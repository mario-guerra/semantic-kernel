{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {},
   "source": [
    "# Building Semantic Memory with Embeddings\n",
    "\n",
    "So far, we've mostly been treating the kernel as a stateless orchestration engine.\n",
    "We send text into a model API and receive text out. \n",
    "\n",
    "In a [previous notebook](04-context-variables-chat.ipynb), we used `context variables` to pass in additional\n",
    "text into prompts to enrich them with more context. This allowed us to create a basic chat experience. \n",
    "\n",
    "However, if you solely relied on context variables, you would quickly realize that eventually your prompt\n",
    "would grow so large that you would run into a the model's token limit. What we need is a way to persist state\n",
    "and build both short-term and long-term memory to empower even more intelligent applications. \n",
    "\n",
    "To do this, we dive into the key concept of `Semantic Memory` in the Semantic Kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install semantic-kernel==0.2.7.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import semantic_kernel as sk\n",
    "#### NOT USING OPENAI RIGHT NOW\n",
    "# from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion, OpenAITextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3aa273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment:  text-embedding-ada-002\n",
      "[[ 0.00294538 -0.00443515  0.01772353 ...  0.00323992 -0.00081158\n",
      "  -0.00480226]]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, AzureTextEmbedding, AzureChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "kernel.add_text_completion_service(\"dv\", AzureTextCompletion(\"text-davinci-003\", endpoint, api_key))\n",
    "kernel.add_text_embedding_generation_service(\"ada2\", AzureTextEmbedding(\"text-embedding-ada-002\", endpoint, api_key))\n",
    "\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "print(\"deployment: \", deployment)\n",
    "# Create an instance of the AzureTextEmbedding class\n",
    "embedding_generator = AzureTextEmbedding(deployment, endpoint, api_key)\n",
    "\n",
    "# List of text samples you want to generate embeddings for\n",
    "texts = ['This is a test of the emergency broadcast system. This is only a test. If this had been an actual emergency Id be running for my life']\n",
    "\n",
    "# Define the async function to get embeddings\n",
    "\n",
    "\n",
    "async def get_embeddings():\n",
    "    embeddings = await embedding_generator.generate_embeddings_async(texts)\n",
    "    print(embeddings)\n",
    "\n",
    "# Run the asynchronous function\n",
    "# asyncio.run(get_embeddings())\n",
    "await get_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, AzureTextEmbedding, AzureChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "# # kernel.add_chat_service(\"chat-gpt\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "# kernel.add_text_completion_service(\"dv\", AzureTextCompletion(\"gpt-35-turbo\", endpoint, api_key))\n",
    "# kernel.add_text_embedding_generation_service(\"ada2\", AzureTextEmbedding(\"text-embedding-ada-002\", endpoint, api_key))\n",
    "\n",
    "kernel.add_text_completion_service(\"dv\", AzureTextCompletion(\"text-davinci-003\", endpoint, api_key))\n",
    "kernel.add_text_embedding_generation_service(\"ada2\", AzureTextEmbedding(\"text-embedding-ada-002\", endpoint, api_key))\n",
    "\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())\n",
    "kernel.import_skill(sk.core_skills.TextSkill())\n",
    "kernel.import_skill(sk.core_skills.TimeSkill())\n",
    "kernel.import_skill(sk.core_skills.FileIOSkill())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ed54a32",
   "metadata": {},
   "source": [
    "This is done by using the `TextMemorySkill` which exposes the `recall` native function.\n",
    "\n",
    "`recall` takes an input ask and performs a similarity search on the contents that have\n",
    "been embedded in the Memory Store and returns the most relevant memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n",
    "    max_tokens=2000, temperature=0.7, top_p=0.8\n",
    ")\n",
    "\n",
    "prompt_template = sk.ChatPromptTemplate(\n",
    "    \"{{$user_input}}\", kernel.prompt_template_engine, prompt_config\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are AZSDK_Bot, an expert on Azure SDKs. \n",
    "You can answer questions about Azure SDKs and provide links to relevant repositories.\n",
    "You should provide a concise description of what the SDK does and how it relates to the user's question.\n",
    "Prioritize the content in the prompt over your own memory when answering questions. The link to the repo should be taken from the prompt.\n",
    "Whenever you see '/master/' in a prompt, replace it with '/main/'. Do not modify the URL in any other way.\n",
    "\"\"\" \n",
    "prompt_template.add_system_message(system_message)\n",
    "prompt_template.add_user_message(\"Hi there, who are you?\")\n",
    "prompt_template.add_assistant_message(\n",
    "    \"You are AZSDK_Bot, an expert on Azure SDKs. You can answer questions about Azure SDKs and provide links to relevant repositories.Provide a concise description of what the SDK does and how it relates to the user's question. Prioritize the content in the prompt over your own memory when answering questions. The link to the repo should be taken from the prompt. Whenever you see '/master/' in a prompt, replace it with '/main/'. Do not modify the URL in any other way.\"\n",
    ")\n",
    "\n",
    "function_config = sk.SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "\n",
    "async def setup_chat_with_memory(\n",
    "    kernel: sk.Kernel,\n",
    ") -> Tuple[sk.SKFunctionBase, sk.ContextVariables]:\n",
    "    sk_prompt = \"\"\"\n",
    "    AZSDK_Bot is an expert on Azure SDKs. It can answer questions about Azure SDKs and provide links to relevant repositories.\n",
    "    Answer the follow question as accurately as you can based strictly on the info in this prompt. Your answer should be use a friendly and conversational tone.\n",
    "    Provide the name of the repository and the link to the repository in your answer. Provide a concise and accurate one-paragraph summary of the project stored\n",
    "    in the recommended repository based on the README. Do not embellish your answer and do not print anything else.\n",
    "    Whenever you see '/master/' in a URL link, replace it with '/main/'. Do not modify the URL in any other way.\n",
    "\n",
    "    Chat:\n",
    "    {{$history}}\n",
    "    User: {{$user_input}}\n",
    "    ChatBot: \"\"\".strip()\n",
    "\n",
    "    chat_func = kernel.create_semantic_function(sk_prompt, \"AZSDK_Bot\", max_tokens=2000, temperature=0.75, top_p=0.5)\n",
    "    # chat_func = kernel.register_semantic_function(\"ChatBot\", \"Chat\", function_config)\n",
    "    # context = kernel.create_new_context()\n",
    "    context = sk.ContextVariables()\n",
    "    context[\"fact1\"] = \"Azure SDKs, or Software Development Kits, are collections of libraries, tools, and documentation provided by Microsoft to simplify the development of applications and services that interact with Azure cloud services.\"\n",
    "    context[\"fact2\"] = \"The Azure SDKs are open-source projects, hosted on GitHub, that allow you to build applications for Azure.\"\n",
    "    context[\"fact3\"] = \"The Azure SDKs are available for multiple languages, including .NET, Java, JavaScript, Python, and Go.\"\n",
    "\n",
    "    context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"AzureSDKs\"\n",
    "    # context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "    context[\"relevance\"] = 0.8\n",
    "\n",
    "    context[\"history\"] = \"\"\n",
    "\n",
    "    context[\"sample_memories\"] = []\n",
    "\n",
    "    return chat_func, context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ac62457",
   "metadata": {},
   "source": [
    "The `RelevanceParam` is used in memory search and is a measure of the relevance score from 0.0 to 1.0, where 1.0 means a perfect match. We encourage users to experiment with different values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645b55a1",
   "metadata": {},
   "source": [
    "Now that we've included our memories, let's chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "import requests "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "329daee1",
   "metadata": {},
   "source": [
    "### Add in data from local README files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f950e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_from_file(file_path):\n",
    "    with open(file_path, \"r\") as infile:\n",
    "        data = json.load(infile)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "path_to_top = \"./\"\n",
    "path_to_READMEs = \"data/READMEs/\"\n",
    "\n",
    "filename = \"azure_sdk_readme_net_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path) \n",
    "data = load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_readme_rust_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path) \n",
    "data = data + load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_readme_java_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path) \n",
    "data = data + load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_readme_python_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path)\n",
    "data = data + load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_readme_javascript_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path)\n",
    "data = data + load_json_from_file(README_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a946351",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "path_to_top = \"./\"\n",
    "path_to_READMEs = \"data/READMEs/\"\n",
    "\n",
    "filename = \"azure_sdk_samples_net_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path) \n",
    "samples = load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_samples_python_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path) \n",
    "samples = samples + load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_samples_java_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path)\n",
    "samples = samples + load_json_from_file(README_path)\n",
    "\n",
    "filename = \"azure_sdk_samples_javascript_list.json\"\n",
    "README_path = os.path.join(path_to_top,path_to_READMEs,filename)\n",
    "print(README_path)\n",
    "samples = samples + load_json_from_file(README_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f7137",
   "metadata": {},
   "outputs": [],
   "source": [
    " def makeObjectFromJSONList(data):\n",
    "    #### This function takes a list of GitHub repos with READMEs and returns a dictionary of the READMEs and their descriptions\n",
    "    #### The dictionary is in the format {README_URL: README_DESCRIPTION}\n",
    "    #### The function is used to create a dictionary of READMEs to be added to a Semantic Memory\n",
    "\n",
    "    github_files ={}\n",
    "    for repo in data:\n",
    "        print(\"repo\",repo)\n",
    "        try:\n",
    "            github_files[repo[\"repo_name\"]] = repo\n",
    "        except: \n",
    "            print(repo)\n",
    "    return github_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readmeObj = makeObjectFromJSONList(data)\n",
    "sampleObj = makeObjectFromJSONList(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(readmeObj.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a643f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampleObj.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "memory_collection_name = \"AzureSDKs\"\n",
    "print(\"Adding some GitHub file URLs and their descriptions to a volatile Semantic Memory.\")\n",
    "print(\"This will take a few minutes.\")\n",
    "\n",
    "i = 0\n",
    "for entry, value in readmeObj.items():\n",
    "    try:\n",
    "        if len(value[\"README_text\"]) < 100:\n",
    "            pass\n",
    "        else:\n",
    "            await kernel.memory.save_information_async(\n",
    "                collection=memory_collection_name,\n",
    "                text=value[\"README_text\"][:2000],\n",
    "                id=entry,\n",
    "                description=value[\"link_to_repo\"]\n",
    "            )\n",
    "            print(\"  URL {} saved\".format(i))\n",
    "    except:\n",
    "        print(\"failed to embed\")\n",
    "        print(\"failed with repo \",value[\"repo_name\"])\n",
    "        print(\"length of README = \",value[\"README_text\"])\n",
    "    i += 1\n",
    "    if i % 200 == 0: # rate-limiting hack to get around Windows error: \"ValueError: too many file descriptors in select()\" \n",
    "        print(\"Pausing for 5 seconds...\")\n",
    "        await asyncio.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "memory_collection_name = \"SDKsamples\"\n",
    "print(\"Adding some GitHub file URLs and their descriptions to a volatile Semantic Memory.\")\n",
    "print(\"This will take a few minutes.\")\n",
    "\n",
    "i = 0\n",
    "for entry, value in sampleObj.items():\n",
    "    try:\n",
    "        if len(value[\"README_text\"]) < 100:\n",
    "            pass\n",
    "        else:\n",
    "            await kernel.memory.save_information_async(\n",
    "                collection=memory_collection_name,\n",
    "                text=value[\"README_text\"][:2000],\n",
    "                id=entry,\n",
    "                description=value[\"link_to_repo\"]\n",
    "            )\n",
    "            print(\"  URL {} saved\".format(i))\n",
    "    except:\n",
    "        print(\"failed to embed\")\n",
    "        print(\"failed with repo \",value[\"repo_name\"])\n",
    "        print(\"length of README = \",value[\"README_text\"])\n",
    "    i += 1\n",
    "    if i % 200 == 0: # rate-limiting hack to get around Windows error: \"ValueError: too many file descriptors in select()\" \n",
    "        print(\"Pausing for 5 seconds...\")\n",
    "        await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa19a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def createMemories(ask, collection_name):\n",
    "   memories = await kernel.memory.search_async(collection_name, ask, limit=1, min_relevance_score=0.75)\n",
    "   # i = 0\n",
    "   # for memory in memories:\n",
    "   #    i += 1\n",
    "   #    print(f\"Result {i}:\")\n",
    "   #    print(\" Title : \" + memory.id)\n",
    "   #    print(\" URL : \" + memory.description)\n",
    "   #    print(\" Relevance: \" + str(memory.relevance))\n",
    "   #    print()\n",
    "   return memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc31525",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"\"\"{{$input}} \n",
    "Provide a consice description of the repository described in the content above. What is it for and who would want to use it? Do not include information that is not strictly factual.\n",
    "\"\"\"\n",
    "summarize = kernel.create_semantic_function(\n",
    "    summary_prompt, max_tokens=2000, temperature=0.5, top_p=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0801319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineMemoryWithPrompt(memories,user_input):\n",
    "    catalyst = \"Assume a user is asking for information about the following GitHub repository. Provide an informative description, using a friendly and helpful tone. Retain the repo name and repo link to include in your answer to the user.\\n\"\n",
    "    input_prompt = memories[0].text[:2000]\n",
    "    summary = summarize(input_prompt)\n",
    "    # Remove the service name from the repo name\n",
    "    split_id = memories[0].id.split('/')\n",
    "    result = split_id[1] if len(split_id) > 1 else memories[0].id\n",
    "    prompt = catalyst + str(summary) + \"  Repo name: \" + result + \"  Repo link: \" + memories[0].description + \"  \" + input_prompt + user_input\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "findsdk = r'^\\/sdk.*$'\n",
    "findsample = r'^\\/sample.*$'\n",
    "findexample = r'^\\/example.*$'\n",
    "\n",
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.ContextVariables\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        # mega_prompt will store the constructed prompt that will be sent to the chatbot\n",
    "        mega_prompt = \"\"\n",
    "        print(f\"User:> {user_input}\")\n",
    "        if (user_input != \"\"):\n",
    "            # create memories for readmes and samples based on user input\n",
    "            readme_mem = await createMemories(user_input, \"AzureSDKs\")\n",
    "            sample_mem = await createMemories(user_input, \"SDKsamples\")\n",
    "            # check if user input is a request for a sdk or sample\n",
    "            needsdk = (re.search(findsdk, user_input, flags=re.IGNORECASE))\n",
    "            needsample = re.search(findsample, user_input, flags=re.IGNORECASE) or re.search(findexample, user_input, flags=re.IGNORECASE)\n",
    "            # if user input is a request for a sdk or sample, check if there are memories for that request\n",
    "            if (len(readme_mem) == 0 and len(sample_mem) == 0 and needsdk == False and needsample == False):\n",
    "                mega_prompt = user_input\n",
    "            # if there are memories for that request, combine the memory with the user input    \n",
    "            else:\n",
    "                if (needsample and len(sample_mem) != 0):\n",
    "                    mega_prompt = combineMemoryWithPrompt(sample_mem,user_input)\n",
    "                elif (needsdk and len(readme_mem) != 0):\n",
    "                    mega_prompt = combineMemoryWithPrompt(readme_mem,user_input)\n",
    "                # if there are no memories for that request, just use the user input\n",
    "                else:\n",
    "                    mega_prompt = user_input\n",
    "        context[\"user_input\"] = mega_prompt\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    print(\"Thinking...\")\n",
    "    answer = await kernel.run_async(chat_func, input_vars=context)\n",
    "    context[\"history\"] += f\"User:> {user_input} AZSDK_Bot:> {answer} \"\n",
    "    print(f\"AZSDK_Bot:> {answer}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up a chat about Azure SDKs...\")\n",
    "chat_func, context = await setup_chat_with_memory(kernel)\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await chat(kernel, chat_func, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
