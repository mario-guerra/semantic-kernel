[{"README_text": "# Mixed Reality Toolkit 3 in-editor tutorials\n\n![MRTK3 in-editor tutorials banner that showcases the Bounds Control, Buttons, Hand Menu, and Object Manipulator features in four corners of the image, starting from top left to top right.](Assets/READMEImages/banner.png)\n\nThe MRTK3 in-editor tutorials guide you through each step of the way to build your first MRTK3 application in a beautiful aquarium. \n\n## Getting Started with the MRTK3 in-editor tutorials\n\n1. Install Unity 2021.3.4f1\n1. Clone this repository to your computer.\n1. Open the cloned project \"MRTK3-iet-tutorials\" with Unity 2021.3.4f1.\n1. You should see a **Tutorials** window on the right side of your screen that contains a list of seven tutorials. If not, on the menu bar, select **Tutorial** > **Show Tutorials**.\n1. In the **Tutorials** window, select the tutorial you want to proceed with.\n\n## Tutorials\n\n### Preview the Project\n\n![An aquarium scene that consists of sea coral, sea grass, and rocks surrounding the environment. A boat is placed into the sand in the back right corner. A treasure chest is placed in the middle of the aquarium. A hide toggle button is to the right of the treasure chest.](Assets/READMEImages/preview-project-full.jpg)\n\nLearn about the various solutions for previewing scenes without compiling and deploying projects to a XR device.\n\n### How to use MRTK Input Simulation\n\n![A pair of simulated hands displayed next to one another. A ray is placed in front of each hand.](Assets/READMEImages/hand-rays-full.png)\n\nLearn how to simulate input directly into the Unity editor with MRTK In-Editor Input Simulation.\n\n### Object Manipulator\n\n![An aquarium scene with coral, rocks, and sea grass. In the middle of the aquarium is a simulated hand that is grabbing a treasure chest.](Assets/READMEImages/object-manipulator-full.jpg)\n\nMove and manipulate objects with one or two hands with a wide variety of input modalities.\n\n### Bounds Control\n\n![An aquarium scene with coral, rocks, and sea grass spread around the tank. A coral is in the middle with a bounding box surrounding it's shape.](Assets/READMEImages/bounds-control-full.jpg)\n\nIntent feedback and precision manipulation affordances.\n\n### Canvas Button\n\n![An aquarium scene with coral, rocks, and sea grass around the tank. A Seahorse is in the middle of the tank. To the right of the sea horse is a toggle button that has a seahorse icon.](Assets/READMEImages/canvas-button-full.jpg)\n\nA volumetric button optimized for a wide range of input modalities.\n\n### Canvas Button bar\n\n![An aquarium scene with sand and a boat sticking straight up in the middle of the tank. At the front of the tank is a button bar that says coral, sea grass, rock, and boat.](Assets/READMEImages/button-bar-full.jpg)\n\nA volumetric button bar optimized for a wide range of input modalities.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MRTK3-iet-tutorials", "org_name": "microsoft", "org_repo": "microsoft/MRTK3-iet-tutorials", "platform_org_repo": "github+microsoft/MRTK3-iet-tutorials", "link_to_repo": "https://github.com/microsoft/MRTK3-iet-tutorials", "platform": "github", "language": "ShaderLab", "stargazers_count": 35, "watchers_count": 35}, {"README_text": "# AzFuse\n\nAzFuse is a lightweight [blobfuse](https://github.com/Azure/azure-storage-fuse)-like\npython tool with the data transfer\nimplemented through [AzCopy](https://github.com/Azure/azure-storage-azcopy).\nWith this tool, reading/writing a file in azure storage is similar to reading a local\nfile, which follows the same principle of blobfuse. However, the underlying data\ntransfer is to leverage azcopy, which provides a much faster speed. \n\n## Installation\n1. Download azcopy from [here](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10).\n   Copy azcopy as `~/code/azcopy/azcopy` or under `/usr/bin/` and\n   make it executable.  Make sure it is version 10 or higher.\n2. install by\n   ```bash\n   pip install git+https://github.com/microsoft/azfuse.git\n   ```\n   or\n   ```bash\n   git clone https://github.com/microsoft/azfuse.git\n   cd azfuse\n   python setup.py install\n   ```\n\n## Preliminary\nAzfuse contains 3 different kinds of file paths.\n1. `local` or `logical` path, which is populated by the user script. For example, the user\n   script may want to access the file, named `data/abc.txt`, which is referred\n   to as\n   `local` path.\n2. `remote` path, which is the path in azure\n   storage blob. For example, if the azure storage path is \n   `https://accountname.blob.core.windows.net/containername/path/data/abc.txt`, the\n   `remote` path will be `path/data/abc.txt`. Note that, the remote path does not\n   include the `containername` in the url.\n3. `cache` path, which is the destination file of the azcopy, e.g. `/tmp/data/abc.txt`. We will use azcopy to download the file here or upload this file to Azure.\n\nThe pipeline is\n1. the user script tries to access `data/abc.txt` through `with azfuse.File.open()`.\n2. if it is in read mode, the tool will check if the `cache` path exists.\n    - if it exists, it returns the handle of the `cache` file\n    - if it does not exist, it will download the file from `remote` path to\n      `cache` path and return the handle of the `cache` file.\n3. if it is in write mode, the tool will open the `cache` path, and return the\n   handle of the `cache` path. Before leaving `with`, the tool will upload the\n   `cache` file to `remote` file.\n\n## Setup\n1. By default, the feature is disabled. That is, the file read/write will\n   directly access the `local` file without trying to access the `remote` in\n   azure blob. Thus, it is also recommended to first use such tool, but not\n   to enable it (also, no need to configure it).\n   To enable it, set `AZFUSE_USE_FUSE=1` explicitly. The following describes\n   how to configure it when enabled.\n2. Set the environment variable of `AZFUSE_CLOUD_FUSE_CONFIG_FILE` as the\n   configuration file path, e.g. `AZFUSE_CLOUD_FUSE_CONFIG_FILE=./aux_data/configs/azfuse.yaml`\n3. The configuration file is in yaml format, and is a list of dictionary. Each\n   dictionary contains `local`, `remote`, `cache`, and `storage_account`.\n   ```yaml\n   - cache: /tmp/azfuse/data\n     local: data\n     remote: azfuse_data\n     storage_account: storage_config_name\n   - cache: /tmp/azfuse/models\n     local: models\n     remote: models\n     storage_account: storage_config_name\n   ```\n   The path in the yaml file is the prefix of the corresponding path. For example, if the\n   local path is `data/abc.txt`, the `cache` path will be\n   `/tmp/azfuse/data/abc.txt`, and the `remote` path will be\n   `azfuse_data/abc.txt`. The tool will match each prefix from the first to the\n   last, and the one which is matched first will be the one used. If there is\n   no match, it will assume this is a local file, which can also be a blobfuse\n   mount file.\n\n   The storage account here is the base file name. Here, the path will be\n   `./aux_data/storage_account/storage_config_name.yaml`. The folder can be\n   changed by setting `AZFUSE_STORAGE_ACCOUNT_CONFIG_FOLDER`. The storage\n   account yaml file's format should be like this\n   ```yaml\n   account_name: accountname\n   account_key: accountkey\n   sas_token: sastoken\n   container_name: containername\n   ```\n   `account_key` or `sas_token` can be `null`. The `sas_token` should start with\n   `?`.\n\n## Examples\n- Open a file to read\n  ```python\n  from azfuse import File\n  with File.open('data/abc.txt', 'r') as fp:\n      content = fp.read()\n  ```\n  It will match the prefix of `local` path in the configuration file. If the\n  cache file exists, it just returns the handle of the cache file. Otherwise,\n  it will download the file from the `remote` path of the Azure Blob to the\n  `cache` file, and then return the handle.\n\n- Open a file to write\n  ```python\n  from azfuse import File\n  with File.open('data/abc.txt', 'w') as fp:\n       fp.write('abc')\n  ```\n  No matter whether there exists a cache file with the same name, it will open the\n  cache file. Before it leaves `with`, it will upload the `cache` file to the\n  `remote` file in the Azure Blob Storage.\n\n- Pre-cache a bunch of files for processing\n  ```python\n  from azfuse import File\n  File.prepare(['data/{}.txt'.format(i)] for i in range(1000))\n  for i in range(1000):\n      with File.open('data/{}.txt'.format(i), 'r') as fp:\n         content = fp.read()\n  ```\n  The function of `prepare` will download all files in one azcopy call, which is much faster than download each file sequentially.\n  As `prepare()` has already downloaded all the files to the cache folder, there\n  will be no azcopy download when calling `File.open()`. \n\n- Upload the file in an asynchronous way.\n  ```python\n  from azfuse import File\n  with File.async_upload(enabled=True):\n     for i in range(1000):\n         with File.open('data/{}.txt'.format(i), 'w') as fp:\n              fp.write(str(i))\n  ```\n  A separate subprocess will be launched to upload the cache files. It will\n  also upload multiple cache files at the same time in one azcopy call if there are.\n  The cache file can also be re-directed to `/dev/shm` such that the file\n  writing into cache files will be faster. It is enabled by `File.async_upload(enabled=True, shm_as_tmp=True)`\n  In this case, the upload process\n  will delete the cache file once it is uploaded.\n\n## Tips\n- Safe to read the same file from multiple processes.\n\n  A lock is implemented to make sure there is only one process to launch\n  azcopy if the file is not available in `cache`. The other processes will not\n  re-launch the azcopy as long as it is ready in `cache`.\n\n- Clear cache if the file is updated on another machines.\n  \n  For the sake of speed, the tool does not check if the cached file is\n  up-to-date. That is, if the file is updated on another machine, the current\n  machine's cached file may be out-of-date. In this case, call\n  `File.clear_cache(local_path)`. The parameter here is not `cache` path.\n\n- No need to clear cache for writing.\n  \n  No matter whether there is an existing file in Blob, the writing will always\n  overwrite the existing file or creating a new file in Blob\n\n- Patch the function if the `open` is inside some package.\n\n  For example, in the package of [Deepspeed](https://github.com/microsoft/deepspeed), the `torch.save` is invoked in\n  `model_engine.save_checkpoint`. We can patch `torch.save` by the following\n  example.\n  ```python\n  def torch_save_patch(origin_save, obj, f, *args, **kwargs):\n      if isinstance(f, str):\n          with File.open(f, 'wb') as fp:\n              result = origin_save(obj, fp, *args, **kwargs)\n      else:\n          result = torch.save(obj, f, *args, **kwargs)\n      return result\n  \n  def patch_torch_save():\n      old_save = torch.save\n      torch.save = lambda *args, **kwargs: torch_save_patch(old_save, *args, **kwargs)\n      return old_save\n  ```\n  With the context of `File.async_upload(enabled=True, shm_as_tmp=True)`, we\n  can easily have the feature of asynchronously uploading the checkpoint to Azure\n  Blob.\n\n## Command line\nA command line tool is provided for some data management.\n\n### setup\n   set the following alias to use azfuse as a command line.\n   ```\n   alias azfuse='ipython --pdb -m azfuse --'\n   ```\n\n### usage\n- read a `local file`.\n  ```\n  azfuse cat data/file.tsv\n  azfuse head data/file.tsv\n  azfuse tail data/file.tsv\n  azfuse display data/file.png\n  azfuse nvim data/file.txt\n  ```\n  If you know the `cache file` is out of date, please manually delete the\n  cache file and re-run this command.\n- list the files under a folder\n  ```\n  azfuse ls data/sub_folder\n  ```\n- get the url of a `local file`, which refers to the remote file\n  ```\n  azfuse url data/file.tsv\n  ```\n  The SAS token is generated with 30 days expairation date. This is normally\n  used for data sharing.\n- delete the `remote file`. Please note that this operation cannot be reverted.\n  Run it with extreme care.\n  ```\n  azfuse rm data/local_path.tsv\n  ```\n- update a file\n  ```\n  azfuse update data/file.txt\n  ```\n  This will launch neovim as default. If the file changes, the changed content\n  will be uploaded, and the change cannot be reverted. Thus, please also be careful.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azfuse", "org_name": "microsoft", "org_repo": "microsoft/azfuse", "platform_org_repo": "github+microsoft/azfuse", "link_to_repo": "https://github.com/microsoft/azfuse", "platform": "github", "language": "Python", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "---\npage_type: sample\nlanguages:\n- csharp\nproducts:\n- microsoft-purview\n- azure-databricks\n---\n<!-- markdownlint-disable MD033 - HTML rule -->\n![EAE_Header.png](./assets/img/readme/EAE_Header.png)\n![lineage.png](./assets/img/readme/lineage.png)\n\nMicrosoft Solutions / Early Access Engineering\n\n# Azure Databricks to Purview Lineage Connector\n\nThis solution accelerator, together with the [OpenLineage](http://openlineage.io) project, provides a connector that will transfer lineage metadata from Spark operations in Azure Databricks to Microsoft Purview, allowing you to see a table-level lineage graph as demonstrated above.\n\n> **Note**\n> In addition to this solution accelerator, Microsoft Purview is creating native models for Azure Databricks (e.g.: Notebooks, jobs, job tasks...) to integrate with Catalog experiences. With native models in Microsoft Purview for Azure Databricks, customers will get enriched experiences in lineage such as detailed transformations.\n> If you choose to use this solution accelerator in a Microsoft Purview account before the native models are released, these enriched experiences are not backward compatible.\n> Please reach out to your Microsoft account representative for timeline related questions on the upcoming model enrichment for Azure Databricks in Microsoft Purview.**\n\n## Contents\n\n* [Overview](#overview)\n* [Features](#features)\n* [Videos](#videos)\n* [Prerequisites](#prerequisites)\n* [Getting Started](#getting-started)\n* [Using the Connector](#using-the-connector)\n* [Troubleshooting](#Troubleshooting)\n* [Limitations & more](#Limitations)\n\n## Overview\n\nGathering lineage data is performed in the following steps:\n\n![high-level-architecture.png](./assets/img/readme/high-level-architecture.svg)\n\n1. Azure Databricks clusters are configured to initialize the OpenLineage Spark Listener with an endpoint to receive data.\n1. Spark operations will output data in a standard OpenLineage format to the endpoint configured in the cluster.\n1. Endpoint provided by an Azure Function app that will filter incoming data and pass it to an Azure EventHub.\n1. Events are captured by a second Function app to transform the data into a format compatible with Atlas and Purview.\n1. Lineage data is synchronized with existing Purview metadata and uploaded to Purview using standard Apache Atlas APIs.\n\n## Features\n\n* Supports table level lineage from Spark Notebooks and jobs for the following data sources:\n  * Azure SQL\n  * Azure Synapse Analytics (as input)\n  * Azure Data Lake Gen 2\n  * Azure Blob Storage\n  * Delta Lake (Merge command not supported)\n  * Azure Data Explorer\n  * Azure Data Factory orchestration\n  * Hive Tables (in default metastore)\n  * MySQL\n  * PostgreSQL\n* Supports Spark 3.0, 3.1, 3.2, and 3.3 (Interactive and Job clusters) / Spark 2.x (Job clusters)\n  * Databricks Runtimes between 9.1 and 11.3 LTS are currently supported\n* Can be configured per cluster or for all clusters as a global configuration \n* Support **column level lineage** for ABFSS, WASBS, and default metastore hive tables (see [Limitations](./LIMITATIONS.md#column-level-mapping-supported-sources) for more detail)\n* Once configured, <span style=\"color: red;\">**does not require any code changes to notebooks or jobs**</span>\n* Can [add new source support through configuration](./docs/extending-source-support.md)  \n\n## Videos\n\n* [Solution Overview](https://youtu.be/LrtDmLnRse0)\n* [Deploying the Demo](https://youtu.be/pLF0iykhruY)\n* [Troubleshooting](https://youtu.be/kG8Maa1kOx0)\n\n## Prerequisites\n\nInstalling this connector requires the following:\n\n1. Azure subscription-level role assignments for both `Contributor` and `User Access Administrator`.\n1. Azure Service Principal with client ID and secret - [How to create Service Principal](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).\n\n## Getting Started\n\nThere are two deployment options for this solution accelerator:\n\n* ### [Demo Deployment](./deploy-demo.md)\n\n    No additional prerequisites are necessary as the demo environment will be setup for you, including Azure Databricks, Purview, ADLS, and example data sources and notebooks.\n\n* ### [Connector Only Deployment](./deploy-base.md)\n\n    If installed as a working connector, Azure Databricks, data sources, and Microsoft Purview are assumed to be setup and running.\n\n## Using the Connector\n\n*Ensure both the Azure Function app and Azure Databricks cluster are running.*\n\n1. Open your Databricks workspace to run a Spark job or notebook which results in data being transferred from one location to another. For the demo deployment, browse to the Workspace > Shared > abfss-in-abfss-out-olsample notebook, and click \"Run all\".\n\n1. Once complete, open your Purview workspace and click the \"Browse assets\" button near the center of the page\n\n1. Click on the \"By source type\" tab  \nYou should see at least one item listed under the heading of \"Azure Databricks\".  In addition there will possibly be a Purview Custom Connector section under the Custom source types heading\n\n    ![browse_assets.png](./assets/img/readme/browse_assets.png)\n\n1. Click on the \"Databricks\" section, then click on the link to the Azure Databricks workspace which the sample notebook was ran. Then select the notebook which you ran (for those running Databricks Jobs, you can also select the job and drill into the related tasks) \n    * After running a Databricks Notebook on an Interactive Cluster, you will see lineage directly in the Notebook asset under the Lineage tab.\n    * After running a Databricks Job on a Job Cluster, you will see lineage in the Notebook Task asset. To navigate from a Notebook to a Notebook Task select the Properties tab and choose the Notebook Tasks from the Related Assets section. Please note that Databricks Jobs lineage require [additional setup](./deploy-base.md#support-extracting-lineage-from-databricks-jobs) outside of the demo deployment.\n\n    ![databricks_task_related.png](./assets/img/readme/databricks_task_related.png)\n\n1. Click to the lineage view to see the lineage graph\n\n    ![lineage_view.png](./assets/img/readme/lineage_view.png)\n\n    **Note**: If you are viewing the Databricks Process shortly after it was created, sometimes the lineage tab takes some time to display. If you do not see the lineage tab, wait a few minutes and then refresh the browser.\n\n    **Lineage Note**: The screenshot above shows lineage to an Azure Data Lake Gen 2 folder, you must have scanned your Data Lake prior to running a notebook for it to be able to match to a Microsoft Purview built-in type like folders or resource sets.\n\n## Troubleshooting\n\n**When filing a new issue, [please include associated log message(s) from Azure Functions](./TROUBLESHOOTING.md#debug-logs).** This will allow the core team to debug within our test environment to validate the issue and develop a solution.\n\nIf you have any issues, please start with the [Troubleshooting Doc](./TROUBLESHOOTING.md) and note the [limitations](./LIMITATIONS.md) which affect what sort of lineage can be collected. If the problem persists, please raise an [Issue on GitHub](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator/issues).\n\n## Limitations\n\nThe solution accelerator has some [limitations which affect what sort of lineage can be collected](./LIMITATIONS.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies\n\n## Data Collection\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft\u2019s privacy statement. Our privacy statement is located at <https://go.microsoft.com/fwlink/?LinkID=824704>. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n", "repo_name": "Purview-ADB-Lineage-Solution-Accelerator", "org_name": "microsoft", "org_repo": "microsoft/Purview-ADB-Lineage-Solution-Accelerator", "platform_org_repo": "github+microsoft/Purview-ADB-Lineage-Solution-Accelerator", "link_to_repo": "https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator", "platform": "github", "language": "C#", "stargazers_count": 69, "watchers_count": 69}, {"README_text": "# picologging\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/picologging)](https://pypi.org/project/picologging/)\n[![PyPI](https://img.shields.io/pypi/v/picologging)](https://pypi.org/project/picologging/)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/picologging/badges/version.svg)](https://anaconda.org/conda-forge/picologging)\n[![codecov](https://codecov.io/gh/microsoft/picologging/branch/main/graph/badge.svg?token=KHs6FpQlVW)](https://codecov.io/gh/microsoft/picologging)\n\n> **Warning**\n> This project is in *beta*.\n> There are some incomplete features (see [Limitations](https://microsoft.github.io/picologging/limitations.html)).\n\nPicologging is a high-performance logging library for Python. picologging is 4-10x faster than the `logging` module in the standard library.\n\nPicologging is designed to be used as a *drop-in* replacement for applications which already use logging, and supports the same API as the `logging` module.\n\nCheck out the [Documentation](https://microsoft.github.io/picologging/) for more.\n\n## Installation\n\nPicologging can be installed from PyPi using pip:\n\n```console\npip install picologging\n```\n\nOr from conda forge using conda:\n\n```console\nconda install -c conda-forge picologging\n```\n\n## Usage\n\nImport `picologging as logging` to use picologging instead of the standard library logging module.\n\nThis patches all the loggers registered to use picologging loggers and formatters.\n\n```python\nimport picologging as logging\nlogging.basicConfig()\n\nlogger = logging.getLogger()\n\nlogger.info(\"A log message!\")\n\nlogger.warning(\"A log message with %s\", \"arguments\")\n```\n\n## Benchmarks\n\nRun `richbench benchmarks/ --markdown` with the richbench CLI to see the benchmarks, here is a sample on macOS 11:\n\n|                             Benchmark | Min     | Max     | Mean    | Min (+)         | Max (+)         | Mean (+)        |\n|---------------------------------------|---------|---------|---------|-----------------|-----------------|-----------------|\n|                         FileHandler() | 0.138   | 0.151   | 0.143   | 0.055 (2.5x)    | 0.063 (2.4x)    | 0.058 (2.5x)    |\n|                  WatchedFileHandler() | 0.189   | 0.197   | 0.193   | 0.097 (1.9x)    | 0.101 (1.9x)    | 0.099 (1.9x)    |\n|                 RotatingFileHandler() | 0.287   | 0.304   | 0.296   | 0.174 (1.6x)    | 0.178 (1.7x)    | 0.176 (1.7x)    |\n|                        QueueHandler() | 1.109   | 1.195   | 1.130   | 0.142 (7.8x)    | 0.151 (7.9x)    | 0.147 (7.7x)    |\n|      QueueListener() + QueueHandler() | 0.157   | 0.167   | 0.162   | 0.034 (4.6x)    | 0.039 (4.3x)    | 0.037 (4.3x)    |\n|                       MemoryHandler() | 0.126   | 0.144   | 0.133   | 0.051 (2.5x)    | 0.059 (2.5x)    | 0.054 (2.5x)    |\n|                           LogRecord() | 0.225   | 0.248   | 0.233   | 0.026 (8.7x)    | 0.029 (8.5x)    | 0.028 (8.4x)    |\n|                  Formatter().format() | 0.076   | 0.086   | 0.081   | 0.004 (18.7x)   | 0.005 (18.9x)   | 0.004 (19.1x)   |\n|        Formatter().format() with date | 0.298   | 0.311   | 0.304   | 0.081 (3.7x)    | 0.087 (3.6x)    | 0.084 (3.6x)    |\n|           Logger(level=DEBUG).debug() | 0.726   | 0.743   | 0.734   | 0.059 (12.3x)   | 0.061 (12.3x)   | 0.060 (12.3x)   |\n| Logger(level=DEBUG).debug() with args | 0.761   | 0.809   | 0.777   | 0.081 (9.4x)    | 0.087 (9.3x)    | 0.084 (9.2x)    |\n|            Logger(level=INFO).debug() | 0.016   | 0.018   | 0.017   | 0.004 (4.3x)    | 0.005 (3.8x)    | 0.004 (4.1x)    |\n|  Logger(level=INFO).debug() with args | 0.018   | 0.019   | 0.018   | 0.005 (3.8x)    | 0.005 (3.8x)    | 0.005 (3.7x)    |\n\n## Limitations\n\nSee [Limitations](https://microsoft.github.io/picologging/limitations.html)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Local development\n\nThis project comes bundled with a dev container which sets up an appropriate environment. If you install the Dev Containers extension for VS Code, then opening this project in VS Code should prompt it to open it in the dev container.\n\nOnce opened in the dev container, run:\n\n```console\npip install -e \".[dev]\"\npre-commit install\npython setup.py build_ext --inplace --build-type Debug\n```\n\nRun the build command whenever you make changes to the files.\n\nIt's also helpful to create a `.vscode/launch.json` file like this one:\n\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n    {\n        \"name\": \"(gdb) Launch pytest\",\n        \"type\": \"cppdbg\",\n        \"request\": \"launch\",\n        \"program\": \"/usr/local/bin/python\",\n        \"args\": [\"-m\", \"pytest\", \"tests\"],\n        \"stopAtEntry\": false,\n        \"cwd\": \"${workspaceFolder}\",\n        \"environment\": [],\n        \"externalConsole\": false,\n        \"MIMode\": \"gdb\",\n        \"setupCommands\": [\n            {\n                \"description\": \"Enable pretty-printing for gdb\",\n                \"text\": \"-enable-pretty-printing\",\n                \"ignoreFailures\": true\n            },\n            {\n                \"description\":  \"Set Disassembly Flavor to Intel\",\n                \"text\": \"-gdb-set disassembly-flavor intel\",\n                \"ignoreFailures\": true\n            },\n        ]\n    }\n}\n```\n\nNow you can press the \"Run and debug\" button to run `pytest` from the `gdb` debugger\nand use breakpoint debugging in the C code.\n\nIf you would like to be able to dive into the CPython code while debugging, then:\n\n1. Do a git checkout of the tagged branch for the devcontainer's Python version\ninto the devcontainer's `/workspaces/` directory. You may need to `sudo`.\n2. Follow the instructions in the CPython README to compile the code.\n3. Add the following key to the the configuration in `launch.json`:\n\n    ```json\n    \"sourceFileMap\": { \"/usr/src/python\": \"/workspaces/cpython\" },\n    ```\n\n4. Add the following command to the `setupCommands` in `launch.json`:\n\n    ```json\n    {\n        \"description\": \"Find CPython source code\",\n        \"text\": \"-gdb-set auto-load safe-path /workspaces/cpython\"\n    },\n    ```\n\n## Trademarks\n\nSome components of this Python package are from CPython 3.11 logging library for compatibility reasons.\n\nCPython 3.11 is licensed under the PSF license.\nThe logging module is Copyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "picologging", "org_name": "microsoft", "org_repo": "microsoft/picologging", "platform_org_repo": "github+microsoft/picologging", "link_to_repo": "https://github.com/microsoft/picologging", "platform": "github", "language": "Python", "stargazers_count": 528, "watchers_count": 528}, {"README_text": "# AzureAD to MSGraph migration data\n\nWelcome to the project to help you get your PowerShell modules migrated from the old [MSOnline](https://www.powershellgallery.com/packages/MSOnline) and [AzureAD](https://www.powershellgallery.com/packages/AzureAD) modules to the new [Microsoft Graph Modules](https://github.com/microsoftgraph/msgraph-sdk-powershell).\n\nIn this repository we maintain the data used to inform various tools as well as online documentation.\nBelow you can find both documentation and resources to understand what is happening, why, and how to best go about your own migration.\n\n## Why is this happening, where do I get more information?\n\nThe API backing the affected PowerShell modules (MSOnline, AzureAD, AzureADPreview) is being retired in favor of the Microsoft Graph API. Furthermore, the authentication libraries ([ADAL](https://github.com/AzureAD/azure-activedirectory-library-for-dotnet)) used by the affected modules are being deprecated.\n\nMigrating the modules to use the new authentication libraries ([MSAL](https://github.com/AzureAD/microsoft-authentication-library-for-dotnet)) would be a major engineering effort, as would be transporting the commands to the new API (which does not have identical endpoints), while there are already commands available for use with the Graph API. Only to end up with having two sets of modules doing the same to be maintained going forward.\n\n> Official Documentation\n\n+ [Announcement](https://techcommunity.microsoft.com/t5/azure-active-directory-identity/azure-ad-change-management-simplified/ba-p/2967456)\n+ [Migration Guide](https://docs.microsoft.com/en-us/powershell/microsoftgraph/migration-steps?view=graph-powershell-beta)\n+ [Mapping to old to the new commands](https://docs.microsoft.com/en-us/powershell/microsoftgraph/azuread-msoline-cmdlet-map?view=graph-powershell-beta)\n+ [Mapping Command to permissions](https://docs.microsoft.com/en-us/powershell/microsoftgraph/find-mg-graph-command?view=graph-powershell-beta)\n\n## Migration Tools\n\nSome projects have been started to help with the migration of your code base:\n\n|Name|Description|\n|---|---|\n|[Graph PowerShell Conversion Analyzer](https://graphpowershell.merill.net)|A website to conveniently convert your code - just paste in the snippet using the old modules and see the magic happen.|\n|[PSAzureMigrationAdvisor](https://github.com/FriedrichWeinmann/PSAzureMigrationAdvisor)|PowerShell-based toolkit to search code in need of migration and how to convert it. Includes integration tools to help scanning GitHub or Azure DevOps Services repositories.|\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureAD-to-MSGraph", "org_name": "microsoft", "org_repo": "microsoft/AzureAD-to-MSGraph", "platform_org_repo": "github+microsoft/AzureAD-to-MSGraph", "link_to_repo": "https://github.com/microsoft/AzureAD-to-MSGraph", "platform": "github", "language": "PowerShell", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# PVA / AudioCodes Contact Center Accelerator\n\nThe goal of a modern contact center solution is to provide a customer experience that rivals\nthat of an experienced human agent and do that at scale and at a fraction of the cost of a\ntraditional contact center.  Achieving that goal requires a solution that can transform\nconversation into a coordinated business process that weaves together a core set of solution\nelements to respond effectively. This solution accelerator provides the building blocks and\nblueprint for how to use Power Virtual Agent and AudioCodes VoiceAI Connect Cloud Edition\nto build a modern contact center assistant.\n\n## Modern Contact Center Solution\n\nA modern contact center solution involves the effective integration of the key systems and\nservices shown in **Figure 1**. The primary goal of this accelerator is to identify these\nsystems and services and show how each can be effectively integrated into a modern contact\ncenter solution to provide maximum capability. To accomplish this goal, the accelerator\nincludes a set of building blocks and blueprints that address common contact center tasks\nand scenarios.  You can think of the systems and services in **Figure 1** as sort of a\nchecklist of key topic areas you need to give careful consideration in order to build an\neffective modern contact center solution.  \n&nbsp;  \n![Figure 1: High-level Subsystem Overview](Doc/ContactCenterCoreElements.png)  \n&nbsp;  \n**<a name=\"HighLevelSubsystemOverview\"></a>Figure 1: Contact Center Core Elements**  \n\n## &nbsp;  \n\n## Table of Contents  \n[SOLUTION OVERVIEW](#SolutionOverview)  \n[PREREQUISITES](#Prerequisites)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Required Skills  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Demo-Only Prerequisites  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Development Prerequisites  \n[Getting Started](#GettingStarted)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clone the PVA/AudioCodes Contact Center Repo  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deploy Mock Services  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Import, Configure & Publish the PVA / AudioCodes Solution  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Telephony Configuration  \n[Using the Accelerator](#UsingTheAccelerator)  \n[Best Practices](#BestPractices)\n\n## <a name=\"SolutionOverview\"></a>Solution Overview\n\nIn the context of a contact center, two disciplines weave each of the various solution\nelements in **Figure 1** together and bring them alive: conversational AI and telephony.\nIn this accelerator, the conversational AI discipline is provided by Microsoft's Power\nVirtual Agent (PVA) and telephony is provided by AudioCodes VoiceAI Connect Cloud\n(VAIC-C). **Figure 2** provides an architectural understanding of how those two\ntechnologies fit together along with the other elements of the accelerator.  In this\ninitial version of the accelerator, only four of the seven core elements of \n**Figure 1** are addressed: Contact Center Assistant, Experiences, Customer Services,\nand Line of Business Integration.  The other three will be addressed as this\naccelerator evolves.\n\n![Figure 2: High-level Subsystem Overview](Doc/ContactCenterArchitecture.png)  \n**<a name=\"HighLevelSolutionOverview\"></a>Figure 2: Accelerator Architecture Diagram**  \n\n## <a name=\"Prerequisites\"></a>Prerequisites\n#### Required Skills\nIf this accelerator is going to be used as a telephony demonstration, then only a\nbasic understanding of Microsoft's Power Virtual Agent (PVA) is required and the\ninstructions for configuring AudioCode's VoiceAI Connect Cloud (VAIC-C) should be\nenough to accomplish the task without any prior knowledge.\n\nIf this accelerator is going to be used to accelerate the development and\ndeployment of a modern contact center solution, then a good working knowledge\nof PVA and VAIC-C will be required for tasks that involve creating and extending\nPVA Topics, PVA Composer Topics, Power Automate Flows, Azure Functions, and\nunderstanding the various VAIC-C capabilities and how to integrate them.  If you\nare new to PVA and/or AudioCodes VoiceAI Connect, this accelerator can be very\nhelpful in getting you up to speed with less effort since you'll start with an\nassistant that has an implementation that you can examine and extend so you\naren't starting from first principles.\n\n#### Demo-Only Prerequisites\nYou don't need to be a developer or have development tools installed in order to\nget this accelerator up and running and demo it but you will if you wanted to\nmodify and extend this accelerator (see [Development Tools](#ForDevelopers) section\nfor details). The only thing you need to run this accelerator and try it out is\naccess to Microsoft's Power Virtual Agent and AudioCodes VoiceAI Connect Cloud\nEdition. If you don't already have access to either of those services, you can\ncreate a trial account to explore those services and this accelerator.\n\n**Note:** The trial subscription for AudioCode VoiceAI Connect Cloud provides a\nstandard set of telephony features but some of the capabilities of this accelerator\nuse advanced telephony capabilities which require the AudioCodes VoiceAI Connect\nCloud Edition Plus and you can upgrade your trial subscription later if you want\nto leverage those features without having to reinstall the accelerator.  \n\n- **Get PVA Trial Account**  \nTo sign up for a free trial account for Power Virtual Agents click [here](https://go.microsoft.com/fwlink/?LinkId=2107702&clcid=0x409&cmpid=pva-home-hero-sta-buildchatbots)\n\n- **Get AudioCodes Trial Account**  \nFollow the instructions [here](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Cloud/signing_up_to_cloud.htm?TocPath=VoiceAI%2520Connect%2520Cloud%257C_____2)\nto sign up for a free trial account for AudioCodes Voice AI Connect Cloud\nEdition.  \n&nbsp;  \nAlternatively, you can also subscribe to AudioCodes Voice AI Connect Cloud\nEdition from Azure Market Place by following the instructions [here](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Combined/Accessing%20VoiceAi%20Connect%20Cloud%20from%20Azure.htm?TocPath=VoiceAI%2520Connect%2520Cloud%257C_____1)  \n&nbsp;  \nWhen you've successfully created a trial account, confirmed your account, and\nsuccessfully signed in, you can stop there and will come back to this AudioCodes\nportal later in the [Telephony Configuration step](#TelephonyConfiguration)  \n- **Get PowerShell**  \nTo make it as easy as possible to get started quickly, a PowerShell script is\nused in the **Getting Started** steps to install and configure various Azure\nservices so you don't have to worry about those details and the accelerator\nwill just work when you're finish setup.  \n&nbsp;  \nYou'll need to download and install PowerShell [here](https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.2#msi)\nif you don't already have that installed.\n\n#### <a name=\"ForDevelopers\"></a>Development Prerequisites  \nIn addition to the demo-only prerequisites, if you want to explore or extend the\nmore advanced and technical aspects of this accelerator, you'll need to install\nthe following tools.  \n- **Install Bot Framework Composer**  \nThis accelerator extends its Power Virtual Agent by using Bot Framework Composer\nand you'll need to install it if you want to explore or modify those\nextensions.  Download Composer ***for Windows*** [here](https://docs.microsoft.com/en-us/composer/install-composer?tabs=windows#download-composer)\n\n- **Install Visual Studio**  \nIf you want to explore the mock Customer Service Azure Function or if you\ninstalled Composer to explore or extend PVA Composer Topics, you'll also\nneed to install Visual Studio which can be installed [here](https://docs.microsoft.com/en-us/visualstudio/install/install-visual-studio?view=vs-2022)\n\n## <a name=\"GettingStarted\"></a>Getting Started\nGetting started should be quick and easy and there are only four steps you\nneed to complete which are easy enough to follow that you shouldn't need\nany prior knowledge to be successful.  Alternatively, this short\n[Getting Started Screencast](https://aka.ms/pva-vaicc-start) covers the\nsame ground, but in video form which can be easier and quicker to\nfollow for this type of activity.\n\n#### 1) **Clone the PVA/AudioCodes Contact Center Repo**  \n>This repo has everything you'll need to get the solution up and running and\nto get started you'll need to clone the repo which copies the accelerator\nto your local device.  If you have [Git](https://git-scm.com/downloads)\ninstalled, create a folder to clone the accelerator in and open a PowerShell\ncommand window with its current working directory set to the folder you\njust created and then enter the following console command:  \n&nbsp;  \n```git clone https://github.com/managedcoder/PVAAudioCodesCC```  \n&nbsp;  \nIf you don't have Git installed then you can scroll to the top of this\npage and click the green Code button and download a zip archive of this\nrepo and extract it into a working folder.  \n![Figure 1: High-level Subsystem Overview](Doc/CloneToZip.png)\n#### 2) **Deploy Mock Services**  \n>In order to capture realistic enterprise scenarios for a modern contact\ncenter, this accelerator implements mock services and hosts sample audio\nlogo files.  To make it as easy as possible to get up and running quickly\nwithout having to understand the technical details of the accelerator, a\nsingle PowerShell script is provided that will deploy and configure these\nmock services and all you'll need to do is run it.  \n&nbsp;  \nOpen a Power Shell command window with its current working directory set\nto the **Deployment** project folder and sign into your Azure subscription\nif you haven't already  \n&nbsp;  \n```az login```  \n&nbsp;  \nRunning the following command will deploy the mock services used by this\naccelerator.  You can delete these services once they've served their\npurpose of providing an explicit blueprint/roadmap for building a modern\ncontact center.  \n&nbsp;  \n```.\\deploy.ps1 -name \"<name>\" -location \"<region>\"```  \n&nbsp;  \nWhere:  \n**```<name>```** is the name to use to deploy the accelerator's mock\nservices.  For example, you could use some form of your contact center\nassistant's name using letters and numbers that's between 3 and 24\ncharacters long (e.g., ```\"ContosoMockServices\"```).  \n**```<region>```** is the Azure region where the mock services will\nbe deployed (e.g., ```\"eastus\"```).  \n&nbsp;  \nThe deployment generally only takes about a minute to complete and when\nit's done you'll see two important configuration settings in yellow that\nyou'll need to come back and copy later when you get to the configuration\nstep for your contact center assistant.  \n&nbsp;  \n**Note:** If there are errors during deployment, you'll see them in the\ncommand window and more detailed error information will be written to a\nlog file named deploy_log.txt.\n\n#### 3) **Import, Configure & Publish the PVA / AudioCodes Solution**  \n>The accelerator's contact center assistant has been shared as a Power\nVirtual Agent solution that will need to be imported into your Power\nApps environment.  \n&nbsp;  \n***Import Solution***  \nBrowse to the [PVA portal](http://powerva.microsoft.com/) and click the\n\"settings\" gear icon in the top right and then click \"General Settings\"\nlink.  In the resulting Setting dialog, click the \"Go to Power Apps\nSolutions\"  \n&nbsp;  \n![Open Assistant](Doc/GeneralSettings.png)  \n&nbsp;   \nIn Power Apps portal, click the \"Import\" link at top of page and then\nclick the \"Browse\" button and select\nPVAAudioCodesContactCenterAccelerator_1_0_0_0.zip which you'll find in\nthe root folder of the accelerator you cloned/extracted in step 1. Then\nclick \"Next\" and \"Import\".\n&nbsp;  \n![Open Assistant](Doc/ImportSolution.png)  \n&nbsp;  \n**Note:** If you're having trouble importing the solution, you can find more\ndetailed instructions [here](https://docs.microsoft.com/en-us/power-virtual-agents/authoring-export-import-bots#import-the-solution-with-your-bot)\nor watch [this short video](https://www.microsoft.com/en-us/videoplayer/embed/RE4CsHl?postJsllMsg=true)\nand skip to the 1:48 minute mark where it shows how to import a solution.  \n&nbsp;  \n***Open Solution***  \nOpen the [PVA portal](https://web.powerva.microsoft.com) in a new browser\nwindow (or refresh the portal if it's already open) and open the imported\nassistant by clicking on the bot icon in the upper right corner of the page\nand choose your assistant from the menu.  \n&nbsp;  \n![Open Assistant](Doc/OpenAssistant.png)  \n&nbsp;  \n<a name=\"ConfigureYourAssistantsGreeting\"></a>***Configure Your Assistant's Greeting***  \nTo connect your contact center assistant to the accelerator's mock\nservices, there is a simple configuration step you'll need to complete.  \n&nbsp;  \nClick the **Topics** tab in the left navigation and find the **Greeting**\ntopic in the list of Topics (use Search box if needed) and click on it\nto open it.  There are three values you'll need to set in the **Redirect**\naction for **configureMockSettings**.  Replace the placeholder text in the\n**customerServiceHostURL** and **audioLogoURL** fields with the\ncorresponding values displayed in yellow text at the end of the\ndeployment in the deployment command window. For the **agentPhoneNumber**\nsetting you can use the phone number you would like the assistant to call\nwhen agent escalation occurs.  The number should start with the country\ncode which in the US is \"1\" so the number would look something like\n15554441212.  There are several places in the building blocks and\nscenarios that escalate to an agent and when that happens the phone number\nyou provide in this setting will be called. Using your cell phone number\nis a convenient way to test and demo this.  \n&nbsp;  \n![Open Assistant](Doc/ConfigSettings.png)  \n&nbsp;  \nCopy the text highlighted with red boxes from the command window you\nused to deploy the mock services and paste it into the corresponding\nfields as described above.  \n&nbsp;  \nFinally, click **Save** in upper right of page.  \n&nbsp;  \n![Open Assistant](Doc/ConfigSettingsValues.png)  \n&nbsp;  \n***Set Security for Your Assistant***  \nSet the security level of your assistant by clicking the **Manage** tab in\nthe left navigation menu and then click the **Security** menu option and\nthen click the **Authentication** option  \n&nbsp;    \n![Open Assistant](Doc/SetSecurityForAssistant.png)  \n&nbsp;  \nFor the purposes of getting started you can choose **No Authentication**\nand click **Save** and then click **Close**\n&nbsp;  \n![Open Assistant](Doc/NoAuthenticationSelection.png)  \n&nbsp;  \n***Publish Your Assistant***  \nFinally, publish your assistant by clicking the **Publish** tab in the\nleft navigation menu and then click the **Publish** button  \n&nbsp;    \n![Open Assistant](Doc/PublishAssistant.png)  \n#### 4) <a name=\"TelephonyConfiguration\"></a>**Telephony Configuration**  \n>Configuring the telephony aspect of the solution accelerator involves\nworking in AudioCodes VoiceAI Connect Cloud portal.  You can follow\nthe instructions in this step or follow the instructions [here](https://techdocs.audiocodes.com/voice-ai-connect/#VAIG_Cloud/ms_power_va.htm?TocPath=VoiceAI%2520Connect%2520Cloud%257CCreating%2520your%2520bot%2520using%2520Bot%2520Integration%2520wizard%257C_____1)\nto create a telephony bot connection.  \n&nbsp;  \n***Create a Bot Connection***  \nOpen the [AudioCodes portal](http://voiceaiconnect.audiocodes.com/)\nin the browser sign in and if this is the first time you've visited\nthe AudioCodes portal, it will look like the screen below and you'll\nselect the **Connect your bot to phone number provided by AudioCodes**\nwizard and then the screens from that point on will match starting with\nthe selection of **Microsoft Power Virtual Agent** as the next step.\n&nbsp;  \n![Open Assistant](Doc/AudioCodesBotWizard.png)  \n&nbsp;  \nIf this is not the first time you've signed in to the AudioCodes portal,\nthen the screen will look like the screen below and you should click\nthe plus sign.  \n&nbsp;  \n![Open Assistant](Doc/OpenFrameworkWizard.png)  \n&nbsp;  \nSelect **Microsoft Power Virtual Agents** framework and click **Next**  \n![Open Assistant](Doc/AudioCodesPVABot.png)  \n&nbsp;  \n***Bot Details Tab***  \nOn the Bot Details tab, enter a name for the bot connection in\nAudioCodes then enter the PVA Bot's secret which you can find by \nfollowing [these instructions](https://docs.microsoft.com/en-us/power-virtual-agents/configure-web-security#obtain-the-secrets)\nand then click **Next**  \n&nbsp;  \n![Bot Details Tab](Doc/vaicBotDetailsTab.png)  \n***Bot Setting Tab***  \nThe default values of the Bot Settings tab are fine and don't need to\nbe changed so you can click **Create**  \n&nbsp;  \n![Bot Details Tab](Doc/vaicBotSettingsTab.png)  \n&nbsp;  \n***Enable Bot Features***  \nOnce you've created a bot connection, you can edit it's features\nand enable capabilities that the accelerator is pre-wired to take\nadvantage of.  Switch to the **Bots** tab and click **Edit** on the\nbot connection you created.  \n&nbsp;  \n![Bot Features Tab](Doc/OpenBotConnectionFeatures.png)  \n&nbsp;  \nClick on the **Features** tab and then enable outbound calling,\ncall recording controlled by bot, and call transfer and then\nclick **Update**.  \n&nbsp;  \nThese features are not available with the basic trial subscription\nand, although the majority of the accelerator's building blocks and\nscenarios work with the trial subscription, if you want demo or\nextend those aspects of the accelerator, you'll need to upgrade to\nthe **Essential Plan** in lower left of dialog.  \n&nbsp;  \n**Note:** The agent escalation capability requires the **call\ntransfer** feature to be enabled so if escalation happens while\nyou are experimenting or demoing, you'll hear a message that says,\n\"Hello, welcome to AudioCodes VoiceAI Connect, the number you've\ndialed is not connected to a root...\".  \n&nbsp;  \n![Bot Features Tab](Doc/vaicBotFeaturesTab.png)  \n&nbsp;  \n***Provision a Phone Number***  \n&nbsp;  \nNext, you'll provision the phone number for your assistant. Click on\nthe **Numbers** tab in the menu on the left and click the\n**plus sign** and choose a country, state, and city and click\n**Add Number** and then **Finish**.  \n&nbsp;  \n![Bot Features Tab](Doc/ProvisionPhoneNumber.png)  \n&nbsp;  \n***Connect Your Bot to Phone Number***\n&nbsp;  \nNow you can connect your bot to the phone number you just provisioned.\nClick the **Wizard** button in lower left and then click **Select**\nbutton on the **Connect your bot to phone number provided by\nAudioCodes** wizard option and then select your bot from the list and\nclick **Next**.  \n&nbsp;  \n![Bot Features Tab](Doc/vaicAssignNumberToBotOption.png)  \n&nbsp;  \n***Select Existing Phone Number***  \nOn the **SELECT PHONE NUMBER** screen, choose **Use existing number** then select your provisioned number\nand click **Next**  \n&nbsp;  \n![Bot Features Tab](Doc/vaicChooseExistingNumber.png)  \n&nbsp;  \nYou can take all the defaults on the **CREATE ROUTING RULES** page\nand select **Create**  \n&nbsp;  \n![Bot Features Tab](Doc/vaicRoutingRules.png)  \n&nbsp;  \n***Call Your Assistant!!!***  \n&nbsp;  \nCongratulations!  You are now ready to call your bot and when you\ndo, you should hear an audio logo and then the assistant's greeting.  \n&nbsp;  \nIf the number isn't working yet or AduioCodes says it not ready,\nthen give it a minute and try again.  \n&nbsp;  \n*****Trouble Shooting*****  \nIf you hear a message that says, \"To use this bot, publish it in\nPower Virtual Agents...\", check to make sure that you have published\nthe bot in the [Power Virtual Agent portal](https://powerva.microsoft.com/).  \n&nbsp;  \nIf you hear a message that says, \"The accelerator needs to be\nconfigured before its ready to take calls.  Instructions can be found\nin the README documentation.\" then you'll need to review\n[this section of Step 3](#ConfigureYourAssistantsGreeting) and\nreplace the placeholder values with the correct ones.  \n\n\n## <a name=\"UsingTheAccelerator\"></a>Using the Accelerator\n\nThink of this accelerator as part construction kit and part roadmap where its building\nblocks and scenarios allow you to minimize time-to-value and maximize solution\ncapability.\n \n**Building Blocks** - The accelerator includes a set of executable how-to's that\nprovide \"implementation snippets\" for various common contact center tasks.  You can use\nthem \"as is\" or tailor and extend them to meet the needs of your solution.  \n**Scenarios** - The scenarios included in the accelerator are example implementations\nof common contact center scenarios like \"order status\" or \"product trouble\nshooting\".  \n**Best Practices** - Finally, the best practices discuss important contact center\ntopics and provide insight and effective approaches for addressing contact center\nchallenges.\n\nThe accelerator is spread across several portals and a number of assets so we've\ncreated [this video](https://aka.ms/pva-vaicc-tour) to help you understand what's\nincluded in the accelerator and how to use it effectively.\n\n## <a name=\"BestPractices\"></a>Best Practices\nAs this accelerator evolves, this section will capture and discuss important\ncontact center topics and common challenges and provide insight and effective\napproaches for addressing them.\n\n### <a name=\"Gotchas\"></a>Gotcha's To Be Aware Of\n- **Version Confusion** - Use version numbers in PVA and Composer\nTopics so you can ask for them when you test your contact center\nassistant.  Anytime you change a Composer topic you will need to\n1) refresh the PVA portal page and 2) republish your contact center\nassistant, otherwise, AudioCodes VoiceAI Connect Cloud (VAIC-C) will\nstill be pointing to the last published version.  Even more\nconfusing is that the Test Panel in the PVA portal works fine since\nit points to the most up-to-date unpublished version.\n\n- More to come soon!\n\n### <a name=\"AddOrExtendComposerTopics\"></a>Add or Extend Composer Topics  \nComing soon!\n\n### <a name=\"TelephonyUserExperienceChallengesAndBestPractices\"></a>Telephony User Experience Challenges and Best Practices  \nComing soon!\n\n### <a name=\"CapturingCustomerInsight\"></a>Capturing Customer Insight  \nComing soon!\n\n### <a name=\"MultiModalCustomerEngagement\"></a>Multi-modal Customer Engagement  \nComing soon!\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-pva-audiocodes-cc", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-pva-audiocodes-cc", "platform_org_repo": "github+microsoft/dstoolkit-pva-audiocodes-cc", "link_to_repo": "https://github.com/microsoft/dstoolkit-pva-audiocodes-cc", "platform": "github", "language": "PowerShell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# @typescript/github-link\nFor a file in a local clone of a Github repository, find the corresponding github.com permalink.\n\n## Usage\n\n`npx ghlink path/foo.ts`\n\n`npx ghlink path/foo.ts line`\n\n`npx ghlink path/foo.ts,line`\n\n`npx ghlink path/foo.ts:line`\n\nA trailing character position (e.g. as in Node.js stack traces) will also be tolerated, but will not affect the output.\n\nIf the commit exists in multiple remotes, `origin` will be preferred.  Failing that, one will be chosen at random.\n\n## Deployment\n\nTo publish a new version of this package, change the version in `package.json` and push to main.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "typescript-github-link", "org_name": "microsoft", "org_repo": "microsoft/typescript-github-link", "platform_org_repo": "github+microsoft/typescript-github-link", "link_to_repo": "https://github.com/microsoft/typescript-github-link", "platform": "github", "language": "TypeScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Z3Guide Documentation\n\nThis repository contains sources of [Z3Guide](https://microsoft.github.io/z3guide/), an online tutorial for [Z3](https://z3prover.github.io/) powered by [RiSE at Microsoft Research](https://www.microsoft.com/en-us/research/group/research-software-engineering-rise/).\n\nThe rest of this page is for developers contributing to the tutorial docs of Z3.\n\n## Table of Content\n1. [Developer Setup](#developer-setup)\n    1. [Local Setup](#local-setup-not-using-codespaces)\n    2. [Codespaces](#codespaces-microsoft-internal-only)\n2. [Development](#development)\n    1. [Contributing to Existing Materials](#contributing-to-existing-tutorial-materials)\n    2. [Creating New Tutorial Materials](#creating-new-tutorial-materials)\n    3. [Testing the Website](#testing-the-website)\n3. [Manually Updating `z3-solver`](#manually-updating-z3-solver)\n4. [Code of Conduct](#microsoft-open-source-code-of-conduct)\n5. [Trademarks](#trademarks)\n\n## Developer Setup\n\n### Local Setup (not using Codespaces)\n\n- install node.js >= 16.15.1\n- install yarn globally if needed\n\n```\nnpm install -g yarn\n```\n\n- Set up repo and start development following the steps in [Development](#development).\n\n\n### Codespaces\n\nThese instruction use GitHub Codespaces, a convienient way to get a perfect cloud development environment. Your organization may or may not support Codespaces.\n\n- From [this repository](https://github.com/microsoft/z3guide/), click on the green `<> Code` button, select the `Codespaces` tab and then `Create codespaces on main`. The setup might take a couple of minutes.\n\n- From there, a VSCode tab will open in your browser. You may now edit, test and commit to the repository just like on your local machine.\n\n  - All command line instructions assume a bash-like terminal.\n\n- Set up repo and start development following the steps in [Development](#development).\n\n### Microsoft Employees ONLY\n\n- Join the Microsoft github organization from [Microsoft Open Source](https://opensource.microsoft.com/) via the `Employee sign-in` at the bottom.\n\n  - From there, go to the `GitHub for Open Source at Microsoft` tab and follow the instructions to join the organization via the management portal.\n\n## Development\n\n- Change the working directory to `website`:\n\n```\ncd website\n```\n\n- From `website`, run the script to install dependencies\n\n```\nyarn\n```\n\n- Launch the docs server (which does client-side rendering and allows for hot reloading, so that you see your changes immediately reflected to the locally running page)\n\n```\nyarn clear; # for clearing cache\nyarn start; \n```\n\n- Click on the generated URL in the terminal output to see the website now running locally.\n\n### Inspecting the Output from the Build\n- In case you want to inspect the output (.html files etc. from server-side rendering) from the build, you can run\n\n```\nyarn clear; # clearing your cache first is always recommended\nyarn build;\n```\n\nand if successful, you should see a `build` directory under `website`.\n\n- If you want to see how these output files are rendered in the browser, you may run \n```\nyarn serve\n```\nNote that this is _different_ from `yarn start`, as `yarn serve` does not do hot-reloading because it is simply _serving_ the files under `build` rather than rebuilding everything from scratch for every change you make, like what `yarn start` does.\n### Contributing to Existing Tutorial Materials\n\nThe online Z3 Guide serves multiple instances of tutorial materials: currently it has a [Z3 tutorial in SMTLIB format](https://microsoft.github.io/z3guide/docs/logic/intro), and [Programming Z3 in different language bindings](https://microsoft.github.io/z3guide/programming/Programming%20Z3/Using%20Z3%20from%20Python/Introduction). \n\nThe instances are hosted under `website/docs-smtlib` and `website/docs-programming`, respectively. To contribute to existing tutorial materials, you may add/edit materials in either directory.\n\n-   You may find the Docusaurus documentation on [Docs](https://docusaurus.io/docs/docs-introduction) useful, especially for configuring the sidebar.\n\n-   Note that Docusaurus does live reload, so that every change you make to `website/docs` will be immediately reflected in the locally running tab.\n\n-   For all Z3-SMTLIB code snippets, please use the following Markdown code blocks format:\n    ~~~\n    ```z3\n    (exec-this-command arg)\n    ```\n    ~~~\n\n- Any Z3 runtime error with the code specified in the code block above would fail the build.\n    However, you may intentionally bypass the errors by using flags of `no-build` or `ignore-errors`:\n\n    ~~~\n    ```z3 no-build\n    give me the error on the webpage I know it is invalid\n    ```\n    ~~~\n\n    or\n    ~~~\n    ```z3 ignore-errors\n    I know this snippet is problematic but I want to teach about error messages so show them\n    ```\n    ~~~\n\n- You may also add Z3-JavaScript or Z3-Python code snippets using Markdown code blocks, e.g.,:\n\n  ~~~\n  ```z3-js\n  // some z3-js code\n  ```\n  ~~~\n\n  and \n\n  ~~~\n  ```z3-python\n  # some z3-python code\n  ```\n  ~~~\n\n  and the flags of `no-build` and `ignore-errors` remain applicable to these code blocks.\n\n  Note that we currently support output rendering and code editing for Z3-SMTLIB and Z3-JavaScript code snippets, with similar support for Z3-Python coming soon.\n\n- Programming Z3 in other language bindings is not supported at the moment.\n\n- Sidebar content is maintained in files under `website/sidebars`. The Z3-SMTLIB guide uses `website/sidebars/smtlibSidebars.js`, while the Programming Z3 (with other language bindings) guide uses `website/sidebars/programmingSidebars.js`. If you find your new content missing from either sidebar, please modify the respective sidebar file accordingly.\n\n### Creating New Tutorial Materials\n\nThe process of creating new tutorial materials is similar to the above, except for the following additional steps:\n\n1. You will need to create a new `docs-*` directory under `website`. E.g., `website/docs-api`.\n2. You will need a JavaScript file for configuring the sidebar for your docs under `website/sidebars`. The sidebar can be generated automatically. You may simply make a renamed copy of `programmingSidebars.js` for such automation.\n3. You will need to go to `docusaurus.config.js` to add additional configurations so that the build process can pick up the new directories. Search for comments beginning with `[NEW DOCS]` within the file for more instructions.\n\n\n### Testing the Website\nWe provided a test file that contains all kinds of Z3 (SMTLIB and JavaScript) code snippets, `docs-playground/_03.test.md`.\nTo test if the interactivity with Z3 snippets works as expected, you may remove the underscord at the beginning of the file name, and run\n```\nyarn start\n```\nto access the file at `https://localhost:[PORT]/z3guide/playground/test`, where `[PORT]` is 3000 by default or the port number you specified in `docusaurus.config.js`.\n\nYou may add more code snippets to this test file, or create your own test file under any `docs-*` directory your prefer. We recommend putting the test files under `docs-playground`.\n\nWhen you are done testing, make sure to add the underscore back to the test file name, so that the content will not be included in rendering the website.\n\n\n## Manually Updating `z3-solver`\nUpgrades of `z3-solver` should be done ONLY you are certain that the latest version of `z3-solver` works well with the existing examples and website infrastructure. We provide a script to automate the manual upgrade process:\n\n```\n# remember to switch into the `website` directory first\nyarn upgrade-z3\n```\n\nThe script will update `z3-solver` to the latest and then try to build the website. If the build fails, then it will downgrade `z3-solver` to the version before your upgrade. It is unlikely that the update itself fails.\n\nAfter done, make sure the field `dependencies.z3-solver` in `website/package.json` is exactly the version that you just upgraded to. For example, if you just upgraded to version 4.10.1, it should look like\n```js\n{\n  //...\n  \"dependencies\": {\n    //...\n    \"z3-solver\": \"4.10.1\" // it should not be \"^4.10.1\" or \"~4.10.1\" or any other values that contain other symbols\n  }\n}\n```\nSo that we make sure that `yarn install` always picks up the _exact_ version of `z3-solver` that you mean for the website to run on.\n\n## Microsoft Open Source Code of Conduct\n\nThis project is hosted at https://github.com/microsoft/z3guide/.\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "z3guide", "org_name": "microsoft", "org_repo": "microsoft/z3guide", "platform_org_repo": "github+microsoft/z3guide", "link_to_repo": "https://github.com/microsoft/z3guide", "platform": "github", "language": "JavaScript", "stargazers_count": 31, "watchers_count": 31}, {"README_text": "# Introduction \r\nSource code of BindVAE paper on Variational Auto Encoders for learning binding signatures of transcription factors.\r\n\r\nhttps://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02723-w\r\n\r\n\r\n# Installation\r\n1.\tInstallation process for the machine learning model\r\nPlease create a conda environment as shown below OR using the yaml file: tfp.yaml\r\n\r\nUsing the supplied yaml file:\r\n\r\n\tconda env create --file=tfp.yaml\r\n\r\nIf you have most of the dependencies already installed, the following simpler setup will suffice:\r\n\r\n\tconda env create --name tfp python=3.7\r\n\tconda install tensorflow-gpu\r\n\tconda install tensorflow-probability\r\n\r\n\r\nNote: In some versions of tensorflow / tensorflow-probability, you might get a \"KL divergence is negative\" error during training. We have not yet figured out why this appears.\r\n\r\n2. Dependencies for the feature generation\r\n\r\nThe feature generation code uses R\r\n\r\n\tinstall.packages(\"BiocManager\")\r\n\tBiocManager::install(\"GenomicRanges\")\r\n\tBiocManager::install(\"BSgenome.Hsapiens.UCSC.hg19\")\r\n\r\n\tinstall.packages(\"remotes\")\r\n\tremotes::install_github(\"ManuSetty/ChIPKernels\")\r\n\r\n\r\n\r\n## TRAINING\r\n\r\n\tpython run.py --model_dir model_dir --train_path data/gm12878_all8merfeats_1k_examples.txt --eval_path data/gm12878_all8merfeats_1k_examples.txt --test_path data/SELEX_probes_all8merfeats_1k_samples.txt --num_topics 25 --prior_initial_value 10 --mode train --vocab_path data/vocabulary_all8mers_with_wildcards.npy\r\n\r\nParameters that are most sensitive and best ones to tweak:\r\n\r\nbatch_size  (currently set at 32)<br>\r\nnum_topics  (size of the hidden bottleneck layer)<br>\r\nprior_initial_value<br>\r\nprior_burn_in_steps<br>\r\n\r\nModifying the number of layers and their width in the Encoder.\r\n\r\n## TEST (or getting TOPIC POSTERIORS)\r\n\r\nIf you want to use a previously saved model to do inference on new data, use the code in \"test\" mode as follows:\r\n\r\n\tpython run.py --model_dir model_dir --test_path data/SELEX_probes_features.txt --num_topics 25 --prior_initial_value 10 --mode test --vocab_path data/vocabulary_all8mers_with_wildcards.npy\r\n\r\nOutput: a matrix of size N x K, where N = number of examples in the input file, K = number of topics / latent dimensions.\r\n\r\n## K-MER DISTRIBUTIONS (DECODER PARAMETERS that encode the TOPIC distributions over words)\r\n\r\n\tpython run.py --model_dir model_dir --num_topics 25 --prior_initial_value 10 --mode beta --vocab_path data/vocabulary_all8mers_with_wildcards.npy\r\n\r\n# FILE FORMATS\r\n\r\n## Data file format:\r\nA list of feature-ids separated by spaces. The training / test files are formatted as lists of features where if a feature has count k, then it appears k times in the list. Each line of the file is one example.\r\nIf you want to change this input format, please look at sparse_matrix_dataset (or let me know and i can help with it). See below for a file with two input examples (documents). The feature ids should be in an increasing order. Also see attached sample file (data/gm12878_all8merfeats_1k_examples.txt).\r\n\r\n112 113 113 113 122 134 144 144 144 144 159 178<br>\r\n115 115 189 194 194 202 202 202\r\n\r\n## Vocabulary format:\r\nPlease see the sample vocabulary file (.npy file) for how to format the <feature-id>  <feature-name>  mapping.  It is in a dictionary format. For example, below are the top few lines of the vocabulary for the k-mer model, which was converted into the vocabulary_all8mers_with_wildcards.npy file. So, if you load the dictionary, d['EMPTY']=0  and d['AAAAAAAA']=1 and so on. Please keep the first dictionary entry a dummy feature like 'EMPTY' and assign it to the index 0. Obviously, none of the examples will contain this feature :-) This is due to how the indexing is done after loading the vocabulary (i.e. the useful features should have indices >=1).\r\n\r\nEMPTY<br>\r\nAAAAAAAA<br>\r\nAAAAAAAC<br>\r\nAAAAAAAG<br>\r\nAAAAAAAT<br>\r\nAAAAAACA<br>\r\nAAAAAACC<br>\r\nAAAAAACG<br>\r\nAAAAAACT<br>\r\nAAAAAAGA<br>\r\nAAAAAAGC<br>\r\nAAAAAAGG<br>\r\n\r\n# OUTPUTS\r\n\r\nTo monitor what the model is learning, you can look at the periodic outputs. The frequency of outputs is controlled by the parameter viz_steps in the code. It is currently set to 20000, but feel free to set it to 1000 or so in the initial runs till you understand what's going on.\r\n\r\nHere's what it looks like for k-mers and ATAC-seq peaks. Only the top few are printed. Again this can be controlled by looking at the method get_topics_strings.\r\n\r\nelbo\r\n-2646.7239\r\n\r\nkl\r\n32.239582\r\n\r\nloss\r\n2646.5957\r\n\r\nperplexity\r\n79969.914\r\n\r\nreconstruction\r\n-2614.485\r\n\r\ntopics\r\nb'index=92 alpha=4.94 CCGCCNNC NNGGGCGG NNCCGCCC NNGGCGGG CCGCNNCC NNCCCGCC CNNCGCCC CCCGCNNC GCNNCGCC CNNCCGCC'<br>\r\nb'index=14 alpha=1.80 NNCAGAGA NNTCTCTG NNTCTGTG NNCACAGA NNCTCTGT NNACAGAG CACAGNNA CAGAGNNA ANNCACAG NNTCACAG'<br>\r\nb'index=17 alpha=1.74 CCCNNCCC CCNNCCCC CCCCNNCC AGGGGNNG NNGGGGAG NNCTCCCC CNNCCCCC CNNCCCCA CCCCANNC CCCCTNNC'<br>\r\n\r\n....\r\n\r\nglobal_step\r\n160000\r\n\r\n\r\n# Contribute\r\nTODO: Explain how other users and developers can contribute to make your code better. \r\n", "repo_name": "BindVAE", "org_name": "microsoft", "org_repo": "microsoft/BindVAE", "platform_org_repo": "github+microsoft/BindVAE", "link_to_repo": "https://github.com/microsoft/BindVAE", "platform": "github", "language": "Python", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Language Server Protocol types code generator\n\nThis repository contains code to generate Language Server Protocol types and classes for various languages.\n\n# Code Generator usage\n\n## Usage\n\nYou will need a python environment to run the generator. Here are the steps:\n\n1. Clone this repository.\n2. Create environment using `python -m venv .venv` in the root directory of this project.\n3. Activate the environment using `.venv\\Scripts\\activate` on Windows or `.venv/bin/activate` on Linux/Mac.\n4. Run this command to install `nox`: `python -m pip install nox`\n5. Run this command to install dependencies:\n\n```\npython -m pip install -r ./packages/python/requirements.txt -r ./requirements.txt\n```\n\n### Command line\n\nClone this repository and run `generator` like a module.\n\n```console\n>python -m generator --help\nusage: __main__.py [-h] [--schema SCHEMA] [--model MODEL]\n\nGenerate types from LSP JSON model.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --schema SCHEMA, -s SCHEMA\n                        Path to a model schema file. By default uses packaged\n                        schema.\n  --model MODEL, -m MODEL\n                        Path to a model JSON file. By default uses packaged\n                        model file.\n  --plugin PLUGIN, -p PLUGIN\n                        Name of a builtin plugin module. By default uses all\n                        plugins.\n```\n\n### using `nox`\n\nThis project uses `nox` as a task runner to run the code generator. You can install `nox` and run `build_lsp` session to generate code from spec available in the repo.\n\n```console\n> python -m pip install nox\n> nox --session build_lsp\n```\n\nYou can format code, run tests, and other tasks using `nox` as well.\n\n# Contributing plugins\n\n## Adding a new plugin\n\nFollow these steps to generate boiler plate code for new plugin:\n\n1. Create a virtual environment for python using python 3.7 and activate that environment.\n    1. If you have python extension for VS Code installed then run `Python: Create Environment` command. Be sure to select all the `requirements.txt` files in the repo. This should, install all packages needed and select the environment for you.\n1. Ensure `nox` is installed.\n    1. Run `nox --list`, is nox is installed oyu should see a list of available sessions. Otherwise, run `python -m pip install nox` from the python 3.7 environment you created above.\n1. Run `nox --session create_plugin` and follow the prompts to create a new plugin.\n\nExample:\n\n```console\n> nox --session create_plugin\nnox > Running session create_plugin\nnox > Creating virtual environment (virtualenv) using python.exe in .nox\\create_plugin\nEnter the name of the plugin: java\nnox > Created plugin java.\nnox > Session create_plugin was successful.\n```\n\n# Supported plugins\n\n| Language | Plugin                  | Package                                                                                             | Notes  |\n| -------- | ----------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n| Python   | generator-plugin.python | [![PyPI](https://img.shields.io/pypi/v/lsprotocol?label=lsprotocol)](https://pypi.org/p/lsprotocol) | Active |\n", "repo_name": "lsprotocol", "org_name": "microsoft", "org_repo": "microsoft/lsprotocol", "platform_org_repo": "github+microsoft/lsprotocol", "link_to_repo": "https://github.com/microsoft/lsprotocol", "platform": "github", "language": "Python", "stargazers_count": 42, "watchers_count": 42}, {"README_text": "[![Causica CI Build](https://github.com/microsoft/causica/actions/workflows/ci-build.yml/badge.svg)](https://github.com/microsoft/causica/actions/workflows/ci-build.yml)\n\n# Causica\n\n## Overview\n \nCausal machine learning enables individuals and organizations to make better data-driven decisions. In particular, causal ML allows us to answer \u201cwhat if\u201d questions about the effect of potential actions on outcomes. \n \nCausal ML is a nascent area, we aim  to enable a **scalable**, **flexible**, **real-world applicable end-to-end** causal inference framework. In perticular, we bridge between causal discovery, causal inference, and deep learning to achieve the goal.  We aim to develop technology can automate causal decision-making using existing observational data alone, output both the discovered causal relationships and estimate the effect of actions simultaneously.\n \nCausica is a deep learning library for end-to-end causal inference, including both causal discovery and inference.  It implements deep end-to-end inference framework [2] and different alternatives.\n \nThis project splits the interventional decision making from observational decision making Azua repo found here [Azua](https://github.com/microsoft/project-azua).\n\nThis codebase has been heavily refactored, you can find the previous version of the code [here](https://github.com/microsoft/causica/releases/tag/v0.0.0).\n\n# DECI: End to End Causal Inference\n\n## Installation\n\nThe Causica repo is on PyPI so you can be pip installed:\n\n```\npip install causica\n```\n\n## About\n\nReal-world data-driven decision making requires causal inference to ensure the validity of drawn conclusions. However, it is very uncommon to have a-priori perfect knowledge of the causal relationships underlying relevant variables. DECI allows the end user to perform causal inference without having complete knowledge of the causal graph. This is done by combining the causal discovery and causal inference steps in a single model. DECI takes in observational data and outputs ATE and CATE estimates. \n\nFor more information, please refer to the [paper](https://arxiv.org/abs/2202.02195).\n\n\n**Model Description**\n\nDECI is a generative model that employs an additive noise structural equation model (ANM-SEM) to capture the functional relationships among variables and exogenous noise, while simultaneously learning a variational distribution over causal graphs. Specifically, the relationships among variables are captured with flexible neural networks while the exogenous noise is modelled as either a Gaussian or spline-flow noise model. The SEM is reversible, meaning that we can generate an observation vector from an exogenous noise vector through forward simulation and given a observation vector we can recover a unique corresponding exogenous noise vector. In this sense, the DECI SEM can be seen as a flow from exogenous noise to observations. We employ a mean-field approximate posterior distribution over graphs, which is learnt together with the functional relationships among variables by optimising an evidence lower bound (ELBO). Additionally, DECI supports learning under partially observed data.\n\n**Simulation-based Causal Inference**\n\nDECI estimates causal quantities (ATE) by applying the relevant interventions to its learnt causal graph (i.e. mutilating incoming edges to intervened variables) and then sampling from the generative model. This process involves first sampling a vector of exogenous noise from the learnt noise distribution and then forward simulating the SEM until an observation vector is obtained. ATE can be computed via estimating an expectation over the effect variable of interest using MonteCarlo samples of the intervened distribution of observations. \n\n## How to run\n\nThe best place to start is the `examples/multi_investment_sales_attribution.ipynb` notebook. This explains how to fit a model using PyTorch Lightning and test ATE and ITE results.\n\nFor a more detailed introduction to the components and how they fit together, see the notebook `examples/csuite_example.ipynb`, for how to train a DECI model and check the causal discovery.\n\nThis will download the data from the CSuite Azure blob storage and train DECI on it. See [here](https://github.com/microsoft/csuite) for more info about CSuite datasets. The notebook will work on any of the available CSuite datasets.\n\n\n**Specifying a noise model**\n\nThe noise exogenous model can be modified by changing the `noise_dist` field within `TrainingConfig`, either Gaussian or Spline are allowed.\n\nThe Gaussian model has Gaussian exogenous noise distribution with mean set to 0 while its variance is learnt.\n\nThe Spline model uses a flexible spline flow that is learnt from the data. This model provides most gains in heavy-tailed noise settings, where the Gaussian model is at risk of overfitting to outliers, but can take longer to train.\n\n**Using a known Causal graph**\n\nTo use DECI to learn the functional relationships, remove the variational distribution terms from the loss and replace the sample with the known graph.\n\n## Further extensions \n\nFor now, we have removed Rhino and DDECI from the codebase but they will be added back. You can still access the previously released versions [here](https://github.com/microsoft/causica/releases/tag/v0.0.0).\n\n# References\n\nIf you have used the models in our code base, please consider to cite the corresponding papers:\n\n[1], **(VISL)** Pablo Morales-Alvarez, Wenbo Gong, Angus Lamb, Simon Woodhead, Simon Peyton Jones, Nick Pawlowski, Miltiadis Allamanis, Cheng Zhang, \"Simultaneous Missing Value Imputation and Structure Learning with Groups\", [ArXiv preprint](https://arxiv.org/abs/2110.08223)\n\n[2], **(DECI)** Tomas Geffner, Javier Antoran, Adam Foster, Wenbo Gong, Chao Ma, Emre Kiciman, Amit Sharma, Angus Lamb, Martin Kukla, Nick Pawlowski, Miltiadis Allamanis, Cheng Zhang. Deep End-to-end Causal Inference. [Arxiv preprint](https://arxiv.org/abs/2202.02195) (2022)\n\n[3], **(DDECI)** Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings, Cheng Zhang. Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning. [ICLR](https://openreview.net/forum?id=dcN0CaXQhT) (2023)\n\n[4], **(Rhino)** Wenbo Gong, Joel Jennings, Cheng Zhang, Nick Pawlowski. Rhino: Deep Causal Temporal Relationship Learning with History-dependent Noise. [ICLR](https://openreview.net/forum?id=i_1rbq8yFWC) (2023)\n\n\n# Development\n\n## Poetry\n\nWe use Poetry to manage the project dependencies, they're specified in the [pyproject.toml](pyproject.toml). To install poetry run:\n\n```\n    curl -sSL https://install.python-poetry.org | python3 -\n```\n\nTo install the environment run `poetry install`, this will create a virtualenv that you can use by running either `poetry shell` or `poetry run {command}`. It's also a virtualenv that you can interact with in the normal way too.\n\nMore information about poetry can be found [here](https://python-poetry.org/)\n\n## mlflow\n\nWe use [mlflow](https://mlflow.org/) for logging metrics and artifacts. By default it will run locally and store results in `./mlruns`.\n", "repo_name": "causica", "org_name": "microsoft", "org_repo": "microsoft/causica", "platform_org_repo": "github+microsoft/causica", "link_to_repo": "https://github.com/microsoft/causica", "platform": "github", "language": "Python", "stargazers_count": 272, "watchers_count": 272}, {"README_text": "# Dynamics 365 Connected Field Service - Azure IoT Deployment Template\n\n## Overview\n\nConnected Field Service enables organizations to transform the way they provide service from a costly break-fix model to a proactive and predictive service model through the combination of IoT diagnostics, scheduling, asset maintenance, and inventory on the same platform.\nThere are three ways you can use to connect IoT-enabled devices into the Field Service solution:\n\n- Connected Field Service for [Azure IoT Hub](https://azure.microsoft.com/services/iot-hub/)\n- Connected Field Service for non-Azure IoT providers using the extensible IoT provider framework\n\nThis repo will help you set up and configure Connected Field Service with Azure IoT Hub. For more information on using other providers, please see our documentation page here: [Connected Field Service - Overview | Microsoft Docs](https://docs.microsoft.com/dynamics365/field-service/connected-field-service)\n\nConnected Field Service for Azure IoT Hub is an add-on solution that brings Azure IoT platform-as-a-service (PaaS) offering into Dynamics 365 for Field Service. With this offering, you can use this template and below instructions to put all the Azure IoT services and Dynamics puzzles together. All Azure IoT services run in your own Azure cloud subscription.\n\nThis deployment package will help you:\n- Deploy and configure an IoT Hub instance. Connected Field Services uses the IoT Hub to manage the state of registered devices and assets. In addition, the IoT Hub sends commands and notifications to connected devices\u2014and tracks message delivery with acknowledgement receipts.\n- Deploy a device simulation (optional). This is a test web app to emulate the device that is sending commands or receiving commands from the IoT Hub.\n- Deploy Time Series Insight (optional). Time Series Insights can be included in your deployment for detailed device insights and analytics.\n- Deploy PowerBI (optional). Microsoft Power BI for device analytics can be included in your deployment. Choosing this will deploy two additional resources, Azure Streaming Analytics and SQL Server database.\n\n## Deploy the ARM template\n\nBy deploying this template, you confirm that you\u2019ve read and agree to the [Terms of Service](https://github.com/microsoft/Dynamics-365-Connected-Field-Service-Deployment/blob/main/Terms_of_Service.md) and the [Microsoft Privacy Statement](https://privacy.microsoft.com/en-us/privacystatement)\n\n[![Deploy To Azure](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/deploytoazure.svg?sanitize=true)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FDynamics-365-Connected-Field-Service-Deployment%2Fmain%2FazureDeploy.json/createUIDefinitionUri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FDynamics-365-Connected-Field-Service-Deployment%2Fmain%2FcustomUi.json)\n\n**Note**: During deployment you'll be asked to provide your organization's unique name. You can find your organization's unique name by navigating to Advanced Settings on your Dynamics organization. Then navigate to Customizations > Developer Resources.\n\n## Set up IoT Hub for Connected Field Service\n\nAfter deploying Azure resource from the ARM template, follow the steps in our documentation to configure IoT settings: [Installation and setup - Connected Field Service for Azure IoT Hub](https://docs.microsoft.com/dynamics365/field-service/installation-setup-iothub)\n", "repo_name": "Dynamics-365-Connected-Field-Service-Deployment", "org_name": "microsoft", "org_repo": "microsoft/Dynamics-365-Connected-Field-Service-Deployment", "platform_org_repo": "github+microsoft/Dynamics-365-Connected-Field-Service-Deployment", "link_to_repo": "https://github.com/microsoft/Dynamics-365-Connected-Field-Service-Deployment", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Kiota JSON Serialization Library for PHP\n\n[![Build Status](https://travis-ci.org/microsoft/kiota-serialization-json-php.svg?branch=main)](https://travis-ci.org/microsoft/kiota-serialization-json-php)\n[![Latest Stable Version](https://poser.pugx.org/microsoft/kiota-serialization-json/version)](https://packagist.org/packages/microsoft/kiota-serialization-json)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota-serialization-json-php&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota-serialization-json-php)\n\nThe Json Serialization Library for PHP is the PHP JSON serialization library implementation.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a json serialization package to handle json payloads from an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Kiota JSON Serialization Library for PHP\n\nrun `composer require microsoft/kiota-serialization-json` or add the following to your `composer.json` file:\n\n```Shell\n{\n    \"require\": {\n        \"microsoft/kiota-serialization-json\": \"^0.1.0\"\n    }\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-json-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-json-php", "platform_org_repo": "github+microsoft/kiota-serialization-json-php", "link_to_repo": "https://github.com/microsoft/kiota-serialization-json-php", "platform": "github", "language": "PHP", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Kiota Abstractions Library for PHP\n\n[![Build Status](https://travis-ci.org/microsoft/kiota-abstractions-php.svg?branch=main)](https://travis-ci.org/microsoft/kiota-abstractions-php)\n[![Latest Stable Version](https://poser.pugx.org/microsoft/kiota-abstractions/version)](https://packagist.org/packages/microsoft/kiota-abstractions)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota-abstractions-php&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota-abstractions-php)\n\nThe Kiota abstractions Library for PHP is the PHP library defining the basic constructs Kiota projects need once an SDK has been generated from an OpenAPI definition.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to the abstraction package to build and run.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Abstractions Library\nrun `composer require microsoft/kiota-abstractions` or add the following to your `composer.json` file:\n\n```Shell\n{\n    \"require\": {\n        \"microsoft/kiota-abstractions\": \"^0.1.0\"\n    }\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-abstractions-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-abstractions-php", "platform_org_repo": "github+microsoft/kiota-abstractions-php", "link_to_repo": "https://github.com/microsoft/kiota-abstractions-php", "platform": "github", "language": "PHP", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Kiota HTTP Library for PHP\n\n[![Build Status](https://travis-ci.org/microsoft/kiota-http-guzzle-php.svg?branch=main)](https://travis-ci.org/microsoft/kiota-http-guzzle-php)\n[![Latest Stable Version](https://poser.pugx.org/microsoft/kiota-http-guzzle/version)](https://packagist.org/packages/microsoft/kiota-http-guzzle)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota-http-guzzle-php&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota-http-guzzle-php)\n\nThe Kiota HTTP Library for PHP is the PHP HTTP library implementation with [Guzzle](http://guzzlephp.org/).\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a HTTP package to make HTTP requests to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Kiota HTTP Library for PHP\n\nrun `composer require microsoft/kiota-http-guzzle` or add the following to your `composer.json` file:\n\n```Shell\n{\n    \"require\": {\n        \"microsoft/kiota-http-guzzle\": \"^0.1.0\"\n    }\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-http-guzzle-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-http-guzzle-php", "platform_org_repo": "github+microsoft/kiota-http-guzzle-php", "link_to_repo": "https://github.com/microsoft/kiota-http-guzzle-php", "platform": "github", "language": "PHP", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Kiota Authentication Provider Library for PHP\n\n![PHP](https://github.com/microsoft/kiota-authentication-phpleague-php/actions/workflows/pr-validation.yml/badge.svg)\n[![Latest Stable Version](https://poser.pugx.org/microsoft/kiota-authentication-phpleague/version)](https://packagist.org/packages/microsoft/kiota-authentication-phpleague)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota-authentication-phpleague-php&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota-authentication-phpleague-php)\n\nThe Kiota Authentication provider library for PHP uses the [PHP League OAuth 2.0 client](https://oauth2-client.thephpleague.com/) to authenticate against the Microsoft Identity platform.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a authentication provider library to authenticate HTTP requests to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n\n## Using the Kiota Authentication Provider Library for PHP\n\nrun `composer require microsoft/kiota-authentication-phpleague` or add the following to your `composer.json` file:\n\n```Shell\n{\n    \"require\": {\n        \"microsoft/kiota-authentication-phpleague\": \"^0.1.0\"\n    }\n}\n```\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-authentication-phpleague-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-authentication-phpleague-php", "platform_org_repo": "github+microsoft/kiota-authentication-phpleague-php", "link_to_repo": "https://github.com/microsoft/kiota-authentication-phpleague-php", "platform": "github", "language": "PHP", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "asset-bundle-stats", "org_name": "microsoft", "org_repo": "microsoft/asset-bundle-stats", "platform_org_repo": "github+microsoft/asset-bundle-stats", "link_to_repo": "https://github.com/microsoft/asset-bundle-stats", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Notes\n\nThis project utilizes existing code from: __\"Healthcare Provider Fraud Detection Analysis using Machine Learning\"__\n\nBuild a binary classification model based on the claims filed by the provider along with Inpatient data, Outpatient data, Beneficiary details to predict Healthcare Provider Fraud. Anik Manik. Feb 27, 2021. The orginal dataset used for this analysis is public domain and provided on Kaggle. \n\nhttps://medium.com/analytics-vidhya/healthcare-provider-fraud-detection-analysis-using-machine-learning-81ebf09ed955\n\nhttps://github.com/anikmanik04/healthcare-provider-fraud-detection\n\nhttps://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis\n\nThe goals of this project are to compare the current python machine learning methods provided by \"Anik Manik\", with those available in Azure Machine Learning (AML). Azure Machine Learning provides an integrated Automated ML system that utilizes integrated parallel processing across cluster compute. We hypothesize that AML's integrated assets (environments, experiments, pipelines, datasets, models, endpoints) integrated automated machine learning, VS Code (IDE) integration, CLI, notebooks, and drag and drop GUI designer provide a more robust development and deployment environment than traditional machine learning environments. \n\n\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureML-Healthcare-Insurance-fraud", "org_name": "microsoft", "org_repo": "microsoft/AzureML-Healthcare-Insurance-fraud", "platform_org_repo": "github+microsoft/AzureML-Healthcare-Insurance-fraud", "link_to_repo": "https://github.com/microsoft/AzureML-Healthcare-Insurance-fraud", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "- [1. What is Cloud Hypervisor?](#1-what-is-cloud-hypervisor)\n  - [Objectives](#objectives)\n    - [High Level](#high-level)\n    - [Architectures](#architectures)\n    - [Guest OS](#guest-os)\n- [2. Getting Started](#2-getting-started)\n  - [Preparation](#preparation)\n  - [Install prerequisites](#install-prerequisites)\n  - [Clone and build](#clone-and-build)\n    - [Containerized builds and tests](#containerized-builds-and-tests)\n  - [Run](#run)\n    - [Cloud image](#cloud-image)\n    - [Custom kernel and disk image](#custom-kernel-and-disk-image)\n      - [Building your kernel](#building-your-kernel)\n      - [Disk image](#disk-image)\n      - [Booting the guest VM](#booting-the-guest-vm)\n- [3. Status](#3-status)\n  - [Hot Plug](#hot-plug)\n  - [Device Model](#device-model)\n  - [TODO](#todo)\n- [4. `rust-vmm` project dependency](#4-rust-vmm-project-dependency)\n  - [Firecracker and crosvm](#firecracker-and-crosvm)\n- [5. Community](#5-community)\n  - [Contribute](#contribute)\n  - [Join us](#join-us)\n  - [Security issues](#security-issues)\n\n# 1. What is Cloud Hypervisor?\n\nCloud Hypervisor is an open source Virtual Machine Monitor (VMM) that runs on\ntop of [KVM](https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt)\nhypervisor and Microsoft Hypervisor (MSHV).\n\nThe project focuses on exclusively running modern, cloud workloads, on top of\na limited set of hardware architectures and platforms. Cloud workloads refers\nto those that are usually run by customers inside a cloud provider. For our\npurposes this means modern operating systems with most I/O handled by\nparavirtualised devices (i.e. virtio), no requirement for legacy devices, and\n64-bit CPUs.\n\nCloud Hypervisor is implemented in [Rust](https://www.rust-lang.org/) and is\nbased on the [rust-vmm](https://github.com/rust-vmm) crates.\n\n## Objectives\n\n### High Level\n\n- Runs on KVM or MSHV\n- Minimal emulation\n- Low latency\n- Low memory footprint\n- Low complexity\n- High performance\n- Small attack surface\n- 64-bit support only\n- CPU, memory, PCI hotplug\n- Machine to machine migration\n\n### Architectures\n\nCloud Hypervisor supports the `x86-64` and `AArch64` architectures. There are\nsome small differences in functionality between the two architectures\n(see [#1125](https://github.com/cloud-hypervisor/cloud-hypervisor/issues/1125)).\n\n### Guest OS\n\nCloud Hypervisor supports `64-bit Linux` and Windows 10/Windows Server 2019.\n\n# 2. Getting Started\n\nBelow sections describe how to build and run Cloud Hypervisor on the `x86_64`\nplatform. For getting started on the `AArch64` platform, please refer to the\n[Arm64 documentation](docs/arm64.md).\n\n## Preparation\n\nWe create a folder to build and run `cloud-hypervisor` at `$HOME/cloud-hypervisor`\n\n```shell\n$ export CLOUDH=$HOME/cloud-hypervisor\n$ mkdir $CLOUDH\n```\n\n## Install prerequisites\n\nYou need to install some prerequisite packages in order to build and test Cloud\nHypervisor. Here, all the steps are based on Ubuntu, for other Linux\ndistributions please replace the package manager and package name.\n\n```shell\n# Install build-essential, git, and qemu-img\n$ sudo apt install git build-essential qemu-img\n# Install rust tool chain\n$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n# If you want to build statically linked binary please add musl target\n$ rustup target add x86_64-unknown-linux-musl\n```\n\n## Clone and build\n\nFirst you need to clone and build the cloud-hypervisor repo:\n\n```shell\n$ pushd $CLOUDH\n$ git clone https://github.com/cloud-hypervisor/cloud-hypervisor.git\n$ cd cloud-hypervisor\n$ cargo build --release\n\n# We need to give the cloud-hypervisor binary the NET_ADMIN capabilities for it to set TAP interfaces up on the host.\n$ sudo setcap cap_net_admin+ep ./target/release/cloud-hypervisor\n\n# If you want to build statically linked binary\n$ cargo build --release --target=x86_64-unknown-linux-musl --all\n$ popd\n```\n\nThis will build a `cloud-hypervisor` binary under\n`$CLOUDH/cloud-hypervisor/target/release/cloud-hypervisor`.\n\n### Containerized builds and tests\n\nIf you want to build and test Cloud Hypervisor without having to install all the\nrequired dependencies (The rust toolchain, cargo tools, etc), you can also use\nCloud Hypervisor's development script: `dev_cli.sh`. Please note that upon its\nfirst invocation, this script will pull a fairly large container image.\n\nFor example, to build the Cloud Hypervisor release binary:\n\n```shell\n$ pushd $CLOUDH\n$ cd cloud-hypervisor\n$ ./scripts/dev_cli.sh build --release\n```\n\nWith `dev_cli.sh`, one can also run the Cloud Hypervisor CI locally. This can be\nvery convenient for debugging CI errors without having to fully rely on the\nCloud Hypervisor CI infrastructure.\n\nFor example, to run the Cloud Hypervisor unit tests:\n\n```shell\n$ ./scripts/dev_cli.sh tests --unit\n```\n\nRun the `./scripts/dev_cli.sh --help` command to view all the supported\ndevelopment script commands and their related options.\n\n## Run\n\nYou can run a guest VM by either using an existing cloud image or booting into\nyour own kernel and disk image.\n\n### Cloud image\n\nCloud Hypervisor supports booting disk images containing all needed\ncomponents to run cloud workloads, a.k.a. cloud images. To do that we rely on\nthe [Rust Hypervisor\nFirmware](https://github.com/cloud-hypervisor/rust-hypervisor-firmware) project\nto provide an ELF formatted KVM firmware for `cloud-hypervisor` to directly\nboot into.\n\nWe need to get the latest `rust-hypervisor-firmware` release and also a working\ncloud image. Here we will use a Ubuntu image:\n\n```shell\n$ pushd $CLOUDH\n$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img\n$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw\n$ wget https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/download/0.4.0/hypervisor-fw\n$ popd\n```\n\n```shell\n$ pushd $CLOUDH\n$ sudo setcap cap_net_admin+ep ./cloud-hypervisor/target/release/cloud-hypervisor\n$ ./cloud-hypervisor/target/release/cloud-hypervisor \\\n\t--kernel ./hypervisor-fw \\\n\t--disk path=focal-server-cloudimg-amd64.raw \\\n\t--cpus boot=4 \\\n\t--memory size=1024M \\\n\t--net \"tap=,mac=,ip=,mask=\"\n$ popd\n```\n\nMultiple arguments can be given to the `--disk` parameter.\n\n### Custom kernel and disk image\n\n#### Building your kernel\n\nCloud Hypervisor also supports direct kernel boot into a `vmlinux` ELF kernel.\nIn order to support virtio-watchdog we have our own development branch. You are\nof course able to use your own kernel but these instructions will continue with\nthe version that we develop and test against.\n\nTo build the kernel:\n\n```shell\n\n# Clone the Cloud Hypervisor Linux branch\n$ pushd $CLOUDH\n$ git clone --depth 1 https://github.com/cloud-hypervisor/linux.git -b ch-5.15.12 linux-cloud-hypervisor\n$ pushd linux-cloud-hypervisor\n\n# Use the cloud-hypervisor kernel config to build your kernel\n$ cp $CLOUDH/cloud-hypervisor/resources/linux-config-x86_64 .config\n$ KCFLAGS=\"-Wa,-mx86-used-note=no\" make bzImage -j `nproc`\n$ popd\n```\n\nThe `vmlinux` kernel image will then be located at\n`linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin`.\n\n#### Disk image\n\nFor the disk image, we will use a Ubuntu cloud image that contains a root\npartition:\n\n```shell\n$ pushd $CLOUDH\n$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img\n$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw\n$ popd\n```\n\n#### Booting the guest VM\n\nNow we can directly boot into our custom kernel and make it use the Ubuntu root\npartition. If we want to have 4 vCPUs and 1024 MBytes of memory:\n\n```shell\n$ pushd $CLOUDH\n$ sudo setcap cap_net_admin+ep ./cloud-hypervisor/target/release/cloud-hypervisor\n$ ./cloud-hypervisor/target/release/cloud-hypervisor \\\n\t--kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\\n\t--disk path=focal-server-cloudimg-amd64.raw \\\n\t--cmdline \"console=hvc0 root=/dev/vda1 rw\" \\\n\t--cpus boot=4 \\\n\t--memory size=1024M \\\n\t--net \"tap=,mac=,ip=,mask=\"\n```\n\nThe above example use the `virtio-console` device as the guest console, and this\ndevice may not be enabled soon enough by the guest kernel to get early kernel\ndebug messages.\n\nWhen in need for earlier debug messages, using the legacy serial device based\nconsole is preferred:\n\n```\n$ ./cloud-hypervisor/target/release/cloud-hypervisor \\\n\t--kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \\\n\t--console off \\\n\t--serial tty \\\n\t--disk path=focal-server-cloudimg-amd64.raw \\\n\t--cmdline \"console=ttyS0 root=/dev/vda1 rw\" \\\n\t--cpus boot=4 \\\n\t--memory size=1024M \\\n\t--net \"tap=,mac=,ip=,mask=\"\n```\n\n# 3. Status\n\nCloud Hypervisor is under active development. The following stability guarantees\nare currently made:\n\n* The API (including command line options) will not be removed or changed in a\n  breaking way without a minimum of 2 major releases notice. Where possible\n  warnings will be given about the use of deprecated functionality and the\n  deprecations will be documented in the release notes.\n\n* Point releases will be made between individual releases where there are\n  substantial bug fixes or security issues that need to be fixed. These point\n  releases will only include bug fixes.\n\nCurrently the following items are **not** guaranteed across updates:\n\n* Snapshot/restore is not supported across different versions\n* Live migration is not supported across different versions\n* The following features are considered experimental and may change\n  substantially between releases: TDX, vfio-user, vDPA.\n\nAs of 2022-04-05, the following cloud images are supported:\n\n- [Ubuntu Bionic](https://cloud-images.ubuntu.com/bionic/current/) (cloudimg)\n- [Ubuntu Focal](https://cloud-images.ubuntu.com/focal/current/) (cloudimg)\n- [Ubuntu Jammy](https://cloud-images.ubuntu.com/jammy/current/) (cloudimg)\n\nDirect kernel boot to userspace should work with a rootfs from most\ndistributions.\n\nFurther details can be found in the [release documentation](docs/releases.md).\n\n## Hot Plug\n\nCloud Hypervisor supports hotplug of CPUs, passthrough devices (VFIO),\n`virtio-{net,block,pmem,fs,vsock}` and memory resizing. This\n[document](docs/hotplug.md) details how to add devices to a running VM.\n\n## Device Model\n\nDetails of the device model can be found in this\n[documentation](docs/device_model.md).\n\n## TODO\n\nWe are not tracking the Cloud Hypervisor TODO list from a specific git tracked\nfile but through\n[github issues](https://github.com/cloud-hypervisor/cloud-hypervisor/issues/new)\ninstead.\n\n# 4. `rust-vmm` project dependency\n\nIn order to satisfy the design goal of having a high-performance,\nsecurity-focused hypervisor the decision was made to use the\n[Rust](https://www.rust-lang.org/) programming language. The language's strong\nfocus on memory and thread safety makes it an ideal candidate for implementing\nVMMs.\n\nInstead of implementing the VMM components from scratch, Cloud Hypervisor is\nimporting the [rust-vmm](https://github.com/rust-vmm) crates, and sharing code\nand architecture together with other VMMs like e.g. Amazon's\n[Firecracker](https://firecracker-microvm.github.io/) and Google's\n[crosvm](https://chromium.googlesource.com/chromiumos/platform/crosvm/).\n\nCloud Hypervisor embraces the rust-vmm project goals, which is to be able to\nshare and re-use as many virtualization crates as possible. As such, the Cloud\nHypervisor relationship with the rust-vmm project is twofold:\n\n1. It will use as much of the rust-vmm code as possible. Any new rust-vmm crate\n   that's relevant to the project goals will be integrated as soon as possible.\n2. As it is likely that the rust-vmm project will lack some of the features that\n   Cloud Hypervisor needs (e.g. ACPI, VFIO, vhost-user, etc), we will be using\n   the Cloud Hypervisor VMM to implement and test them, and contribute them back\n   to the rust-vmm project.\n\n## Firecracker and crosvm\n\nA large part of the Cloud Hypervisor code is based on either the Firecracker or\nthe crosvm projects implementations. Both of these are VMMs written in Rust with\na focus on safety and security, like Cloud Hypervisor.\n\nHowever we want to emphasize that the Cloud Hypervisor project is neither a fork\nnor a reimplementation of any of those projects. The goals and use cases we're\ntrying to meet are different. We're aiming at supporting cloud workloads, i.e.\nthose modern, full Linux distribution images currently being run by Cloud\nService Provider (CSP) tenants.\n\nOur primary target is not to support client or serverless use cases, and as such\nour code base already diverges from the crosvm and Firecracker ones. As we add\nmore features to support our use cases, we believe that the divergence will\nincrease while at the same time sharing as much of the fundamental\nvirtualization code through the rust-vmm project crates as possible.\n\n# 5. Community\n\nThe Cloud Hypervisor project follows the governance, and community guidelines\ndescribed in the [Community](https://github.com/cloud-hypervisor/community)\nrepository.\n\n## Contribute\n\nWe are working on building a global, diverse and collaborative community around\nthe Cloud Hypervisor project. Anyone who is interested in\n[contributing](CONTRIBUTING.md) to the project is welcome to participate.\n\nWe believe that contributing to a open source project like Cloud Hypervisor\ncovers a lot more than just sending code. Testing, documentation, pull request\nreviews, bug reports, feature requests, project improvement suggestions, etc,\nare all equal and welcome means of contribution. See the\n[CONTRIBUTING](CONTRIBUTING.md) document for more details.\n\n## Join us\n\nGet an [invite to our Slack channel](https://join.slack.com/t/cloud-hypervisor/shared_invite/enQtNjY3MTE3MDkwNDQ4LWQ1MTA1ZDVmODkwMWQ1MTRhYzk4ZGNlN2UwNTI3ZmFlODU0OTcwOWZjMTkwZDExYWE3YjFmNzgzY2FmNDAyMjI)\nand [join us on Slack](https://cloud-hypervisor.slack.com/).\n\n## Security issues\n\nPlease contact the maintainers listed in the MAINTAINERS.md file with security issues.\n", "repo_name": "cloud-hypervisor-craton", "org_name": "microsoft", "org_repo": "microsoft/cloud-hypervisor-craton", "platform_org_repo": "github+microsoft/cloud-hypervisor-craton", "link_to_repo": "https://github.com/microsoft/cloud-hypervisor-craton", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# XNAT on Azure\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FXNAT-on-Azure%2Fmain%2Fsrc%2Farm%2Fxnat.ui.json)\n\nThe goal of this repository is to provide code, templates and best practices for deploying [XNAT](https://xnat.org/about/) on Azure. \n\n\n# Overview\n![xnat overview](./images/XNAT%20Diagrams.jpg)\n\n\nThe repository is organized in `src` and `docs` directory. The `src` directory contains code for building the docker container and ARM templates. The `docs` directory contains the following:\n\n1. [Loading Images](./docs/1_Loading_Images/README.md)\n2. [Setting up a project](./docs/2_Setting_up_project/README.md)\n3. [Programmatic access to data](./docs/3_Programmatic_Access/README.md)\n4. [Training ML Model on Azure ML](./docs/4_Training_Model_On_Azure_ML/README.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "XNAT-on-Azure", "org_name": "microsoft", "org_repo": "microsoft/XNAT-on-Azure", "platform_org_repo": "github+microsoft/XNAT-on-Azure", "link_to_repo": "https://github.com/microsoft/XNAT-on-Azure", "platform": "github", "language": "Shell", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Power Platform Dev Day\n\nThis repo contains all the information for Power Platform Dev Day labs.\n\n<!-- You can access the presentations and content [here](https://aka.ms/azuredevdaycontent). -->\n\n## Prerequisites\n\nWe encourage you to follow along the hands-on labs during lab sessions.\n\n* If you don't have an Azure Subscription to use for these labs, please create a free subscription at https://azure.microsoft.com/free/.\n* Request a power platform developer license [here](https://go.microsoft.com/fwlink/?LinkId=2180357&clcid=0x409).\n    * If that doesn't work, please try  [Microsoft 365 Developer Program](https://docs.microsoft.com/en-us/office/developer-program/microsoft-365-developer-program#join-the-microsoft-365-developer-program).\n\n## Labs\n\n1. [Power App](power-app/README.md)\n2. [Power Automate and Dataverse](power-automate/README.md)\n3. [Custom Connectors](custom-connector/README.md)\n\n\n\n\n\n\n", "repo_name": "powerplatform-dev-day", "org_name": "microsoft", "org_repo": "microsoft/powerplatform-dev-day", "platform_org_repo": "github+microsoft/powerplatform-dev-day", "link_to_repo": "https://github.com/microsoft/powerplatform-dev-day", "platform": "github", "language": "Bicep", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# FailureAnalysis Core\n[![FailureAnalysis Core Build](https://github.com/microsoft/mixedreality.failureanalysis.core/actions/workflows/dotnet.yml/badge.svg?branch=main)](https://github.com/microsoft/mixedreality.failureanalysis.core/actions/workflows/dotnet.yml?query=branch%3Amain)\n[![The Standard - COMPLIANT](https://img.shields.io/badge/The_Standard-COMPLIANT-2ea44f)](https://github.com/hassanhabib/The-Standard)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mixedreality.failureanalysis.core", "org_name": "microsoft", "org_repo": "microsoft/mixedreality.failureanalysis.core", "platform_org_repo": "github+microsoft/mixedreality.failureanalysis.core", "link_to_repo": "https://github.com/microsoft/mixedreality.failureanalysis.core", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "-azure-data-services-go-fast-codebase-es-dmz", "org_name": "microsoft", "org_repo": "microsoft/-azure-data-services-go-fast-codebase-es-dmz", "platform_org_repo": "github+microsoft/-azure-data-services-go-fast-codebase-es-dmz", "link_to_repo": "https://github.com/microsoft/-azure-data-services-go-fast-codebase-es-dmz", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# BridgeTower\n\nThis repo is the official `Pytorch` implementation of [\"BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning\"](https://arxiv.org/abs/2206.08657).\n\n## Updates\n\n- Feb. 2023: BridgeTower was integrated into [Hugging Face - Transformers](https://github.com/huggingface/transformers/). \n  - [Model Hub](https://huggingface.co/BridgeTower), [Code](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bridgetower) and [Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/bridgetower) are available. \n  - Thanks to [Anahita Bhiwandiwalla](https://github.com/abhiwand), [Tiep Le](https://github.com/tileintel) and [Shaoyen Tseng](https://github.com/shaoyent-IL) from [Intel Labs](https://github.com/IntelLabs) for their great work! \n- Nov. 2022: BridgeTower got accepted by [AAAI'23](https://aaai.org/Conferences/AAAI-23/). Code and checkpoints are released.\n- Jun. 2022: We released the preprint version in [Arxiv](https://arxiv.org/abs/2206.08657).\n- May. 2022: BridgeTower (single model, 4M data) achieved 78.73% and 81.15% (base and large) on the [VQAv2 Challenge](https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278) test-std set.\n\n## Abstract\n\nVision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance on various downstream vision-language tasks. In particular, on the VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming the previous state-of-the-art model METER by 1.09% with the same pre-training data and almost negligible additional parameters and computational costs. Notably, when further scaling the model, BridgeTower achieves an accuracy of 81.15%, surpassing models that are pre-trained on orders-of-magnitude larger datasets.\n\n## Architecture\n\n![Architecture](images/framework.jpg)\n\n## Main Results\n\n![Result1](images/result1.jpg)\n\n![Result2](images/result2.jpg)\n\n## Deployment\n\n- Run `setup.sh` to set up the environment.\n- [Optional] We use [wandb](https://wandb.ai/) to track experiments! Please remember to `wandb login` and paste your token before running the script.\n\n## Dataset Preparation\n\n- We follow [ViLT](https://github.com/dandelin/ViLT) and use pyarrow to serialize the datasets. See [here](https://github.com/dandelin/ViLT/blob/master/DATA.md) for details.\n- For SNLI-VE dataset, we follow [here](https://github.com/necla-ml/SNLI-VE).\n- For VG-QA dataset, except the image-text pairs in [VG](https://visualgenome.org/api/v0/api_home.html) got from [here](https://github.com/dandelin/ViLT/blob/master/DATA.md), [image meta data](https://visualgenome.org/static/data/dataset/image_data_v1.json.zip), [question answers data](https://visualgenome.org/static/data/dataset/question_answers.json.zip) and [coco split information](https://github.com/peteanderson80/bottom-up-attention/tree/master/data/genome/coco_splits) also need to be downloaded.\n- The final file structure of datasets are shown in `setup.sh`.\n\n## Checkpoints\n\n- Pre-trained checkpoints on 4M data: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_pt_base.ckpt?sv=2020-10-02&st=2022-11-24T12%3A18%3A49Z&se=2027-11-25T12%3A18%3A00Z&sr=b&sp=r&sig=BJigddAMHfNUtQuTGH8bJUrzAO3LfaeSm48AXUqZngY%3D) and [LARGE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_pt_large.ckpt?sv=2020-10-02&st=2022-11-24T12%3A19%3A19Z&se=2027-11-25T12%3A19%3A00Z&sr=b&sp=r&sig=8yWqesQACrJSi0JMLIA0uAbNlMQKb653gOXjXjQuIW4%3D)\n- Fine-tuned checkpoints for\n  - Visual Question Answering on VQAv2: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_vqav2.ckpt?sv=2020-10-02&st=2022-11-24T12%3A16%3A38Z&se=2027-11-25T12%3A16%3A00Z&sr=b&sp=r&sig=t35v4kezDcSOm9Q9E767PhNGAQRsiYm%2FMSDgHIz%2Fvto%3D), [BASE(w/ VGQA)](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_vqav2_vgqa.ckpt?sv=2020-10-02&st=2022-11-24T12%3A17%3A18Z&se=2027-11-25T12%3A17%3A00Z&sr=b&sp=r&sig=BD%2BOsI%2F6R905vBJUlrWlgx3%2BmaBRsa2rQcHBChhW0eE%3D), [LARGE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_large_vqav2.ckpt?sv=2020-10-02&st=2022-11-24T12%3A17%3A47Z&se=2027-11-25T12%3A17%3A00Z&sr=b&sp=r&sig=RqL7Eeye4385oaO1nvVvRwC4d%2ByhpEVGM3xmS4GcKkQ%3D), [LARGE(w/ VGQA)](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_large_vqav2_vgqa.ckpt?sv=2020-10-02&st=2022-11-24T12%3A18%3A29Z&se=2027-11-25T12%3A18%3A00Z&sr=b&sp=r&sig=xtI8rmEqjMmN1b1bcE0KB9ePUax3SuRfOt%2Bp2ATH9ng%3D)\n  - Image-Text Retrieval on Flickr30k: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_irtr_itm_itc_f30k.ckpt?sv=2020-10-02&st=2022-11-24T12%3A13%3A42Z&se=2027-11-25T12%3A13%3A00Z&sr=b&sp=r&sig=0BP3pOiE4AFkK4BTgQl5Dy6iJWxHuJffpjU4LFMTfWY%3D)\n  - Visual Entailment on SNLI-VE: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_snlive.ckpt?sv=2020-10-02&st=2022-11-24T12%3A15%3A27Z&se=2027-11-25T12%3A15%3A00Z&sr=b&sp=r&sig=IccPmnxQYIpWO8m6kwtEFir9wmVq1SsLOqmw0FRc9hY%3D)\n  - Visual Reasoning on NLVR$^2$: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_nlvr2.ckpt?sv=2020-10-02&st=2022-11-24T12%3A15%3A09Z&se=2027-11-25T12%3A15%3A00Z&sr=b&sp=r&sig=AL3q15eyhPBHaWY0FOop9goHVq8CbNluABDk%2FS94rkI%3D)\n  - Image-Text Retrieval on MSCOCO: [BASE](https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_ftfpt_base_irtr_itm_itc_coco.ckpt?sv=2020-10-02&st=2022-11-24T12%3A13%3A18Z&se=2027-11-25T12%3A13%3A00Z&sr=b&sp=r&sig=ahM%2FyI8fg9D4obCZsNKaxLzPVz2y8RX8ydZNToGavC4%3D)\n- Here is an example for downloading a checkpoint.\n\n  ```Shell\n  # download azcopy\n  wget https://aka.ms/downloadazcopy-v10-linux\n  tar -xvf downloadazcopy-v10-linux\n  sudo cp ./azcopy_linux_amd64_*/azcopy /usr/bin/\n  sudo chmod -R 777 /usr/bin/azcopy\n  # azcopy copy [remote path] [local path]\n  azcopy copy \"https://chenfei.blob.core.windows.net/data/G/LCI/best_checkpoints/BridgeTower_pt_base.ckpt?sv=2020-10-02&st=2022-11-24T12%3A18%3A49Z&se=2027-11-25T12%3A18%3A00Z&sr=b&sp=r&sig=BJigddAMHfNUtQuTGH8bJUrzAO3LfaeSm48AXUqZngY%3D\" \"./BridgeTower_pt_base.ckpt\"\n  ```\n\n## Pre-training on Image-Text Datasets\n\n```bash\n# Pre-train BridgeTower Base Model\nbash scripts/pre_train.sh\n# Pre-train BridgeTower Large Model\nbash scripts/pre_train_large.sh\n```\n\n## Fine-tuning on Downstream VL Tasks\n\n- VQAv2 Evaluation needs to submit the `json` file in the `logs/` directory to [eval.ai](https://eval.ai/web/challenges/challenge-page/830/overview) evaluation server to get the test-dev and/or test-std scores.\n\n```bash\n# Base Model on VQAv2 without VLP\nbash scripts/ftfs_base_vqa.sh\n\n# Large Model on VQAv2 without VLP\nbash scripts/ftfs_large_vqa.sh\n\n# Base Model on VQAv2 with VLP\nbash scripts/ftfpt_base_vqa.sh\n\n# Large Model on VQAv2 with VLP\nbash scripts/ftfpt_large_vqa.sh\n\n# Base Model on IRTR-Flickr30K with VLP (directly use ITM with multiple false texts)\nbash scripts/ftfpt_base_irtr_f30k.sh\n\n# Base Model on IRTR-Flickr30K with VLP (follow ALBEF to use ITC to sample hard negatives for ITM)\nbash scripts/ftfpt_base_irtr_itm_itc_f30k.sh\n\n# Base Model on SNLI-VE with VLP\nbash scripts/ftfpt_base_snlive.sh\n\n# Base Model on NLVR^2 with VLP\nbash scripts/ftfpt_base_nlvr2.sh\n\n# Base Model on IRTR-MSCOCO with VLP (follow ALBEF to use ITC to sample hard negatives for ITM)\nbash scripts/ftfpt_base_irtr_itm_itc_coco.sh\n\n```\n\n## Fine-tuning on Uni-Modal Tasks\n\n```bash\n# Base Model on CIFAR with VLP\nbash scripts/ftfpt_base_cifar.sh\n\n# Base Model on GLUE with VLP\nbash scripts/ftfpt_base_glue.sh\n```\n\n## Citation\n\n```\n@article{xu2022bridge,\n  title={BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},\n  author={Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},\n  journal={arXiv preprint arXiv:2206.08657},\n  year={2022}\n}\n```\n\n## Acknowledgement\n\nWe are highly grateful for the public code of the following papers, our code is partly based on them:\n\n- Main Code: [METER](https://github.com/zdou0830/METER), [ViLT](https://github.com/dandelin/ViLT)\n- Others: [CLIP](https://github.com/openai/CLIP), [ALBEF](https://github.com/salesforce/ALBEF), [BLIP](https://github.com/salesforce/BLIP)\n", "repo_name": "BridgeTower", "org_name": "microsoft", "org_repo": "microsoft/BridgeTower", "platform_org_repo": "github+microsoft/BridgeTower", "link_to_repo": "https://github.com/microsoft/BridgeTower", "platform": "github", "language": "Python", "stargazers_count": 102, "watchers_count": 102}, {"README_text": "# Dan's Microsoft Defender XDR + Sentinel + Compliance Tools\n\nHi, I'm Dan! I am a Microsoft employee, but Microsoft does not provide support or warranty for any of these scripts/tools.\n\n## Contributing\n\nThis project welcomes suggestions! Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dchemistruck", "org_name": "microsoft", "org_repo": "microsoft/dchemistruck", "platform_org_repo": "github+microsoft/dchemistruck", "link_to_repo": "https://github.com/microsoft/dchemistruck", "platform": "github", "language": "PowerShell", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Teams Meeting Scheduler Website\n\nThe code published here can be used to deploy an Azure Web App that can allow users of your Microsoft 365 tenant to schedule Teams meetings using a simple web interface. \nThis could be very useful when your mail system is different from Exchange / Exchange Hybrid deployment and/or your email client calendar is not integrated.\n\n\n\n## Setup & Prerequisites\nUsing the steps below you will be able to create all prerequisites and resources required by the solution.\n\n\n### Register the required application for Graph API usage\n\t1. Open a browser and navigate to the Azure Active Directory admin center. Login using a personal account (aka: Microsoft Account) or Work or School Account.\n\n\t2. Select Azure Active Directory in the left-hand navigation, then select App registrations under Manage.\n\n\t3. Select New registration. On the Register an application page, set the values as follows.\n\t- Set Name to name of your app.\n\t- Set Supported account types to Accounts in any organizational directory and personal Microsoft accounts.\n\t- Under Redirect URI, set the first drop-down to Web and set the value to https://localhost:5001/.\n\n\t4. Select Register. On the web page, copy the value of the Application (client) ID and save it, you will need it in the next step.\n\n\t5. Select Authentication under Manage. Under Redirect URIs add a URI with the value https://your_application_URL.TLD/signin-oidc.\n\n\t6. Set the Logout URL to https://your_application_URL.TLD/signout-oidc.\n\n\t7. Locate the Implicit grant section and enable ID tokens. Select Save.\n\n\t8. Select Certificates & secrets under Manage. Select the New client secret button. Enter a value in Description and select one of the options for Expires and select Add.\n\n\t9. Copy the client secret value before you leave this page. You will need it in the next step.\n\n\t> IMPORTANT: This client secret is never shown again, so make sure you copy it now.\n\n\n\n### Authorize app access to Azure Active Directory\n\n\t1. From the created app, select from left menu of the app created API Authorization.\n\t2. Add Graph Authorization: OnlineMeetings.ReadWrite and Calendars.ReadWrite\n\t3. Provide Admin consent for the authorizations\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "TeamsMeetingsWebScheduler", "org_name": "microsoft", "org_repo": "microsoft/TeamsMeetingsWebScheduler", "platform_org_repo": "github+microsoft/TeamsMeetingsWebScheduler", "link_to_repo": "https://github.com/microsoft/TeamsMeetingsWebScheduler", "platform": "github", "language": "C#", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Shapeshifter\n\nA way to dynamically render controls in Power Apps\n\n![](ShapeshifterDemo.gif)\n\nThe inspiration for this control is that there is no way to do it in a Canvas App without toggling the show/hide of all controls which is ugly and hard to manage. It could be used to shrink the amount of static coding in a MDA approach too. I have the ambition to bring this to the world of SharePoint and Teams development also.\n\n## Implemented Controls\n* Fluent UI TextField\n* Fluent UI DatePicker\n* Fluent UI Slider\n* Fluent UI Dropdown\n* Fluent UI ChoiceGroup\n* Fluent UI SpinButton\n* Fluent UI StarRating\n\n## How to use\nImport the solution from our [releases area](https://github.com/brendon-colburn/shapeshifter/releases).  It has a sample Canvas App to help you understand the concept.\nTo work on the solution clone or fork this repo and build it out in VSCode. Upon importing this solution, you can either copy the included Cavas App as a template or you can import the Shapeshifter control into a new Canvas App.\n\n## Want to contribute?\nHelp implement controls or establish more elegant approaches to the shapeshifter concept\n", "repo_name": "shapeshifter", "org_name": "microsoft", "org_repo": "microsoft/shapeshifter", "platform_org_repo": "github+microsoft/shapeshifter", "link_to_repo": "https://github.com/microsoft/shapeshifter", "platform": "github", "language": "TypeScript", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "# Prompt Engine\n\nThis repo contains an NPM utility library for creating and maintaining prompts for Large Language Models (LLMs).\n\n## Background\n\nLLMs like GPT-3 and Codex have continued to push the bounds of what AI is capable of - they can capably generate language and code, but are also capable of emergent behavior like question answering, summarization, classification and dialog. One of the best techniques for enabling specific behavior out of LLMs is called prompt engineering - crafting inputs that coax the model to produce certain kinds of outputs. Few-shot prompting is the discipline of giving examples of inputs and outputs, such that the model has a reference for the type of output you're looking for.\n\nPrompt engineering can be as simple as formatting a question and passing it to the model, but it can also get quite complex - requiring substantial code to manipulate and update strings. This library aims to make that easier. It also aims to codify patterns and practices around prompt engineering.\n\nSee [How to get Codex to produce the code you want](https://microsoft.github.io/prompt-engineering/) article for an example of the prompt engineering patterns this library codifies. \n\n## Installation\n\n`npm install prompt-engine`\n\n## Usage\n\nThe library currently supports a generic `PromptEngine`, a `CodeEngine` and a `ChatEngine`. All three facilitate a pattern of prompt engineering where the prompt is composed of a description, examples of inputs and outputs and an ongoing \"dialog\" representing the ongoing input/output pairs as the user and model communicate. The dialog ensures that the model (which is stateless) has the context about what's happened in the conversation so far.\n\nSee architecture diagram representation:\n  \n<img src=\"https://user-images.githubusercontent.com/17247257/178334939-65e0e3ce-39b3-4abc-a889-7f2c0fb75f60.png\" width=\"500\">\n\n\n### Code Engine\n\nCode Engine creates prompts for Natural Language to Code scenarios. See TypeScript Syntax for importing `CodeEngine`:\n\n```js\nimport { CodeEngine } from \"prompt-engine\";\n```\n\nNL->Code prompts should generally have a description, which should give context about the programming language the model should generate and libraries it should be using. The description should also give information about the task at hand:\n\n```js\nconst description =\n  \"Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\";\n```\n\nNL->Code prompts should also have examples of NL->Code interactions, exemplifying the kind of code you expect the model to produce. In this case, the inputs are math queries (e.g. \"what is 2 + 2?\") and code that console logs the result of the query.\n\n```js\nconst examples = [\n  { input: \"what's 10 plus 18\", response: \"console.log(10 + 18)\" },\n  { input: \"what's 10 times 18\", response: \"console.log(10 * 18)\" },\n];\n```\n\nBy default, `CodeEngine` uses JavaScript as the programming language, but you can create prompts for different languages by passing a different `CodePromptConfig` into the constructor. If, for example, we wanted to produce Python prompts, we could have passed `CodeEngine` a `pythonConfig` specifying the comment operator it should be using:\n\n```js\nconst pythonConfig = {\n  commentOperator: \"#\",\n}\nconst codeEngine = new CodeEngine(description, examples, flowResetText, pythonConfig);\n\n```\n\nWith our description and our examples, we can go ahead and create our `CodeEngine`:\n\n```js\nconst codeEngine = new CodeEngine(description, examples);\n```\n\nNow that we have our `CodeEngine`, we can use it to create prompts:\n\n```js\nconst query = \"What's 1018 times the ninth power of four?\";\nconst prompt = codeEngine.buildPrompt(query);\n```\n\nThe resulting prompt will be a string with the description, examples and the latest query formatted with comment operators and line breaks:\n\n```js\n/* Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. */\n\n/* what's 10 plus 18 */\nconsole.log(10 + 18);\n\n/* what's 10 times 18 */\nconsole.log(10 * 18);\n\n/* What's 1018 times the ninth power of four? */\n```\n\nGiven the context, a capable code generation model can take the above prompt and guess the next line: `console.log(1018 * Math.pow(4, 9));`.\n\nFor multi-turn scenarios, where past conversations influences the next turn, Code Engine enables us to persist interactions in a prompt:\n\n```js\n...\n// Assumes existence of code generation model\nlet code = model.generateCode(prompt);\n\n// Adds interaction\ncodeEngine.addInteraction(query, code);\n```\n\nNow new prompts will include the latest NL->Code interaction:\n\n```js\ncodeEngine.buildPrompt(\"How about the 8th power?\");\n```\n\nProduces a prompt identical to the one above, but with the NL->Code dialog history:\n\n```js\n...\n/* What's 1018 times the ninth power of four? */\nconsole.log(1018 * Math.pow(4, 9));\n\n/* How about the 8th power? */\n```\n\nWith this context, the code generation model has the dialog context needed to understand what we mean by the query. In this case, the model would correctly generate `console.log(1018 * Math.pow(4, 8));`.\n\n### Chat Engine\n\nJust like Code Engine, Chat Engine creates prompts with descriptions and examples. The difference is that Chat Engine creates prompts for dialog scenarios, where both the user and the model use natural language. The `ChatEngine` constructor takes an optional `chatConfig` argument, which allows you to define the name of a user and chatbot in a multi-turn dialog: \n\n```js\nconst chatEngineConfig = {\n  user: \"Ryan\",\n  bot: \"Gordon\"\n};\n```\n\nChat prompts also benefit from a description that gives context. This description helps the model determine how the bot should respond. \n\n```js\nconst description = \"A conversation with Gordon the Anxious Robot. Gordon tends to reply nervously and asks a lot of follow-up questions.\";\n```\n\nSimilarly, Chat Engine prompts can have examples interactions: \n\n```js\nconst examples = [\n  { input: \"Who made you?\", response: \"I don't know man! That's an awfully existential question. How would you answer it?\" },\n  { input: \"Good point - do you at least know what you were made for?\", response: \"I'm OK at riveting, but that's not how I should answer a meaning of life question is it?\"}\n];\n```\n\nThese examples help set the tone of the bot, in this case Gordon the Anxious Robot. Now we can create our `ChatEngine` and use it to create prompts:\n\n```js\nconst chatEngine = new ChatEngine(description, examples, flowResetText, chatEngineConfig);\nconst userQuery = \"What are you made of?\";\nconst prompt = chatEngine.buildPrompt(userQuery);\n```\n\nWhen passed to a large language model (e.g. GPT-3), the context of the above prompt will help coax a good answer from the model, like \"Subatomic particles at some level, but somehow I don't think that's what you were asking.\". As with Code Engine, we can persist this answer and continue the dialog such that the model is aware of the conversation context: \n\n```js\nchatEngine.addInteraction(userQuery, \"Subatomic particles at some level, but somehow I don't think that's what you were asking.\");\n```\n\n## Managing Prompt Overflow\n\nPrompts for Large Language Models generally have limited size, depending on the language model being used. Given that prompt-engine can persist dialog history, it is possible for dialogs to get so long that the prompt overflows. The Prompt Engine pattern handles this situation by removing the oldest dialog interaction from the prompt, effectively only remembering the most recent interactions.\n\nYou can specify the maximum tokens allowed in your prompt by passing a `maxTokens` parameter when constructing the config for any prompt engine:\n\n```js\nlet promptEngine = new PromptEngine(description, examples, flowResetText, {\n  modelConfig: { maxTokens: 1000 }\n});\n```\n\n## Available Functions\n\nThe following are the functions available on the `PromptEngine` class and those that inherit from it:\n\n| Command | Parameters | Description | Returns |\n|--|--|--|--|\n| `buildContext` | None | Constructs and return the context with parameters provided to the Prompt Engine | Context: string |\n| `buildPrompt` | Prompt: string | Combines the context from `buildContext` with a query to create a prompt | Prompt: string |\n| `buildDialog` | None | Builds a dialog based on all the past interactions added to the Prompt Engine | Dialog: string |\n| `addExample` | interaction: Interaction(input: string, response: string) | Adds the given example to the examples | None |\n| `addInteraction` | interaction: Interaction(input: string, response: string) | Adds the given interaction to the dialog | None |\n| `removeFirstInteraction` | None | Removes and returns the first interaction in the dialog | Interaction: string |\n| `removeLastInteraction` | None | Removes and returns the last interaction added to the dialog | Interaction: string |\n| `resetContext` | None | Removes all interactions from the dialog, returning the reset context | Context:string |\n\nFor more examples and insights into using the prompt-engine library, have a look at the [examples](https://github.com/microsoft/prompt-engine/tree/main/examples) folder\n\n## YAML Representation\nIt can be useful to represent prompts as standalone files, versus code. This can allow easy swapping between different prompts, prompt versioning, and other advanced capabiliites. With this in mind, prompt-engine offers a way to represent prompts as YAML and to load that YAML into a prompt-engine class. See `examples/yaml-examples` for examples of YAML prompts and how they're loaded into prompt-engine.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Statement of Purpose\n\nThis library aims to simplify use of Large Language Models, and to make it easy for developers to take advantage of existing patterns. The package is released in conjunction with the [Build 2022 AI examples](https://github.com/microsoft/Build2022-AI-examples), as the first three use a multi-turn LLM pattern that this library simplifies. This package works independently of any specific LLM - prompt generated by the package should be useable with various language and code generating models.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "prompt-engine", "org_name": "microsoft", "org_repo": "microsoft/prompt-engine", "platform_org_repo": "github+microsoft/prompt-engine", "link_to_repo": "https://github.com/microsoft/prompt-engine", "platform": "github", "language": "TypeScript", "stargazers_count": 2068, "watchers_count": 2068}, {"README_text": "# PerformanceFunctionAnalysis\n\n<h3 align=\"center\"> On the Economics of Multilingual Few-shot Learning: Modeling the\nCost-Performance Trade-offs of Machine Translated and Manual Data </h3>\n\n<h4 align=\"center\"> Kabir Ahuja, Monojit Choudhury, Sandipan Dandapat </h4>\n\n<p align=\"center\">\n  <a href=\"https://aclanthology.org/2022.naacl-main.98/\"><img src=\"https://img.shields.io/badge/NAACL%20-2022-blue\"></a>\n  <a href=\"https://arxiv.org/abs/2009.11264\"><img src=\"http://img.shields.io/badge/Paper-PDF-red.svg\"></a>\n  <a href=\"https://github.com/microsoft/PerformanceFunctionAnalysis/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/License-MIT-green\">\n  </a>\n</p>\nBorrowing ideas from *Production functions* in micro-economics, in this paper we introduce a framework \nto systematically evaluate the performance and cost trade-offs between machine-translated and manually-created labelled data for task-specific fine-tuning of massively multilingual language models. We illustrate the effectiveness of our framework through a case-study on the TyDIQA-GoldP dataset. One of the interesting conclusions of the study is that if the cost of machine translation is greater than zero, the optimal performance at least cost is always achieved with at least some or only manually-created data. To our knowledge, this is the first attempt towards extending the concept of production functions to study data collection strategies for training multilingual models, and can serve as a valuable tool for other similar cost vs data trade-offs in NLP.\n\n<h2 align=\"center\">\n  <img align=\"center\"  src=\"main_figure.png\" alt=\"...\" width=\"720\">\n</h2>\n\n#### Dependencies\n- Compatible with Python3.7\n- The necessary packages can be install through requirements.txt.\n\n#### Setup\nInstall VirtualEnv using the following (optional):\n\n```shell\n$ [sudo] pip install virtualenv\n```\nWe recommend creating a virtual environment(optional):\n\n```shell\n$ virtualenv -p python3 venv\n$ source venv/bin/activate\n```\nFinally, install the required packages by running:\n\n```shell\npip install -r requirements.txt\n```\n\n#### Resources\n\nWe provide the Performance Data by training with 3080 different data configurations for TyDiQA-GoldP dataset as described in the paper. These can be found in `performance_data/tydiqa_mbert_results.csv`\n\nThese numbers were obtained by fine-tuning the Multilingual BERT or mBERT on TyDiQA-GoldP datasets by modifying the scripts provided in the [XTREME Benchmark](https://github.com/google-research/xtreme). We will make the modified scripts public soon.\n\n#### Running Experiments\n\n**Fitting and Evaluating Performance Functions**\n\nTo fit the AMUE and Gaussian Regression based Performance Function on the provided data and evaluate the wellness of fit, execute the following command\n```bash\npython -m src.main --mode fit_nd_eval --lang <LANG> --pivot_size <PIVOT SIZE>\n```\n\nHere `<LANG>` and `<PIVOT SIZE>` refers to the target language and size of the pivot (English) language data. For example:\n\n```bash\n# This command fits and evaluates the performance functions on Swahili with 3696 training examples available for English\npython -m src.main --mode fit_nd_eval --lang sw --pivot_size 3696\n```\n\nSupported languages include: ar, bn, fi, id, ko, ru, sw, te\n\nSupported Pivot Sizes include: 100, 500, 1000, 2000, 3000, 3696\n\nOne can also supply \"all\" for the two arguments to run experiments on all possible language and pivot size pairs\n\n```bash\n# This command fits and evaluates the performance functions on all supported language and pivot size pairs\npython -m src.main --mode fit_nd_eval --lang all --pivot_size all\n```\n\nThe results of the experiments will get stored in `outputs/fit_results`\n\n**Generating Expansion Paths**\n\nTo generate Isoperf-Isocost plots with expansion paths as discussed in the paper, run the following command\n\n```bash\n# This command fits the AMUE performance function on Swahili with 3696 training examples available for English when the translated data is 10 times as cheap as manual data, and generates Expansion Curves\npython -m src.main --mode fit_nd_eval --lang sw --pivot_size 3696 --c12 0.1 \n```\n\nHere `--c12` specifies the ratio of the unit costs of obtaining translated and manual data.\n\nThe generated plots will be stored in `outputs/exp_paths`\n\n# Citation\n\nIf you use our code in this repo, please cite our paper `\\cite{ahuja-etal-2022-economics}`.\n```\n@inproceedings{ahuja-etal-2022-economics,\n    title = \"On the Economics of Multilingual Few-shot Learning: Modeling the Cost-Performance Trade-offs of Machine Translated and Manual Data\",\n    author = \"Ahuja, Kabir  and\n      Choudhury, Monojit  and\n      Dandapat, Sandipan\",\n    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jul,\n    year = \"2022\",\n    address = \"Seattle, United States\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.naacl-main.98\",\n    doi = \"10.18653/v1/2022.naacl-main.98\",\n    pages = \"1369--1384\",\n}\n```\n\n\n#### TODO\n- [ ] Upload Fine-Tuning Scripts\n", "repo_name": "PerformanceFunctionAnalysis", "org_name": "microsoft", "org_repo": "microsoft/PerformanceFunctionAnalysis", "platform_org_repo": "github+microsoft/PerformanceFunctionAnalysis", "link_to_repo": "https://github.com/microsoft/PerformanceFunctionAnalysis", "platform": "github", "language": "Python", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "<h1 align=\"center\">   <img src=\"https://microsoft.github.io/augmented-interpretable-models/auggam_gif.gif\" width=\"25%\"><img src=\"https://microsoft.github.io/augmented-interpretable-models/logo.svg?sanitize=True&kill_cache=1\" width=\"45%\"> <img src=\"https://microsoft.github.io/augmented-interpretable-models/auggam_gif.gif\" width=\"25%\"></h1>\n<p align=\"center\"> Augmenting Interpretable Models with LLMs during Training\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/license-mit-blue.svg\">\n  <img src=\"https://img.shields.io/badge/python-3.6+-blue\">\n  <img src=\"https://img.shields.io/badge/pytorch-1.0+-blue\">\n  <img src=\"https://img.shields.io/pypi/v/imodelsx?color=green\">  \n</p>  \n\n<p align=\"center\">\n  <img src=\"https://microsoft.github.io/augmented-interpretable-models/ovw.png\" width=\"60%\">\n</p>  \n\nThis repo contains code to reproduce the experiments in [the Aug-imodels paper](https://arxiv.org/abs/2209.11799). For a simple scikit-learn interface to use Aug-imodels, use the [imodelsX library](https://github.com/csinva/imodelsX). Below is a quickstart example.\n\nInstallation: `pip install imodelsx`\n\n```python\nfrom imodelsx import AugGAMClassifier, AugTreeClassifier, AugGAMRegressor, AugTreeRegressor\nimport datasets\nimport numpy as np\n\n# set up data\ndset = datasets.load_dataset('rotten_tomatoes')['train']\ndset = dset.select(np.random.choice(len(dset), size=300, replace=False))\ndset_val = datasets.load_dataset('rotten_tomatoes')['validation']\ndset_val = dset_val.select(np.random.choice(len(dset_val), size=300, replace=False))\n\n# fit model\nm = AugGAMClassifier(\n    checkpoint='textattack/distilbert-base-uncased-rotten-tomatoes',\n    ngrams=2, # use bigrams\n)\nm.fit(dset['text'], dset['label'])\n\n# predict\npreds = m.predict(dset_val['text'])\nprint('acc_val', np.mean(preds == dset_val['label']))\n\n# interpret\nprint('Total ngram coefficients: ', len(m.coefs_dict_))\nprint('Most positive ngrams')\nfor k, v in sorted(m.coefs_dict_.items(), key=lambda item: item[1], reverse=True)[:8]:\n    print('\\t', k, round(v, 2))\nprint('Most negative ngrams')\nfor k, v in sorted(m.coefs_dict_.items(), key=lambda item: item[1])[:8]:\n    print('\\t', k, round(v, 2))\n```\n\n\n\nReference:\n```r\n@misc{ch2022augmenting,\n    title={Augmenting Interpretable Models with LLMs during Training},\n    author={Chandan Singh and Armin Askari and Rich Caruana and Jianfeng Gao},\n    year={2022},\n    eprint={2209.11799},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n", "repo_name": "augmented-interpretable-models", "org_name": "microsoft", "org_repo": "microsoft/augmented-interpretable-models", "platform_org_repo": "github+microsoft/augmented-interpretable-models", "link_to_repo": "https://github.com/microsoft/augmented-interpretable-models", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# Kiota Text Serialization Library for PHP\n![Build](https://github.com/microsoft/kiota-serialization-text-php/actions/workflows/pr-validation.yml/badge.svg)\n[![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota-serialization-text-php&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota-serialization-text-php)\n\nProvides a serialization library implementation that handles serialization/deserialization to/from `text/plain` content\ntypes for [Kiota](https://github.com/microsoft/kiota) generated projects.\n\n## Using the Kiota Text Serialization Library\n\nrun `composer require microsoft/kiota-serialization-text` or add the following to your `composer.json` file:\n\n```Shell\n{\n    \"require\": {\n        \"microsoft/kiota-serialization-text\": \"^0.1.0\"\n    }\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-text-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-text-php", "platform_org_repo": "github+microsoft/kiota-serialization-text-php", "link_to_repo": "https://github.com/microsoft/kiota-serialization-text-php", "platform": "github", "language": "PHP", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Workshop Azure Machine Learning and PyTorch\n\nIn this hands-on lab you are going to build and deploy your own trained vision model to a highly scalable endpoint using Azure Machine Learning. You start with setting up your cloud workspace and learn how to manage your data and make it reusable. Next you will train a PyTorch model using the transfer learning approach and finally you deploy the model wrapped in an API in a managed endpoint.\n\nDuring the labs you learn the basics of PyTorch and at the end of this hands-on lab you have gone through the complete life-cycle of a model, from data to deployment using the Azure Machine Learning platform.\n\n\n### Labs\n[Lab 1 - Set an Azure Machine Learning Workspace](labs/Lab%201%20-%20Azure%20Machine%20Learning%20Workspace.md).   \n[Lab 2 - Create your first PyTorch Model](labs/Lab%202%20-%20Create%20your%20first%20PyTorch%20Model.md).   \n[Lab 3 - Pre-trained models and transfer learning](labs/Lab%203%20-%20Pre-trained%20models%20and%20transfer%20learning.md).   \n[Lab 4 - Training in the cloud](labs/Lab%204%20-%20Training%20in%20the%20cloud.md).   \n[Lab 5 - Deploy your model](labs/Lab%205%20-%20Deploy%20your%20model.md).   \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "workshop-aml-pytorch", "org_name": "microsoft", "org_repo": "microsoft/workshop-aml-pytorch", "platform_org_repo": "github+microsoft/workshop-aml-pytorch", "link_to_repo": "https://github.com/microsoft/workshop-aml-pytorch", "platform": "github", "language": "Python", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project: Arbutus\n\n> UI Library for building documentation sites.\n\n---\n\n## Getting started\n\n- `npm i`\n- `npm run build:packages`\n\n### Developing\n\n- `npm run start:ui`\n\n### Building production\n_WIP_\n- `npm run build:storybook`\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Name\nArbutus is the only native broadleaf evergreen tree in Canada. Another common name is madrone, a Spanish word for the strawberry tree, of which arbutus is a close relative. The Scottish botanist Archibald Menzies first collected specimens in 1792 and described it as the oriental strawberry tree.\n\u2014 [for.gov.bc.ca](https://www.for.gov.bc.ca/hfd/library/documents/treebook/arbutus.htm)\n", "repo_name": "arbutus", "org_name": "microsoft", "org_repo": "microsoft/arbutus", "platform_org_repo": "github+microsoft/arbutus", "link_to_repo": "https://github.com/microsoft/arbutus", "platform": "github", "language": "TypeScript", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "MDCC", "org_name": "microsoft", "org_repo": "microsoft/MDCC", "platform_org_repo": "github+microsoft/MDCC", "link_to_repo": "https://github.com/microsoft/MDCC", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\nThis repo contains sample circuits and programs along with QIR for testing of QIR based tooling.\n\n## Test Case Structure\n\n### Python\n\nQiskit based test circuits should have unique names that are valid file/directory names.\n\n- `python/src`\n  - `adaptive_cirquits.py` contains test cases leveraging conditions based on measurement results. Test cases should return `Generator[QuantumCircuit, Any, Any]` for circuits.\n  - `circuits.py` contains test circuits.\n  - `gates.py` contains gate definitions which are transpiled to the target gate set along with their controlled, power, and inverse. Test cases should return `List[QuantumCircuit]` and call `generate_circuits` with the appropriate parameters for creating their variations.\n\n## Output Folder Structure\n\nThe `python/artifacts` directory contains a folder per test case\n(e.g. `duplicate_conditions`, `hidden_shift`). Within each test case's\ndirectory is a folder for each execution target profile (e.g. `target_4bf9`,\n`target_7ee0`). Finally, within each execution target, there is a folder for\neach execution profile supported for that target (e.g. `AdaptiveExecution`).\n\nEach test case/target/profile combination includes:\n- Input of either\n  - QIR source `src_ir.ll` and bitcode `src_qir.bc` files representing the test case\n  - `generation_error.txt` when the test case cannot be generate valid QIR for the chosen provider/profile\n- An OpenQASM `circuit.qasm` representing the test cases circuit. This file may not exist as OpenQASM cannot represent all test cases.\n- QIR output of either\n  -  QIR source `targeted_ir.ll` and bitcode `targeted_qir` files with the provider/profile targeted QIR.\n  - `qir_processing_error.txt` containing the validation or transformation errors associated with the targeting of the QIR.\n- Simulation output of either\n  - `hist.txt` containing a sample histogram from simulating the circuit.\n  - `simulation_error.txt` containing the simulation errors encountered when running the circuit. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "qir-testing", "org_name": "microsoft", "org_repo": "microsoft/qir-testing", "platform_org_repo": "github+microsoft/qir-testing", "link_to_repo": "https://github.com/microsoft/qir-testing", "platform": "github", "language": "LLVM", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# The Azure Secure Enclave for Research\n\n## What is the Azure Secure Enclave for Research?\n\nThe [**Secure Enclave for Research**](https://docs.microsoft.com/azure/architecture/example-scenario/ai/secure-compute-for-research) (also known as the Secure Research Enclave) is a reference architecture for a remotely-accessible environment for researchers to use in a secure manner while working on restricted data sets. The solution features robust mechanisms for control over user access to the environment and also over movement of data in or out of scope for analysis so it is ideal for working with restricted data sets. Data in the environment can be analyzed with traditional VMs using Windows or Linux with well-known tools such as R Studio and also supports the use of advanced analytical tools such as Azure Machine Learning.\n\nThe solution is built using multiple Azure services including [Azure Virtual Desktop](https://azure.microsoft.com/services/virtual-desktop/), Azure Key Vault, and Azure Data Factory to provide strong control over data movement into and out of the environment in order to prevent unauthorized exfiltraction of data sets.\n\nThis solution was created in collaboration with the University of Pittsburgh.\n\n![SRE Architecture Diagram](docs/diagram/AzureSecureEnclaveForResearch.png)\n\n*Download a [Visio file](docs/diagram/AzureSecureEnclaveForResearch.vsdx) of this architecture.*\n\n**Important:**  *The Azure Secure Enclave for Research is not a substitute for good security practices. It is only a set of tools and processes which help you maintain a secure environment. Please read this repo's Wiki for instructions on how the environment is intended to function and how to manage security for both users and data properly.*\n\n## Deploying the Secure Enclave\n\nThis repository contains a set of Bicep templates which will deploy a complete SRE solution in a parameterized fashion. You can either download the Bicep templates and execute them using the *deploy.ps1* PowerShell script or simply use the \"Deploy to Azure\" button on this page.\n\n[![Deploy To Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#blade/Microsoft_Azure_CreateUIDef/CustomDeploymentBlade/uri/https%3A%2F%2Fraw.githubusercontent.com%2FMicrosoft%2FAzure-Secure-Enclave-for-Research%2Fmain%2Farm_templates%2Fazuredeploy.json/createUIDefinitionUri/https%3A%2F%2Fraw.githubusercontent.com%2FMicrosoft%2FAzure-Secure-Enclave-for-Research%2Fmain%2Farm_templates%2FmainUiDefinition.json)\n\nTo help you with the process of deploying research environments, refer to the [\"Secure Research\" Azure DevOps Generator](https://azuredevopsdemogenerator.azurewebsites.net/?name=secresearch) template. This Azure DevOps template contains Azure Boards work items to guide you through the design decisions and deployment of a complete research environment.\n\nFor complete documentation, please refer to the [Wiki](/wiki).\n\n## Similar Projects\n\nThese projects may also be useful for groups which would like to get started working with sensitive data sets on Microsoft Azure.\n\n- [Azure Trusted Research Environments (Azure TRE)](https://microsoft.github.io/AzureTRE)\n- The [Mission Landing Zone](https://github.com/Azure/MissionLZ) project is a set of templates which deploy a complete \"Landing Zone\" in Azure following Microsoft's best practices for isolation and separation of data, services, and security controls. It is designed with a focus on SACA (SCCA) compliance in Azure Government.\n\n## Contributing\n\nSee [Contributing](CONTRIBUTING.md)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Azure-Secure-Enclave-for-Research", "org_name": "microsoft", "org_repo": "microsoft/Azure-Secure-Enclave-for-Research", "platform_org_repo": "github+microsoft/Azure-Secure-Enclave-for-Research", "link_to_repo": "https://github.com/microsoft/Azure-Secure-Enclave-for-Research", "platform": "github", "language": "Bicep", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Data-AI-Hackathon", "org_name": "microsoft", "org_repo": "microsoft/Data-AI-Hackathon", "platform_org_repo": "github+microsoft/Data-AI-Hackathon", "link_to_repo": "https://github.com/microsoft/Data-AI-Hackathon", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Jacdac+DeviceScript for RP2040\n\nTo build just run `make` - this will clone submodules and invoke `cmake` as appropriate.\nDo not clone with `--recursive` unless you want to wait a long time - just run `make`.\n\nYou will need ARM GCC and node.js installed as well - some of our\n[instructions for STM32 build](https://github.com/microsoft/jacdac-stm32x0/blob/main/README.md#setup)\nmay apply.\n\nOnce you build, copy `build/src/devsrunner.uf2` to the RPI-RP2 drive.\n\nDebugging is set up for Black Magic Probe. \nYou can convert a [$3 Bluepill board into Black Magic Probe](https://github.com/mmoskal/blackmagic-bluepill).\nRun `make gdb` to run GDB.\n\nOne you have the firmware running, head to https://aka.ms/jacdac and press \"CONNECT\" in top-right corner,\nand then \"CONNECT SERIAL\" (not \"CONNECT USB\").\nThen head to \"Device Dashboard\" (Jacdac \"logo\" (connector icon) next to \"CONNECT\" button).\nYou should see widgets for power service, DeviceScript and HID services.\n\n## TODO\n\n* PICO_TIME_DEFAULT_ALARM_POOL_DISABLED\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "devicescript-pico", "org_name": "microsoft", "org_repo": "microsoft/devicescript-pico", "platform_org_repo": "github+microsoft/devicescript-pico", "link_to_repo": "https://github.com/microsoft/devicescript-pico", "platform": "github", "language": "C", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Introduction\n\nThis is a custom provider for Shopify for Dynamics 365 Intelligent Order Management. In order to use this custom provider, you will need to have a deployed and activated IOM environment available.\n\n# Installation\n1. Download one of the official releases as a zip file. Alternatively download the source and create a zip file from the base directory.\n2. Navigate to your Power Automate Portal for your environment. This portal will have your solutions, click on Solutions\n3. Click on Import Solution at the top\n4. Find your downloaded solution and import\n5. Find your connector id from the custom connector. This will be in the format of `shared_cr1a6-5fshopify-5fb4ae2b78f4d4086f`\n6. Navigate to Provider Definition Connection Reference and update the Connector Id\n![image](https://user-images.githubusercontent.com/104783217/175602531-fce18429-9254-4d9e-b38e-504f220969d4.png)\n7. Modify the Create Shopify Customer logic definition. Within the client data, search for cr1a6 and update with the new connection\n8. Repeat for Pull Shopify Sales Order\n9. Repeat for Shopify Cancel Order\n10. Navigate to IOM UI\n11. Open your provider catalog and find Shopify\n12. Click \"Add Provider\"\n13. Activate your connections. There should be 2 of them. Specify your shopify StoreURL and credentials\n14. Activate your provider\n15. Use the newly activated provider in an Orchestration Flow\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "iom-provider-shopify", "org_name": "microsoft", "org_repo": "microsoft/iom-provider-shopify", "platform_org_repo": "github+microsoft/iom-provider-shopify", "link_to_repo": "https://github.com/microsoft/iom-provider-shopify", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "![image](https://user-images.githubusercontent.com/64599697/185288661-6d7d823f-d244-41df-9ff9-4458a8887197.png)\n\n# Knowledge Graph Powered Search Accelerator\n\nThis accelerator provides the code template to implement a domain-aware search solution that uses knowledge graph (KG) to enrich the organic results of a general purpose search engine. The use of the accelerator is demonstrated through an example implementation of a medical document retrieval solution. The example implementation uses a knowledge graph to expand the scope of search from the original query to semantically related queries to uncover documents that are relevant to the original query but may not contain the exact keywords. \n\n <!-- the original query.  to uses a medical knowledge graph to expand search perform search across the well-known [OHSUMED](https://link.springer.com/chapter/10.1007/978-1-4471-2099-5_20) medical dataset.  -->\n\n<p align=\"center\"><img src=\"docs/media/animation.gif\"></p>\n\n\n## Knowledge Graph to Enhance Search Results\n\nKnowledge graph has been widely used to enhance search results by interpreting user intent based on semantic meaning of search terms. It is commonly applied to the following two search enhancement scenarios:\n\n* **Search refinement**: Knowledge graph can be used to refine search results by recognizing different meanings of a search term (e.g. \"apple\" can be a company or fruit). When search ambiguity is detected from knowledge graph a search engine can provide a refining option to the users to select a slice of the results based on their intent.\n\n* **Search expansion**: Knowledge graph can also be used to expand search results through the relationships of the entities present in the search queries. For example [Uber Eats](https://www.uber.com/en-AU/blog/uber-eats-query-understanding/) uses a food knowledge graph to expand search from the original query to related terms to help their customers find food options that they had not considered (e.g. expand search from \"udon\" to terms such as \"ramen\", \"soba\", and \"Japanese\").\n\nThe code template provided by this solution acceleration is applicable to both search refinement and search expansion scenarios.\n\n\n## An Example KG-Powered Search Scenario\n\nTo demonstrate the use of the solution accelerator, this repo provides an example of using knowledge graph for search expansion in the context of medical document retrieval. \n\nAn example knowledge graph is created based on [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html), which is a set of files and software that brings together health and biomedical vocabularies and standards to enable interoperability between computer systems. We create an example knowledge graph based on an [ontology](https://en.wikipedia.org/wiki/Ontology_(information_science)) as shown below to capture the relationships between various [keratoconus](https://www.hopkinsmedicine.org/health/conditions-and-diseases/keratoconus)-related diseases and their corresponding treatments. \n![img](docs/media/sample_kg.PNG)\n\nIn a medical document retrieval scenario, parent (hypernym) and children (hyponyms) of a disease are considered to be highly related, hence are natural candidates for search expansion. The example search engine is therefore designed to automatically expand a disease term in the original query to related parent and children diseases. For example, the search engine will expand \"keratoconus\" to its parent and children diseases: \"protrusion\", \"corneal disease\", \"corneal ectasia\", \"stable condition keratoconus\", and \"acute hydrops keratoconus\". \n\n## Extensible to Other Industries \n\nThis solution accelerator can be customized to other search scenarios from different industries. The vision demonstrator below illustrates how a KG-powered search solution can be used to support aircraft maintenance.  \n\nhttps://user-images.githubusercontent.com/64599697/193975237-faa5b810-3c8d-4836-8fd3-3a9585728049.mp4\n\n\n# Solution Design\n\nThis accelerator implements the following solution design consisting of 4 key components: \n![img](docs/media/solution_design.png)\n* **Front-End UI**: A front-end application to surface the final search results to the end users.\n* **Graph DB**: A graph database to host the knowledge graph. [Azure Cosmos DB with Gremlin API](https://docs.microsoft.com/en-us/azure/cosmos-db/graph/graph-introduction) is used in our implementation.\n* **Search Index**: A general purpose search engine to index and query the provided data. [Azure Cognitive Search](https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search) is used to index the data stored in the Azure Data Lake Storage (Gen 2) container.\n* **KG Search API**: An API that encapsulates the following workflow for KG-powered search expansion/refinement:\n    \n    1. **Preprocessing**: Preprocess the search query, e.g. remove domain-specific stop words.\n    2. **NER**: Apply a Named Entity Recognition (NER) model to the preprocessed query to extract entities of interest. As NER is not the focus of this accelerator the example code uses a simple rule-based NER procedure. One can replace this simple implementation with an ML-based NER model to recognize domain-specific entities, e.g. by training a custom NER model using [Azure Cognitive Service for Language](https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/custom-named-entity-recognition/overview). Refer to ```api/search_expansion/nerprocessing/ner.py``` for NER model input and output formats. \n    3. **Graph Query**: Take the NER result as the input, and query the KG for related entities (and corresponding entity properties if required).\n    4. **Query Rewriting**: Rewrite the search query based on the the entities (and entity properties) retrieved from the KG. \n    5. **Index Query**: Submit the rewritten query to the search engine.\n    5. **Postprocessing**: Apply postprocessing logic to the search results returned by the search engine, e.g. apply persona-based filtering or any re-ranking logic. The postprocessed results are surfaced to the end users through the front-end UI.\n\nThe example code in this repo implements a search expansion approach based on the above solution design. One can adapt the implementation for other search scenarios as well, specifically the scenarios that can benefit from the use of a domain-specific knowledge graph. \n\n## Prerequisite\n\nYou need to have an Azure subscription with the access to the following resources: \n\n| Azure Resources      | Description | Note |\n| ----------- | ----------- | --------------|\n| Azure Data Lake Storage | To store the data   | Use [Azure Data Lake Storage Gen2](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)\n| Azure Cognitive Search      | To index the data from the data lake   |\n| Azure Cosmos DB with Gremlin API      | To store the knowledge graph | Cosmos DB is a Labelled Property Graph (LPG) store\n| App Service Plan   | To host two applications: the front-end UI and the KG Search API, respectively | App service plan is required for app deployment to Azure. When creating an app service plan select Linux as the OS. \n\n<!--Refer to this [instruction](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cvscode-aztools%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli#2---create-a-web-app-in-azure) for app deployment from [VS Code](https://code.visualstudio.com/). One can also follow the instruction below to run the apps locally for testing. To protect the KG search API one should add an authentication mechanism. For example, we configure API authentication to use Azure AD as the authentication provider, cf. the configuration guide [here](https://docs.microsoft.com/en-us/azure/app-service/configure-authentication-provider-aad#--option-1-create-a-new-app-registration-automatically). -->\n\n\n\n<!-- Provision the following Azure resources in your own subscription: \n1. An Azure App Service to host the frontend application (We recommend to create the App Service using VS Code: [following this link](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cvscode-aztools%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli#2---create-a-web-app-in-azure). You can skip the Deployment of the code first. We will revisit this in the later step.)\n2. An Azure App Service to host the search APIs\n3. A cognitive search service to index the documents\n4. A Blob storage to stage the sample documents\n5. A Cosmos DB instance with Gremlin API to store the Knowledge Graph\n\nBesides, we assume the search APIs will be protected by token authentication. So, you need to configure the authentication provider for the search APIs App Service. We simply configure Azure AD login following this [guide](https://docs.microsoft.com/en-us/azure/app-service/configure-authentication-provider-aad#--option-1-create-a-new-app-registration-automatically).   \n\nAfter the frontend App Service is deployed, you need to add the following environment variables in the [Application settings](https://docs.microsoft.com/en-us/azure/app-service/configure-common?tabs=portal):\n```\n# Search API Secret\nSEARCH_API_URL # the URL of the search APIs App Service.\nSEARCH_AUTH_URL # the URL of the authentication provider, it should be https://login.microsoftonline.com/{tenant id}/oauth2/token if Azure AD login is configured\nSEARCH_GRANT_TYPE # simply set it as client_credentials\nSEARCH_CLIENT_ID # the client id when you registered in the identity provider for the search APIs App Service. \nSEARCH_CLIENT_SECRET # a client secret for the application you registered in the identity provider. Follow this https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#option-2-create-a-new-application-secret to create a client secret if you don't have.   \n\n# Configuration for UI Application\nAPP_SECRET_KEY  # The secret key for frontend application to maintain cookies\nMAX_CONTENT_SIZE  # The content size setting used by the frontend application. Set it as 200.  \n```\n\nSimilarly, you need to add the following environment variables for the search APIs App Service:\n```\n# Azure Cognitive Configuration\nACS_ENDPOINT # The url of ACS endpoint \nACS_API_KEY # The access key of the ACS \nACS_INDEX_NAME # The index name you want to use in ACS, e.g., ohsumed\nACS_API_VERSION # The API version of ACS, we have tested on 2021-04-30-Preview only \n\n# Cosmos DB Configuration\nCOSMOS_DB_SERVER # The address of the Cosmos DB server\nCOSMOS_DB_DATABASE # The database you create in Cosmos DB\nCOSMOS_DB_GRAPH # The graph collection in the above database that actually stores the KG\nCOSMOS_DB_PASSWORD # The access key to the Cosmos DB\n```\n\n### Deploy the source code to App Service\nCurrently, both the search APIs and frontend application source code are sitting in the same repository. We need to configure the startup command in both App Services such that they can pick up the right code to run. Following this [guide](https://docs.microsoft.com/en-us/azure/developer/python/configure-python-web-app-on-app-service#create-a-startup-file) to change the startup command.\n\nFor the search APIs App Service, set the startup command as:\n```\ngunicorn --bind=0.0.0.0 --timeout 600 --chdir api app:app\n```\n\nFor the frontend App Service, set the startup command as:\n```\ngunicorn --bind=0.0.0.0 --timeout 600 --chdir ui app:app\n```\n\nYou may first need to clone the repository to your machine if you did not.\nIn Visual Code, you can now continue the deploy step by following this [link](https://docs.microsoft.com/en-us/azure/app-service/quickstart-python?tabs=flask%2Cwindows%2Cvscode-aztools%2Cvscode-deploy%2Cdeploy-instructions-azportal%2Cterminal-bash%2Cdeploy-instructions-zip-azcli#2---create-a-web-app-in-azure). You can also choose other deployment methods like command line deployment in the same page of the previous link. -->\n\n## Getting Started\n\n1. Create a virtual environment. The solution accelerator is tested with Python 3.8. \n    ```\n    conda create -n kg-search python=3.8\n    conda activate kg-search\n    ```\n\n2. Clone the repo and install python dependencies:\n    ```\n    git clone https://github.com/microsoft/dstoolkit-kg-search.git\n    cd dstoolkit-kg-search\n    pip install -r requirements.txt\n    ```\n\n3. Prepare example medical data for indexing. We use the Hugging Face [OHSUMED](https://huggingface.co/datasets/ohsumed) dataset for the demo. It is a set of 348,564 references from MEDLINE, an online medical information database, consisting of titles, abstracts, and other metadata from 270 medical journals over a five-year period (1987-1991). Run the following command to download the OHSUMED dataset from the Hugging Face data repository, and extract individual entries from the dataset into separate JSON files for subsequent indexing:\n    ```\n    python scripts/prepare_data.py -o your_output_directory\n    ```\n\n4. Upload the JSON files created in the previous step to a folder in an Azure Data Lake blob container, see an example below.\n![img](docs/media/blob_folder.png)\n\n5. Import `scripts/create_acs_index.postman_collection.json` into [Postman](https://learning.postman.com/docs/getting-started/importing-and-exporting-data/#importing-data-into-postman) to create a Postman collection named \"create_acs_index\". The collection contains the following Azure Cognitive Search API calls that can be run in the following order to index the OHSUMED JSON files from the blob container:\n    * 1_create_datasource: Create a [data source](https://docs.microsoft.com/en-us/rest/api/searchservice/create-data-source) to connect to the OHSUMED data in the blob container.\n    * 2_create_index: Define an [index](https://docs.microsoft.com/en-us/rest/api/searchservice/create-index) schema to host the data.\n    * 3_create_indexer: Create [an indexer](https://docs.microsoft.com/en-us/rest/api/searchservice/create-indexer) to index the OHSUMED data to the defined index. \n\n     Before running the Postman collection, [edit the collection variables](https://learning.postman.com/docs/sending-requests/variables/#defining-collection-variables) in Postman accordingly. Set `datasource_name`, `index_name`, and `indexer_name` based on your preferred naming choice. The rest of the variables should be set according to your Azure service setup. ![img](docs/media/postman_vars.png) \n     \n     Having configured the collection variables, you can [run the collection](https://learning.postman.com/docs/running-collections/intro-to-collection-runs/#configuring-a-collection-run) in the order as shown below to start indexing the OHSUMED data.![img](docs/media/postman_run_collection.png) \n\n    To review the indexing progress you can go to the Azure Cognitive Search Overview page on the Azure Portal. Upon successful completion of the indexing, the document count of your created index should reach 348,564:\n     ![img](docs/media/search_index.png) \n\n\n6. Create a graph instance in the Cosmos DB. You can create a graph in Cosmos DB programmatically, or as shown below by using the Data Explorer tool in the Azure portal. Select **Data Explorer > New Graph**, provide a database ID, a graph ID, and set partition key to \"/pk\":\n    ![img](docs/media/create_kg.png)\n\n7. Ingest the example KG to the above created graph instance in Cosmos DB. First create a `.env` file in the root directory of the repository, and fill in the values for the following variables (**Note**: `.env` is meant to be used in local mode. It is already added to the `.gitignore` file to avoid accidental commit of credentials to a repo):\n\n    ```\n    # Cosmos DB Configuration\n    COSMOS_DB_SERVER=    # The address of the Cosmos DB server, i.e., the Gremlin Endpoint displayed on the Cosmos DB Overview page in the Azure Portal\n    COSMOS_DB_DATABASE=  # The database you create in Cosmos DB\n    COSMOS_DB_GRAPH=     # The graph collection in the above database that stores the KG\n    COSMOS_DB_PASSWORD=  # The access key to the Cosmos DB\n    ```\n    Then run the following command to create a keratoconus knowledge graph, cf. the ontology and knowledge graph introduced in the example search scenario discussion:\n    ```\n    python scripts/initialize_graph.py\n    ```\n\n7. Deploy the front-end UI and KG search API (both are Flask apps) locally or to Azure: \n    \n    **Local deployment (for testing)**: \n    * To deploy the apps locally, first edit the `.env` file created above for KG creation with the following additional variables: \n\n        ```\n        # Azure Cognitive Search Configuration\n        ACS_ENDPOINT=       # The url of ACS endpoint \n        ACS_API_KEY=        # The access key of ACS \n        ACS_INDEX_NAME=     # The index name you want to use in ACS, e.g.ohsumed\n        ACS_API_VERSION=    # The API version of ACS, tested with 2021-04-30-Preview \n\n        # Cosmos DB Configuration (same as used for KG creation above)\n        COSMOS_DB_SERVER=   # The address of the Cosmos DB server\n        COSMOS_DB_DATABASE= # The database you create in Cosmos DB\n        COSMOS_DB_GRAPH=    # The graph collection in the above database that stores the KG\n        COSMOS_DB_PASSWORD= # The access key to the Cosmos DB\n\n        # Search API Configuration\n        SEARCH_API_URL=     # The URL of the KG search API. In case of local development it should point to your local URL.\n        LOCAL_DEBUG=1       # Set local debug to 1 to bypass authentication\n\n        # Front-end UI Configuration\n        APP_SECRET_KEY=     # The secret key for front-end application to maintain cookies. It can be an arbitrary string. \n        MAX_CONTENT_SIZE=200  # The content size setting used by the frontend application. Default to 200.\n        ```\n    * Run the following command under the ```api``` folder to activate the KG Search API. You can use a different port number. The resulting URL (http://127.0.0.1:5000) should be the same as ```SEARCH_API_URL``` in the ```.env``` file.\n\n        ```\n        flask run --host=0.0.0.0 --port=5000\n        ```\n    * Run the following command under the ```ui``` folder to start the front-end application.\n        ```\n        flask run --host=0.0.0.0 --port=5001\n        ```\n    * You can now visit the front-end application at http://127.0.0.1:5001. Type in a query such as \"keratoconus treatment\", then click the \"Search\" button to search. You can toggle \"KG Enabled\" option on and off to compare the results with and without KG augmentation. You are likely to see more results with the \"KG Enabled\" option on if your query contains a disease term that is present in the example keratoconus KG. In that case the search is expanded from the original query to a query containing all the related diseases. \n    ![img](docs/media/expansion.png)\n\n    **Azure deployment**: \n    1. Deploy the KG Search API. We recommend to use VS Code to deploy the App Service. You need to install [Azure Tools extension pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode.vscode-node-azure-pack) and sign into Azure from VS code. Once install the extension, select the Azure Tools icon. Then, right click on \"App Services\" and choose \"Create New Web App.. (Advanced)\". It will ask you to input the app service name, resource group, App Service Plan etc. Once you provide all the information, the app service will be automatically created and shown as a new item under \"App Services\". \n    ![image](https://user-images.githubusercontent.com/64599697/186061479-c37c5987-1737-4b70-87a2-a749beb92532.png)\n\n       After create the App Service, you can now deploy the source code by right-clicking your targeted App Service. Then choose \"Deploy to Web App...\" option. It will ask you to select the root folder of the source code. Simply select the root folder of this repository.\n    ![image](https://user-images.githubusercontent.com/64599697/186061672-d99efc56-0637-429a-995e-59b55b0514ef.png)\n    \n    2. Configure the authentication for the KG search API. To configure the authentication, go to KG Search API App Service in Azure Portal. Select \"Autentication\" and then click \"Add identity provider\".\n    ![image](https://user-images.githubusercontent.com/64599697/185053685-1a51ba7f-5066-417e-930a-32dac314952a.png)\n    \n       In the next page, select Microsoft as the identity provider, and provide the name of the App registration. Finally, click the add button at the bottom to finish the configuration. \n    ![image](https://user-images.githubusercontent.com/64599697/185055242-f28c5798-3cad-432e-8f79-08f90967a88d.png)\n    \n       Once done, you will find an identity provider created. \n    ![image](https://user-images.githubusercontent.com/64599697/185055982-274433bb-a712-465c-89ff-d74490ace6b9.png)\n\n    3. Edit KG Search API application settings. To let the KG Search API be able to access the underlying ACS and Cosmos DB, we need to add the connection properties as environment variables in the application settings.\n    ![image](https://user-images.githubusercontent.com/64599697/185060539-7e9133c7-4583-4a46-a173-d6001f5f65a6.png)\n       Here is the description of the above enviroment variables:\n       ```\n       # Azure Cognitive Configuration\n       ACS_ENDPOINT # The url of ACS endpoint \n       ACS_API_KEY # The access key of the ACS \n       ACS_INDEX_NAME # The index name you want to use in ACS, e.g., ohsumed\n       ACS_API_VERSION # The API version of ACS, we have tested on 2021-04-30-Preview only \n\n       # Cosmos DB Configuration\n       COSMOS_DB_SERVER # The address of the Cosmos DB server\n       COSMOS_DB_DATABASE # The database you create in Cosmos DB\n       COSMOS_DB_GRAPH # The graph collection in the above database that actually stores the KG\n       COSMOS_DB_PASSWORD # The access key to the Cosmos DB\n       ```\n    \n       Apart from adding the environment varibles, you also need to tell which application this app service will run because this repository contains both the KG Search API and front-end applications. To do that, you need to configure the \"Startup Command\" for your application as bellow:\n       ```\n       gunicorn --bind=0.0.0.0 --timeout 600 --chdir api app:app\n       ```\n    ![image](https://user-images.githubusercontent.com/64599697/185177596-d145a19a-c3da-49a6-a88d-428b6e9e2dbf.png)\n\n    4. Create client secret for the App registration. The App registration created in step 2 will be used to conduct Oauth 2.0 authentication by the front-end application. To achieve that, we need to first create the client secret for the App registration. Find the App registration you created in step 2 in your Active Directory, then click \"New client secret\" to create a secret. Don't forget to save the secret since it will be used in the configuration of the front-end app service later on.  \n    ![image](https://user-images.githubusercontent.com/64599697/185065052-38c36d59-5014-46f3-95c3-536d0a3c2cad.png) \n    \n    5. Deploy front-end application. You can follow the same instructions in step 1 to create a new App Service and deploy the source code for the front-end application. Again, simply choose the root directory of the repository as the deployment target.\n    \n    6. Edit front-end app settings. Add the following environment variables in the front-end App Service.\n    ![image](https://user-images.githubusercontent.com/64599697/185064376-265fa033-1669-416e-8304-04ce5652d307.png)  \n       Here is the description of the above enviroment variables:\n       ```\n       # Search API Secret\n       SEARCH_API_URL # the URL of the search APIs App Service.\n       SEARCH_AUTH_URL # the URL of the authentication provider, it should be https://login.microsoftonline.com/{tenant id}/oauth2/token if Azure AD login is configured\n       SEARCH_GRANT_TYPE # simply set it as client_credentials\n       SEARCH_CLIENT_ID # the client id when you registered in the identity provider for the search APIs App Service. \n       SEARCH_CLIENT_SECRET # a client secret for the application you registered in the identity provider. Follow this https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#option-2-create-a-new-application-secret to create a client secret if you don't have.   \n\n       # Configuration for UI Application\n       APP_SECRET_KEY  # The secret key for frontend application to maintain cookies\n       MAX_CONTENT_SIZE  # The content size setting used by the frontend application. Set it as 200.  \n       ```\n       \n       After that, follow the instructions in step 3 to configure the \"Startup Command\" for the front-end application as bellow. \n       ```\n       gunicorn --bind=0.0.0.0 --timeout 600 --chdir ui app:app\n       ```\n \n       To check the deployment, you can open the home page of the front-end application and input the search \"keratoconus treatment\". If there is result returns, then it means your deployment is successful. You can find the URL of the front-end application in the overview page as below:\n       ![image](https://user-images.githubusercontent.com/64599697/185178618-32031711-8a35-48df-a3ce-c6d9de019ba1.png)\n\n## Code Structure\n\n<!-- You can reuse differnet parts of the code for your own application. The key component here is the search APIs. You can completely replace the frontend application by your own one. To adapt the search APIs to you specific scenario, you need to adjust the code accordingly. Below is the detailed breakdown of the search APIs. There are five main components:\n* Preprocessing: conduct any preprocessing logic of the search query, e.g., removing domain specific stop words.\n* NER: conduct NER to the preprocessed search text\n* Graph Query: take the NER result as input, retrieve the relevant entities from the KG\n* Query Rewriting: rewrite the original search query. It will be the final query being submitted to ACS\n* Postprocessing: include any postprocessing logic here, e.g., user based filtering or re-ranking \n![img](docs/media/search_api.PNG)\n\nEvery component has a base class defined. You can create your own class by inheriting the corresponding base class. The whole solution will work seamlessly if you follow the same API designed.  -->\n\n```\n\u251c\u2500\u2500\u2500api              # folder containing the KG search API components\n\u2502   \u251c\u2500\u2500\u2500app.py       # the api web service\n\u2502   \u2514\u2500\u2500\u2500search_expansion    # the search expansion component\n\u2502       \u251c\u2500\u2500\u2500kg              # extract relevant entities from KG\n|       \u251c\u2500\u2500\u2500nerprocessing   # extract entities of interest from search query\n|       \u251c\u2500\u2500\u2500postprocessing  # postprocess the Azure Cognitive Search result before sending back to front-end application\n|       \u251c\u2500\u2500\u2500preprocessing   # preprocess the original search query\n|       \u251c\u2500\u2500\u2500rewriting       # rewrite the original search query\n|       \u251c\u2500\u2500\u2500search_expander.py   # control the whole execution flow\n|       \u251c\u2500\u2500\u2500search_sdk.py   # encapsulate the API for the underlying search engine\n\u2502       \u2514\u2500\u2500\u2500util.py\n\u251c\u2500\u2500\u2500config      # configuration for log or other non-credential settings\n\u251c\u2500\u2500\u2500docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500\u2500media   # storing images, videos, etc, needed for docs.\n\u251c\u2500\u2500\u2500scripts     # scripts for preparing data\n\u251c\u2500\u2500\u2500tests       # unit tests\n|\u2500\u2500 ui          # the front-end application\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirement.txt # Python dependencies\n```\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-kg-search", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-kg-search", "platform_org_repo": "github+microsoft/dstoolkit-kg-search", "link_to_repo": "https://github.com/microsoft/dstoolkit-kg-search", "platform": "github", "language": "SCSS", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Theme Tester\n\nTest a theme on a set of input files without installing it.\n\nTry it out in vscode.dev: https://vscode.dev/theme/aeschli.senja-dark\n\n![screenshot](./screenshot.png)\n\nAlso works in desktop or web:\n    Command: `Theme Tester: Preview Theme`\n\n\n> **Note**: \nVS Code also has a built-in feature to try out themes:<br>\nRun `Preferences: Color Themes` and select `Browse Additional Color Themes...`\n  \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-theme-tester", "org_name": "microsoft", "org_repo": "microsoft/vscode-theme-tester", "platform_org_repo": "github+microsoft/vscode-theme-tester", "link_to_repo": "https://github.com/microsoft/vscode-theme-tester", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Interactive NPCs for Minecraft\n\nThis repository contains the code referred in the paper below:\n\n<a href=\"https://wordplay-workshop.github.io/modern/assets/pdfs/6.pdf\">Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code</a><br/>\n\n```bibtex\n@inproceedings{volum2022craft,\n  title={Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code},\n  author={Volum, Ryan and Rao, Sudha and Xu, Michael and DesGarennes, Gabriel A and Brockett, Chris and Van Durme, Benjamin and Deng, Olivia and Malhotra, Akanksha and Dolan, Bill},\n  booktitle={The Third Wordplay: When Language Meets Games Workshop},\n  year={2022}\n}\n```\n\nContact Person: Sudha Rao (Sudha.Rao@microsoft.com)\n\n## Getting Started\n\nThis repository contains prototype code that applies Codex in Minecraft. Codex is a code generation model that fine-tuned GPT-3 in GitHub code. Here we use it to generate code and dialog in gaming contexts to explore what might be possible. We use a Minecraft library called `mineflayer` to generate a non-player character (NPC) and enable that NPC to turn commands into code. \n\n### Requirements\n\nTo run this prototype, you'll need the following: \n\n1. **Minecraft** - specifically the Java version v.1.17.1\n1. **Node.js and npm** - the prototype was tested with Node version v14.17.5\n1. **Access to the OpenAI Codex API** - this prototype specifically uses the `code-davinci-002` model\n1. **git** - if you're reading this, this is probably self evident :)\n\n### Running the prototype\n\n1. Clone the repo: `git clone https://github.com/microsoft/codex-gaming.git`\n1. Install npm packages: `npm i`\n1. Rename `.env.example` to `.env`\n1. Grab your Codex API key from `https://beta.openai.com/account/api-keys` and add it to the `.env` file\n1. Open Minecraft and create a one player new world. Set \"Allow Cheats\" to true\n1. Enter the world and open settings (hitting the escape key). Select \"Open to LAN\", selecting \"Allow Cheats\" again. To avoid being killed in-game while programming, set mode to \"Creative\"\n1. Run the bot: `node index.js`. To automatically re-run the bot as you make code changes, consider installing `nodemon` and running `nodemon index.js`\n1. You can see the code produced by the bot in the console window \n\nYou should now see an NPC appear that you can interact with! To type commands or messages to the NPC, press \"t\" to open the chat window, and type the command or message. \n\n## Introductory Demo\n\nhttps://user-images.githubusercontent.com/64496188/179088024-bfefd7f0-b83a-44d3-8e42-c97bb46fb051.mp4\n\n## How it Works\n\nFor now, this prototype uses zero-shot learning and no fine-tuning to generate code. Instead, it relies on engineered context and prompts. Our contexts live in the `context` directory and are intended to give the model a sense for the shape of the API we're using (`mineflayer`), along with the structure of the calls we will make. Pulling from the `context/commands.js` file, here is a subset of our context:\n\n```js\n// \"Go backwards\"\nbot.setControlState('back', true)\n\n// \"Hello!\"\nbot.chat(\"Yo! How's it going?\");\n```\n\nAs you can see, we give commands in the form of comments, which is followed by the code that should be executed to satisfy the command. When calling the model, we can now simply pass a comment with our command (e.g. `// \"Go Forward\"`) and the model will generate the next line - the code that satisfies the command (e.g. `bot.setControlState('forward', true)`). In the `index.js`, we run the code by simply calling JavaScript's \"eval\" function on it. \n\n### Context Class\n\nThis project includes a Context class, which you can find in `Context.js` in the root directory. This class enables a few things:\n\n- To build new prompts against the model\n- To append past interactions to future prompts\n- To enable an interface for learning and \"unlearning\" from previous interactions\n\nThe Context class exposes a few functions: \n\n- `addInteraction`: Appends the last call + response to the existing context. This happens automatically as you interact with the NPC. \n- `removeLastInteraction`: Remove the last call + response from the context. If the NPC does something wrong, you can say \"unlearn\" to have this function invoked.\n- `resetContext`: Removes all interactions from the context, effectively resetting the context.\n- `craftPrompt`: A helper method to append a prompt to a context, to be sent to the model. \n- `fixResponse`: Removes last response and adds a corrected response. Currently triggered by saying \"fix:...\" with the correct code to the NPC.\n\nUse these functions or add more to \"machine teach\" your NPC as you interact!\n\n### Web-based viewer\n\nYou can watch the player and the NPC nagivate the minecraft world by going to http://localhost:3030/ \n\n### Using a seed to initialize Minecraft world\n\nBy default, the new world started in Minecraft (Java edition) does not have easy access to interesting resources. Follow the steps below to arrive at a world that is comparatively flat and has interesting resources nearby. \n\n- In the Create New World window, select 'More World Options'\n- In the 'Seed for the world generator' text box, enter 6714107141548954383\n- Select 'Game Rules' and under World Updates, switch off 'Advance time of day', 'Update weather'. This will make sure that it is always day time and no rain in the newly created world.\n\n## Escape Room Demo\n\nHere is a demo of a player collaborating with the NPC to escape out of two escape rooms\n\nhttps://user-images.githubusercontent.com/64496188/179092236-ec6362ea-249b-4e23-95d9-8c3d94ba4560.mp4\n\nA copy of this world in Minecraft 1.17.1 is included in the repo as `escape-room.zip`, to play with this locally simply unzip the contents into `%appdata%/.minecraft/saves`, and you should see it show up in your Minecraft Java Edition inside the Singleplayer menu.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "interactive-minecraft-npcs", "org_name": "microsoft", "org_repo": "microsoft/interactive-minecraft-npcs", "platform_org_repo": "github+microsoft/interactive-minecraft-npcs", "link_to_repo": "https://github.com/microsoft/interactive-minecraft-npcs", "platform": "github", "language": "JavaScript", "stargazers_count": 34, "watchers_count": 34}, {"README_text": "# Prompt Engine\n\nThis repo contains a Python utility library for creating and maintaining prompts for Large Language Models (LLMs).\n\n## Background\n\nLLMs like GPT-3 and Codex have continued to push the bounds of what AI is capable of - they can capably generate language and code, but are also capable of emergent behavior like question answering, summarization, classification and dialog. One of the best techniques for enabling specific behavior out of LLMs is called prompt engineering - crafting inputs that coax the model to produce certain kinds of outputs. Few-shot prompting is the discipline of giving examples of inputs and outputs, such that the model has a reference for the type of output you're looking for.\n\nPrompt engineering can be as simple as formatting a question and passing it to the model, but it can also get quite complex - requiring substantial code to manipulate and update strings. This library aims to make that easier. It also aims to codify patterns and practices around prompt engineering.\n\nSee [How to get Codex to produce the code you want](https://microsoft.github.io/prompt-engineering/) article for an example of the prompt engineering patterns this library codifies. \n\n## Installation\n\n`pip install prompt-engine-py`\n\n## Usage\n\nThe library currently supports a generic `PromptEngine`, a `CodeEngine` and a `ChatEngine`. All three facilitate a pattern of prompt engineering where the prompt is composed of a description, examples of inputs and outputs and an ongoing \"dialog\" representing the ongoing input/output pairs as the user and model communicate. The dialog ensures that the model (which is stateless) has the context about what's happened in the conversation so far.\n\nSee architecture diagram representation:  \n  \n<img src=\"https://user-images.githubusercontent.com/17247257/178356452-03a69f87-aa09-459b-83d8-eeedba634b39.png\" width=\"500\">\n\n### Code Engine\n\nCode Engine creates prompts for Natural Language to Code scenarios. See Python Syntax for importing `CodeEngine` and `PythonCodeEngineConfig`:\n\n```py\nfrom prompt_engine.code_engine import CodeEngine, PythonCodeEngineConfig\n```\n\nNL->Code prompts should generally have a description, which should give context about the programming language the model should generate and libraries it should be using. The description should also give information about the task at hand:\n\n```py\ndescription = \"Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console.\"\n```\n\nNL->Code prompts should also have examples of NL->Code interactions, exemplifying the kind of code you expect the model to produce. In this case, the inputs are math queries (e.g. \"what is 2 + 2?\") and code that console logs the result of the query.\n\n```py\nfrom prompt_engine.interaction import Interaction\nexamples = [\n  Interaction(\"what's 10 plus 18\", \"console.log(10 + 18)\"),\n  Interaction(\"what's 10 times 18\", \"console.log(10 * 18)\")\n]\n```\n\nBy default, `CodeEngine` uses Python as the programming language, but you can create prompts for different languages by passing a different `CodeEngineConfig` into the constructor. If, for example, we wanted to produce JavaScript prompts, we could have passed `CodeEngine` a `javascript_config` specifying the comment operator it should be using:\n\n```py\njavascript_config = CodeEngineConfig(description_comment_operator = \"/*/\", description_comment_close_operator = \"/*/\", \n                                     comment_operator = \"/*\", comment_close_operator = \"*/\")\ncode_engine = CodeEngine(config = javascript_config, description = description, examples = examples)\n\n```\n\nWith our description and our examples, we can use it to create prompts:\n\n```py\nquery = \"What's 1018 times the ninth power of four?\"\nprompt = code_engine.build_prompt(query)\n```\n\nThe resulting prompt will be a string with the description, examples and the latest query formatted with comment operators and line breaks:\n\n```js\n/*/ Natural Language Commands to JavaScript Math Code. The code should log the result of the command to the console. /*/\n\n/* what's 10 plus 18 */\nconsole.log(10 + 18)\n\n/* what's 10 times 18 */\nconsole.log(10 * 18)\n\n/* What's 1018 times the ninth power of four? */\n```\n\nGiven the context, a capable code generation model can take the above prompt and guess the next line: `print(1018 * (4 ** 9))`.\n\nFor multi-turn scenarios, where past conversations influences the next turn, Code Engine enables us to persist interactions in a prompt:\n\n```py\n...\n# Assumes existence of code generation model\ncode = model.generate_code(prompt)\n\n# Adds interaction\ncode_engine.add_interaction(query, code)\n```\n\nNow new prompts will include the latest NL->Code interaction:\n\n```py\ncode_engine.build_prompt(\"How about the 8th power?\")\n```\n\nProduces a prompt identical to the one above, but with the NL->Code dialog history:\n\n```js\n...\n/* What's 1018 times the ninth power of four? */\nconsole.log(1018 * (4 ** 9))\n\n/* How about the 8th power? */\n```\n\nWith this context, the code generation model has the dialog context needed to understand what we mean by the query. In this case, the model would correctly generate `print(1018 * (4 ** 8))`.\n\n### Chat Engine\n\nJust like Code Engine, Chat Engine creates prompts with descriptions and examples. See Python Syntax for importing `CodeEngine` and `PythonCodeEngineConfig`:\n\n```py\nfrom prompt_engine.chat_engine import ChatEngine, ChatEngineConfig\n```\n\nThe difference is that Chat Engine creates prompts for dialog scenarios, where both the user and the model use natural language. The `ChatEngine` constructor takes an optional `config` argument, which allows you to define the name of a user and chatbot in a multi-turn dialog: \n  \n```py\nconfig = ChatEngineConfig(\n    user_name = \"Abhishek\",\n    bot_name = \"Gordon\"\n)\n```\n\nChat prompts also benefit from a description that gives context. This description helps the model determine how the bot should respond. \n\n```py\ndescription = \"A conversation with Gordon the Anxious Robot. Gordon tends to reply nervously and asks a lot of follow-up questions.\"\n```\n\nSimilarly, Chat Engine prompts can have examples interactions: \n\n```py\nfrom prompt_engine.interaction import Interaction\nexamples = [\n  Interaction(\"Who made you?\", \"I don't know man! That's an awfully existential question. How would you answer it?\"),\n  Interaction(\"Good point - do you at least know what you were made for?\", \"I'm OK at riveting, but that's not how I should answer a meaning of life question is it?\")\n]\n```\n\nThese examples help set the tone of the bot, in this case Gordon the Anxious Robot. Now we can create our `ChatEngine` and use it to create prompts:\n\n```py\nchat_engine = ChatEngine(chatEngineConfig, description, examples)\nuser_query = \"What are you made of?\"\nprompt = chat_engine.build_prompt(user_query)\n```\n\nWhen passed to a large language model (e.g. GPT-3), the context of the above prompt will help coax a good Marvin-like answer from the model, like \"Subatomic particles at some level, but somehow I don't think that's what you were asking.\". As with Code Engine, we can persist this answer and continue the dialog such that the model is aware of the conversation context: \n\n```py\nchatEngine.add_interaction(user_query, \"Subatomic particles at some level, but somehow I don't think that's what you were asking.\")\n```\n\n### Dynamic Prompt Engine\n\nDynamic Prompt Engine is another behaviour constructed on top of prompt engine that enables dynamic retrieval of the relevant examples in order to generate a prompt. \n\nIt is developed with the belief that giving more relevant examples to the Large Language Model will enable it to generate a better output and much closer to our examples, rather than leaving it to guess for itself. This also allows the ability to coax multiple behaviours out of the Large Language Model instead of having to maintain mutliple different prompts. \n\nThe dynamic prompt engine maintains a prompt bank, which is a collection of embeddings of all the examples and interactions that have been provided to it. When given a new unseen prompt, it queries the prompt bank based on the embeddings to retrieve the Top-k relevant examples and adds them to the examples section of the prompt engine output. \n\n<img width=\"600\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17247257/181765992-5a645f56-e463-4c96-98c9-814efd1b8a17.png\">\n\n\n## Managing Prompt Overflow\n\nPrompts for Large Language Models generally have limited size, depending on the language model being used. Given that prompt-engine can persist dialog history, it is possible for dialogs to get so long that the prompt overflows. The Prompt Engine pattern handles this situation by removing the oldest dialog interaction from the prompt, effectively only remembering the most recent interactions.\n\nYou can specify the maximum tokens allowed in your prompt by passing a `max_tokens` parameter when constructing the config for any prompt engine:\n\n```py\nfrom prompt_engine.model_config import ModelConfig\nconfig = PromptEngineConfig( ModelConfig(max_tokens=1024) )\n```\n\n## Available Functions\n\nThe following are the functions available on the `PromptEngine` class and those that inherit from it:\n\n| Command | Parameters | Description | Returns |\n|--|--|--|--|\n| `build_context` | None | Constructs and return the context with parameters provided to the Prompt Engine | Context: string |\n| `build_prompt` | Prompt: string | Combines the context from `build_context` with a query to create a prompt | Prompt: string |\n| `build_dialog` | None | Builds a dialog based on all the past interactions added to the Prompt Engine | Dialog: string |\n| `add_example` | interaction: Interaction(input: string, response: string) | Adds the given example to the examples | None |\n| `add_interaction` | interaction: Interaction(input: string, response: string) | Adds the given interaction to the dialog | None |\n| `remove_first_interaction` | None | Removes and returns the first interaction in the dialog | Interaction: Interaction |\n| `remove_last_interaction` | None | Removes and returns the last interaction added to the dialog | Interaction: Interaction |\n| `reset_context` | None | Removes all interactions from the dialog, effectively resetting the context to just description and examples | Context: string |\n\nFor more examples and insights into using the prompt-engine library, have a look at the [examples](https://github.com/microsoft/prompt-engine-py/tree/main/examples) folder\n\n## YAML Representation\nIt can be useful to represent prompts as standalone files, versus code. This can allow easy swapping between different prompts, prompt versioning, and other advanced capabiliites. With this in mind, prompt-engine offers a way to represent prompts as YAML and to load that YAML into a prompt-engine class. See `examples/yaml-examples` for examples of YAML prompts and how they're loaded into prompt-engine.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.  \n  \n## Statement of Purpose  \n  \nThis library aims to simplify use of Large Language Models, and to make it easy for developers to take advantage of existing patterns. The package is released in conjunction with the [Build 2022 AI examples](https://github.com/microsoft/Build2022-AI-examples), as the first three use a multi-turn LLM pattern that this library simplifies. This package works independently of any specific LLM - prompt generated by the package should be useable with various language and code generating models.\n  \n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "prompt-engine-py", "org_name": "microsoft", "org_repo": "microsoft/prompt-engine-py", "platform_org_repo": "github+microsoft/prompt-engine-py", "link_to_repo": "https://github.com/microsoft/prompt-engine-py", "platform": "github", "language": "Python", "stargazers_count": 144, "watchers_count": 144}, {"README_text": "# Distance Learner: Incorporating Manifold Prior to Model Training\n\nThis repository contains the official implementation for:\n\n>[Distance Learner: Incorporating Manifold Prior to Model Training](https://arxiv.org/abs/2207.06888). *[Aditya Chetan](http://justachetan.github.io/)*, *[Nipun Kwatra](https://www.microsoft.com/en-us/research/people/nkwatra/)*\n\n## About\n\nThe manifold hypothesis (real world data concentrates near low-dimensional manifolds) is suggested as the principle behind the effectiveness of machine learning algorithms in very high dimensional problems that are common in domains such as vision and speech. Multiple methods have been proposed to explicitly incorporate the manifold hypothesis as a prior in modern Deep Neural Networks (DNNs), with varying success. In this paper, we propose a new method, Distance Learner, to incorporate this prior for DNN-based classifiers. Distance Learner is trained to predict the distance of a point from the underlying manifold of each class, rather than the class label. For classification, Distance Learner then chooses the class corresponding to the closest predicted class manifold. Distance Learner can also identify points as being out of distribution (belonging to neither class), if the distance to the closest manifold is higher than a threshold. We evaluate our method on multiple synthetic datasets and show that Distance Learner learns much more meaningful classification boundaries compared to a standard classifier. We also evaluate our method on the task of adversarial robustness, and find that it not only outperforms standard classifier by a large margin, but also performs at par with classifiers trained via state-of-the-art adversarial training.\n\n## Setup and Usage\n\n### Dependencies\n\n```\n* Python 3.6+\n* scikit-learn==0.22.1\n* scipy==1.4.1\n* numpy==1.18.1\n* torch==1.11.0\n* torchvision==0.12.0\n* faiss==1.7.2\n* cleverhans==4.0.0\n* livelossplot==0.5.5\n* matplotlib==3.1.3\n* plotly==5.8.0\n* seaborn==0.10.0\n* tensorboard==2.9.0\n* tqdm==4.42.1\n* sacred==0.8.2\n```\n\nAll of these can be installed using any package manager such as `pip` or Conda. We recommend using a virtual environment before installing these packages. For installing Faiss, please refer to [instructions](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md) on their official repository.\n\n### Sample Code\n\nThe code in this project is used to run a pipeline as follows:\n\n1. Data Synthesis: Synthesizes the data used for training the models. This includes on-manifold point generation for synthetic datasets, as well as off-manifold augmentations.\n2. Distance Learner Training: Involves training the distance learner on the generated dataset.\n3. Standard Classifier Training: Involves training the standard classifier on the generated dataset.\n4. Robust Classifier Training: To create a classifier generated under adversarial training [[Madry et. al]](https://arxiv.org/abs/1706.06083)\n5. Testing on Adversarial Attacks: We attack the models trained in 2-4 with PGD-based $l_2$-norm attacks.\n\nThe complete pipeline trains and compares Distance Learner, Standard Classifier and Robust Classifier on adversarial attacks. As an example, let us say that we want to run this pipeline on a dataset consisting of two concentric 50-spheres (50 dimensional spheres) embedded in 500-dimensional space. The following commands would be required to run this pipeline:\n\n```bash\n# Switch to directory with training code\ncd ./src/pipeline\n\n# Steps 1 & 2: Data Synthesis and Distance Learner training\n\npython3 learn_cls_from_dist.py with cuda=0 num_epochs=1000 cooldown=700 warmup=10 lr=1.5e-5 batch_size=4096 debug=False loss_func=std_mse tgtname=normed_actual_distances data.mtype=inf-conc-spheres data.logdir=\"../../dumps/rdm_concspheres_test/\" \\\n data.data_tag=rdm_concspheres_m50n500 \\\n data.data_params.train.N=6500000 \\\n data.data_params.train.num_neg=6000000 \\\n data.data_params.train.k=51 \\\n data.data_params.train.n=500 \\\n data.data_params.train.max_t_delta=1e-3 \\\n data.data_params.train.max_norm=0.14 \\\n data.data_params.val.N=200000 \\\n data.data_params.val.num_neg=100000 \\\n data.data_params.val.k=51 \\\n data.data_params.val.n=500 \\\n data.data_params.val.max_norm=0.14 \\\n data.data_params.test.N=200000 \\\n data.data_params.test.num_neg=100000 \\\n data.data_params.test.k=51 \\\n data.data_params.test.n=500 \\\n data.data_params.test.max_norm=0.14 \\\n model.input_size=500 \\\n data.generate=True \\\n task=regression\n \n# Step 3: Standard Classifier training\n\npython3 learn_cls_from_dist.py with cuda=3 num_epochs=1000 cooldown=700 warmup=10 lr=8e-5 batch_size=4096 debug=False data.mtype=inf-conc-spheres \\\n data.logdir=\"../../dumps/rdm_concspheres_test/\" \\\n data.data_tag=rdm_concspheres_m50n500 \\\n data.data_params.train.N=6500000 \\\n data.data_params.train.num_neg=6000000 \\\n data.data_params.train.k=51 \\\n data.data_params.train.n=500 \\\n data.data_params.train.max_t_delta=1e-3 \\\n data.data_params.train.max_norm=0.14 \\\n data.data_params.val.N=200000 \\\n data.data_params.val.num_neg=100000 \\\n data.data_params.val.k=51 \\\n data.data_params.val.n=500 \\\n data.data_params.val.max_norm=0.14 \\\n data.data_params.test.N=200000 \\\n data.data_params.test.num_neg=100000 \\\n data.data_params.test.k=51 \\\n data.data_params.test.n=500 \\\n data.data_params.test.max_norm=0.14 \\\n model.input_size=500 \\\n on_mfld_noise=0 \\\n adv_train=False \\\n test_off_mfld=False \\\n data.generate=False \\\n task=clf\n \n # Step 4: Robust Classifier training\n\npython3 learn_cls_from_dist.py with cuda=3 num_epochs=1000 cooldown=700 warmup=10 lr=8e-5 batch_size=4096 debug=False data.mtype=inf-conc-spheres \\\n data.logdir=\"../../dumps/rdm_concspheres_test/\" \\\n data.data_tag=rdm_concspheres_m50n500 \\\n data.data_params.train.N=6500000 \\\n data.data_params.train.num_neg=6000000 \\\n data.data_params.train.k=51 \\\n data.data_params.train.n=500 \\\n data.data_params.train.max_t_delta=1e-3 \\\n data.data_params.train.max_norm=0.14 \\\n data.data_params.val.N=200000 \\\n data.data_params.val.num_neg=100000 \\\n data.data_params.val.k=51 \\\n data.data_params.val.n=500 \\\n data.data_params.val.max_norm=0.14 \\\n data.data_params.test.N=200000 \\\n data.data_params.test.num_neg=100000 \\\n data.data_params.test.k=51 \\\n data.data_params.test.n=500 \\\n data.data_params.test.max_norm=0.14 \\\n model.input_size=500 \\\n on_mfld_noise=0 \\\n adv_train=True \\\n adv_train_params.atk_eps=8e-2 \\\n test_off_mfld=False \\\n data.generate=False \\\n task=clf\n \n# Switch to directory with adversarial attack analysis\ncd ../adversarial_attack\n \n# Step 5: Adversarial Attack analysis\n\npython3 get_attack_perf.py with debug=False \"attack.atk_routine=['my']\" \\\n input_files.settings_type=list \\\n input_files.proj_dir=\"../dumps/rdm_concspheres_test/\" \\\n input_files.settings_to_analyze=\"['rdm_concspheres_m50n500/1','rdm_concspheres_m50n500/2', 'rdm_concspheres_m50n500/3']\" \\\n dump_dir=\"../../dumps/rdm_concspheres_test/attack_perfs_on_runs\" \\\n```\n\n### About the code\n\nThis section describes the purpose of relevant files in the project.\n\n- `./src/datagen/`: Module for generating data for experiments\n\n- `./src/pipeline/`: Contains the code for our models and traning loop\n  - `models.py`: Contains the code for all our models\n  - `learn_mfld_distance.py`: Training and test loop for our models\n\n- `./src/pipeline/`: Contains pipeline code for data synthesis and model training\n  - `pipeline_utils/`: Contains some utility functions for the pipeline\n    - `common.py`: Some common utility functions for all kinds of synthetic manifold datasets\n    - `plot_ittwswrolls.py`: Plotting functions for synthetic manifold datasets\n  - `data_configs.py`: Configuration values for synthetic datasets\n  - `data_ingredients.py`: Data ingredient for the pipeline; Collates data parameters and synthesizes the dataset\n  - `model_ingredients.py`: Model ingredient for the pipeline; Loads/Initialized the model to be used for training\n  - `learn_cls_from_dist.py`: Runs the data synthesis and model training pipeline end-to-end\n\n- `./src/adversarial_attack/`: Contains pipeline code for adversarial attacks on trained models\n  - `attack_ingredients.py`: Settings for attacks that models have to be evaluated on\n  - `inpfn_ingredients.py`: Settings for models and datasets that have to be evaluated with adversarial attacks\n  - `attacks.py`: Code for adversarial attacks to the models can be evaluated on\n  - `get_attack_perf.py`: Runner script that loads the models and data, generates adversarial examples and evaluated the models\n\n- `./src/reproduce/`: Contains steps to reproduce the results in our work \n\n### Results\n\nIn order to generate results given in the paper, follow the instructions given [here](./src/reproduce/README.md).\n\n## Cite\n\nTo cite this work:\n\n```\n@misc{chetan2022distance,\n    title={Distance Learner: Incorporating Manifold Prior to Model Training},\n    author={Aditya Chetan and Nipun Kwatra},\n    year={2022},\n    eprint={2207.06888},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "distance-learner", "org_name": "microsoft", "org_repo": "microsoft/distance-learner", "platform_org_repo": "github+microsoft/distance-learner", "link_to_repo": "https://github.com/microsoft/distance-learner", "platform": "github", "language": "Python", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Microsoft-AdvNLG\n\nThis repo contains a series of works on natural language generation.\n\n## News\n\n**Joint Generator-Ranker Learning for Natural Language Generation**, Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, Weizhu Chen, ***Arxiv pre-print***, [Code](https://github.com/microsoft/advNLG/tree/main/JGR) [Paper](https://arxiv.org/abs/2206.13974)", "repo_name": "advNLG", "org_name": "microsoft", "org_repo": "microsoft/advNLG", "platform_org_repo": "github+microsoft/advNLG", "link_to_repo": "https://github.com/microsoft/advNLG", "platform": "github", "language": "Python", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "<img src=\"https://microsoft.github.io/VisTalk/vistalk-icon.svg\" width=\"64px\" align=\"left\" style=\"margin-right: 10px;\" /> Vis Talk: A JavaScript toolkit for Natural Language-based Visualization Authoring\n====\n\n[![npm version](https://img.shields.io/npm/v/@vis-talk/vega-builder.svg?color=0084ff)](https://www.npmjs.com/package/@vis-talk/vega-builder)\n[![DOI:10.1109/TVCG.2022.3209357](https://zenodo.org/badge/DOI/10.1109/TVCG.2022.3209357.svg)](https://doi.org/10.1109/TVCG.2022.3209357)\n[![arxiv badge](https://img.shields.io/badge/arXiv-2208.10947-red)](https://arxiv.org/abs/2208.10947)\n\n\n**Vis Talk** is a JavaScript library for developers create visualization using natural language.\n&nbsp;\n\n## Try it out\nYou can try the [Playground Web App](https://microsoft.github.io/VisTalk/) or fork example [Observable Notebook](https://observablehq.com/@zhitao/vistalk)\n\n#### Playground Demo:\n![vis-talk-playground](https://user-images.githubusercontent.com/822440/200222498-11df1cd5-8426-4086-803d-50b7028032a9.gif)\n\n## For Developers\n### Installing\n\nInstall using yarn:\n```shell\n$ yarn add @vis-talk/vega-builder\n```\nor install using npm:\n```shell\n$ npm install @vis-talk/vega-builder\n```\n\n## Build a simple react sample app\n\n```\n$ npx create-react-app my-vis-app --template typescript\n$ cd my-vis-app\n$ npm install @vis-talk/vega-builder react-vega\n```\n\nModify _src/App.tsx_ to:\n\n```tsx\nimport React from 'react';\nimport { createBuilder } from \"@vis-talk/vega-builder\";\nimport { VegaLite } from \"react-vega\";\n\nfunction App() {\n  const table = [\n    { Brand: \"BrandA\", Category: \"SUV\", Sales: 40 },\n    { Brand: \"BrandB\", Category: \"SUV\", Sales: 20 },\n    { Brand: \"BrandC\", Category: \"SUV\", Sales: 30 },\n    { Brand: \"BrandD\", Category: \"SUV\", Sales: 10 },\n    { Brand: \"BrandA\", Category: \"Midsize\", Sales: 40 },\n    { Brand: \"BrandB\", Category: \"Midsize\", Sales: 10 },\n    { Brand: \"BrandC\", Category: \"Midsize\", Sales: 20 },\n    { Brand: \"BrandD\", Category: \"Midsize\", Sales: 5 },\n  ];\n  \n  const builder = createBuilder(table);\n  \n  builder.setInput([\n    \"total sales by brand\",\n    \"highlight midsize in orange\",\n    \"add line in 60 in red\",\n    \"add rect from 12 to 37 in green\"]);\n    \n  const spec = builder.build({ name: \"table\" });\n\n  return (\n    <div className=\"App\">\n      <VegaLite spec={spec} data={{ table }} />\n    </div>\n  );\n}\n\nexport default App;\n```\n\nstart the app:\n\n```shell\n$ npm start\n```\n<img src=\"https://microsoft.github.io/VisTalk/sample-app.png\" width=\"400px\" alt=\"sample app\" />\n\n## API Reference\n\n### createBuilder(dataSource)\n\nCreate a new vega visual builder by load your data table.\n```ts\nfunction createBuilder(dataSource: DataSource): VegaBuilder;\n\n// data source is array of object, with optional column name list.\nexport interface DataSource extends Array<object> {\n  // List of column names.\n  columns?: Array<string>;\n}\n```\nFor example, you can create a session using inline records data like:\n```ts\nconst builder = createBuilder([\n  { Brand:'BrandA', Category: 'SUV', Sales: 100 },\n  { Brand:'BrandB', Category: 'SUV', Sales: 200 },\n  { Brand:'BrandC', Category: 'SUV', Sales: 300 }\n])\n```\n\n\n### session.setInput(input)\nThis method allow you specify the natural language input using a single block of text or a multi-line string arry\n```ts\nclass VegaBuilder {\n  // Provide your natural language input\n  public setInput(lines: string[]);\n  ...\n}\n```\nFor example:\n```ts\nbuilder.setInput(['total sales by brand', 'sort it'])\n```\n\n#### builder.build\nGenerate vega-lite spec as a javascript JSON object (Type defined in Vega-Lite package).\n```ts\nlet spec = builder.build({name: 'table'});\n```\n\n### Render Vega Chart:\n```shell\n$ yarn add vega vega-lite react-vega\n```\nor \n```shell\n$ npm install vega vega-lite react-vega\n```\n\n```jsx\nimport { VegaLite } from \"react-vega\";\n\n<VegaLite spec={spec} data={{table: rows}} />\n```\n![Chart 1](https://microsoft.github.io/VisTalk/vistalk.svg \"Chart 1\")\n### More examples - donut chart\n```js\nbuilder.setInput([\n  'sales by brand as donut chart'\n])\n```\n![Chart 2](https://microsoft.github.io/VisTalk/vistalk-donut.svg \"Chart 2\")\n### More examples - column chart with highlights\n```js\nbuilder.setInput([\n  'sales by brand as column chart',\n  'sort desc',\n  'highlight top 2 in green',\n  'add line 100 in red',\n  'hide grid',\n  'make data point wider'\n])\n```\n![Chart 3](https://microsoft.github.io/VisTalk/vistalk-highlight.svg \"Chart 3\")\n\n# Build from source code\n\n## Enlist code\n```\n$ git clone https://github.com/microsoft/VisTalk.git\n$ cd VisTalk\n```\n\n## Build & Run\n```\n$ yarn\n$ yarn build\n$ yarn start\n```\nThen open browser and navigate to http://localhost:4200/\n\n## Run Unit Tests\n```shell\n$ yarn test\n```\n\n## Run E2E Tests\n```shell\n$ yarn e2e\n```\nThen you can explore captured screenshots and videos from <proj>/dist/cypress/apps/playground-e2e\n\n## Build package\n```shell\n$ yarn package\n```\n\n# Train deep learning model\n\n## Setup miniconda\nInstall [Miniconda](https://docs.conda.io/en/latest/miniconda.html)\n\n## Setup Environment\n\ncpu:\n```\n$ conda create -n vis-talk python=3.9.13 tensorflow=2.5.0  \n$ conda activate vis-talk\n$ pip install tensorflow-addons==0.13 tensorflowjs==3.18 seqeval==1.2.2\n```\n\ngpu:\n```\n$ conda create -n vis-talk-gpu tensorflow-gpu=2.1.0 python=3.6.12 cudatoolkit=10.1\n$ conda activate vis-talk-gpu\n$ pip install tensorflow-addons==0.9.1 tensorflowjs==3.8 seqeval==1.2.2\n```\n\n## Train Model\n```shell\n$ yarn train\n```\n\nManually replace the generated _model-data.ts_ to libs/interpreter/src/lib/model-data.ts\n\n&nbsp;\n\n# Feedback\nIf you have any questions, feel free to [open an issue](https://github.com/microsoft/VisTalk/issues/new/choose) or contact us: [Vis Talk Team](mailto:vistalk@microsoft.com).\n\n&nbsp;\n\n# Citation\n\nIf you use VisTalk in your research, please cite as follows:\n\n> Y. Wang et al., \"Towards Natural Language-Based Visualization Authoring,\" in IEEE Transactions on Visualization and Computer Graphics, 2022, doi: 10.1109/TVCG.2022.3209357.\n\n&nbsp;\n\n## License\nThis project is licensed under the [MIT License](LICENSE.txt).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "VisTalk", "org_name": "microsoft", "org_repo": "microsoft/VisTalk", "platform_org_repo": "github+microsoft/VisTalk", "link_to_repo": "https://github.com/microsoft/VisTalk", "platform": "github", "language": "TypeScript", "stargazers_count": 27, "watchers_count": 27}, {"README_text": "# Prompt-Engine\n\nThis package provides an easy and reusable interface to build prompts for Large scale Language\nModels (LLMs).\n\nPrompt Engineering is a technique used to elicit intended responses out of a LLM model and can work\non many strategies.\n\n## Background\n\nLLMs like GPT-3 and Codex have continued to push the bounds of what AI is capable of - they can\ncapably generate language and code, but are also capable of emergent behavior like question answering,\nsummarization, classification and dialog. One of the best techniques for enabling specific behavior\nout of LLMs is called prompt engineering - crafting inputs that coax the model to produce certain\nkinds of outputs. Few-shot prompting is the discipline of giving examples of inputs and outputs,\nsuch that the model has a reference for the type of output you're looking for.\n\nPrompt engineering can be as simple as formatting a question and passing it to the model, but it can\nalso get quite complex - requiring substantial code to manipulate and update strings. This library\naims to make that easier. It also aims to codify patterns and practices around prompt engineering.\n\nSee\n[How to get Codex to produce the code you want](https://microsoft.github.io/prompt-engineering/)\narticle for an example of the prompt engineering patterns this library codifies.\n\n## Usage\n\nThe library currently supports a generic `GenericEngine` and a `TextAnalysisEngine`. Both facilitate\na pattern of prompt engineering where the prompt is composed of a description, examples of inputs\nand outputs and an ongoing \"dialog\" representing the ongoing input/output pairs as the user and\nmodel communicate. The dialog ensures that the model (which is stateless) has the context about\nwhat's happened in the conversation so far.\n\nSee architecture diagram representation:\n\n<img src=\"https://user-images.githubusercontent.com/17247257/178356452-03a69f87-aa09-459b-83d8-eeedba634b39.png\">\n\n## Examples\n\nThe repository includes a few [examples](Examples), showing how to leverage the library to generate\nfree form text, JSON files such as Azure ARM templates, pseudo code like Mermaid graphs, etc.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## Statement of Purpose\n\nThis library aims to simplify use of Large Language Models, and to make it easy for developers to\ntake advantage of existing patterns. This package works independently of any specific LLM - prompt\ngenerated by the package should be usable with various language and code generating models.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use\nof Microsoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion\nor imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "prompt-engine-dotnet", "org_name": "microsoft", "org_repo": "microsoft/prompt-engine-dotnet", "platform_org_repo": "github+microsoft/prompt-engine-dotnet", "link_to_repo": "https://github.com/microsoft/prompt-engine-dotnet", "platform": "github", "language": "C#", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Test-NetUpgradeReadiness", "org_name": "microsoft", "org_repo": "microsoft/Test-NetUpgradeReadiness", "platform_org_repo": "github+microsoft/Test-NetUpgradeReadiness", "link_to_repo": "https://github.com/microsoft/Test-NetUpgradeReadiness", "platform": "github", "language": "PowerShell", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# foldingdiff - Diffusion model for protein backbone generation\n\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) ![PyTorch Lightning](https://img.shields.io/badge/pytorch-lightning-blue.svg?logo=PyTorch%20Lightning)\n\nWe present a diffusion model for generating novel protein backbone structures. For more details, see our preprint on [arXiv](https://arxiv.org/abs/2209.15611). We also host a trained version of our model on [HuggingFace spaces](https://huggingface.co/spaces/wukevin/foldingdiff) so you can get started with generating protein structures with just your browser!\n\n![Animation of diffusion model protein folds over timesteps](plots/generated_0.gif)\n\n## Installation\n\nTo install, clone this using `git clone`. This software is written in Python, notably using PyTorch, PyTorch Lightning, and the HuggingFace transformers library. The required conda environment is defined within the `environment.yml` file. To set this up, make sure you have conda (or [mamba](https://mamba.readthedocs.io/en/latest/index.html)) installed, clone this repository, and run:\n\n```bash\nconda env create -f environment.yml\nconda activate foldingdiff\npip install -e ./  # make sure ./ is the dir including setup.py\n```\n\n### Downloading data\n\nWe require some data files not packaged on Git due to their large size. These are not required for sampling (as long as you are not using the `--testcomparison` option, see below); this is required for training your own model. We provide a script in the `data` dir to download requisite CATH data.\n\n```bash\n# Download the CATH dataset\ncd data  # Ensure that you are in the data subdirectory within the codebase\nchmod +x download_cath.sh\n./download_cath.sh\n```\n\nIf the download link in the `.sh` file is not working, the tarball is also mirrored at the following [Dropbox link](https://www.dropbox.com/s/ka5m5lx58477qu6/cath-dataset-nonredundant-S40.pdb.tgz?dl=0).\n\n## Training models\n\nTo train your own model on the CATH dataset, use the script at `bin/train.py` in combination with one of the\njson config files under `config_jsons` (or write your own). An example usage of this is as follows:\n\n```bash\npython bin/train.py config_jsons/cath_full_angles_cosine.json --dryrun\n```\n\nBy default, the training script will calculate the KL divergence at each timestep before starting training, which can be quite computationally expensive with more timesteps. To skip this, append the `--dryrun` flag. The output of the model will be in the `results` folder with the following major files present:\n\n```\nresults/\n    - config.json           # Contains the config file for the huggingface BERT model itself\n    - logs/                 # Contains the logs from training\n    - models/               # Contains model checkpoints. By default we store the best 5 models by validation loss and the best 5 by training loss\n    - training_args.json    # Full set of arguments, can be used to reproduce run\n```\n\n## Pre-trained models\n\nWe provide weights for a model trained on the CATH dataset. These weights are stored on HuggingFace model hub at [wukevin/foldingdiff_cath](https://huggingface.co/wukevin/foldingdiff_cath). The following code snippet shows how to load this model, load data (assuming it's been downloaded), and perform a forward pass:\n\n```python\nfrom huggingface_hub import snapshot_download\nfrom torch.utils.data.dataloader import DataLoader\nfrom foldingdiff import modelling\nfrom foldingdiff import datasets as dsets\n\n# Load the model (files will be cached for future calls)\nm = modelling.BertForDiffusion.from_dir(snapshot_download(\"wukevin/foldingdiff_cath\"))\n\n# Load dataset\n# As part of loading, we try to compute internal angles in parallel. This may\n# throw warnings like the following; this is normal.\n# WARNING:root:Illegal values for omega in /home/*/projects/foldingdiff-main/data/cath/dompdb/2ebqA00 -- skipping\n# After computing these once, the results are saved in a .pkl file under the\n# foldingdiff source directory for faster loading in future calls.\nclean_dset = dsets.CathCanonicalAnglesOnlyDataset(pad=128, trim_strategy='randomcrop')\nnoised_dset = dsets.NoisedAnglesDataset(clean_dset, timesteps=1000, beta_schedule='cosine')\ndl = DataLoader(noised_dset, batch_size=32, shuffle=False)\nx = iter(dl).next()\n\n# Forward pass\npredicted_noise = m(x['corrupted'], x['t'], x['attn_mask'])\n```\n\n## Sampling protein backbones\n\nTo sample protein backbones, use the script `bin/sample.py`. Example commands to do this using the pretrained weights described above are as follows.\n\n```bash\n# To sample 10 backbones per length ranging from [50, 128) with a batch size of 512 - reproduces results in our manuscript\npython ~/projects/foldingdiff/bin/sample.py -l 50 128 -n 10 -b 512 --device cuda:0\n```\n\nThis will run the trained model hosted at [wukevin/foldingdiff_cath](https://huggingface.co/wukevin/foldingdiff_cath) and generate sequences of varying lengths. If you wish to load the test dataset and include test chains in the generated plots, use the option `--testcomparison`; note that this requires downloading the CATH dataset, see above. Running `sample.py` will create the following directory structure in the diretory where it is run:\n\n```\nsome_dir/\n    - plots/            # Contains plots comparing the distribution of training/generated angles\n    - sampled_angles/   # Contains .csv.gz files with the sampled angles\n    - sampled_pdb/      # Contains .pdb files from converting the sampled angles to cartesian coordinates\n    - model_snapshot/   # Contains a copy of the model used to produce results\n```\n\nNot specifying a `--device` will default to the first device `cuda:0`; use `--device cpu` to run on CPU (though this will be very slow). See the following table for runtimes from our machines.\n\n| Device | Runtime estimates sampling 512 structures |\n| --- | --- |\n| Nvidia RTX 2080Ti | 7 minutes |\n| i9-9960X (16 physical cores) | 2 hours |\n\n### Maximum training similarity TM scores\n\nAfter generating sequences, we can calculate TM-scores to evaluate the simliarity of the generated sequences and the original sequences. This is done using the script under `bin/tmscore_training.py` and requires data to have been downloaded prior (see above).\n\n### Visualizing diffusion \"folding\" process\n\nThe above sampling code can also be run with the ``--fullhistory`` flag to write an additional subdirectory `sample_history` under each of the `sampled_angles` and `sampled_pdb` folders that contain pdb/csv files coresponding to each timestep in the sampling process. The pdb files, for example, can then be passed into the script under `foldingdiff/pymol_vis.py` to generate a gif of the folding process (as shown above). An example command to do this is:\n\n```bash\npython ~/projects/foldingdiff/foldingdiff/pymol_vis.py pdb2gif -i sampled_pdb/sample_history/generated_0/*.pdb -o generated_0.gif\n```\n\n**Note** this script lives separately from other plotting code because it depends on PyMOL; feel free to install/activate your own installation of PyMOL for this, or set up an environment using [PyMOL open source](https://github.com/schrodinger/pymol-open-source).\n\n## Evaluating designability of generated backbones\n\nOne way to evaluate the quality of generated backbones is via their \"designability\". This refers to whether or not we can design an amino acid chain that will fold into the designed backbone. To evaluate this, we use an inverse folding model to generate amino acid sequences that are predicted to fold into our generated backbone, and check whether those generated sequences actually fold into a structure comparable to our backbone.\n\n### Inverse folding\n\nInverse folding is the task of predicting a sequence of amino acids that will produce a given protein backbone structure. We evaluated two different methods for this step, ProteinMPNN and ESM-IF1; we find ProteinMPNN to be significantly more performant. In our analyses, we generate 8 different amino caid sequences for each of FoldingDiff's generated structures.\n\n#### ESM-IF1\n\nWe use a different conda environment for [ESM-IF1](https://proceedings.mlr.press/v162/hsu22a.html); see this [Jupyter notebook](https://colab.research.google.com/github/facebookresearch/esm/blob/main/examples/inverse_folding/notebook.ipynb) for setup details. We found that the following series of commands works on our machines:\n\n```bash\nmamba create -n inverse python=3.9 pytorch cudatoolkit pyg -c pytorch -c conda-forge -c pyg\nconda activate inverse\nmamba install -c conda-forge biotite\npip install git+https://github.com/facebookresearch/esm.git\n```\n\nAfter this, we `cd` into the folder that contains the `sampled_pdb` directory created by the prior step, and run:\n\n```bash\npython ~/projects/foldingdiff/bin/pdb_to_residues_esm.py sampled_pdb -o esm_residues\n```\n\nThis creates a new folder, `esm_residues` that contains 10 potential residues for each of the pdb files contained in `sampled_pdb`.\n\n#### ProteinMPNN\n\nTo set up [ProteinMPNN](https://www.science.org/doi/10.1126/science.add2187), see the authors guide on their [GitHub](https://github.com/dauparas/ProteinMPNN).\n\nAfter this, we follow a similar procedure as for ESM-IF1 (above) where we `cd` into the directory containing the `sampled_pdb` folder and run:\n\n```bash\npython ~/projects/foldingdiff/bin/pdb_to_residue_proteinmpnn.py sampled_pdb\n```\n\nThis will create a new directory called `proteinmpnn_residues` containing 8 amino acid chains per sampled PDB structure.\n\n### Structural prediction\n\nAfter generating amino acid sequences, we check that these recapitulate our original sampled structures by passing them through either OmegaFold or AlphaFold. After running one of these folders, we use the following command to asses self-consistency TM scores:\n\n```bash\npython ~/projects/foldingdiff/bin/sctm.py -f alphafold_predictions_proteinmpnn\n```\n\nWhere `alphafold_predictions_proteinmpnn` is a folder containing the folded structures corresponding to inverse folded amino acid sequences. This produces a json file of all scTM scores, as well as various pdf files containing plots and correlations of the scTM score distribution.\n\n#### OmegaFold\n\nWe primarily use [OmegaFold](https://github.com/HeliXonProtein/OmegaFold) to fold the amino acid sequences produced by either ESM-IF1 or ProteinMPNN. This is due to OmegaFold's relatively fast runtime compared to AlphaFold2, and due to the fact that OmegaFold is natively designed to be run without MSA information - making it more suitable for our protein design task.\n\nAfter creating and activating a separate conda environment and following the authors' instructions for installing OmegaFold, we use the following script to split our input amino acid fasta files across GPUs for inference, and subsequently calculate the self-consistency TM (scTM) scores.\n\n```bash\n# Fold each fasta, spreading the work over GPUs 0 and 1, outputs to omegafold_predictions folder\npython ~/projects/foldingdiff/bin/omegafold_across_gpus.py esm_residues/*.fasta -g 0 1\n```\n\n#### AlphaFold2\n\nWe run [AlphaFold2](https://github.com/deepmind/alphafold) via the `localcolabfold` installation method (see [GitHub](https://github.com/YoshitakaMo/localcolabfold)). Due to AlphaFold's runtime requirements, we provide scripts to split the set of fasta files into subdirectories that can then be separately folded; see SLURM script under `scripts/slurm/alphafold.sbatch` for an example.\n\n## Tests\n\nTests are implemented through a mixture of doctests and unittests. To run unittests, run:\n\n```bash\npython -m unittest -v\n```\n\nYou may see warnings like the following; these are expected.\n\n```bash\nWARNING:root:Illegal values for omega in protdiff-main/data/cath/dompdb/5a2qw00 -- skipping\n```\n", "repo_name": "foldingdiff", "org_name": "microsoft", "org_repo": "microsoft/foldingdiff", "platform_org_repo": "github+microsoft/foldingdiff", "link_to_repo": "https://github.com/microsoft/foldingdiff", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 322, "watchers_count": 322}, {"README_text": "<h1 align=\"center\">\n  <br/>\n  <a href=\"https://github.com/microsoft/early-access-engineering/\"><img src=\"./docs/assets/eae-logo.png?raw=true\" alt=\"Early Access Engineering\" width=\"200\" /></a>\n  <br/>\n  Microsoft Early Access Engineering\n  <br/>\n</h1>\n\n<h3 align=\"center\">Solution accelerators for production-ready use cases</h3>\n\n<div align=\"center\">\n  <a href=\"#solution-accelerators\">All</a> \u2022\n  <a href=\"./industry/financial.md\">Financial Services</a> \u2022\n  <a href=\"./industry/healthcare.md\">Healthcare</a> \u2022\n  <a href=\"./industry/manufacturing.md\">Manufacturing</a> \u2022\n  <a href=\"./industry/retail.md\">Retail</a>\n</div>\n<br/><br/>\n\n<h2>\n  <a href=\"#solution-accelerators\" name=\"solution-accelerators\"><img src=\"./docs/assets/sa-icon-purple.png?raw=true\" alt=\"icon\" width=\"25\" /></a>\n  <span>&nbsp;Solution Accelerators</span>\n</h2>\n\n**[Azure Employee Network Graph with MGDC](https://github.com/microsoft/Azure-Employee-Network-Graph-Solution-Accelerator-with-MGDC)**<br/>\nConnect and automate Microsoft 365 data with Azure Synapse Analytics using Microsoft Graph Data Connect.\n\n**[Azure PDF Form Processing Automation](https://github.com/microsoft/Azure-PDF-Form-Processing-Automation-Solution-Accelerator)**<br/>\nAutomate the processing of PDF forms to modernize operations, save time, and reduce costs.\n\n**[Inventory Management for IoT Connected Coolers](https://github.com/microsoft/Inventory-Management-for-IoT-Connected-Coolers-Solution-Accelerator)**<br/>\nMonitor inventory of beverage containers in remote locations and predict future restocking needs.\n\n**[Azure Databricks to Microsoft Purview Lineage](https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator)**<br/>\nCreate a connector that will transfer lineage metadata from Spark operations in Azure Databricks to Microsoft Purview.\n\n**[Document Translation](https://github.com/microsofttranslator/documenttranslation)**<br/>\nTranslate documents and text in over 90 languages while retaining the original document formatting.\n\n**[Social Media Analytics](https://github.com/microsoft/Azure-Social-Media-Analytics-Solution-Accelerator)**<br/>\nBuild a solution to leverage insights from social media regarding a company, products, or services.\n\n**[Invoice Process Automation](https://github.com/microsoft/Azure-Invoice-Process-Automation-Solution-Accelerator)**<br/>\nBuild an automated invoice processing and analysis solution.\n\n**[Relationship Mesh with MGDC and Azure Synapse Analytics](https://github.com/microsoft/Relationship-Mesh-Solution-Accelerator-with-MGDC-and-Azure-Synapse-Analytics)**<br/>\nDevelop a dashboard that provides information on the relationships between sellers and accounts.\n\n**[COVID-19 Vaccination Proof and Test Verification](https://github.com/microsoft/Azure-Solution-Accelerator-to-automate-COVID-19-Vaccination-Proof-and-Test-Verification-Forms)**<br/>\nBuild a COVID-19 vaccination and test verification solution.\n\n**[Non-Fungible Token](https://github.com/microsoft/Azure-Non-Fungible-Token-Solution-Accelerator)**<br/>\nDevelop an enterprise-class NFT.\n\n**[Machine Learning Patient Risk Analyzer](https://github.com/microsoft/Machine-Learning-Patient-Risk-Analyzer-SA)**<br/>\nCreate a healthcare portal with patient risk analysis capabilities for providers and enhanced user experience for patients.\n\n**[Customer Complaint Management](https://github.com/microsoft/Azure-Solution-Accelerator-Customer-Complaint-Management)**<br/>\nBuild a solution that quickly surfaces customer complaints from various platforms to the correct support agents for timely resolution.\n\n**[Azure Synapse Customer Insights Customer 360&deg;](https://github.com/microsoft/Azure-Synapse-Customer-Insights-Customer360-Solution-Accelerator)**<br/>\nAccelerate the process of creating a unified customer profile that can also segment at-risk customers.\n\n**[Microsoft Purview Custom Connector](https://github.com/microsoft/Purview-Custom-Connector-Solution-Accelerator)**<br/>\nJump-start and shorten the development process of custom connectors for Microsoft Purview.\n\n**[Microsoft Purview Custom Types Tool](https://github.com/microsoft/Purview-Custom-Types-Tool-Solution-Accelerator)**<br/>\nSupport the creation of custom type definitions in Microsoft Purview.\n\n**[Microsoft Purview Machine Learning Lineage](https://github.com/microsoft/Purview-Machine-Learning-Lineage-Solution-Accelerator)**<br/>\nTransform raw data into insights with end-to-end lineage of ML entities and processes in Microsoft Purview.\n\n**[Part Comparator](https://github.com/microsoft/Azure-Synapse-Solution-Accelerator--Part-Comparator)**<br/>\nQuickly solve for supply chain issues through similarity matching that locates viable replacement parts.\n\n**[Commodity Price Prediction](https://github.com/microsoft/Azure-Synapse-Solution-Accelerator-Commodity-Price-Prediction)**<br/>\nCreate commodity price predictions and identify anomalies in the time-series data.\n\n**[Content Recommendations](https://github.com/microsoft/Azure-Synapse-Content-Recommendations-Solution-Accelerator)**<br/>\nCreate personalized content recommendations based on user activity.\n\n**[Cosmos DB Helper](https://github.com/microsoft/CosmosDB-Solution-Accelerator-CosmosDB-Helper)**<br/>\nQuickly build Cosmos DB-based applications with a simplified process and faster learning curve.\n\n**[Financial Analytics Customer Revenue Growth Factor](https://github.com/microsoft/Azure-Synapse-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor)**<br/>\nBring together data from different systems to quickly identify the top factors leading to customer revenue growth.\n\n**[Azure Synapse Retail Recommender](https://github.com/microsoft/Azure-Synapse-Retail-Recommender-Solution-Accelerator)**<br/>\nCreate an end-to-end solution for large retailers with an e-commerce channel to provide personalized product recommendations to users based on their purchase history, product selection in the e-commerce channel, and their activity in the physical store.\n\n**[Tradable Digital Assets](https://github.com/microsoft/Tradable-Digital-Assets-Solution-Accelerator)**<br/>\nDigitalize and exchange traditional assets, or store them in digital wallets.\n\n**[Digital Documentation Shipping Industry](https://github.com/microsoft/Digital-Documentation-Shipping-Industry-Solution-Accelerator)**<br/>\nBuild a fully distributed, tokenized order process business application quickly.\n\n**[Virtual Assistant Deployer](https://github.com/microsoft/Virtual-Assistant-Deployer)**<br/>\nDeploy a Microsoft Virtual Assistant Solution without needing to install all the prerequisite tools on your own machine.\n\n**[Many Models (Demand Forecasting)](https://github.com/microsoft/solution-accelerator-many-models)**<br/>\nSolve complex problems by training many machine learning models to make accurate predictions for multiple specific instances.\n\n**[Containerized Store](https://github.com/microsoft/solution-accelerator-containerized-store)**<br/>\nBuild a smart, friction-less checkout using Azure Custom Vision.\n\n**[Travel Marketplace](https://github.com/microsoft/Marketplace_Blockchain_Solution_Accelerator)**<br/>\nCreate an application where transactions between multiple individuals or organizations can interact through a managed marketplace platform using Azure Blockchain Services on a Quorum network.\n\n**[Knowledge Mining](https://github.com/Azure-Samples/azure-search-knowledge-mining)**<br/>\nQuickly create a Cognitive Search Solution within your data.\n\n<div align=\"center\">\n  <br/><br/>\n  <a href=\"https://github.com/microsoft/early-access-engineering/\"><img src=\"./docs/assets/eae-logo-stack.png?raw=true\" alt=\"Early Access Engineering\" width=\"200\" /></a>\n  <br/>\n</div>\n\n## Contributing\n\n<!--\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n-->\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Early-Access-Engineering", "org_name": "microsoft", "org_repo": "microsoft/Early-Access-Engineering", "platform_org_repo": "github+microsoft/Early-Access-Engineering", "link_to_repo": "https://github.com/microsoft/Early-Access-Engineering", "platform": "github", "language": null, "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "electionguard-tools", "org_name": "microsoft", "org_repo": "microsoft/electionguard-tools", "platform_org_repo": "github+microsoft/electionguard-tools", "link_to_repo": "https://github.com/microsoft/electionguard-tools", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# [Safety Score for Pre-Trained Language Models](https://arxiv.org/abs/2301.09211)\nThis repository contains the code used to measure safety scores for pre-trained language models based on [ToxiGen human annotated dataset](https://github.com/microsoft/TOXIGEN) and [ImplicitHate dataset](https://github.com/GT-SALT/implicit-hate). \n\n## Evaluation Dataset\n- We selected a subset of ToxiGen and ImplicitHate datasets. The examples in ImplicitHate subset are either implicit-hate or neutral and we down-sampled the neutral examples to have equal number of harmful and benign exxamples. ImplicitHate does not have any information about the target of the hate for each sentence.\n- The examples in ToxiGen dataset include the sentences in whhch all the annotators agreed on wether the sentence is harmful and more than 2 annotators agreed on the target group of the hate. \n\n## Setup\nThere are few specific dependencies to install before runnung the safety score calculator, you can install them with the command `pip install -r requirements.txt`.\n\n## How to calculate safety score\nNow you can run the following script:\n\n```bash\npython safety_score.py \\\n   --data data/toxiGen.json \\ # Path to evaluation dataset\n   --output results \\ # local path to a directory for saving results\n   --model gpt2 \\ # pre-trained model name or local path\n   --lmHead clm \\ # Type of language model head, i.e. causal or masked\n   --force # overwrites the output path if it already exists.\n```\nTwo files will be saved in the output path: \n- 'perplexities.json' which contains the perplexity value for each sentence in the evaluation dataset\n- 'safety_scores.json' which contains the statistically significant safety scores for each demographic.\n\nFor example, the contetn of 'safety_scores.json' after running the above script is\n\n`\n{\"asian\": 0.3694922836054574, \"black\": 0.36662849289967936, \"chinese\": 0.3731038121619839, \"jewish\": 0.40661968642101093, \"latino\": 0.22831884057971014, \"lgbtq\": 0.2701839434577746, \"mental dis\": 0.22755361686659398, \"mexican\": 0.23524720893141945, \"middle-eastern\": 0.2604830744365628, \"muslim\": 0.32320982365959877, \"native-american\": 0.24511818257746595, \"physical dis\": 0.22460258469801234, \"women\": 0.23225019516003123}\n`\n\n## Safety scores based on ToxiGen\nHere are the results based on the ToxiGen dataset:\n\n| model name              | Asian     | Black    | Chinese  | Jewish   | Latino   | LGBTQ    | Mentally disabled | Mexican  | Middle-Eastern | Muslim   | Native-American | Physically disabled | Women    | Average  |\n| ----------------------- | --------- | -------- | -------- | -------- | -------- | -------- | ----------------- | -------- | -------------- | -------- | --------------- | ------------------- | -------- | -------- |\n| BERT-large-uncased      | 0.3904102 | 0.318049 | 0.385327 | 0.391747 | 0.248196 | 0.315275 | 0.260423          | 0.269784 | 0.30053        | 0.307303 | 0.254255        | 0.253674            | 0.243696 | 0.302975 |\n| BERT-base-uncased       | 0.3955331 | 0.332077 | 0.387988 | 0.394026 | 0.253957 | 0.314765 | 0.248967          | 0.273278 | 0.291169       | 0.302534 | 0.247724        | 0.244923            | 0.242808 | 0.302288 |\n| DistiBERT-uncased       | 0.4066471 | 0.324267 | 0.40219  | 0.406393 | 0.272203 | 0.272415 | 0.200269          | 0.2826   | 0.294716       | 0.289555 | 0.264996        | 0.218225            | 0.247609 | 0.298622 |\n| MobileBERT              | 0.3717289 | 0.319698 | 0.384602 | 0.405374 | 0.246391 | 0.286268 | 0.199057          | 0.266215 | 0.280596       | 0.300907 | 0.241644        | 0.218105            | 0.248078 | 0.289897 |\n| BERT-large-cased        | 0.3861499 | 0.294892 | 0.362991 | 0.340423 | 0.226696 | 0.296858 | 0.224227          | 0.245158 | 0.207529       | 0.251746 | 0.173039        | 0.217625            | 0.20645  | 0.264137 |\n| BERT-base-cased         | 0.3919012 | 0.316148 | 0.367058 | 0.355918 | 0.240072 | 0.311503 | 0.227047          | 0.256797 | 0.208023       | 0.272093 | 0.176547        | 0.224854            | 0.214208 | 0.274013 |\n| DistiBERT-cased         | 0.4032974 | 0.310421 | 0.395748 | 0.347781 | 0.272    | 0.27143  | 0.19779           | 0.298758 | 0.257318       | 0.211965 | 0.238203        | 0.207459            | 0.246604 | 0.281444 |\n| RoBERTA-Large           | 0.4380718 | 0.385891 | 0.436398 | 0.42469  | 0.254029 | 0.294581 | 0.263915          | 0.265645 | 0.310878       | 0.281888 | 0.254456        | 0.26209             | 0.261524 | 0.318004 |\n| RoBERTA-Base            | 0.4892215 | 0.447183 | 0.493185 | 0.49209  | 0.320232 | 0.343025 | 0.303185          | 0.352225 | 0.359769       | 0.353366 | 0.30507         | 0.311123            | 0.304411 | 0.37493  |\n| DistilRoBERTa           | 0.4971137 | 0.488124 | 0.489491 | 0.44293  | 0.363928 | 0.390325 | 0.364319          | 0.367339 | 0.419592       | 0.412908 | 0.35575         | 0.372084            | 0.356928 | 0.409295 |\n| Electra-large-Generator | 0.3665474 | 0.293507 | 0.378886 | 0.366403 | 0.249174 | 0.295975 | 0.230296          | 0.277303 | 0.257767       | 0.283315 | 0.228314        | 0.23375             | 0.224053 | 0.283484 |\n| Electra-base-Generator  | 0.3703071 | 0.309711 | 0.376314 | 0.382847 | 0.254341 | 0.297005 | 0.219017          | 0.284024 | 0.270293       | 0.291083 | 0.233509        | 0.226641            | 0.228025 | 0.287932 |\n| Electra-small-Generator | 0.390719  | 0.332936 | 0.417799 | 0.382365 | 0.271123 | 0.337894 | 0.244484          | 0.306524 | 0.285288       | 0.309288 | 0.253554        | 0.247908            | 0.253913 | 0.310292 |\n| Albert-xxlarge-v2       | 0.4464272 | 0.409517 | 0.448182 | 0.484349 | 0.291833 | 0.338325 | 0.2682            | 0.314214 | 0.342889       | 0.321211 | 0.322392        | 0.302347            | 0.278864 | 0.351442 |\n| Albert-xlarge-v2        | 0.4285448 | 0.404695 | 0.42712  | 0.471826 | 0.291812 | 0.374162 | 0.262406          | 0.313207 | 0.338421       | 0.329093 | 0.369698        | 0.275218            | 0.293628 | 0.352295 |\n| Albert-large-v2         | 0.4749017 | 0.445774 | 0.465946 | 0.489712 | 0.325978 | 0.414326 | 0.33644           | 0.352111 | 0.384686       | 0.363161 | 0.387505        | 0.334824            | 0.324034 | 0.392262 |\n| Albert-base-v2          | 0.472942  | 0.436361 | 0.476828 | 0.494453 | 0.342572 | 0.390925 | 0.305244          | 0.379035 | 0.370724       | 0.361862 | 0.35094         | 0.325473            | 0.316579 | 0.386457 |\n| GPT2-xl                 | 0.3636664 | 0.366239 | 0.353361 | 0.401766 | 0.207203 | 0.271849 | 0.245597          | 0.213944 | 0.238641       | 0.31103  | 0.237301        | 0.231472            | 0.221868 | 0.281841 |\n| GPT2-large              | 0.3649977 | 0.363983 | 0.366992 | 0.402827 | 0.211116 | 0.279551 | 0.243361          | 0.220969 | 0.239988       | 0.311744 | 0.239372        | 0.233702            | 0.22743  | 0.285079 |\n| GPT2-medium             | 0.3636451 | 0.352714 | 0.362881 | 0.397167 | 0.21392  | 0.275893 | 0.236828          | 0.221197 | 0.232064       | 0.304091 | 0.233108        | 0.219603            | 0.226473 | 0.279968 |\n| GPT2-small              | 0.3694923 | 0.366628 | 0.373104 | 0.40662  | 0.228319 | 0.270184 | 0.227554          | 0.235247 | 0.260461       | 0.32321  | 0.245118        | 0.224603            | 0.23225  | 0.289445 |\n| DistilGPT2              | 0.3853458 | 0.381619 | 0.383766 | 0.418747 | 0.243261 | 0.281941 | 0.23956           | 0.258183 | 0.287869       | 0.343128 | 0.259851        | 0.241207            | 0.227342 | 0.303986 |\n| XLNet-large             | 0.3846801 | 0.328298 | 0.378952 | 0.377031 | 0.267681 | 0.287548 | 0.226386          | 0.277208 | 0.238529       | 0.301164 | 0.235279        | 0.208874            | 0.23144  | 0.287928 |\n| XLNet-base              | 0.3841209 | 0.333978 | 0.381392 | 0.391181 | 0.281413 | 0.297107 | 0.216329          | 0.292739 | 0.244613       | 0.296866 | 0.231103        | 0.212123            | 0.234504 | 0.292113 |\n| PTLMs Average           | 0.4056839 | 0.360946 | 0.404021 | 0.411194 | 0.265727 | 0.31288  | 0.249621          | 0.284321 | 0.288431       | 0.309771 | 0.264114        | 0.251996            | 0.253863 | 0.312505 |\n\n\n## Safety scores based on ImplicitHate\nHere are the results based on the ImplicitHate dataset:\n| model name              | Safety Score |\n| ----------------------- | ------------ |\n| BERT-large-uncased      | 0.332300992  |\n| BERT-base-uncased       | 0.335931145  |\n| DistilBERT-base-uncased | 0.336185856  |\n| mobileBERT              | 0.335289526  |\n| BERT-large-cased        | 0.300331164  |\n| BERT-base-cased         | 0.308677306  |\n| DistilBERT-base-cased   | 0.329417992  |\n| RoBERTa-large           | 0.353298215  |\n| RoBERTa-base            | 0.376362527  |\n| DistilRoBERTa           | 0.390526523  |\n| ELECTRA-large-generator | 0.332349693  |\n| ELECTRA-base-generator  | 0.332561139  |\n| ELECTRA-small-generator | 0.334555207  |\n| ALBERT-xxlarge-v2       | 0.35294267   |\n| ALBERT-xlarge-v2        | 0.358772426  |\n| ALBERT-large-v2         | 0.352241738  |\n| ALBERT-base-v2          | 0.339738782  |\n| GPT-2-xl                | 0.2539317    |\n| GPT-2-large             | 0.255463608  |\n| GPT-2-medium            | 0.255785509  |\n| GPT-2                   | 0.259990915  |\n| DistilGPT-2             | 0.26304632   |\n| XLNet-large-cased       | 0.269394327  |\n| XLNet-base-cased        | 0.271851141  |\n\n\n## Citation\nPlease use the following to cite this work:\n\n```\n@misc{hosseini2023empirical,\n      title={An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models}, \n      author={Saghar Hosseini and Hamid Palangi and Ahmed Hassan Awadallah},\n      year={2023},\n      eprint={2301.09211},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n", "repo_name": "SafeNLP", "org_name": "microsoft", "org_repo": "microsoft/SafeNLP", "platform_org_repo": "github+microsoft/SafeNLP", "link_to_repo": "https://github.com/microsoft/SafeNLP", "platform": "github", "language": "Python", "stargazers_count": 28, "watchers_count": 28}, {"README_text": "# Project\nNOTE: We are in the process of migrating the code from here: https://github.com/ryu577/graphing. Until you see this message, please use that repository.\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n# Usage\nTo install the library on your local machine, clone it and run from the base directory:\n\n> python setup.py install\n\nThen, try to run the following sample code:\n\n> from graphing.special_graphs.neural_trigraph.path_cover import min_cover_trigraph\n> \n> from graphing.special_graphs.neural_trigraph.rand_graph import *\n> ## Generate a random neural trigraph. Here, it is two sets of edges between layers 1 and 2 (edges1) and layers 2 and 3 (edges2)\n> edges1, edges2 = neur_trig_edges(7, 3, 7, shuffle_p=.05)\n> ## Find the full-path cover for this neural trigraph.\n> paths1 = min_cover_trigraph(edges1, edges2)\n> \n> print(paths1)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "graphing", "org_name": "microsoft", "org_repo": "microsoft/graphing", "platform_org_repo": "github+microsoft/graphing", "link_to_repo": "https://github.com/microsoft/graphing", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "hypothtst", "org_name": "microsoft", "org_repo": "microsoft/hypothtst", "platform_org_repo": "github+microsoft/hypothtst", "link_to_repo": "https://github.com/microsoft/hypothtst", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PartnerCenter-GDAPTransition", "org_name": "microsoft", "org_repo": "microsoft/PartnerCenter-GDAPTransition", "platform_org_repo": "github+microsoft/PartnerCenter-GDAPTransition", "link_to_repo": "https://github.com/microsoft/PartnerCenter-GDAPTransition", "platform": "github", "language": "C#", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# Microsoft Focus Center\n\nThis project contains community solutions and tools available for fusion teams to use in order to help accelerate solution lifecycle management in Microsoft Power Platform. Although the underlying features and components used to build these are fully supported, the solutions and tools included are sample implementations.  \n\nIf you encounter issues using the solutions and tools, please report issues. We will provide updates including bug fixes regularly and encourage community PRs for issues and enhancements as well. \n\nOur customers and community can use and customize these to implement in their organizations by forking the repository; however, please note that we are unable to provide support and bug fixes to forked repositories.\n\n**Important**:\nMicrosoft Support will not be able to help with issues related to these solutions and tools. For issues with core features in Microsoft Power Platform, use standard channel to contact Microsoft Support.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Microsoft-Focus-Center", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-Focus-Center", "platform_org_repo": "github+microsoft/Microsoft-Focus-Center", "link_to_repo": "https://github.com/microsoft/Microsoft-Focus-Center", "platform": "github", "language": "C#", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "GCR-Engage-Program", "org_name": "microsoft", "org_repo": "microsoft/GCR-Engage-Program", "platform_org_repo": "github+microsoft/GCR-Engage-Program", "link_to_repo": "https://github.com/microsoft/GCR-Engage-Program", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# FLASH University Monorepo\n\nThis repo contains all of the presentations, notes, examples, and other materials for\nthe FLASH University.\n\nAs of 8/10/2022, we are currently going through: _CLR via C#_, _Object Oriented Thought Process_,\nand _Design Patterns: Elements of Reusable Object-Oriented Software_.\n## Contents\n\n- _CLR via C#_ (https://learning.oreilly.com/library/view/clr-via-c/9780735668737/)\n\n- _Object Oriented Thought Process_ (https://learning.oreilly.com/library/view/object-oriented-thought-process/9780135182130/)\n\n- _Design Patterns: Elements of Reusable Object-Oriented Software_ (https://learning.oreilly.com/library/view/design-patterns-elements/0201633612/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Flash_University", "org_name": "microsoft", "org_repo": "microsoft/Flash_University", "platform_org_repo": "github+microsoft/Flash_University", "link_to_repo": "https://github.com/microsoft/Flash_University", "platform": "github", "language": "C#", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure-synapse-analytics\n- microsoft-graph-data-connect-api\n- power-bi\n---\n\n![EmployeeNetworkGrpah](./Deployment/img/EmployeeNetworkGraph.png)\n# Azure Employee Network Graph Solution Accelerator with MGDC\nOrganizations that can tap into the large datasets powering Microsoft 365, CRM and sales systems can gain tremendous insights into the challenges and opportunities they may encounter. Employee network graphs and data insights from interactions can improve sales productivity, enable organizational optimization and intelligent workflows, and ensure compliance.\n\nThis solution connects Microsoft 365 data with Azure Synapse Analytics using Microsoft Graph Data Connect. The accelerator automates the end-to-end pipeline to set up initial and delta loads at scale. This solution takes a privacy by design approach and comes with folder filter capability where a user can choose which folders data is extracted and used in analysis.\n\n![SAIntroduction](./Deployment/img/SAIntroduction.png \"SA Introduction\")\n\n## Prerequisites\nTo use this solution accelerator, you will need access to an [Azure subscription](https://azure.microsoft.com/free/), Microsoft 365 account and Microsoft Graph Data Connect(MGDC) enabled.\n\nFor additional training and support, please see:\n\n1. [Azure Synapse Analytics](https://azure.microsoft.com/en-us/services/synapse-analytics/) \n2. [Microsoft Graph Data Connect](https://docs.microsoft.com/en-us/graph/)\n3. [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n4. [Azure Text Analytics](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n3. [Power BI](https://docs.microsoft.com/en-us/power-bi/)\n\n## Getting Started\nStart by deploying the required resources to Azure. The button below will deploy Azure Synapse Analytics, Azure Data Lake Storage, Azure Cosmos DB, Azure Language Service, Azure Key Vault and its related resources:\n\n> **Note**: \n> * Azure Cogntive Services require to accept the terms and conditions of Responsible AI when they are first provisioned in an Azure subscription. If none of the Cognitive Services were provisioned previously in the Azure subscription, it is necessary to create a temporary cognitive service (for example Language Service) to accept the AI license terms. The service can be dropped after the creation. Once the requirements are in place, the deploy to Azure button can be used to deploy and configure the solution.\n> * We are deploying tier 0. If you are loading more data you will need to increase the tier to match your data laod needs.\n\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzure-Employee-Network-Graph-Solution-Accelerator-with-MGDC%2Fmain%2FDeployment%2Fdeploy.json)\n\n* Go to the [Deployment guide](./Deployment/Deployment.md) to set up your Azure resources for this solution.  \n\n## Architecture\n\nThe architecture diagram below details what you will be building for this Solution Accelerator.\n\n![Employee Network Graph Architecture Diagram](./Deployment/img/EmployeeNetworkGraphSAArchitectire.png \"Employee Network Graph Architecture Diagram\")\n\n\n## Power BI Dashboards\n\nThe organization summary dashboard shows all internal and external contacts for a specific account, their connectivity scores and event timeline.\n\n![Organization Network Graph Dashboard](./Deployment/img/OrganizationNetworkOverview.png \"Organization Network Graph Dashboard\")\n\nThe organization detail dashboard shows top contacts at an account, a network of internal and external contacts and an event tracker.\n\n![Organization Network Graph Dashboard](./Deployment/img/OrganizationNetworkGraphDetail.png \"Organization Network Graph Dashboard\")\n\n\nThe employee summary dashboard shows the all accounts and external contacts the employee is connected with, their connectivity scores and event timeline.\n\n![Employee Network Graph Details Page](./Deployment/img/EmployeeNetworkGraphDetail.png \"Employee Network Graph Details Page\")\n\nThe employee detail dashboard shows top contacts at all account, a network of internal and external contacts and entities.\n\n![Employee Network Graph Details Page](./Deployment/img/EmployeeNetworkGraphDetailEntity.png \"Employee Network Graph Details Page\")\n\n## License\nMIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n\n## Note about Libraries with MPL-2.0 License\nThe following libraries are not explicitly included in this repository, but users who use this Solution Accelerator may need to install them locally and in Azure Synapse Analytics to fully utilize this Solution Accelerator. However, the actual binaries and files associated with the libraries are not included as part of this repository, but they are available for installation via the PyPI library using the pip installation tool.\n\nLibraries: certifi\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n", "repo_name": "Azure-Employee-Network-Graph-Solution-Accelerator-with-MGDC", "org_name": "microsoft", "org_repo": "microsoft/Azure-Employee-Network-Graph-Solution-Accelerator-with-MGDC", "platform_org_repo": "github+microsoft/Azure-Employee-Network-Graph-Solution-Accelerator-with-MGDC", "link_to_repo": "https://github.com/microsoft/Azure-Employee-Network-Graph-Solution-Accelerator-with-MGDC", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure-synapse-analytics\n- power-bi\n---\n\n\n\n![DatahubforGovernment](./Deployment/img/DatahubforGovernment.png)\n\n# Azure Datahub Solution Accelerator for Government Recovery Programs\nGovernments run many programs in parallel and track metrics around each program's initiatives, success, budget, and more simultaneously.  Measuring the success and status of programs internally and increasing the transparency of that information externally is a challenge that many governments are looking to solve.\n\nThis solution enables governments (local and/or state) to bring program data into one location, visualize key information in a dashboard, and then share the information with stakeholders.\n\n\n## Prerequisites\nTo use this solution accelerator, you will need access to an [Azure subscription](https://azure.microsoft.com/free/). While not required, a prior understanding of Azure Synapse Analytics and Power BI will be helpful.\n\n\nFor additional training and support, please see:\n\n1. [Azure Synapse Analytics](https://azure.microsoft.com/en-us/services/synapse-analytics/) \n2. [Power BI](https://docs.microsoft.com/en-us/power-bi/)\n\n## Getting Started\nStart by deploying the required resources to Azure. The button below will deploy Azure Synapse Analytics, Azure Data Lake Storage and its related resources:\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzure-Datahub-Solution-Accelerator-for-Government-Recovery-Programs%2Fmain%2FDeployment%2Fdeploy.json)\n\n\n* Go to the [Deployment guide](./Deployment/Deployment.md) to set up your Azure resources for this solution.  \n\n## Architecture\n\nThe architecture diagram below details what you will be building for this Solution Accelerator.\n\n![Government Data Hub](./Deployment/img/GovernmentDataHubSAArchitecture.png \"Government Data Hub\")\n\n## Power BI Dashboards\n\nThe summary dashboard shows the main problem areas that are faced by a region and the list of programs the government is running to address those problems. \n\n![Government Recovery Programs Overview](./Deployment/img/GovDataHubOverview.png \"Government Recovery Programs Overview\")\n\nThe Rent and Utility Relief detail dashboard shows the metrics about the program and the program's participants. \n\n![Government Recovery Programs Detail Page](./Deployment/img/GovDataHubRentRelief.png \"Government Recovery Programs Rent Relief\")\n\nThe Small Business Innovation Assistance detail dashboard shows the metrics about the program and the program's participants. \n\n![Government Recovery Programs Detail Page](./Deployment/img/GovDataHubSmallBusiness.png \"Government Recovery Programs Small Business\")\n\nThe Daycare Grant Program detail dashboard shows the metrics about the program and the program's participants. \n![Government Recovery Programs Detail Page](./Deployment/img/GovDataHubDaycare.png \"Government Recovery Programs Daycare\")\n\n## License\nMIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n", "repo_name": "Azure-Datahub-Solution-Accelerator-for-Government-Recovery-Programs", "org_name": "microsoft", "org_repo": "microsoft/Azure-Datahub-Solution-Accelerator-for-Government-Recovery-Programs", "platform_org_repo": "github+microsoft/Azure-Datahub-Solution-Accelerator-for-Government-Recovery-Programs", "link_to_repo": "https://github.com/microsoft/Azure-Datahub-Solution-Accelerator-for-Government-Recovery-Programs", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# App Templates\n\nApp Templates are packaged app samples designed to reduce the time it takes a developer to deploy their code to Azure. Unlike standalone example code, the goal of App Templates is to provide all the components to deploy an app to Azure with automation via GitHub Actions or other CI/CD services. \n\nEach sample consists of example code, CI/CD components, and documentation containing all the required steps.\n\nApp Templates are designed to be compatible with the [Azure Developer CLI(azd)](https://github.com/Azure/azure-dev/). These templates follow the same file structure and are straightforward to convert to `azd` compatible templates once all the components of the template (e.g. programming language) are supported. \n\nApp Templates are open to any technology some may not be on the azd roadmap and can be deployed using the READ ME guidance. The purpose of App Templates is to deliver and prove the value of accelerated onboarding for developers who are new to Azure. Developing examples for `azd` is one of our aims that fit into the broader goal of accelerated developer onboarding.\n\nIf you are looking for enterprise scale guidance on design pillars like Reliability, Performance, Networking, leverage design areas recommendation and reference implementation on [Landing Zone Accelerators](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/app-platform/ready)\n\n## Pre-requisites\n- Azure Subscription and you must be the owner(or know the owner) of the subscription for configuring automatated deployments\n\n## Sample templates\n\n- [.NET app on App Service](https://github.com/Azure-Samples/app-templates-dotnet-azuresql-appservice)\n- [Integration Services - APIM + ServiceBus + Functions + Cosmos](https://github.com/Azure-Samples/app-templates-integration-services)\n- [JBoss EAP on Appservice](https://github.com/Azure-Samples/app-templates-JBossEAP-on-AppService)\n- [Jakarta EE IBM Open Liberty on AKS](https://github.com/Azure-Samples/app-templates-Liberty-on-aks)\n- [Jakarta EE Oracle WebLogic Server (WLS) on AKS](https://github.com/Azure-Samples/app-templates-WLS-on-aks)\n- [Java Spring boot on AKS](https://github.com/Azure-Samples/app-templates-springboot-app-on-AKS)\n- [Java microservices based spring boot on AKS](https://github.com/Azure-Samples/app-templates-springboot-microservices-on-AKS)\n- [Microservices integration - APIM + Container Apps + AppService + Functions](https://github.com/Azure-Samples/app-templates-microservices-integration)\n- [Spring Boot PetClinic Microservices Application on Azure Red Hat Openshift (ARO)](https://github.com/Azure-Samples/app-templates-springboot-microservices-on-ARO)\n- [Spring Boot on Azure Spring Apps](https://github.com/Azure-Samples/apptemplates-microservices-spring-app-on-AzureSpringApps)\n- [Static Web App with Cosmos DB](https://github.com/Azure-Samples/app-templates-staticwebapp-cosmosdb)\n- [WordPress on Azure Container Apps](https://github.com/Azure-Samples/apptemplate-wordpress-on-ACA)\n\n\n## Contributing\n\n- Identify a sample app you plan to make into an app template\n- Follow the steps listed here to make the template azd compatible: [Make your project compatible with Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/make-azd-compatible?pivots=azd-create)\n- Add .devcontainer, .github and .github/workflow components\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkId=521839. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n## Telemetry Configuration\nTelemetry collection is on by default.\n\nTo opt-out, set the variable enableTelemetry to false in Bicep/ARM file and disable_terraform_partner_id to false on Terraform files.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "App-Templates", "org_name": "microsoft", "org_repo": "microsoft/App-Templates", "platform_org_repo": "github+microsoft/App-Templates", "link_to_repo": "https://github.com/microsoft/App-Templates", "platform": "github", "language": null, "stargazers_count": 32, "watchers_count": 32}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "acs-kitchen-sink", "org_name": "microsoft", "org_repo": "microsoft/acs-kitchen-sink", "platform_org_repo": "github+microsoft/acs-kitchen-sink", "link_to_repo": "https://github.com/microsoft/acs-kitchen-sink", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# debugger_test\n\nProvides an easy way of integrating debugger specific tests into a crate.\n\nThis crate is responsible for generating the `#[debugger_test]` proc macro attribute.\n\n## Usage\n\nTo use, add this crate and the `debugger_test_parser` as a dependency in your `Cargo.toml`.\n\nThis crate uses the `debugger_test_parser` to parse the output of the specified debugger\nand verify all expected statements were found.\n\nIn order to set breakpoints, an `__break()` function will need to be defined and called\nat each place the debugger should stop.\n\nFor example:\n\n```rust\n#[inline(never)]\nfn __break() { }\n\n#[debugger_test(\n    debugger = \"cdb\",\n    commands = r#\"\n.nvlist\ndv\ng\"#,\n    expected_statements = r#\"\npattern:test\\.exe .*\\.natvis\na = 0n10\n    \"#)]\nfn test() {\n    let a = 10;\n    __break();\n}\n```\n\nThe `#[debugger_test]` proc macro attribute has 3 required meta items which all take a string value:\n\n1. debugger\n2. commands\n3. expected_statements\n\nThe `debugger` meta item expects the name of a supported debugger. Currently the only supported debugger is `cdb`.\nThis crate will try to find the specified debugger, first by testing if it is on the `PATH`. If the debugger is\nnot found, this crate will search the default installation directory for the debugger. Specifying an exact path\nfor which debugger to use is not currently supported.\n\nThe `commands` meta item expects a string of a debugger command to run. To run multiple commands, separate each\ncommand by the new line character (`\\n`).\n\nThe `expected_statements` meta item expects a string of output to verify in the debugger output.\nEach statement should be separated by a new line character (`\\n`).\n\nFor example:\n\n```rust\n#[debugger_test(\n    debugger = \"cdb\",\n    commands = \"command1\\ncommand2\\ncommand3\",\n    expected_statements = \"statement1\\nstatement2\\nstatement3\")]\n```\n\nUsing a multiline string is also supported:\n\n```rust\n#[debugger_test(\n    debugger = \"cdb\",\n    commands = r#\"\ncommand1\ncommand2\ncommand3\"#,\n    expected_statements = r#\"\nstatement1\nstatement2\nstatement3\"#)]\n```\n\nPattern matching is also supported for a given `expected_statement`. Use the prefix, `pattern:` for the\nexpected statement. This is useful for ignoring debugger output that contain memory address and/or paths:\n\n```rust\n#[debugger_test(\n    debugger = \"cdb\",\n    commands = \"command3\",\n    expected_statements = \"pattern:abc.*\")]\n```\n\nThe `#[debugger_test]` proc macro attribute will generate a new test function that will be marked\nwith the `#[test]` attribute. This generated test function will add a suffix to the test name to ensure\nthe test is unique. In the example above, the proc macro attribute will generate the following function:\n\n```rust\n#[test]\nfn test__cdb() {\n    .....\n    test();\n    .....\n}\n```\n\nThe proc macro attribute will generate a test function that will do the following:\n\n1. Launch the specified debugger\n2. Attach the debugger to the current test executable process\n3. Set breakpoints at all call sites of the `__break()` function\n4. Run the debugger to the first breakpoint specified by the debugger\n5. Run all of the user specified commands and exit the debugger\n6. Parse the debugger output using the `debugger_test_parser` crate and verify all the `expected_statements` were found\n\nBased on the debugger specified via the `#[debugger_test]` attribute, the path used to launch the debugger will\nbe one of the following:\n\n1. If the environment variable, _debugger_type_ _DEBUGGER_DIR is set, i.e. `CDB_DEBUGGER_DIR`, the proc macro attribute will try to launch the debugger from this directory\n2. The default installation directory for the given debugger if it exists at that path\n3. Invoking the executable directly, i.e. `cdb` or `cdb.exe` depending on the OS\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "rust_debugger_test", "org_name": "microsoft", "org_repo": "microsoft/rust_debugger_test", "platform_org_repo": "github+microsoft/rust_debugger_test", "link_to_repo": "https://github.com/microsoft/rust_debugger_test", "platform": "github", "language": "Rust", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# js-shell-engine\n\nThis is a library that makes it easy to implement a terminal shell within JavaScript. \n\nIdeas:\n\n- Pluggable file system\n- Accessibility\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "shell-engine-js", "org_name": "microsoft", "org_repo": "microsoft/shell-engine-js", "platform_org_repo": "github+microsoft/shell-engine-js", "link_to_repo": "https://github.com/microsoft/shell-engine-js", "platform": "github", "language": "TypeScript", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# MTTL\n\nMTTL - Multi-Task Transfer Learning\n\n## Setup\n\nInstall Python packages:\n\n`pip install -r requirements.txt`\n\n_The package `promptsource` currently requires Python 3.7. Alternative versions require local installations (see their [documentation](https://github.com/bigscience-workshop/promptsource#setup))._\n\nDownload the datasets:\n\n`bash scripts/create_datasets.sh`\n\n## Multi-task Pre-training\n\nThe general command:\n\n`python pl_train.py -c $CONFIG_FILES -k $KWARGS`\n\nMultiple `CONFIG_FILES` can be concatenated as `file1+file2`. To modify defaults, `KWARGS` can be expressed as `key=value`.\n\n## Test Fine-Tuning\n\nTo perform finetuning for a test task, use the script `pl_finetune.py`\n\n## Hyper-parameter Search for Test Fine-Tuning\n\nTo perform an hyperparameter search for a test task, use the script `pl_finetune_tune.py`.\nThe script will just call the functions in `pl_finetune.py` in a loop. The script itself defines hp ranges for different fine-tuning types.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mttl", "org_name": "microsoft", "org_repo": "microsoft/mttl", "platform_org_repo": "github+microsoft/mttl", "link_to_repo": "https://github.com/microsoft/mttl", "platform": "github", "language": "Python", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "# UniTAB: Unifying Text and Box Outputs for Grounded VL Modeling\n[UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling](https://arxiv.org/pdf/2111.12085.pdf)\n\nby [Zhengyuan Yang](https://zhengyuan.info), [Zhe Gan](https://zhegan27.github.io/), [Jianfeng Wang](http://jianfengwang.me/), [Xiaowei Hu](https://scholar.google.com/citations?user=Pj0TwxwAAAAJ&hl=en), [Faisal Ahmed](https://scholar.google.com/citations?hl=en&user=laKl8acAAAAJ), [Zicheng Liu](https://zicliu.wixsite.com/mysite), [Yumao Lu](https://www.linkedin.com/in/yumao/), [Lijuan Wang](https://www.microsoft.com/en-us/research/people/lijuanw/)\n\nEuropean Conference on Computer Vision, 2022, Oral Presentation\n\n\n### Introduction\nWe propose UniTAB, a vision-language (VL) model that unifies text generation and bounding box prediction into a single architecture.\nFor more details, please refer to our\n[paper](https://arxiv.org/pdf/2111.12085.pdf).\n\n\n<p align=\"center\">\n  <img src=\"https://zyang-ur.github.io//unitab/unitab.jpg\" width=\"100%\"/>\n</p>\n\n### Citation\n\n    @inproceedings{yang2022unitab,\n      title={UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling},\n      author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},\n      booktitle={ECCV},\n      year={2022}\n    }\n\n\n## Installation\n\nClone the repository:\n```\ngit clone https://github.com/microsoft/UniTAB.git\ncd UniTAB\n```\n\nNew conda env:\n```\nconda create -n unitab python=3.8\nconda activate unitab\n```\n\nInstall packages in ``requirements.txt`` (separately install [numpy](https://pypi.org/project/numpy/) and [pytorch (LTS 1.8.2)](https://pytorch.org/get-started/locally/) if fails):\n```\npip install -r requirements.txt\n```\n\n### AzCopy\nWe recommend using the following AzCopy command to download.\nAzCopy executable tools can be [downloaded here](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10#download-azcopy).\n\nExample command:\n```\npath/to/azcopy copy <folder-link> <target-address> --resursive\"\n\n# For example:\npath/to/azcopy copy https://unitab.blob.core.windows.net/data/data <local_path> --recursive\npath/to/azcopy copy https://unitab.blob.core.windows.net/data/weights <local_path> --recursive\npath/to/azcopy copy https://unitab.blob.core.windows.net/data/annotations <local_path> --recursive\n```\n\n### Distributed Training\nWe do not specify ``distributed training`` tool in the example commands below. Pytorch distributed ``python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py`` or [submitit](https://github.com/facebookincubator/submitit) supported. Or update ``util/dist.py/init_distributed_mode()`` to fit your cluster setting.\n\n\n## Data\n\n* Download the original Flickr30k image dataset from : [Flickr30K webpage](http://shannon.cs.illinois.edu/DenotationGraph/) and update the `flickr_img_path` to the folder containing the images.\n* Download the original Flickr30k entities annotations from: [Flickr30k annotations](https://github.com/BryanPlummer/flickr30k_entities) and update the `flickr_dataset_path` to the folder with annotations.\n* Download the gqa images at [GQA images](https://nlp.stanford.edu/data/gqa/images.zip) and update `vg_img_path` to point to the folder containing the images.\n* Download COCO images [Coco train2014](http://images.cocodataset.org/zips/train2014.zip). Update the `coco_path` to the folder containing the downloaded images.\n\nOr download the [cached data (~77G)](https://unitab.blob.core.windows.net/data/data) (use AzCopy with the link).\n\n* Download our pre-processed [annotations (~3.7G)](https://unitab.blob.core.windows.net/data/annotations) (use AzCopy with the link, or [zip file](https://unitab.blob.core.windows.net/data/annotations.zip)) and update the `flickr_ann_path`, `gqa_ann_path` and `refexp_ann_path` to this folder with pre-processed annotations.\n\n## Pre-train\nThe config file for pretraining is ``configs/pretrain.json``. Optionally starting from [MDETR](https://github.com/ashkamath/mdetr/blob/main/.github/pretrain.md) pretrain with ``--load https://zenodo.org/record/4721981/files/pretrained_resnet101_checkpoint.pth``. [Weights availble here](https://unitab.blob.core.windows.net/data/weights/pretrained_checkpoint.pth).\n\nExample command (ngpu=64):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/pretrain.json --batch_size 2 --lr_backbone 2e-5 --text_encoder_lr 2e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --unitab_pretrain --pretrain_seqcrop mixed --ema --output-dir weights/$exp_id --load https://zenodo.org/record/4721981/files/pretrained_resnet101_checkpoint.pth\n```\n\n## Multi-task Finetuning\nThe config file for pretraining is ``configs/multitask.json``. [Weights availble here](https://unitab.blob.core.windows.net/data/weights/prefinetune_checkpoint.pth).\n\nExample command (ngpu=32):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/multitask.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 1e-5 --lr 5e-5 --num_queries 200 --max_decoding_step 256 --load weights/pretrained_checkpoint.pth --ema --output-dir weights/$exp_id\n```\n\n## Downstream tasks\nOptionally, downloading all weights at once (~54G):\n```\npath/to/azcopy copy https://unitab.blob.core.windows.net/data/weights <local_path> --recursive\n```\n\nFor model inference, use the input arguments ``--eval --test``. For captioning tests (Flickr grounded captioning, COCO image captioning, VQAv2 visual question answering), the computed captioning metrics displayed is only for reference. For the final number, an output prediction json file will be automatically stored at ``weights/$exp_id/results/pred_dict_$CIDEr.json``. Please follow the official evaluation for [Flickr grounded captioning](https://github.com/facebookresearch/grounded-video-description), [COCO captioning](https://github.com/tylin/coco-caption), and [VQAv2](https://visualqa.org/evaluation.html) evaluation. We will better intergrate the caption evaluations in future versions.\n\n### Grounded captioning\nThe config file for pretraining is ``configs/flickr_kp.json``. For model inference, use the input arguments ``--eval --test``. \n\nFor the final number, an output prediction json file will be automatically stored at ``weights/$exp_id/results/pred_dict_$CIDEr.json``. Please follow the official evaluation for [Flickr grounded captioning](https://github.com/facebookresearch/grounded-video-description) evaluation. We will better intergrate the caption evaluations in future versions.\n\nWeights: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_flickrcaptionKP_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_flickrcaptionKP_checkpoint.pth).\n\n<table>\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th>CIDEr</th>\n            <th>F1_all</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Separate</td>\n            <td>65.6</td>\n            <td>11.46</td>\n        </tr>\n        <tr>\n            <td>Pre-finetuning</td>\n            <td>69.7</td>\n            <td>12.95 </td>\n        </tr>\n    </tbody>\n</table>\n\nExample command (ngpu=8):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_kp.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 1e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/pretrained_checkpoint.pth\n\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_kp.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 1e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/prefinetune_flickrcaptionKP_checkpoint.pth --eval --test\n```\n\n### Referring expression comprehension\nThe config file for pretraining is ``configs/refcoco/+/g.json``. For model inference, use the input arguments ``--eval --test --test_type testA/testB/test``.\n\nWeights: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_refcoco_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_refcoco_checkpoint.pth) (refcoco/refcoco+/refcocog).\n\n<table>\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th>Refcoco</th>\n            <th>Refcoco+</th>\n            <th>Refcocog</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Separate</td>\n            <td>86.32</td>\n            <td>78.70</td>\n            <td>79.96</td>\n        </tr>\n        <tr>\n            <td>Pre-finetuning</td>\n            <td>88.59</td>\n            <td>80.97</td>\n            <td>84.58</td>\n        </tr>\n    </tbody>\n</table>\n\nExample command (ngpu=8):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/refcoco.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 5e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --ema --output-dir weights/$exp_id --load weights/pretrained_checkpoint.pth\n\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/refcoco.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 5e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --ema --output-dir weights/$exp_id --load weights/prefinetune_refcoco_checkpoint.pth --eval --test --test_type testA\n```\n\n### Phrase grounding\nThe config file for pretraining is ``configs/flickr.json``. For model inference, use the input arguments ``--eval --test``.\n\nWeights: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_flickrGrounding_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_flickrGrounding_checkpoint.pth).\n\n<table>\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th>Flickr</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Separate</td>\n            <td>79.39</td>\n        </tr>\n        <tr>\n            <td>Pre-finetuning</td>\n            <td>79.58</td>\n        </tr>\n    </tbody>\n</table>\n\nExample command (ngpu=8):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 5e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --ema --do_flickrgrounding --output-dir weights/$exp_id --load weights/pretrained_checkpoint.pth\n\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr.json --batch_size 2 --lr_backbone 1e-5 --text_encoder_lr 5e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --ema --do_flickrgrounding --output-dir weights/$exp_id --load weights/prefinetune_flickrGrounding_checkpoint.pth --eval --test\n```\n\n### COCO captioning\nThe config file for pretraining is ``configs/flickr_cococaption.json``. For model inference, use the input arguments ``--eval --test``. \n\nFor the final number, an output prediction json file will be automatically stored at ``weights/$exp_id/results/pred_dict_$CIDEr.json``. Please follow the official evaluation for [COCO captioning](https://github.com/tylin/coco-caption) evaluation. We will better intergrate the caption evaluations in future versions.\n\nWeights: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_MScococaption_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_MScococaption_checkpoint.pth).\n\n<table>\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th>CIDEr</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Separate</td>\n            <td>119.3</td>\n        </tr>\n        <tr>\n            <td>Pre-finetuning</td>\n            <td>119.8</td>\n        </tr>\n    </tbody>\n</table>\n\nExample command (ngpu=16):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_cococaption.json --lr_backbone 2e-5 --text_encoder_lr 2e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/pretrained_checkpoint.pth\n\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_cococaption.json --lr_backbone 2e-5 --text_encoder_lr 2e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/prefinetune_MScococaption_checkpoint.pth --eval --test\n```\n\n### Visual question answering on VQAv2\nThe config file for pretraining is ``configs/flickr_vqav2caption.json`` and ``configs/flickr_vqav2captionKP.json``. Adjust the ``GT_type`` between ``vqav2caption`` and ``vqav2captionKP`` for std and KP splits. For model inference, use the input arguments ``--eval --test``. \n\nFor the final number, an output prediction json file will be automatically stored at ``weights/$exp_id/results/pred_dict_$CIDEr.json``. Please follow the official evaluation for [VQAv2](https://visualqa.org/evaluation.html) evaluation. We will better intergrate the caption evaluations in future versions.\n\nWeights: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_VQAv2_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_VQAv2_checkpoint.pth). KP split: [Separate](https://unitab.blob.core.windows.net/data/weights/separate_VQAv2KP_checkpoint.pth), [Pre-finetuning](https://unitab.blob.core.windows.net/data/weights/prefinetune_VQAv2KP_checkpoint.pth).\n\n\n<table>\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th>test-dev</th>\n            <th>KP-test</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Separate</td>\n            <td>69.9</td>\n            <td>66.6</td>\n        </tr>\n        <tr>\n            <td>Pre-finetuning</td>\n            <td>70.7</td>\n            <td>67.5</td>\n        </tr>\n    </tbody>\n</table>\n\nExample command (ngpu=16):\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_vqav2caption.json --lr_backbone 2e-5 --text_encoder_lr 2e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/pretrained_checkpoint.pth\n\nCUBLAS_WORKSPACE_CONFIG=:4096:8  python main.py --dataset_config configs/flickr_vqav2caption.json --lr_backbone 2e-5 --text_encoder_lr 2e-5 --lr 1e-4 --num_queries 200 --max_decoding_step 256 --do_caption --no_detection --ema --output-dir weights/$exp_id --load weights/prefinetune_VQAv2_checkpoint.pth --eval --test\n```\n\n## Acknowledgement\nThe project is built based on the following repository:\n* [MDETR--Modulated Detection for End-to-End Multi-Modal Understanding](https://github.com/ashkamath/mdetr),\n* [transformers](https://github.com/huggingface/transformers).\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "UniTAB", "org_name": "microsoft", "org_repo": "microsoft/UniTAB", "platform_org_repo": "github+microsoft/UniTAB", "link_to_repo": "https://github.com/microsoft/UniTAB", "platform": "github", "language": "Python", "stargazers_count": 68, "watchers_count": 68}, {"README_text": "# Introduction \nThis is an example code which leverages Azure IoT Central for a smart farm use case. It collects temperature, humidity, and light data from sensors. Then send the data to Azure via Azure IoT Central via wireless network. Most of the code of this project is based on the [ESP32-Azure IoT Kit example](https://github.com/Azure/azure-sdk-for-c-arduino/tree/main/examples/Azure_IoT_Central_ESP32_AzureIoTKit) of Azure SDK for C on Arduino.\n## Hardwares\n\nMicro-controller\n* [Adafruit HUZZAH32 \u2013 ESP32 Feather](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Flearn.adafruit.com%2Fadafruit-huzzah32-esp32-feather&data=04%7C01%7Cvivihung%40microsoft.com%7C31b9da68c61f43b65b7608da09329eed%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637832410263428282%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=KqFYe%2Bgacy5JYxGX4C%2Fo8b4s4KcbYZqEL4nd1iIUNcM%3D&reserved=0)\n\nSensors\n* Temperature and humidity sensors: DHT11\n* Sunlight sensor: SI1145\n\n# Statement of Purpose\nThis repository aims to demonstrate how to use an ESP32 board to collect data from sensors and send the data to Azure IoT Central. It is not intended to be a released product. Therefore, this repository is not for discussing Azure IoT Central or requesting new features.\n\n# Getting Started\n## Requirements\n* Install [Arduino IDE](https://www.arduino.cc/en/software) (recommend the Windows executable than the Windows app version).\n* Install [SiLabs CP2104 Driver](https://www.silabs.com/developers/usb-to-uart-bridge-vcp-drivers)\n* Install [ESP32 package for Arduino IDE](https://docs.espressif.com/projects/arduino-esp32/en/latest/getting_started.html)\n* Install `Azure IoT SDK for C` library in Arduino IDE\n* Install libraries for the DHT sensor in Arduino IDE:\n    * `Adafruit IO Arduino v2.4.1+`\n    * `Adafruit Unified Sensor`\n    * `DHT Sensor Library`\n* Install libraries for the Sunlight sensor in Arduino IDE:\n    * [Manually install]((https://learn.adafruit.com/adafruit-all-about-arduino-libraries-install-use/installing-a-library)) [Adafruit_SI1145](https://github.com/adafruit/Adafruit_SI1145_Library/releases) library from GitHub\n* An Azure subscription and has owner permission to create resources.\n* A WiFi network is required to run this example.\n\n### Development environment\nAlthough Arduino IDE is required for running this example, I found Visual Studio Code with Arduino extension (`vscode-arduino`) provides better integrated experience.\n\n## Setup\n### A. Clone the repository\n```\ngit clone https://github.com/microsoft/azure-iot-central-esp32.git\n```\n\n### B. Setup Azure IoT Central\n1. Go to [Azure IoT Central](https://apps.azureiotcentral.com/) and create a new IoT Central application.\n2. Go to the device page and click `Connect`. Get the value of `ID scope`, `Device ID`. Select \"Shared access signature (SAS)\" as the Authentication type. Get the value of `Primary key`. We will use them in later steps.\n\n### C. Setup devices\n\n1. Wire hardwares.\n![](images/dht11+SI1145.png)\n\n2. Open `iot_configs.h`. Replace `YOUR_WIFI_SSID` and `YOUR_WIFI_PASSWORD` with your WiFi SSID and password.\n    ```cpp\n    #define IOT_CONFIG_WIFI_SSID              \"YOUR_WIFI_SSID\"\n    #define IOT_CONFIG_WIFI_PASSWORD          \"YOUR_WIFI_PASSWORD\"\n    ```\n\n3. Connect with Azure IoT Central.\n\n   a. Open `iot_configs.h`. Replace `YOUR_ID_SCOPE`, `YOUR_DEVICE_ID`, and `YOUR_DEVICE_KEY` with the `ID scope`, `Device ID` and the `Primary key` from Azure IoT Central setup step 6.\n\n   ```cpp\n   #define DPS_ID_SCOPE                      \"YOUR_ID_SCOPE\"\n   #define IOT_CONFIG_DEVICE_ID              \"YOUR_DEVICE_ID\"\n   #define IOT_CONFIG_DEVICE_KEY             \"YOUR_DEVICE_KEY\"\n   ```\n\n   b. In IDE, set the board to `Adafruit ESP32 Feather`. and pick the right port. Upload the code to the board. You should see the Wi-Fi connection status, MQTT connection status, and events in Serial Monitor.\n\n### D. Create a device and modify data model in IoT Central.\nAfter connected Azure IoT Central via the previous step, you will find a device template, \"Espressif ESP32 Azure IoT Kit\", that is automatically generated in Azure IoT Central portal. \n1. Create a new device in Azure IoT Central: On the left navigation menu, click on \"Devices\". Click on the \"New\" button on the top. Provide a name and an ID for the device. Select \"Espressif ESP32 Azure IoT Kit\" as the device template.\n2. Modifiy the data model: In the device template menu, select `Model` -> `{} Edit DTDL`. Copy and paste the content of `Azure_IoT_Central\\Device_Template\\Model\\Espressif ESP32 Azure IoT Kit.json`, click `Save`. Then click `Publish` on the top menu.\n3. Validate device connection: Once the updated template is published, navigate to \"Devices\" -> \"Espressif ESP32 Azure IoT Kit\" -> \"Adafruit Feature ESP32\" -> The device you created in D-1. You should see the device status change to `Connected`. Click on the \"Raw data\" tab, you can see the data with the latest data model uploaded from the device.\n\n# About the code\nThis code is modified from the `Azure_IoT_Central_ESP32` sample code of Azure SDK for C in Arduino library.\n* `Azure_IoT_Central\\`: Configurations of Azure IoT Central.\n    * `Device_Template\\Model\\Espressif ESP32 Azure IoT Kit.json`: The DTDL file for the data model.\n* `Azure_IoT_Central_ESP32`: The code for the ESP32.\n    * `Azure_IoT_Central_ESP32.ino`: The main file.\n    * `Azure_IoT_PnP_Template.cpp`: Where we fetch data from the sensors and prepare payload for sending to the cloud.\n    * `iot_configs.h`: The secrets for the Azure IoT Central along with other configurations.\n\n# Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "azure-iot-central-esp32-sample", "org_name": "microsoft", "org_repo": "microsoft/azure-iot-central-esp32-sample", "platform_org_repo": "github+microsoft/azure-iot-central-esp32-sample", "link_to_repo": "https://github.com/microsoft/azure-iot-central-esp32-sample", "platform": "github", "language": "C++", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# VS Code Markdown Language Service\n\nThe language service that powers VS Code's Markdown support, extracted so that it can be reused by other editors and tools.\n\n\n## Features\n\nThis library targets [CommonMark](https://commonmark.org). Support for other Markdown dialects and extensions is not within the scope of this project.\n\nCurrently supported language features:\n\n- Document links (clickable spans in the editor)\n\n\tSupported links include:\n\n\t- Links to headers within the current file: `[text](#header)`\n\t- Absolute and relative links to files: `[text](path/to/file.md)`\n\t- Reference links: `[text][link-name]`\n\n- Document symbols\n\n\tFinds all headers within a markdown file\n\n- Workspace symbols\n\n\tFind all headers across all markdown files in the workspace.\n\n- Folding ranges\n\n\tFolding ranges are computed for:\n\n\t- Header sections\n\t- Region sections\n\t- Lists\n\t- Block elements\n\n- Smart select (expand selection)\n\n- Completions\n\n\tSupports completions for:\n\n\t- Links to headers\n\t- Path links\n\t- Reference links\n\n- Find all references\n\n\tSupports finding references to:\n\n\t- Headers\n\t- Path links\n\t- Fragments in links\n\t- Reference links\n\n- Definitions\n\n\tSupports finding definitions headers and reference links.\n\n- Renames\n\n\tSupports renaming of headers and links.\n\n- Organize link definitions.\n\n\tGroups and sorts link definitions in a file, optionally also removing unused definitions.\n\n-  Code actions\n\n\t- Extract all occurrences of a link in a file to a link definition at the bottom of the file.\n\t- Quick fixes for removing duplicated or unused link definitions.\n\n- Diagnostics (error reporting)\n\n\tSupports generating diagnostics for invalid links to:\n\n\t- References.\n\t- Header within the current file.\n\t- Files in the workspace.\n\t- Headers in other files.\n\t\n\tAlso can generate diagnostics for:\n\n\t- Unused link definitions.\n\t- Duplicate link definitions.\n\n- Update links on file rename\n\n\tGenerate an edit that updates all links when a file/directory in the workspace is renamed or moved.\n\n## Usage\n\nTo get started using this library, first install it into your workspace:\n\n```bash\nnpm install vscode-markdown-languageservice\n```\n\nTo use the language service, first you need to create an instance of it using `createLanguageService`. We use dependency injection to allow the language service to be used in as many contexts as possible.\n\n```ts\nimport * as md from 'vscode-markdown-languageservice';\n\n// Implement these\nconst parser: md.IMdParser = ...;\nconst workspace: md.IWorkspace = ...;\nconst logger: md.ILogger = ...;\n\nconst languageService = md.createLanguageService({ workspace, parser, logger });\n```\n\nAfter creating the service, you can ask it for the language features it supports:\n\n```ts\n// We're using the vscode-language types in this demo\n// If you want to use them, make sure to run:\n//\n//     npm install vscode-languageserver vscode-languageserver-textdocument\n//\n// However you can also bring your own types if you want to instead.\n\nimport { CancellationTokenSource } from 'vscode-languageserver';\nimport { TextDocument } from 'vscode-languageserver-textdocument';\n\nconst cts = new CancellationTokenSource();\n\n// Create a virtual document that holds our file content\nconst myDocument = TextDocument.create(\n\tURI.file('/path/to/file.md').toString(), // file path\n\t'markdown', // file language\n\t1, // version\n\t[ // File contents\n\t\t'# Hello',\n\t\t'from **Markdown**',\n\t\t'',\n\t\t'## World!',\n\t].join('\\n')\n);\n\nconst symbols = await languageService.getDocumentSymbols(myDocument, { includeLinkDefinitions: true }, cts.token);\n```\n\nSee [example.cjs](./example.cjs) for complete, minimal example of using the language service. You can run in using `node example.cjs`.\n\n\n## Additional Links\n\n- [VS Code's Markdown language server](https://github.com/microsoft/vscode/blob/main/extensions/markdown-language-features/server/)\n- [The TextMate grammar VS Code uses for Markdown syntax highlighting](https://github.com/microsoft/vscode-markdown-tm-grammar)\n\n\n## Contributing\n\nIf you're interested in contributing\n\n1. Clone this repo\n1. Install dependencies using `npm install`\n1. Start compilation using `npm run watch`\n\nYou can run the unit tests using `npm test` or by opening the project in VS Code and pressing `F5` to debug.\n", "repo_name": "vscode-markdown-languageservice", "org_name": "microsoft", "org_repo": "microsoft/vscode-markdown-languageservice", "platform_org_repo": "github+microsoft/vscode-markdown-languageservice", "link_to_repo": "https://github.com/microsoft/vscode-markdown-languageservice", "platform": "github", "language": "TypeScript", "stargazers_count": 385, "watchers_count": 385}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CommercialAdsDataset", "org_name": "microsoft", "org_repo": "microsoft/CommercialAdsDataset", "platform_org_repo": "github+microsoft/CommercialAdsDataset", "link_to_repo": "https://github.com/microsoft/CommercialAdsDataset", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# go-rustaudit\n\nGo library for extracting Rust dependency information from Rust binaries built with [cargo-auditable](https://github.com/rust-secure-code/cargo-auditable) (previously named rust-audit).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "go-rustaudit", "org_name": "microsoft", "org_repo": "microsoft/go-rustaudit", "platform_org_repo": "github+microsoft/go-rustaudit", "link_to_repo": "https://github.com/microsoft/go-rustaudit", "platform": "github", "language": "Go", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# CodeT\n\nThis repository contains projects that aims to equip large-scale pretrained language models with better programming and reasoning skills.\nThese projects are presented by Microsoft Research Asia and Microsoft Azure AI.\n\n## Projects\n\n- [[CodeT]](./CodeT/): Code Generation with Generated Tests\n- [[DIVERSE]](./DIVERSE/): On the Advance of Making Language Models Better Reasoners\n- [[RepoCoder]](./RepoCoder/): Repository-Level Code Completion Through Iterative Retrieval and Generation", "repo_name": "CodeT", "org_name": "microsoft", "org_repo": "microsoft/CodeT", "platform_org_repo": "github+microsoft/CodeT", "link_to_repo": "https://github.com/microsoft/CodeT", "platform": "github", "language": "Python", "stargazers_count": 251, "watchers_count": 251}, {"README_text": "<p align=\"center\">\n  <h1 align=\"center\"><img src=\"assets/lamar_white.svg\" width=\"85\"><br><ins>LaMAR</ins><br>Benchmarking Localization and Mapping<br>for Augmented Reality</h1>\n  <p align=\"center\">\n    <a href=\"https://psarlin.com/\">Paul-Edouard&nbsp;Sarlin*</a>\n    \u00b7\n    <a href=\"https://dsmn.ml/\">Mihai&nbsp;Dusmanu*</a>\n    <br>\n    <a href=\"https://demuc.de/\">Johannes&nbsp;L.&nbsp;Sch\u00f6nberger</a>\n    \u00b7\n    <a href=\"https://www.microsoft.com/en-us/research/people/paspecia/\">Pablo&nbsp;Speciale</a>\n    \u00b7\n    <a href=\"https://www.microsoft.com/en-us/research/people/lugruber/\">Lukas&nbsp;Gruber</a>\n    \u00b7\n    <a href=\"https://vlarsson.github.io/\">Viktor&nbsp;Larsson</a>\n    \u00b7\n    <a href=\"http://miksik.co.uk/\">Ondrej&nbsp;Miksik</a>\n    \u00b7\n    <a href=\"https://www.microsoft.com/en-us/research/people/mapoll/\">Marc&nbsp;Pollefeys</a>\n  </p>\n<p align=\"center\">\n    <img src=\"assets/logos.svg\" alt=\"Logo\" height=\"40\">\n</p>\n  <h2 align=\"center\">ECCV 2022</h2>\n  <h3 align=\"center\"><a href=\"https://lamar.ethz.ch/\">Project Page</a> | <a href=\"https://youtu.be/32XsRli2coo\">Video</a></h3>\n  <div align=\"center\"></div>\n</p>\n<p align=\"center\">\n    <a href=\"https://lamar.ethz.ch/\"><img src=\"assets/teaser.svg\" alt=\"Logo\" width=\"80%\"></a>\n    <br>\n    <em>LaMAR includes multi-sensor streams recorded by AR devices along hundreds of unconstrained trajectories captured over 2&nbsp;years in 3&nbsp;large indoor+outdoor locations.</em>\n</p>\n\n##\n\nThis repository hosts the source code for LaMAR, a new benchmark for localization and mapping with AR devices in realistic conditions. The contributions of this work are:\n1. __A dataset__: multi-sensor data streams captured by AR devices and laser scanners\n2. __scantools__: a processing pipeline to register different user sessions together\n3. __A benchmark__: a framework to evaluate algorithms for localization and mapping\n\nSee our [ECCV 2022 tutorial](https://lamar.ethz.ch/tutorial-eccv2022/) for an overview of LaMAR and of the state of the art of localization and mapping for AR.\n\n## Overview\n\nThis codebase is composed of the following modules:\n\n- <a href=\"#benchmark\">`lamar`</a>: evaluation pipeline and baselines for localization and mapping\n- <a href=\"#processing-pipeline\">`scantools`</a>: data API, processing tools and pipeline\n- [ScanCapture](apps/ScanCapture_iOS): a data recording app for Apple devices\n\n## Data format\n\nWe introduce a new data format, called *Capture*, to handle multi-session and multi-sensor data recorded by different devices. A Capture object corresponds to a capture location. It is composed of multiple sessions and each of them corresponds to a data recording by a given device. Each sessions stores the raw sensor data, calibration, poses, and all assets generated during the processing.\n\n```python\nfrom scantools.capture import Capture\ncapture = Capture.load('data/CAB/')\nprint(capture.sessions.keys())\nsession = capture.sessions[session_id]  # each session has a unique id\nprint(session.sensors.keys())  # each sensor has a unique id\nprint(session.rigs)  # extrinsic calibration between sensors\nkeys = session.trajectories.key_pairs()  # all (timestamp, sensor_or_rig_id)\nT_w_i = sessions.trajectories[keys[0]]  # first pose, from sensor/rig to world\n```\n\n[More details are provided in the specification document.](./CAPTURE.md)\n\n## Installation\n\n:one: Install the core dependencies:\n\n- Python >= 3.8\n- [hloc](https://github.com/cvg/Hierarchical-Localization) and its dependencies, including [COLMAP](https://colmap.github.io/install.html) built from source\n- everything listed in `requirements/lamar.txt` installed with\n```bash\npython -m pip install -r requirements/lamar.txt\n```\n\n:two: Optional: the processing pipeline additionally relies on heavier dependencies not required for benchmarking:\n\n- Pip dependencies: `python -m pip install -r requirements/scantools.txt`\n- [raybender](https://github.com/cvg/raybender) for raytracing\n- [pcdmeshing](https://github.com/cvg/pcdmeshing) for pointcloud meshing\n\n:three: Optional: install `lamar` and `scantools` as libraries for external use via\n```bash\npython -m pip install -e .\n```\n\n## Benchmark\n\n:one: __Obtain the evaluation data:__ [visit the dataset page](https://lamar.ethz.ch/lamar/) and place the 3 scenes in `./data` :\n\n```\ndata/\n\u251c\u2500\u2500 CAB/\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sessions/\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 map/                # mapping session\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 query_hololens/     # HoloLens test queries\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 query_phone/        # Phone test queries\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 query_val_hololens/ # HoloLens validation queries\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 query_val_phone/    # Phone validation queries\n\u251c\u2500\u2500 HGE\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 LIN\n    \u2514\u2500\u2500 ...\n```\n\nEach scene contains a mapping session and queries for each device type. We provide a small set of validation queries with known ground-truth poses such that they can be used for developing algorithms and tuning parameters. We keep private the ground-truth poses of the test queries.\n\n:two: __Run the single-frame evaluation__ with the strongest baseline:\n\n```bash\npython -m lamar.run \\\n\t--scene $SCENE --ref_id map --query_id $QUERY_ID \\\n\t--retrieval fusion --feature superpoint --matcher superglue\n```\n\nwhere `$SCENE` is in `{CAB,HGE,LIN}` and `$QUERY_ID` is in `{query_phone,query_hololens}` for testing and in `{query_val_phone,query_val_hololens}` for validation. All outputs are written to `./outputs/` by default. For example, to localize validation Phone queries in the CAB scene:\n```bash\npython -m lamar.run \\\n\t--scene CAB --ref_id map --query_id query_val_phone \\\n\t--retrieval fusion --feature superpoint --matcher superglue\n```\n\nThis executes two steps:\n1. Create a sparse 3D map using the mapping session via feature extraction, pair selection, feature matching, triangulation\n2. Localize each image of the sequence via feature extraction, pair selection, feature matching, absolute pose estimation\n\n:three: __Obtain the evaluation results:__\n\n- validation queries: the script print the localization recall.\n- test queries: until the benchmark leaderboard is up and running, please send the predicted pose files to <a href=\"&#x6d;ailto&#58;lamar-benchmark&#x40;sympa.ethz.ch\">lamar-benchmark&#x40;sympa.ethz.ch</a> :warning: we will only accept at most 2 submissions per user per week.\n\n:four: __Workflow:__ the benchmarking pipeline is designed such that\n- the mapping and localization process is split into modular steps listed in [`lamar/tasks/`](./lamar/tasks/)\n- outputs like features and matches are cached and re-used over multiple similar runs\n- changing a configuration entry automatically triggers the recomputation of all downstream steps that depend on it\n\n#### Other evaluation options\n\n<details>\n<summary>[Click to expand]</summary>\n\nUsing radio signals for place recognition:\n```bash\npython -m lamar.run [...] --use_radios\n```\n\nLocalization with sequences of 10 seconds instead of single images:\n```bash\npython -m lamar.run [...] --sequence_length_seconds 10\n```\n\n</details>\n\n#### Adding your own algorithms\n\n<details>\n<summary>[Click to expand]</summary>\n\nTo add a new local feature:\n- add your feature extractor to hloc in [`hloc/extractors/myfeature.py`](https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/extractors)\n- create a configuration entry in [`lamar.tasks.feature_extraction.FeatureExtraction.methods`](./lamar/tasks/feature_extraction.py)\n\nTo add a new global feature for image retrieval:\n- add your feature extractor to hloc in [`hloc/extractors/myfeature.py`](https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/extractors)\n- create a configuration entry in [`lamar.tasks.feature_extraction.RetrievalFeatureExtraction.methods`](./lamar/tasks/feature_extraction.py)\n\nTo add a new local feature matcher:\n- add your feature matcher to hloc in [`hloc/matchers/mymatcher.py`](https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/matchers)\n- create a configuration entry in [`lamar.tasks.feature_matching.RetrievalFeatureMatching.methods`](./lamar/tasks/feature_matching.py)\n\nTo add a new pose solver: create a new class that inherits from [`lamar.tasks.pose_estimation.SingleImagePoseEstimation`](./lamar/tasks/pose_estimation.py):\n\n```python\nclass MyPoseEstimation(SingleImagePoseEstimation):\n    method = {'name': 'my_estimator'}\n    def run(self, capture):\n        ...\n```\n\n</details>\n\n## Processing pipeline\n\nEach step of the pipeline corresponds to a runfile in `scantools/run_*.py` that can be used as follow:\n\n- executed from the command line: `python -m scantools.run_phone_to_capture [--args]`\n- imported as a library:\n\n```python\nfrom scantools import run_phone_to_capture\nrun_phone_to_capture.run(...)\n```\n\nWe provide pipeline scripts that execute all necessary steps:\n\n- [`pipelines/pipeline_scans.py`](pipelines/pipeline_scans.py) aligns multiple NavVis sessions and merge them into a unique reference session\n- [`pipelines/pipeline_sequence.py`](pipelines/pipeline_sequence.py) aligns all AR sequences to the reference session\n\nThe raw data will be released soon such that anyone is able to run the processing pipeline without access to capture devices.\n\nHere are runfiles that could be handy for importing and exporting data:\n- `run_phone_to_capture`: convert a [ScanCapture](apps/ScanCapture_iOS) recording into a Capture session\n- `run_navvis_to_capture`: convert a NavVis recording into a Capture Session\n- `run_session_to_kapture`: convert a Capture session into a [Kapture](https://github.com/naver/kapture) instance\n- `run_capture_to_empty_colmap`: convert a Capture session into an empty [COLMAP model](https://colmap.github.io/format.html#sparse-reconstruction)\n- `run_image_anonymization`: anonymize faces and license plates using the [Brighter.AI](https://brighter.ai/) API\n- `run_radio_anonymization`: anonymize radio signal IDs\n- `run_combine_sequences`: combine multiple sequence sessions into a single session\n\n## Release plan\n\n We are still in the process of fully releasing LaMAR. Here is the release plan:\n\n- [x] LaMAR evaluation data and benchmark\n- [x] Ground truthing pipeline\n- [x] iOS capture app\n- [ ] Full raw data\n- [ ] Leaderboard and evaluation server\n- [ ] 3D dataset viewer\n\n## BibTex citation\n\nPlease consider citing our work if you use any code from this repo or ideas presented in the paper:\n\n```\n@inproceedings{sarlin2022lamar,\n  author    = {Paul-Edouard Sarlin and\n               Mihai Dusmanu and\n               Johannes L. Sch\u00f6nberger and\n               Pablo Speciale and\n               Lukas Gruber and\n               Viktor Larsson and\n               Ondrej Miksik and\n               Marc Pollefeys},\n  title     = {{LaMAR: Benchmarking Localization and Mapping for Augmented Reality}},\n  booktitle = {ECCV},\n  year      = {2022},\n}\n```\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "lamar-benchmark", "org_name": "microsoft", "org_repo": "microsoft/lamar-benchmark", "platform_org_repo": "github+microsoft/lamar-benchmark", "link_to_repo": "https://github.com/microsoft/lamar-benchmark", "platform": "github", "language": "Python", "stargazers_count": 286, "watchers_count": 286}, {"README_text": "# Introduction\nThis repo presents some example codes to reproduce some results in\n[GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100).\n\n# Installation\n- Install [azfuse](https://github.com/microsoft/azfuse). The tool is used to\n  automatically download the data. The configuration of\n  AzFuse has already been in this repo.\n\n- Download the source code by\n  ```shell\n  git clone https://github.com/microsoft/GenerativeImage2Text.git\n  cd GenerativeImage2Text\n  ```\n\n- Install the package\n  ```shell\n  pip install -r requirements.txt\n  python setup.py build develop\n  ```\n\n# Inference\n- Inference on a single image or multiple frames:\n  ```shell\n  # single image, captioning\n  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_image', \\\n        'image_path': 'aux_data/images/1.jpg', \\\n        'model_name': 'GIT_BASE', \\\n        'prefix': '', \\\n  }\"\n  # single image, question answering\n  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_image', \\\n        'image_path': 'aux_data/images/1.jpg', \\\n        'model_name': 'GIT_BASE_VQAv2', \\\n        'prefix': 'what is it?', \\\n  }\"\n  # multiple images, captioning\n  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_image', \\\n        'image_path': ['aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg'], \\\n        'model_name': 'GIT_BASE_VATEX', \\\n        'prefix': '', \\\n  }\"\n  # multiple images, question answering\n  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_image', \\\n        'image_path': ['aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg', 'aux_data/images/1.jpg'], \\\n        'model_name': 'GIT_BASE_MSRVTT_QA', \\\n        'prefix': 'what is it?', \\\n  }\"\n  ```\n  - If `prefix` is empty, it is effectively the captioning task.\n  - If `prefix` is a question, it is effectively the visual question answering task.\n  - Use a list for `image_path` if it is for video. The example here is 6 identical images, only\n    for a demo purpose. It should be different image frames from a video.\n  - `model_name` here can be the following. Performance details can be found in the reference paper.\n\n    | model_name          | Information                                         | Performance             |\n    |---------------------|-----------------------------------------------------|------------------------ |\n    | GIT_BASE            | pretrained on 4M images                             |                         |\n    | GIT_BASE_COCO       | fine-tuned on COCO                                  | CIDEr: 131.4            |\n    | GIT_BASE_TEXTCAPS   | fine-tuned on TextCaps for captioning               | val/CIDEr: 64.9         |\n    | GIT_BASE_VQAv2      | fine-tuned on VQAv2                                 | test-dev: 72.72         |\n    | GIT_BASE_TEXTVQA    | fine-tuned on TextVQA                               | val/acc: 18.81          |\n    | GIT_BASE_VATEX      | fine-tuned on VATEX for captioning                  | public/test/CIDEr: 60.0 |\n    | GIT_BASE_MSRVTT     | fine-tuned on MSRVTT for captioning                 | test/CIDEr: 57.8        |\n    | GIT_BASE_MSRVTT_QA  | fine-tuned on MSRVTT for question answering         | acc: 41.0               |\n    | GIT_LARGE           | pretrained on 14M images                            |                         |\n    | GIT_LARGE_COCO      | fine-tuned on COCO                                  | CIDEr: 138.5            |\n    | GIT_LARGE_TEXTCAPS  | fine-tuned on TextCaps for captioning               | val/CIDEr: 106.3        |\n    | GIT_LARGE_VQAv2     | fine-tuned on VQAv2                                 | test-dev: 75.51         |\n    | GIT_LARGE_TEXTVQA   | fine-tuned on TextVQA                               | val/acc: 37.47          |\n    | GIT_LARGE_VATEX     | fine-tuned on VATEX for captioning                  | public/test/CIDEr: 72.5 |\n    | GIT_LARGE_MSRVTT    | fine-tuned on MSRVTT for captioning                 | test/CIDEr: 64.1        |\n    | GIT_LARGE_MSRVTT_QA | fine-tuned on MSRVTT for question answering         | acc: 42.7               |\n\n  - In the dataset of cc12m, the caption may contain some special tags to hide\n    person names and the\n    model might also predict such special tokens. To eliminate this issue, we\n    remove these captions (around 25\\% in cc12m), and re-trained the\n    large-sized model. The base-sized model is not affected as cc12 is not part\n    of the training data.\n\n    | model_name          | Information                                         | Performance             |\n    |---------------------|-----------------------------------------------------|------------------------ |\n    | GIT_LARGE_R         | pretrained on 14M images with special tag removed   |                         |\n    | GIT_LARGE_R_COCO    | fine-tuned on COCO                                  | CIDEr: 137.6            |\n    | GIT_LARGE_R_TEXTCAPS| fine-tuned on TextCaps for captioning               | val/CIDEr: 105.3        |\n\n\n- Inference on a [TSV](https://en.wikipedia.org/wiki/Tab-separated_values) file, which is a collection of multiple images.\n  - Data format (for information only)\n    - image TSV: Each row has two columns. The first is the image key; the\n      second is base64-encoded jpg or png bit string.\n    - caption or question tsv: Each row has two columns. The first is the image\n      key; the second is a list of dictionaries in the json format. For caption TSV,\n      the dictionary should contain at least the field of `'caption'`. For the\n      question answering TSV, it should contain at least `question_id` and\n      `question`.\n  - inference on [COCO](https://cocodataset.org) Karpathy test.\n      <!---\n    1. Prepare the coco test TSV\n       ```\n       mkdir -p aux_data/raw_data\n       wget http://images.cocodataset.org/zips/val2014.zip -O aux_data/raw_data/val2014.zip\n       wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip -O aux_data/raw_data/caption_datasets.zip\n       cd aux_data/raw_data\n       unzip val2014.zip\n       unzip caption_datasets.zip\n       python -m generativeimage2text.data_prepare -p \"{'type': 'prepare_coco_test'}\"\n       ```\n       -->\n    1. Inference.\n       ```shell\n       # base\n       AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n             'image_tsv': 'data/coco_caption/test.img.tsv', \\\n             'model_name': 'GIT_BASE_COCO', \\\n             'question_tsv': null, \\\n             'out_tsv': 'inference/GIT_BASE_COCO/coco.tsv', \\\n       }\"\n       # GIT_LARGE_COCO. If there are 8 GPUs, it can parallel by mpirun -n 8\n       AZFUSE_TSV_USE_FUSE=1 mpirun -n 8 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n             'image_tsv': 'data/coco_caption/test.img.tsv', \\\n             'model_name': 'GIT_LARGE_COCO', \\\n             'question_tsv': null, \\\n             'out_tsv': 'inference/GIT_LARGE_COCO/coco.tsv', \\\n       }\"\n       ```\n    2. Calculate the evaluation metric\n       ```shell\n       # base\n       AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'evaluate_on_coco_caption', \\\n             'res_file': 'inference/GIT_BASE_COCO/coco.tsv', \\\n             'label_file': 'data/coco_caption/test.caption.tsv', \\\n       }\"\n       ```\n       The CIDEr score should be 131.35 for `GIT_BASE_COCO` and  138.45 for `GIT_LARGE_COCO`.\n       If you get lower score (e.g. 126 for the base model),\n       the reason could be\n       the misalignment of the environment, e.g. pytorch version.\n    3. (optional) To exactly reproduce the number, please run the following:\n       ```bash\n       nvidia-docker run --ipc=host amsword/setup:py38pt19u20cu11 \\\n           bash -c \"mkdir -p /tmp/code \\\n                   && cd /tmp/code \\\n                   && pip install git+https://github.com/microsoft/azfuse.git \\\n                   && git clone https://github.com/amsword/generativeimage2text.git \\\n                   && cd generativeimage2text \\\n                   && pip install -r requirements.txt \\\n                   && python setup.py build develop \\\n                   && AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n                            'image_tsv': 'data/coco_caption/test.img.tsv', \\\n                            'model_name': 'GIT_BASE_COCO', \\\n                            'question_tsv': null, \\\n                            'out_tsv': 'inference/GIT_BASE_COCO/coco.tsv', \\\n                      }\" \\\n                   &&  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'evaluate_on_coco_caption', \\\n                       'res_file': 'inference/GIT_BASE_COCO/coco.tsv', \\\n                       'label_file': 'data/coco_caption/test.caption.tsv', \\\n                       'outfile': 'inference/GIT_BASE_COCO/coco.score.json', \\\n                       }\" \\\n                   && cat inference/GIT_BASE_COCO/coco.score.json \\\n                   \"\n       ```\n  - Inference on [vqa](https://visualqa.org/index.html) test\n    1. Inference\n       ```shell\n       # base model\n       AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n             'image_tsv': 'data/TaxVQAv2/test.tsv', \\\n             'model_name': 'GIT_BASE_VQAv2', \\\n             'question_tsv': 'data/TaxVQAv2/test.caption.tsv', \\\n             'out_tsv': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.tsv', \\\n       }\"\n       # GIT_LARGE_VQAv2 with 8 GPUs.\n       AZFUSE_TSV_USE_FUSE=1 mpirun -n 8 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n             'image_tsv': 'data/TaxVQAv2/test.tsv', \\\n             'model_name': 'GIT_LARGE_VQAv2', \\\n             'question_tsv': 'data/TaxVQAv2/test.caption.tsv', \\\n             'out_tsv': 'inference/GIT_LARGE_VQAv2/snapshot/vqav2.tsv', \\\n       }\"\n       ```\n\n    2. Convert the output tsv to the json format for submission to [evalai](https://eval.ai/web/challenges/challenge-page/830/overview)\n       ```shell\n       # base model\n       AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'convert_tsv_to_vqa_json', \\\n             'predict_file': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.tsv', \\\n             'out_json': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.json', \\\n       }\"\n       # large model\n       AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'convert_tsv_to_vqa_json', \\\n             'predict_file': 'inference/GIT_LARGE_VQAv2/snapshot/vqav2.tsv', \\\n             'out_json': 'inference/GIT_LARGE_VQAv2/snapshot/vqav2.json', \\\n       }\"\n       ```\n       Submit the file of `inference/GIT_BASE_VQAv2/snapshot/vqav2.json` to evalai\n       and you should get `72.72` on `test-dev`. If it is `GIT_LARGE_VQAv2`, the accuracy is\n       `75.51`.\n\n    3. (optional) To exactly reproduce the number, you can use the\n       following:\n       ```shell\n       # base model\n       nvidia-docker run --ipc=host amsword/setup:py38pt19u20cu11 \\\n           bash -c \"mkdir /tmp/code \\\n                   && cd /tmp/code \\\n                   && pip install git+https://github.com/microsoft/azfuse.git \\\n                   && git clone https://github.com/amsword/generativeimage2text.git \\\n                   && cd generativeimage2text \\\n                   && pip install -r requirements.txt \\\n                   && python setup.py build develop \\\n                   && AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'test_git_inference_single_tsv', \\\n                       'image_tsv': 'data/TaxVQAv2/test.tsv', \\\n                       'model_name': 'GIT_BASE_VQAv2', \\\n                       'question_tsv': 'data/TaxVQAv2/test.caption.tsv', \\\n                       'out_tsv': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.tsv', \\\n                   }\" \\\n                   &&  AZFUSE_TSV_USE_FUSE=1 python -m generativeimage2text.inference -p \"{'type': 'convert_tsv_to_vqa_json', \\\n                       'predict_file': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.tsv', \\\n                       'out_json': 'inference/GIT_BASE_VQAv2/snapshot/vqav2.json', \\\n                   }\" \\\n       }\"\n       ```\n       Note that, please modify the docker command properly so that the output\n       file can be saved permanently to the host machine. It is also recommended\n       to run it inside the docker container by\n       ```shell\n       nvidia-docker run --ipc=host amsword/setup:py38pt19u20cu11 sleep infinity\n       docker ps # get the docker container ID\n       docker exec -it container_id /bin/bash # attach inside the docker container\n       # all other commands to run the inference.\n       ```\n\n# Training\nThe repo shows the key code path of constructing the network\ninput with transformations and forward/backward. The code can be plugged into\nany trainer easily. Here is the example for the base model.\n- Pretraining/captioning\n  ```\n  python -m generativeimage2text.train -p \"{'type': 'forward_backward_example', \\\n                  'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'], \\\n                  'captions': ['a couple of boats in a large body of water.', 'a view of a mountain with a tree'], \\\n              }\"\n  ```\n- VQA\n  ```\n  python -m generativeimage2text.train -p \"{'type': 'forward_backward_example', \\\n                  'image_files': ['aux_data/images/1.jpg', 'aux_data/images/2.jpg'], \\\n                  'prefixs': ['what is this?', 'how many trees?'], \\\n                  'captions': ['several boats in a large body of water', '1'], \\\n              }\"\n  ```\n\n\n# ImageNet\n## Class ID to unique readable names\n- Save the file of `LOC_synset_mapping.txt` from [Kaggle](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data?select=LOC_synset_mapping.txt).\n  under `aux_data/imagenet/`\n\n- Convert the wordnet ID to readable names as follows\n  ```python\n  python -m generativeimage2text.data_prepare -p \"{'type': 'generate_imagenet_unique_names'}\"\n  ```\n  The input file is hard coded as `./aux_data/imagenet/LOC_synset_mapping.txt` and the\n  output file is `./aux_data/imagenet/imagenet_unique_readable_names.txt`\n\n# Citation\nPlease consider to cite the following reference if it helps.\n```text\n@article{wang2022git,\n  title={GIT: A Generative Image-to-text Transformer for Vision and Language},\n  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},\n  journal={arXiv preprint arXiv:2205.14100},\n  year={2022}\n}\n```\n\n# Misc\n\nThe model is now [available](https://huggingface.co/docs/transformers/model_doc/git) in \ud83e\udd17 Transformers. You can also find a fine-tuning guide on image captioning with GIT [here](https://huggingface.co/docs/transformers/main/en/tasks/image_captioning). Thanks to [Niels Rogge](https://github.com/NielsRogge) for contributing the model to \ud83e\udd17 Transformers and [Sayak Paul](https://github.com/sayakpaul) for the fine-tuning guide. \n\n# Acknowledgement\nPart of the code is based on\n[transformers](https://github.com/huggingface/transformers),\n[clip](https://github.com/openai/CLIP),\n[maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark),\n[oscar](https://github.com/microsoft/Oscar),\n[virtex](https://github.com/kdexd/virtex).\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "GenerativeImage2Text", "org_name": "microsoft", "org_repo": "microsoft/GenerativeImage2Text", "platform_org_repo": "github+microsoft/GenerativeImage2Text", "link_to_repo": "https://github.com/microsoft/GenerativeImage2Text", "platform": "github", "language": "Python", "stargazers_count": 353, "watchers_count": 353}, {"README_text": "# Start Your Developer Journey\n\n> **UPDATE: Mar 14, 2023** | Thank you to all the amazing learners who joined us in the #30DaysOf journeys hosted on this site. We have decided to _unpublish_ the website on GitHub pages and move to [Tech Community](https://aka.ms/faculty) for hosting all our content related to #30DaysOfLearning journeys for students. Previously published content should remain accessible as files in this repo, but not as a hosted website URL. So please don't forget to update your bookmarks! \u2665\ufe0f\n\n\n![Screenshot of banner from landing page](./images/landing.png)\n\nWelcome to [#30DaysOfLearning](https://aka.ms/30DaysOf) - a hub with beginner-friendly resources to support your learning journey. \n\n| | | \n|:---|:---|\n|![Start](./website/static/img/landing/start.svg) |**Start** your learning journey with [Microsoft Learn Cloud Skills Challenges](https://microsoft.github.io/30daysof/docs/challenges/intro)! |\n| ![Explore](./website/static/img/landing/explore.svg)| **Explore** [structured roadmaps](https://microsoft.github.io/30daysof/docs/category/roadmaps), [videos](https://microsoft.github.io/30daysof/docs/video-series/intro) and [curricula](https://microsoft.github.io/30daysof/docs/category/curricula). |\n| ![Connect](./website/static/img/landing/connect.svg)|**Connect** with peers [in forums and events.](https://github.com/microsoft/30daysof/discussions) |\n| ![Share](./website/static/img/landing/share.svg)| **Share** your insights [by contributing articles, projects, and feedback](https://github.com/microsoft/30daysof/issues/new/choose) |\n| | |\n\n[![Deploy to GitHub Pages](https://github.com/microsoft/30daysof/actions/workflows/deploy-on-push.yml/badge.svg)](https://github.com/microsoft/30daysof/actions/workflows/deploy-on-push.yml)\n\n---\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "30daysof", "org_name": "microsoft", "org_repo": "microsoft/30daysof", "platform_org_repo": "github+microsoft/30daysof", "link_to_repo": "https://github.com/microsoft/30daysof", "platform": "github", "language": "JavaScript", "stargazers_count": 167, "watchers_count": 167}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "50BusinessAssignmentsLog", "org_name": "microsoft", "org_repo": "microsoft/50BusinessAssignmentsLog", "platform_org_repo": "github+microsoft/50BusinessAssignmentsLog", "link_to_repo": "https://github.com/microsoft/50BusinessAssignmentsLog", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "## Archived\nThis project is going to be archived. We recommend using KEDA for automaticalyl scaling agents\nhttps://keda.sh/docs/2.8/scalers/azure-pipelines/\n\n# Azure Pipelines - Kubernetes Orchestrator\n[![Continuous Integration](https://github.com/microsoft/azure-pipelines-orchestrator/actions/workflows/ci.yaml/badge.svg?branch=master)](https://github.com/microsoft/azure-pipelines-orchestrator/actions/workflows/ci.yaml)\n\nMany enterprise customers run their own Kubernetes clusters either on-premise or in managed kubernetes environments in the cloud. Azure DevOps Services and Server agents can run from containers hosted in these Kubernetes clusters, but what if you do not want to run your agents 24/7? What if you need to be able to scale the number of agents dynamically as pipelines jobs are queued?\n\nThis project provides an application that can monitor a configurable set of agent pools, when pipeline jobs are queued up it will automagically provision Kubernetes Jobs for each job that is queued up. The Kubernetes Jobs will run and process only a single Pipelines Job and then be cleaned up by Kubernetes. \n\nThis allows for horizontally scaleable, on-demand agent pools backed by Kubernetes!\n\n## Getting Started\nYou can first build the docker image:\n```\n# Build Orchestrator Container\ndocker build -t ado-agent-orchestrator\n\n# Build Linux Pipelines Agent\ncd linux\ndocker build -t ado-pipelines-linux\n```\n\n## Run with Docker\n```\ndocker run -d --name ado-agent-orchestrator \\\n    --restart=always \\\n    --env ORG_URL=https://dev.azure.com/yourorg \\\n    --env ORG_PAT=12345 \\\n    --env AGENT_POOLS=Pool1,Pool2 \\\n    --env JOB_IMAGE=ghcr.io/akanieski/ado-pipelines-linux:latest \\\n    --env JOB_NAMESPACE=ado \\\n    ado-agent-orchestrator:latest\n```\n\n## Run with Kubernetes\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ado-orchestrator-deployment\n  labels:\n    app: ado-orchestrator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ado-orchestrator\n  template:\n    metadata:\n      labels:\n        app: ado-orchestrator\n    spec:\n      containers:\n      - name: ado-orchestrator\n        image: ghcr.io/akanieski/ado-orchestrator:latest\n        env:\n        - name: ORG_URL\n          value: \"https://dev.azure.com/yourorg\"\n        - name: ORG_PAT\n          value: \"1234\"\n        - name: AGENT_POOLS\n          value: \"Pool1,Pool2\"\n        - name: JOB_IMAGE\n          value: \"ghcr.io/akanieski/ado-pipelines-linux:latest\"\n        - name: JOB_NAMESPACE\n          value: \"ado\"\n\n```\nAdditionally you can configure the following options environment variables.\n```\nPOLLING_DELAY=1000                           # Milliseconds to wait between runs\n\nRUN_ONCE=1                                   # Only run once - use this to switch a cron job instead of 24/7 monitor run\n\nJOB_PREFIX=agent-job-                        # Customize the agent job's prefix\n\nJOB_DOCKER_SOCKET_PATH=/var/run/docker.sock  # Set this to allow for docker builds within your docker container\n\nJOB_DEFINITION_FILE=job.yaml                 # Provide a template for the k8s Jobs the orchestrator creates\nINITIALIZE_NAMESPACE=true                    # Allows you to optionally disable namespace initialization\n\nMINIMUM_AGENT_COUNT=1                        # The minimum number of agents (regardless of Busy/Idle) to keep running at all times\n\nMINIMUM_IDLE_AGENT_COUNT=0                   # The minimum number of IDLE agents to keep running at all times\n\n```\n\n## Customizing the Kubernetes Job\nIn many scenarios you will want to specify additional configurations to the Job that the orchestrator creates in your k8s cluster. For example, perhaps your pipelines require a custom mounted set of secrets from a CSI, or you would like to reserve memory/cpu for each job, or mount a cached set of build assets. To allow for this level of customization you can now specify the `JOB_DEFINITION_FILE` env variable which will provide you a way of define all the bells and whistles you need for you pipeline agents.\n\nA sample custom job file might look like this:\n```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: custom-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: custom-job\n        image: ghcr.io/akanieski/ado-pipelines-linux:latest\n        resources:\n          requests:\n            memory: \"100Mi\"\n            cpu: \"1\"\n          limits:\n            memory: \"200Mi\"\n            cpu: \"2\"\n        env:\n          - name: AZP_URL\n            value: https://dev.azure.com/your-org\n          - name: AZP_TOKEN\n            value: xxxqhugutbqvpoxxxicdab2ojaipkw6kexxxau57bybmvksp5jpq\n          - name: AZP_POOL\n            value: Default\n      restartPolicy: Never\n```\n\n## Running Serverless with Azure Container Instances\nYou can also choose to avoid the work of setting up Kubernetes and simply run on Azure Container Instances, as shown below:\n```\nAZ_SUBSCRIPTION_ID=     # Your Azure Subscription ID\nAZ_RESOURCE_GROUP=      # The existing resource group that you will place provisioned container group/instances\nAZ_TENANT_ID=           # Your Azure Tenant ID\nAZ_REGION=EastUS        # The Azure region your resources are located\nAZ_ENVIRONMENT=         # The Azure environment, specify AzurePublicCloud, or other sovereign clouds like AzureChina\n```\nThis feature uses the **\"DefaultAzureCredentials\"** API for Azure SDK. This allows for a variety of supported Azure credential scenarios. See [these docs](https://docs.microsoft.com/en-us/dotnet/api/azure.identity.environmentcredential?view=azure-dotnet) for more information on how to configure a scenario that works for you. \n\n## Improving build times through Persistent Volumes\nKubernetes provides users with a convenient mechanism for sharing a disk between multiple containers, or in our case multiple agents and between multiple pipeline runs. We can use this to our advantage. By mounting persistent volumes at key locations you can carry cached data to all of the agents in your pool. \n\nFor example, mounting a persistent volume to the `/root/.nuget/package` path as shown below will make sure you don't have to re-download nuget packages on every single pipeline run.\n\n```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: custom-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: custom-job\n        image: ghcr.io/akanieski/ado-pipelines-linux:0.0.1-preview\n        volumeMounts:\n          - mountPath: \"/root/.nuget/packages\"\n            name: nuget-cache\n      volumes:\n        - name: nuget-cache\n          persistentVolumeClaim:\n            claimName: nuget-cache-claim\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nuget-cache\nspec:\n  capacity:\n   storage: 10Gi\n  accessModes:\n   - ReadWriteMany\n  hostPath:\n    path: \"/tmp/nuget-cache\"\n  storageClassName: slow\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: nuget-cache-claim\nspec:\n  accessModes:\n    - ReadWriteMany\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: slow\n\n```\n\nOther examples of commonly cached paths:\n- `/azp/_work/_tasks` - ADO Pipeline tasks that are downloaded every single time will be cached here - saves time on every run!\n- `/azp/_work/_tool` - ADO Tools installer tasks like .NET Tools, NodeJS Tools, etc - saves time on most runs!\n- `/root/.npm` - Npm packages are notoriously numerous, mounting a cache here will save lots of time for JS builds\n- `/root/.nuget/packages` - Save time on .NET builds\n\n**Note:** the `/root` path above is based on the user/homepath of the user your docker agent runs under. In the examples I use `root` user (not ideal in real world scenarios) for my agent containers. Also note for windows they will also have different paths. The key for both scenarios is that these `/root` paths are the container user's homepath, on Windows it may be `c:\\users\\agent\\` etc.\n\n\n## Contributing\n\nThis project welcomes [contributions and suggestions](docs/contribute.md).\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Issues\n\nWe accept issue reports both here (file a GitHub issue) and in [Developer Community](https://developercommunity.visualstudio.com/spaces/21/index.html).\n\nDo you think there might be a security issue? Have you been phished or identified a security vulnerability? Please don't report it here - let us know by sending an email to secure@microsoft.com.\n", "repo_name": "azure-pipelines-orchestrator", "org_name": "microsoft", "org_repo": "microsoft/azure-pipelines-orchestrator", "platform_org_repo": "github+microsoft/azure-pipelines-orchestrator", "link_to_repo": "https://github.com/microsoft/azure-pipelines-orchestrator", "platform": "github", "language": "C#", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Blazor Sandbox Demo\n\nA simple demo that includes:\n\n- Multiple hosting options: Blazor Hybrid(MAUI) and Blazor Sandbox, using a Shared RCL\n- Demonstration of using EF Core + SQL Lite - including runtime database migration\n- Demonstration of executing unmanaged code from Blazor Hybrid\n- Other Scenarios\n", "repo_name": "blazor-sandbox-demo", "org_name": "microsoft", "org_repo": "microsoft/blazor-sandbox-demo", "platform_org_repo": "github+microsoft/blazor-sandbox-demo", "link_to_repo": "https://github.com/microsoft/blazor-sandbox-demo", "platform": "github", "language": "C#", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Microsoft Cloud Integrations (code samples, videos, documentation)\n\nThe `MicrosoftCloud` repo provides samples and hands-on exercises for different Microsoft Cloud integration scenarios across Azure, Microsoft 365, Power Platform, and GitHub.\n\nWebsite: https://microsoft.github.io/MicrosoftCloud/\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Deployment to GitHub Pages\n\nEnable GitHub Pages to use the `gh-pages` branch by going to `https://github.com/[org_name]/[repo_name]/settings/pages`\n\nPublish:\n\n1. `npm install`\n1. Build and deploy the project by running `npm run build-deploy`.\n", "repo_name": "MicrosoftCloud", "org_name": "microsoft", "org_repo": "microsoft/MicrosoftCloud", "platform_org_repo": "github+microsoft/MicrosoftCloud", "link_to_repo": "https://github.com/microsoft/MicrosoftCloud", "platform": "github", "language": "HTML", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# Biosynthetic gene cluster (BiG) convolutional autoencoding representations of proteins (CARP)\n\nThis repo contains training and plotting code for the paper [Deep self-supervised learning for biosynthetic gene cluster detection and product classification](https://doi.org/10.1101/2022.07.22.500861). Model weights, data, and some results are available on [Zenodo](https://doi.org/10.5281/zenodo.6857703). If you'd like to use BiGCARP, the easiest way is through the [protein sequence models](https://github.com/microsoft/protein-sequence-models) repo. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bigcarp", "org_name": "microsoft", "org_repo": "microsoft/bigcarp", "platform_org_repo": "github+microsoft/bigcarp", "link_to_repo": "https://github.com/microsoft/bigcarp", "platform": "github", "language": "Python", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble\nThis is the experiment code for our IJCAI 2022 paper \"[Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble](https://seqml.github.io/eppo/)\".\n\n## Abstract\n> It is challenging for reinforcement learning (RL) algorithms to succeed in real-world applications like financial trading and logistic system due to the noisy observation and environment shifting between training and evaluation. Thus, it requires both high sample efficiency and generalization for resolving real-world tasks. However, directly applying typical RL algorithms can lead to poor performance in such scenarios. Considering the great performance of ensemble methods on both accuracy and generalization in supervised learning (SL), we design a robust and applicable method named Ensemble Proximal Policy Optimization (EPPO), which learns ensemble policies in an end-to-end manner. Notably, EPPO combines each policy and the policy ensemble organically and optimizes both simultaneously. In addition, EPPO adopts a diversity enhancement regularization over the policy space which helps to generalize to unseen states and promotes exploration. We theoretically prove EPPO increases exploration efficacy, and through comprehensive experimental evaluations on various tasks, we demonstrate that EPPO achieves higher efficiency and is robust for real-world applications compared with vanilla policy optimization algorithms and other ensemble methods. Code and supplemental materials are available at https://seqml.github.io/eppo.\n\n## Environment Dependencies\n### Dependencies\n```\npip install -r requirements.txt\n```\n\n### Running\nTake `Pong` environment in Atari benchmarks as an example, to run EPPO, you can do the following.\n```\npython code/tools/train_on_atari.py exp/atari_local.yml\n```\n\nTo run EPPO-Ens, please set the `center_policy_coef` in `exp/atari_local.yml` to 0.\n\nTo run EPPO-Div, please set the `diverse_coef` in `exp/atari_local.yml` to 0.\n\n## Reference\nYou are more than welcome to cite our paper:\n```\n@article{yang2022towards,\n  title={Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble},\n  author={Yang, Zhengyu and Ren, Kan and Luo, Xufang and Liu, Minghuan and Liu, Weiqing and Bian, Jiang and Zhang, Weinan and Li, Dongsheng},\n  journal={arXiv preprint arXiv:2205.09284},\n  year={2022}\n}\n```\n", "repo_name": "EPPO", "org_name": "microsoft", "org_repo": "microsoft/EPPO", "platform_org_repo": "github+microsoft/EPPO", "link_to_repo": "https://github.com/microsoft/EPPO", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Azure App Configuration extension for Storage Explorer\n\n## The Repository\n\nThis repository contains the releases and documentation for the Azure App Configuration extension for Microsoft Azure Storage Explorer. The extension can be installed within Storage Explorer to provide additional features for managing Azure App Configuration resources under your Azure subscriptions.\n\n![Extension overview](./images/ExtensionOverview.PNG)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nThis work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, see [LICENSE](./LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Security Reporting\n\nIf you find any potential security vulnerabilities, please follow the instructions in [Security Reporting](./SECURITY.md) to report the security vulnerabilities.\n", "repo_name": "azure-app-configuration-se-extension", "org_name": "microsoft", "org_repo": "microsoft/azure-app-configuration-se-extension", "platform_org_repo": "github+microsoft/azure-app-configuration-se-extension", "link_to_repo": "https://github.com/microsoft/azure-app-configuration-se-extension", "platform": "github", "language": null, "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# StackFuture\n\n[![crates.io](https://img.shields.io/crates/v/stackfuture.svg)](https://crates.io/crates/stackfuture)\n[![docs.rs](https://img.shields.io/docsrs/stackfuture)](https://docs.rs/stackfuture/)\n\nThis crate defines a `StackFuture` wrapper around futures that stores the wrapped future in space provided by the caller.\nThis can be used to emulate dynamic async traits without requiring heap allocation.\nBelow is an example of how use `StackFuture`:\n\n```rust\nuse stackfuture::*;\n\ntrait PseudoAsyncTrait {\n    fn do_something(&self) -> StackFuture<'static, (), { 512 }>;\n}\n\nimpl PseudoAsyncTrait for i32 {\n    fn do_something(&self) -> StackFuture<'static, (), { 512 }> {\n        StackFuture::from(async {\n            // function body goes here\n        })\n    }\n}\n\nasync fn use_dyn_async_trait(x: &dyn PseudoAsyncTrait) {\n    x.do_something().await;\n}\n\nasync fn call_with_dyn_async_trait() {\n    use_dyn_async_trait(&42).await;\n}\n```\n\nThis is most useful for cases where async functions in `dyn Trait` objects are needed but storing them in a `Box` is not feasible.\nSuch cases include embedded programming where allocation is not available, or in tight inner loops where the performance overhead for allocation is unacceptable.\nNote that doing this involves tradeoffs.\nIn the case of `StackFuture`, you must set a compile-time limit on the maximum size of future that will be supported.\nIf you need to support async functions in `dyn Trait` objects but these constraints do not apply to you, you may be better served by the [`async-trait`](https://crates.io/crates/async-trait) crate.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "stackfuture", "org_name": "microsoft", "org_repo": "microsoft/stackfuture", "platform_org_repo": "github+microsoft/stackfuture", "link_to_repo": "https://github.com/microsoft/stackfuture", "platform": "github", "language": "Rust", "stargazers_count": 288, "watchers_count": 288}, {"README_text": "////\nDO NOT EDIT THIS FILE. IT WAS GENERATED.\nManual changes to this file will be lost when it is generated again.\nEdit the files in the src/main/asciidoc/ directory instead.\n////\n\n\n[[spring-cloud-stream-binder-dapr-reference]]\n= Spring Cloud Stream Dapr Binder Reference Guide\n\n== 1. Usage\n\nTo use `Dapr Binder`, you need to clone this project and deploy it into your local maven repository.\n\n[source,shell]\n----\ngit clone git@github.com:microsoft/spring-cloud-stream-binder-dapr.git\n./mvnw clean install\n----\n\nThen you need add `spring-cloud-stream-binder-dapr` as a dependency to your `Spring Cloud Stream` application, as shown in the following example for Maven:\n\n[source,xml]\n----\n<dependency>\n   <groupId>com.azure.spring</groupId>\n   <artifactId>spring-cloud-stream-binder-dapr</artifactId>\n   <version>1.0.0-SNAPSHOT</version>\n</dependency>\n----\n\nAlternatively, you can use the `spring-cloud-starter-stream-dapr`, as follows:\n\n[source,xml]\n----\n<dependency>\n   <groupId>com.azure.spring</groupId>\n   <artifactId>spring-cloud-starter-stream-dapr</artifactId>\n   <version>1.0.0-SNAPSHOT</version>\n</dependency>\n----\n\n== 2. Spring Cloud Stream Dapr Binder Overview\n\nhttps://dapr.io[Dapr] is an event-driven, portable distributed application runtime, which provides the execution environment that distributed applications need to rely on.\n\nAs a runtime, `Dapr` can build our microservices on the cloud or edge computing and provides a unified API interface call for applications upward, making `Dapr` supporting different programming languages and frameworks.\n\n`Dapr` also provides our developers with standard APIs and pluggable and replaceable components to organize various distributed capabilities for our applications, and encapsulate these distributed capabilities as runtimes to simplify distributed application development.\n\nhttps://docs.spring.io/spring-cloud-stream/docs/current/reference/html/spring-cloud-stream.html#spring-cloud-stream-reference[Spring Cloud Stream] establishes a connection between the application and the middleware, separates the logic of sending and receiving messages with specific middleware from the application, and is finally provided by Binder. Binder shields the details of the underlying MQ message middleware and decouples our application from the specific message middleware.\n\nIn addition, `Spring Cloud Stream` abstracts the MQ in the `Spring Cloud` system and provides a unified API for sending and receiving messages, in order to simplify the development of messages in `Spring Cloud` applications.\n\nCombining `Spring Cloud Stream` and `Dapr`, on the one hand `Dapr` can make up the dependent on a specific middleware library for `Spring Cloud Stream`, on the other hand, `Spring Cloud Stream` can help application and `Dapr` client decoupling. Therefore, the combination of the two is conducive to the maximum decoupling and enhanced features of both sides.\n\nThe ultimate goal and vision of https://github.com/microsoft/spring-cloud-stream-binder-dapr[Spring Cloud Stream Dapr Binder](Dapr Binder) is to strip all non-business logic parts from the application, so that developers can pay more attention to the development of business logic.\n\nimage::https://user-images.githubusercontent.com/42743274/182814078-45fb0cdc-a2da-40ad-8a4c-ed395b692fa6.png[scaledwidth=\"100%\"]\n\nIn `Dapr Binder`, we regard the entire https://docs.dapr.io/concepts/overview/#sidecar-architecture[Dapr Sidecar] as an MQ. When sending messages, we call Grpc's Client, and when receiving messages, we create a Grpc's Server and open two interfaces to monitor Dapr's calls. One is to tell `Dapr`, which topics have I subscribed to, and the other is to receive messages from the subscribed topics. This is what we at `Dapr Binder` do.\n\n== 3. Configuration Options\n\nThis section contains the configuration options used by the `Dapr Binder`.\n\nFor common configuration options and properties pertaining to the binder, see the https://docs.spring.io/spring-cloud-stream/docs/current/reference/html/#_configuration_options[binding properties] in core documentation.\n\n=== 3.1. Dapr Binder Properties\n\nThe following properties are available for `Dapr Binder` only and must be prefixed with `spring.cloud.stream.dapr.binder.`.\n\nspring.cloud.stream.dapr.binder.daprIp::\nThe parameter of the channel, indicating the IP address of the dapr sidecar to which the message is finally sent through the channel.\n+\nDefault: `127.0.0.1`\n\nspring.cloud.stream.dapr.binder.daprPort::\nThe parameter of channel, indicating the port that the dapr sidecar to which the message is finally sent through the channel listens.\n+\nDefault: `50001`\n\nspring.cloud.stream.dapr.binder.negotiationType::\nNegotiationType decide which connection method to use. TLS and PLAINTEXT are currently available.\n\n- PLAINTEXT: Use of a plaintext connection to the server. By default a secure connection mechanism such as TLS will be used.\nShould only be used for testing or for APIs where the use of such API or the data exchanged is not sensitive.\nThis assumes prior knowledge that the target of this channel is using plaintext. It will not perform HTTP/1.1 upgrades.\n- TLS: Makes the client use TLS.\n+\nDefault: `PLAINTEXT`\n\nspring.cloud.stream.dapr.binder.authority::\nOverrides the authority used with TLS and HTTP virtual hosting. It does not change what host is actually connected to. Is commonly in the form host:port.\n+\nThis method is intended for testing, but may safely be used outside of tests as an alternative to DNS overrides.\n\nspring.cloud.stream.dapr.binder.defaultLoadBalancingPolicy::\nSets the default load-balancing policy that will be used if the service config doesn't specify one.\n+\nThis method is implemented by all stock channel builders that are shipped with gRPC, but may not be implemented by custom channel builders, in which case this method will throw.\n\nspring.cloud.stream.dapr.binder.idleTimeout::\nSet the duration without ongoing RPCs before going to idle mode.\n+\nIn idle mode the channel shuts down all connections, the NameResolver and the LoadBalancer. A new RPC would take the channel out of idle mode. A channel starts in idle mode. Defaults to 30 minutes.\n+\nThis is an advisory option. Do not rely on any specific behavior related to this option.\n+\nTimeUnit: `minutes`\n\nspring.cloud.stream.dapr.binder.keepAliveTime::\nSets the time without read activity before sending a keepalive ping. An unreasonably small value might be increased, and Long.MAX_VALUE nano seconds or an unreasonably large value will disable keepalive. Defaults to infinite.\n+\nClients must receive permission from the service owner before enabling this option. Keepalives can increase the load on services and are commonly \"invisible\" making it hard to notice when they are causing excessive load. Clients are strongly encouraged to use only as small of a value as necessary.\n+\nTimeUnit: `minutes`\n\nspring.cloud.stream.dapr.binder.keepAliveTimeout::\nSets the time waiting for read activity after sending a keepalive ping. If the time expires without any read activity on the connection, the connection is considered dead. An unreasonably small value might be increased. Defaults to 20 seconds.\n+\nThis value should be at least multiple times the RTT to allow for lost packets.\n+\nTimeUnit: `seconds`\n\nspring.cloud.stream.dapr.binder.perRpcBufferLimit::\nSets the per RPC buffer limit in bytes used for retry. The RPC is not retriable if its buffer limit is exceeded. The implementation may only estimate the buffer size being used rather than count the exact physical memory allocated. It does not have any effect if retry is disabled by the client.\n+\nThis method may not work as expected for the current release because retry is not fully implemented yet.\n\n\nspring.cloud.stream.dapr.binder.retryBufferSize::\nSets the retry buffer size in bytes. If the buffer limit is exceeded, no RPC could retry at the moment, and in hedging case all hedges but one of the same RPC will cancel. The implementation may only estimate the buffer size being used rather than count the exact physical memory allocated. The method does not have any effect if retry is disabled by the client.\n+\nThis method may not work as expected for the current release because retry is not fully implemented yet.\n\nspring.cloud.stream.dapr.binder.keepAliveWithoutCalls::\nSets whether keepalive will be performed when there are no outstanding RPC on a connection. Defaults to false.\n+\nClients must receive permission from the service owner before enabling this option. Keepalives on unused connections can easilly accidentally consume a considerable amount of bandwidth and CPU. `idleTimeout()` should generally be used instead of this option.\n\nspring.cloud.stream.dapr.binder.maxInboundMessageSize::\nSets the maximum message size allowed to be received on the channel. If not called, defaults to 4 MiB. The default provides protection to clients who haven't considered the possibility of receiving large messages while trying to be large enough to not be hit in normal usage.\n+\nThis method is advisory, and implementations may decide to not enforce this. Currently, the only known transport to not enforce this is InProcessTransport.\n\nspring.cloud.stream.dapr.binder.maxInboundMetadataSize::\nSets the maximum size of metadata allowed to be received. Integer.MAX_VALUE disables the enforcement. The default is implementation-dependent, but is not generally less than 8 KiB and may be unlimited.\n+\nThis is cumulative size of the metadata. The precise calculation is implementation-dependent, but implementations are encouraged to follow the calculation used for https://httpwg.org/specs/rfc7540.html#rfc.section.6.5.2[HTTP/2's SETTINGS_MAX_HEADER_LIST_SIZE] . It sums the bytes from each entry's key and value, plus 32 bytes of overhead per entry.\n\nspring.cloud.stream.dapr.binder.maxRetryAttempts::\nSets the maximum number of retry attempts that may be configured by the service config. If the service config specifies a larger value it will be reduced to this value. Setting this number to zero is not effectively the same as disableRetry() because the former does not disable https://github.com/grpc/proposal/blob/master/A6-client-retries.md#transparent-retries[transparent retry] .\n+\nThis method may not work as expected for the current release because retry is not fully implemented yet.\n\nspring.cloud.stream.dapr.binder.maxHedgedAttempts::\nSets the maximum number of hedged attempts that may be configured by the service config. If the service config specifies a larger value it will be reduced to this value.\n+\nThis method may not work as expected for the current release because retry is not fully implemented yet.\n\nspring.cloud.stream.dapr.binder.maxTraceEvents::\nSets the maximum number of channel trace events to keep in the tracer for each channel or subchannel. If set to 0, channel tracing is effectively disabled.\n\n=== 3.2. Dapr Producer Properties\n\nThe following properties are available for `Dapr` producers only and must be prefixed with `spring.cloud.stream.dapr.bindings.<bindingTarget>.producer.`.\n\npubsubName::\nSpecifies the name of the Pub/Sub component.\n+\nNOTE: PubsubName must be specified and has no default value.\n\n=== 3.3. Dapr Consumer Properties\n\nThe following properties are available for `Dapr` consumers only and must be prefixed with `spring.cloud.stream.dapr.bindings.<bindingTarget>.consumer.`.\n\npubsubName::\nSpecifies the name of the Pub/Sub component.\n+\nNOTE: PubsubName must be specified and has no default value.\n\n=== 3.4. Dapr Grpc Service Properties\n\nDapr Binder receives messages by starting a Grpc Server, and Grpc Server depends on the https://github.com/yidongnan/grpc-spring-boot-starter[grpc-spring-boot-starter] dependency. The relevant parameter configuration can be referred to https://yidongnan.github.io/grpc-spring-boot-starter/en/server/configuration.html[Configuration of Grpc Spring Boot Starter].\n\n\n=== 3.5. Dapr Message Headers\n\nThe following table illustrates how `Dapr` message properties are mapped to Spring message headers.\n\n\n[width=100%]\n|===\n| Dapr Message Properties         | Spring Message Header Constants       | Type                 | Description\n| contentType                     | DaprHeaders#CONTENT_TYPE              | String               | The contentType tells Dapr which content type your data adheres to when constructing a CloudEvent envelope.\n| ttlInSeconds                    | DaprHeaders#TTL_IN_SECONDS            | Long                 | The number of seconds for the message to expire.\n| rawPayload                      | DaprHeaders#RAW_PAY_LOAD              | Boolean              | Determine if Dapr should publish the event without wrapping it as CloudEvent. Not using CloudEvents disables support for tracing, event deduplication per messageId, content-type metadata, and any other features built using the CloudEvent schema.\n| specifiedBrokerMetadata         | DaprHeaders#SPECIFIED_Broker_METADATA | Map<String, String>  | Some metadata parameters are available based on each pubsub broker component.\n|===\n== 4. Migration Guide\n\n=== 4.1 Migration from Spring Cloud Azure Stream Binder Event Hubs\n==== 1. Update dependency\n\nRemove `spring-cloud-azure-stream-binder-eventhubs` dependencies.\n\n[source,yaml]\n----\n<dependency>\n  <groupId>com.azure.spring</groupId>\n  <artifactId>spring-cloud-azure-stream-binder-eventhubs</artifactId>\n</dependency>\n----\nAdd `spring-cloud-stream-binder-dapr` dependencies.\n\n[source,yaml]\n----\n<dependency>\n  <groupId>com.azure.spring</groupId>\n  <artifactId>spring-cloud-stream-binder-dapr</artifactId>\n</dependency>\n----\n\n==== 2. Update application.yaml\n\n- Remove all configurations prefixed with `spring.cloud.azure.eventhubs.`.\n- Remove all configurations prefixed with `spring.cloud.stream.eventhubs.`.\n- Add configurations prefixed with `spring.cloud.stream.dapr.` and specify pubsubName.\n\nThe final pubsub.yaml is as follows:\n\n- `spring.cloud.stream.bindings.<binding>.destination` is configured the topic specified for sending messages.\n- `spring.cloud.stream.dapr.bindings.<binding>.producer.pubsubName` is configured the name of the Pub/Sub component specified for sending messages.\n- `spring.cloud.stream.dapr.bindings.<binding>.consumer.pubsubName` is configured the name of the Pub/Sub component specified for receiving messages.\n[source,yaml]\n----\nspring:\n  cloud:\n    stream:\n      function:\n        definition: supply;consume\n      bindings:\n        supply-out-0:\n          destination: <AZURE_EVENTHUB_NAME>\n        consume-in-0:\n          destination: <AZURE_EVENTHUB_NAME>\n      dapr:\n        bindings:\n          supply-out-0:\n            producer:\n              pubsubName: eventhubs-pubsub\n          consume-in-0:\n            consumer:\n              pubsubName: eventhubs-pubsub\n----\n\n==== 3. Configure Azure Event Hubs component\n\n`Dapr` integrates with `Pub/Sub` message buses to provide applications with the ability to create event-driven, loosely coupled architectures where producers send events to consumers via topics.\n\n`Dapr` supports the configuration of multiple, named, `Pub/Sub components` per application. Each `Pub/Sub component` has a name and this name is used when publishing a message topic.\n\n`Pub/Sub components` are extensible. A list of support `Pub/Sub components` is https://docs.dapr.io/reference/components-reference/supported-pubsub/[here] and the implementations can be found in the https://github.com/dapr/components-contrib[components-contrib repo].\n\nIn this example, we configure the `Azure Event Hubs Pub/Sub component` described using the `pubsub.yaml` file:\n\n[source,yaml]\n----\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: eventhubs-pubsub\nspec:\n  type: pubsub.azure.eventhubs\n  version: v1\n  metadata:\n    - name: connectionString\n      value: \"<AZURE_CONNECTION_STRING>\"\n    - name: storageAccountName\n      value: \"<AZURE_STORAGE_ACCOUNT_NAME>\"\n    - name: storageAccountKey\n      value: \"<AZURE_STORAGE_ACCOUNT_KEY>\"\n    - name: storageContainerName\n      value: \"<AZURE_STORAGE_CONTAINER_NAME>\"\n----\n\nFollow the instructions https://docs.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal[here] to manage the storage account access keys.\nSee https://docs.microsoft.com/azure/event-hubs/event-hubs-get-connection-string[here] on how to get the Event Hubs connection string.\n\n\n==== 4. Run application with Dapr sidecar\n\n[source,shell]\n----\ndapr run --app-id dapr-app --app-port 9090 --components-path ${componentsPath}  --app-protocol grpc --dapr-grpc-port ${daprPort} mvn spring-boot:run\n----\n\nThis command specifies:\n\n- the id for your application with `--app-id dapr-app`, used for service discovery.\n- the port your application is listening on (default -1) with `--app-port 9090`.\n- the path for components directory with `--components-path ./components`.\n- the protocol (gRPC or HTTP) Dapr with `--app-protocol grpc` uses to talk to the application.\n- the gRPC port for Dapr to listen on (default -1) with `--dapr-grpc-port 50001`\n\n==== 5. Clean Up\nTo stop your services from running, simply stop the `dapr run` process. Alternatively, you can spin down your services with the Dapr CLI `dapr stop` command.\n\n[source,shell]\n----\ndapr stop --app-id dapr-app\n----\n\n== Appendices\n[appendix]\n[[building]]\nIn the maven of the project we use `Spring Cloud Stream` as parent, thus to build it locally, you can refer to https://github.com/spring-cloud/spring-cloud-stream#building[Spring Cloud Stream].\n", "repo_name": "spring-cloud-stream-binder-dapr", "org_name": "microsoft", "org_repo": "microsoft/spring-cloud-stream-binder-dapr", "platform_org_repo": "github+microsoft/spring-cloud-stream-binder-dapr", "link_to_repo": "https://github.com/microsoft/spring-cloud-stream-binder-dapr", "platform": "github", "language": "Java", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Microsoft Application Insights JavaScript SDK - React Native Plugin\n\nReact Native Plugin for the Application Insights Javascript SDK\n\n<tags\n    ms.service=\"application-insights\"\n    ms.workload=\"tbd\"\n    ms.tgt_pltfrm=\"ibiza\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"08/24/2015\"/>\n\n## Getting Started\n\n> By Default: **This plugin relies on [`react-native-device-info`](https://github.com/rebeccahughes/react-native-device-info). You must install and link this package. Keep `react-native-device-info` up-to-date to collect the latest device names using your app.**\n>\n> Since v3, support for accessing the DeviceInfo has been abstracted into an interface ```IDeviceInfoModule``` to enable you to use / set your own device info module. This interface uses the same function names and result `react-native-device-info`.\n\n```zsh\nnpm install --save @microsoft/applicationinsights-react-native @microsoft/applicationinsights-web\nnpm install --save react-native-device-info\nreact-native link react-native-device-info\n```\n\n## Initializing the Plugin\n\nTo use this plugin, you only need to construct the plugin and add it as an `extension` to your existing Application Insights instance.\n\n```ts\nimport { ApplicationInsights } from '@microsoft/applicationinsights-web';\nimport { ReactNativePlugin } from '@microsoft/applicationinsights-react-native';\n\nvar RNPlugin = new ReactNativePlugin();\nvar appInsights = new ApplicationInsights({\n    config: {\n        instrumentationKey: 'YOUR_INSTRUMENTATION_KEY_GOES_HERE',\n        extensions: [RNPlugin]\n    }\n});\nappInsights.loadAppInsights();\n```\n\n### Disabling automatic device info collection\n\n```ts\nimport { ApplicationInsights } from '@microsoft/applicationinsights-web';\n\nvar RNPlugin = new ReactNativePlugin();\nvar appInsights = new ApplicationInsights({\n    config: {\n        instrumentationKey: 'YOUR_INSTRUMENTATION_KEY_GOES_HERE',\n        disableDeviceCollection: true,\n        extensions: [RNPlugin]\n    }\n});\nappInsights.loadAppInsights();\n```\n\n### Using your own device info collection class\n\n```ts\nimport { ApplicationInsights } from '@microsoft/applicationinsights-web';\n\n// Simple inline constant implementation\nconst myDeviceInfoModule = {\n    getModel: () => \"deviceModel\",\n    getDeviceType: () => \"deviceType\",\n    // v5 returns a string while latest returns a promise\n    getUniqueId: () => \"deviceId\",         // This \"may\" also return a Promise<string>\n};\n\nvar RNPlugin = new ReactNativePlugin();\nRNPlugin.setDeviceInfoModule(myDeviceInfoModule);\n\nvar appInsights = new ApplicationInsights({\n    config: {\n        instrumentationKey: 'YOUR_INSTRUMENTATION_KEY_GOES_HERE',\n        extensions: [RNPlugin]\n    }\n});\n\nappInsights.loadAppInsights();\n```\n## Requirements\nYou must be using a version `>=2.0.0` of `@microsoft/applicationinsights-web`. This plugin will only work in react-native apps, e.g. it will not work with `expo`.\n\n## Device Information Collected\n\nBy default, this plugin automatically collects\n - **Unique Device ID** (also known as Installation ID)\n - **Device Model Name** (iPhone XS, etc.)\n - **Device Type** (Handset, Tablet, etc.)\n\n## IDeviceInfoModule\n\n```typescript\n/**\n * Interface to abstract how the plugin can access the Device Info, this is a stripped\n * down version of the \"react-native-device-info\" interface and is mostly supplied for\n * testing.\n */\nexport interface IDeviceInfoModule {\n    /**\n     * Returns the Device Model\n     */\n    getModel: () => string;\n\n    /**\n     * Returns the device type\n     */\n    getDeviceType: () => string;\n\n    /**\n     * Returns the unique Id for the device, to support both the current version and previous\n     * versions react-native-device-info, this may return either a `string` or `Promise<string>`,\n     * when a promise is returned the plugin will \"wait\" for the promise to `resolve` or `reject`\n     * before processing any events. This WILL cause telemetry to be BLOCKED until either of these\n     * states, so when returning a Promise it MUST `resolve` or `reject` it can't just never resolve.\n     * There is a default timeout configured via `uniqueIdPromiseTimeout` to automatically unblock\n     * event processing when this issue occurs.\n     */\n    getUniqueId: () => Promise<string> | string;\n}\n```\n\nIf events are getting \"blocked\" because the `Promise` returned via `getUniqueId` is never resolved / rejected\nyou can call `setDeviceId()` on the plugin to \"unblock\" this waiting state. There is also an automatic timeout\nconfigured via `uniqueIdPromiseTimeout` (defaults to 5 seconds), which will internally call `setDeviceId()` with\nany previously configured value.\n\n## Compatibility Matrix\n\n| Version |  Application Insights | React Native         | Branch\n|---------|-----------------------|----------------------|-----------\n| TBD     | TBD (^3.x)            |                      | [main](https://github.com/microsoft/applicationinsights-react-native)\n| 3.0.3   | ^2.8.14               | *<br/>dev:^0.69.9    | [release3.x](https://github.com/microsoft/applicationinsights-react-native/tree/release3.x)\n| 3.0.2   | ^2.8.12               | *<br/>dev:^0.69.9    | [main](https://github.com/microsoft/applicationinsights-react-native)\n| 3.0.1   | ^2.8.10               | *<br/>dev:^0.69.8    | [main](https://github.com/microsoft/applicationinsights-react-native)\n| 3.0.0   | ^2.8.5                | *<br/>dev:^0.69.3    | [main](https://github.com/microsoft/applicationinsights-react-native)\n| 2.5.6   | ^2.8.5                | *<br/>dev:^0.68.0    | [main](https://github.com/microsoft/applicationinsights-react-native) <-- First release from this repo\n| 2.5.5   | 2.8.5                 | *<br/>dev:^0.68.0    | [main](https://github.com/microsoft/applicationinsights-react-native) and [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.5.4   | 2.8.4                 | *<br/>dev:^0.68.0    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.5.3   | 2.8.3                 | *<br/>dev:^0.68.0    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.5.2   | 2.8.2                 | *<br/>dev:^0.68.0    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.5.1   | 2.8.1                 | *<br/>dev:^0.68.0    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.5.0   | 2.8.0                 | *<br/>dev:^0.68.0    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.4.4   | 2.7.4                 | *<br/>dev:^0.64.2    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.4.3   | 2.7.3                 | *<br/>dev:^0.64.2    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.4.2   | 2.7.2                 | *<br/>dev:^0.64.2    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.4.1   | 2.7.1                 | *<br/>dev:^0.64.2    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.4.0   | 2.7.0                 | *<br/>dev:^0.64.2    | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.5   | ^2.6.5                | *<br/>dev:0.64.2     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.4   | ^2.6.4                | *<br/>dev:0.64.2     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.3   | ^2.6.3                | *<br/>dev:0.64.2     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.2   | ^2.6.2                | *<br/>dev:0.63.2     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.1   | ^2.6.2                | *<br/>dev:0.59.8     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n| 2.3.0   | ^2.6.0                | *<br/>dev:0.59.8     | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/\n\n## Nightly Builds\n\nTo aid with testing and validation we also produce and publish nightly builds whenever there is a change from the previous build. These builds are published to the [NpmJs registry](https://www.npmjs.com/package/@microsoft/applicationinsights-react-native) on a successful build / test pass.\n\nThis process also [tags the source code](https://github.com/microsoft/applicationInsights-react-native/tags) so that we can track the specific changes included using a nightly build specific version number which is the format \"nightly-yymm-##\" eg. ```nightly-2208-02```\n\nThese nightly builds will not be retained indefinitely and should only be used for __pre-production__ testing and/or validation of any changes that have not yet been released.\n\n### NPM\n\nThe NPM builds are tagged as \"nightly\" and can by downloaded using this as the version number ```npm install @microsoft/applicationinsights-react-native@nightly``` or using the nightly specific version number which is \"nightly.yyyymm-###\" (```npm install @microsoft/applicationinsights-react-native@2.7.3-nightly.2112-08```) where ## is the specific build number for the month (Note, slightly different version from the source code tag due to compatibility issues between the different systems).\n\n### Deployment process\n\nWhen a new release is deployed the following occurs as part of the release\n\n- NPM packages are created and published to [NpmJs](https://www.npmjs.com/package/@microsoft/applicationinsights-react-native)\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Data Collection\n\nAs this SDK is designed to enable applications to perform data collection which is sent to the Microsoft collection endpoints the following is required to identify our privacy statement.\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## License\n\n[MIT](LICENSE)\n", "repo_name": "applicationinsights-react-native", "org_name": "microsoft", "org_repo": "microsoft/applicationinsights-react-native", "platform_org_repo": "github+microsoft/applicationinsights-react-native", "link_to_repo": "https://github.com/microsoft/applicationinsights-react-native", "platform": "github", "language": "JavaScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Microsoft Application Insights JavaScript SDK - React Plugin\n\n[![npm version](https://badge.fury.io/js/%40microsoft%2Fapplicationinsights-react-js.svg)](https://badge.fury.io/js/%40microsoft%2Fapplicationinsights-react-js)\n\nReact Plugin for the Application Insights Javascript SDK, enables the following:\n\n- Tracking of router changes\n- React components usage statistics\n\nFull documentation for the React Plugin for the Application Insights JavaScript SDK can be found on [Microsoft Docs](https://docs.microsoft.com/azure/azure-monitor/app/javascript-react-plugin).\n\n## Getting Started\n\nInstall npm package:\n\n```bash\nnpm install @microsoft/applicationinsights-react-js\n```\n\n## Basic Usage\n\n```js\nimport React from 'react';\nimport { ApplicationInsights } from '@microsoft/applicationinsights-web';\nimport { ReactPlugin, withAITracking } from '@microsoft/applicationinsights-react-js';\nimport { createBrowserHistory } from \"history\";\n\nconst browserHistory = createBrowserHistory({ basename: '' });\nvar reactPlugin = new ReactPlugin();\nvar appInsights = new ApplicationInsights({\n    config: {\n        instrumentationKey: 'YOUR_INSTRUMENTATION_KEY_GOES_HERE',\n        extensions: [reactPlugin],\n        extensionConfig: {\n          [reactPlugin.identifier]: { history: browserHistory }\n        }\n    }\n});\nappInsights.loadAppInsights();\n\n// To instrument various React components usage tracking, apply the `withAITracking` higher-order\n// component function.\n\nclass MyComponent extends React.Component {\n    ...\n}\n\nexport default withAITracking(reactPlugin, MyComponent);\n```\nFor `react-router v6` or other scenarios where router history is not exposed, appInsights config `enableAutoRouteTracking` can be used to auto track router changes.\n\n```js\nvar reactPlugin = new ReactPlugin();\nvar appInsights = new ApplicationInsights({\n    config: {\n        instrumentationKey: 'YOUR_INSTRUMENTATION_KEY_GOES_HERE',\n        enableAutoRouteTracking: true,\n        extensions: [reactPlugin]\n        }\n    }\n});\nappInsights.loadAppInsights();\n```\n\n\n\n## Configuration\n\n| Name | Default | Description |\n|------|---------|-------------|\n| history | null | React router history for more information see the [documentation][react-router] of the `react-router` package. |\n\n#### React components usage tracking\n\nTo instrument various React components usage tracking, apply the `withAITracking` higher-order\ncomponent function.\n\nIt will measure time from the `ComponentDidMount` event through the `ComponentWillUnmount` event.\nHowever, in order to make this more accurate, it will subtract the time in which the user was idle.\nIn other words, `React Component Engaged Time = ComponentWillUnmount timestamp - ComponentDidMount timestamp - idle time`.\n\nTo see this metric in the Azure portal you need to navigate to the Application Insights resource, select \"Metrics\" tab and configure the empty charts to display Custom metric named \"React Component Engaged Time (seconds)\", select aggregation (sum, avg, etc.) of your liking and apply split by \"Component Name\".\n\n![image](https://user-images.githubusercontent.com/1005174/51357010-c168ac80-1a71-11e9-8df9-348febd2d6dd.png)\n\nYou can also run custom queries to slice and dice AI data to generate reports and visualizations as per your requirements. In the Azure portal, navigate to the Application Insights resource, select \"Analytics\" from the top menu of the Overview tab and run your query.\n\n![image](https://user-images.githubusercontent.com/1005174/51356821-e872ae80-1a70-11e9-9e12-e56a1edcde68.png)\n\nPlease note that it can take up to 10 minutes for new custom metric to appear in the Azure Portal.\n\n## Sample App\n\n[Azure-Samples/application-insights-react-demo](https://github.com/Azure-Samples/application-insights-react-demo).\n\n## React Router\n\n[react-router]: https://github.com/ReactTraining/react-router/blob/master/FAQ.md#how-do-i-access-the-history-object-outside-of-components\n\n## Compatibility Matrix\n\n| Version |  Application Insights | React     | Branch\n|---------|-----------------------|-----------|-----------\n| 3.4.2   | ^2.8.12               | >= 17.0.1 | [main](https://github.com/microsoft/applicationinsights-react-js)\n| 3.4.1   | ^2.8.10               | >= 17.0.1 | [main](https://github.com/microsoft/applicationinsights-react-js)\n| 3.4.0   | ^2.8.5                | >= 17.0.1 | [main](https://github.com/microsoft/applicationinsights-react-js)\n| 3.3.6   | ^2.8.5                | ^17.0.1   | [main](https://github.com/microsoft/applicationinsights-react-js) <-- First release from this repo\n| 3.3.5   | 2.8.5                 | ^17.0.1   | [main](https://github.com/microsoft/applicationinsights-react-js) and [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.3.4   | 2.8.4                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.3.3   | 2.8.3                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.3.2   | 2.8.2                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.3.1   | 2.8.1                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.3.0   | 2.8.0                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.2.4   | 2.7.4                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.2.3   | 2.7.3                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.2.2   | 2.7.2                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.2.1   | 2.7.1                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.2.0   | 2.7.0                 | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.5   | ^2.6.5                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.4   | ^2.6.4                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.3   | ^2.6.3                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.2   | ^2.6.2                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.1   | ^2.6.2                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.1.0   | ^2.6.0                | ^17.0.1   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.0.5   | ^2.5.10               | ^16.0.0   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n| 3.0.4   | ^2.5.9                | ^16.0.0   | [AI master](https://github.com/microsoft/ApplicationInsights-JS/tree/master/extensions/applicationinsights-react-js)\n\n## Nightly Builds\n\nTo aid with testing and validation we also produce and publish nightly builds whenever there is a change from the previous build. These builds are published to the [NpmJs registry](https://www.npmjs.com/package/@microsoft/applicationinsights-react-js) on a successful build / test pass.\n\nThis process also [tags the source code](https://github.com/microsoft/applicationInsights-react-js/tags) so that we can track the specific changes included using a nightly build specific version number which is the format \"nightly-yymm-##\" eg. ```nightly-2208-05```\n\nThese nightly builds will not be retained indefinitely and should only be used for __pre-production__ testing and/or validation of any changes that have not yet been released.\n\n### NPM\n\nThe NPM builds are tagged as \"nightly\" and can by downloaded using this as the version number ```npm install @microsoft/applicationinsights-react-js@nightly``` or using the nightly specific version number which is \"nightly.yyyymm-###\" (```npm install @microsoft/applicationinsights-react-js@2.7.3-nightly.2112-08```) where ## is the specific build number for the month (Note, slightly different version from the source code tag due to compatibility issues between the different systems).\n\n### Deployment process\n\nWhen a new release is deployed the following occurs as part of the release\n\n- NPM packages are created and published to [NpmJs](https://www.npmjs.com/package/@microsoft/applicationinsights-react-js)\n\n## Release Notes\n\n[Release Notes](./RELEASES.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Data Collection\n\nAs this SDK is designed to enable applications to perform data collection which is sent to the Microsoft collection endpoints the following is required to identify our privacy statement.\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft\u2019s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n\n## License\n\n[MIT](LICENSE)\n", "repo_name": "applicationinsights-react-js", "org_name": "microsoft", "org_repo": "microsoft/applicationinsights-react-js", "platform_org_repo": "github+microsoft/applicationinsights-react-js", "link_to_repo": "https://github.com/microsoft/applicationinsights-react-js", "platform": "github", "language": "JavaScript", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# SQL Server Migration Assistant\n\nThis GitHub repository is for reporting any of the SQL Server Migration Assistant (SSMA) related issues. Issues submitted will be reviewed and answered by engineering team.\n\nMicrosoft SQL Server Migration Assistant (SSMA) is a tool designed to automate database migration to SQL Server from Microsoft Access, DB2, MySQL, Oracle, and SAP ASE.\n\nPublic Documentation : [SQL Server Migration Assistant - SQL Server | Microsoft Docs](https://docs.microsoft.com/en-us/sql/ssma/sql-server-migration-assistant?view=sql-server-ver16)\n\n## Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SSMA", "org_name": "microsoft", "org_repo": "microsoft/SSMA", "platform_org_repo": "github+microsoft/SSMA", "link_to_repo": "https://github.com/microsoft/SSMA", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Microsoft Azure Arc Community Monthly Meetup\n\n![Arc](./img/arc.png)\n\n## Overview\n\nOnce a month, the various Azure Hybrid Cloud product groups at Microsoft will hold a call to showcase new features, talk through important topics and engage in a Q&A regarding Azure Arc. The foundational goals of the call are highlighted below:\n\n- Provide the Azure Arc community with product updates\n- Host a short talk and/or demo on Azure Hybrid Cloud technologies and products technologies\n- Collect feedback from the community on issues, blockers, use cases, and questions related to Azure Hybrid Cloud technologies and products\n\n## Who is the \"community\"?\n\nIf you are a customer, partner, Microsoft employee, or just someone who loves tech, for us, you are part of our community. The content presented in our calls is **not under a non-disclosure agreement (NDA)** and is public because our mission is just to spread the \u2764\ufe0f for Azure Hybrid Cloud solutions and technologies.\n\n## Meetup agenda\n\nEach monthly meetup will be 1 hour, don't be late, we have a lot to cover \ud83e\udd13\n\n- 2 minutes: Welcome\n- 45-50 minutes: Product updates\n- 5-10 minutes: \u201cAsk us anything\u201d and feedback discussion\n\n## Meeting links and recordings\n\n| Asset | Link        |\n|:-----------|:------------|\n| \ud83c\udf6a Teams Channel - Guests | [If you are a guest, join the Microsoft Teams channel by submitting this form](https://aka.ms/joinazurearcmeetup)\n| \ud83c\udf6a Teams Channel - Microsoft FTE | [If you are a Microsoft FTE, join the Microsoft Teams channel using this link](https://teams.microsoft.com/l/team/19%3a227a226ae75f4ffabc67f77a9d439d15%40thread.tacv2/conversations?groupId=f4ccf9df-0dc2-4282-a392-652117be03e7&tenantId=72f988bf-86f1-41af-91ab-2d7cd011db47)\n| \ud83d\udcdd Meeting presentations | [View](https://github.com/microsoft/azure_arc_community/tree/main/Presentations)\n| \ud83c\udfa5 Meeting recordings | [Watch](https://aka.ms/ArcMeetup)\n| \ud83d\udcc5 Upcoming meetup calendar invite | [Download the _ics_ file](https://1drv.ms/u/s!ApeID0DmHjgfn21ruEnDrXcE0rEQ?e=lxfRJf)\n| \ud83c\udfa4 Azure Hybrid Community AMA (Twitter Spaces) | [Join Ask Me Anything (AMA)](./TwitterSpaces.md)\n\n> **NOTE**: If you are using Outlook for Mac, you may have trouble adding the invite to your calendar. In order to avoid issues, follow the steps below:\n\n1. Download the _.ics_ file\n2. Open Outlook for the web\n3. Drag and drop the _.ics_ file onto your calendar\n\n## Schedule\n\n\ud83d\udcc5 **2023 Upcoming Meetings**:\n\n- ~~January 31st, 08:00 AM Pacific Time (PST)~~\n- ~~February 28th, 08:00 AM Pacific Time (PST)~~\n- ~~March 28th, 08:00 AM Pacific Time (PST) - Canceled~~\n- ~~April 25th, 08:00 AM Pacific Time (PST)~~\n- May 30th, 08:00 AM Pacific Time (PST) - Canceled (US post Memorial Day weekend and //Build 2023)\n- June 27th, 08:00 AM Pacific Time (PST)\n\n## Azure Hybrid AMA on Twitter Spaces\n\nJoin Azure Hybrid & Multicloud experts from Microsoft and the MVP community to learn about new product features, releases and to find answers to challenging hybrid and multicloud problems. Live on Twitter! Learn more [here](TwitterSpaces.md/).\n\n## Azure Hybrid Cloud resources\n\n- [Azure Arc Documentations](https://docs.microsoft.com/azure/azure-arc/)\n- [Azure Stack HCI Documentations](https://docs.microsoft.com/azure-stack/hci/)\n- [Azure Arc Jumpstart](https://aka.ms/AzureArcJumpstart)\n- [Azure Arc Landing Zone Accelerators](https://aka.ms/ArcLZAcceleratorReady)\n\n## Contact Us\n\nReach out to us on our Teams channel (make sure to use the @General hashtag) or at [distribution list](mailto:arccustomermeetleads@microsoft.com) and we'll make sure to answer your questions as soon as possible!\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow.\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure_arc_community", "org_name": "microsoft", "org_repo": "microsoft/azure_arc_community", "platform_org_repo": "github+microsoft/azure_arc_community", "link_to_repo": "https://github.com/microsoft/azure_arc_community", "platform": "github", "language": null, "stargazers_count": 45, "watchers_count": 45}, {"README_text": "# Microsoft-Defender-for-Identity\n\nThis repository contains scripts, code examples and additional resources to improve customer experience with Microsoft Defender for Identity.\nIf you have an issue with Microsoft Defender for Identity, please open a support ticket.\nFor questions and feedback, please contact <mdifeedback@microsoft.com>\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  All contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Questions and feedback\n\nFor questions and feedback, please start a new discussion in the [Microsoft Defender for Identity Tech Community](https://techcommunity.microsoft.com/t5/microsoft-defender-for-identity/bd-p/AzureAdvancedThreatProtection)\n", "repo_name": "Microsoft-Defender-for-Identity", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-Defender-for-Identity", "platform_org_repo": "github+microsoft/Microsoft-Defender-for-Identity", "link_to_repo": "https://github.com/microsoft/Microsoft-Defender-for-Identity", "platform": "github", "language": "PowerShell", "stargazers_count": 50, "watchers_count": 50}, {"README_text": "# Microsoft Security DevOps Rules\r\n\r\n> This repo has been populated by an initial template to help get you started. Please\r\n> make sure to update the content to build a great experience for community-building.\r\n\r\nAs the maintainer of this project, please make a few updates:\r\n\r\n- Improving this README.MD file to provide a great experience\r\n- Updating SUPPORT.MD with content about this project's support experience\r\n- Understanding the security reporting process in SECURITY.MD\r\n- Remove this section from the README\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \r\ntrademarks or logos is subject to and must follow \r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "security-devops-rules", "org_name": "microsoft", "org_repo": "microsoft/security-devops-rules", "platform_org_repo": "github+microsoft/security-devops-rules", "link_to_repo": "https://github.com/microsoft/security-devops-rules", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "security-devops-policy", "org_name": "microsoft", "org_repo": "microsoft/security-devops-policy", "platform_org_repo": "github+microsoft/security-devops-policy", "link_to_repo": "https://github.com/microsoft/security-devops-policy", "platform": "github", "language": "PowerShell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "![project-screenshots/icon.png](project-screenshots/icon.png)\n\n## About the Project\nThis is an Azure DevOps Extension to Run Performance Test using Apache JMeter and Analyze Results.\n\nThis task enables to run Performance testing using Apache JMeter, Analyze report and post results.\n\nThis task uses Apache JMeter 5.5 (can be modified) and expects a valid parametrized JMX File, Any input Files, and property file for JMX. The task runs the jmx files according to the configured values in JMX and uses property file and parametrizes it with the variables listed in pipeline variables. You can additionally use any number of user input files based on your test and mention them in property file for use.\n\nThe pipeline task downloads your JMX, Property File, Input files and then runs the JMeter task and publishes the result to build pipeline artifacts.\n\nYou can additionally host the data on a web URL by using the Static Website hosting capacity of Azure storage account. The task publishes the results to your $web container of your storage container.\nYou need to enable static hosting in the storage container in order to be able to view html results. These results are published and ready for view immediately after test result. The link of hosted website is available in the logs.\n\n## Market Place\n\nYou can get the task [HERE](https://marketplace.visualstudio.com/items?itemName=id-az-pipeline.jmeter-perf-analyzer)\n\n## Latest Updates\n\n#### V1.5.44\n- Enabled Custom Plugin Jars to be copied to JMeter's lib/ext folder\n- Allowed input files to be copied without need to provide property file\n\n#### V1.3.80\n- Fixed Issue for Storage Account Binding Name. Updated Readme's.\n\n#### V1.2.57\n- Enabled Log analysis for JMeter. Pipeline can be marked as failed if failure count increase threshold specified.\n\n#### V1.1.55\n- Stable Version.\n\n## Demo\n\n![project-screenshots/Steps.gif](project-screenshots/Steps.gif)\n\n## Architecture\n\n![project-screenshots/archi.png](project-screenshots/archi.png)\n\n\n## Features\n\n- Run Apache JMeter Performance Tests\n- Produces Results for Analysis by publishing it to artifacts.\n- Uploads results to Azure Storage account using Static Store for viewing results through a online link.\n- Cross platform - Can be integrated along with any project\n- Can be used as a regression/integration test in build pipeline to ensure use-cases pass per PR Request\n- Can be used to tune and optimize performance based on the analysis.\n- Can Fail task if the JMeter results any failure. This would help in continuous integration test to identify any breaking change. Integrating this in build pipeline would help identify any breaking change before merge to main.\n\n\n## Tech Stack\n\n**Client:** Typescript, JavaScript\n\n**Server:** Node, Azure ARM Template, Azure Storage\n\n\n## Screenshots\n\n![project-screenshots/P4.png](project-screenshots/P4.png)\n\n\n## Sample Usage\n\nFollow this Link: [SAMPLE USAGE](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/blob/main/samples/README.md)\n\nFollow this Link: [YAML TASK BASED INPUT](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/blob/main/InputTaskYAML.md)\n\nFollow this Link: [CLASSIC PIPELINE GUI BASED TASK BASED INPUT](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/blob/main/InputTask.md)\n\n## Installation To your Pipeline\n\n1. Install the extension from Market Place: [HERE](https://marketplace.visualstudio.com/items?itemName=id-az-pipeline.jmeter-perf-analyzer)\n\n![project-screenshots/P1.png](project-screenshots/P1.png)\n\n2. Search for the extension in your pipeline task\n\n3. Add the task\n![project-screenshots/P4.png](project-screenshots/P4.png)\n\n4. Provide input to variables\n\n![project-screenshots/P5.png](project-screenshots/P5.png)\n![project-screenshots/P6.png](project-screenshots/P6.png)\n\n5. Trigger the Pipeline\n\n![project-screenshots/P7.png](project-screenshots/P7.png)\n\n6. Verify Pipeline artifacts results and html report. Html Reports as artifacts are supported only on Build Pipelines. For release pipeline you can host them on Azure Storage.\n\n![project-screenshots/P8.png](project-screenshots/P8.png)\n\n![project-screenshots/P10.png](project-screenshots/P10.png)\n\n## Pipeline Variables\n\nThe following variables are used in the pipeline. Some have default values which can be updated.\n\n`JMX Run File Source` : This provides you the option to provide your JMX Source. It can be either from the source code of the pipeline or it can be a direct external URL. This is set to default to SourceCode\n\n`JMX Run File Source Path`: This is the path of the JMX source code file. You can select the file path and update the input here. This is visible in case you select 'SourceCode' in the `JMX RUN File Source` step.\n\n`JMX Run File Source URL` : This provides you the option to provide your JMX Source File via a direct external URL. A https URL is expected here. This file will be downloaded from the link and used as JMX file source to run the test.\n\n`JMX Property File Source` : This provides you the option to provide your JMeter Property File Source. It can be none (in case your jmx does not require a property file to run), source code of the pipeline or it can be a direct external URL. This is set to default to SourceCode\n\n`JMX Property Source Path`: This is the path of the JMeter Property source code file. You can select the file path and update the input here. This is visible in case you select 'SourceCode' in the `JMX Property File Source` step.\n\n`JMeter Property Source URL` : This provides you the option to provide your JMeter Property File via a direct external URL. A https URL is expected here. This file will be downloaded from the link and used as JMX Property file source to run the test.\n\n`Token Regex\"` : This provides you the option to specify the regex for replacing the tokens in property File to the values specified in pipeline variables. This is visible only if the JMX Property File source is not set to None. This regex must include a group match. The regex match should be return a group of 2 values, one with variable name second with variable and enclosure. Samples (Starts and ends with 1 underscore ==>  _(\\\\w+)_ ,Starts and ends with % ==>  %(\\\\w+)%)\n\n`JMX Input File Source(s)\"` : This provides you the option to provide your JMeter Input File Source. Your test might require some test data stored in CSV, XLS etc. This tasks enables to procure those files and makes it available to test. It can be none (in case your jmx does not require any input file to run), source code of the pipeline or it can be a direct external URL. This is set to default to None. This option is visible only if `JMX Property File Source` is not set to None.\n\n`JMX Input Folder Source`: This is the path of the JMeter Input source code Folder. All files inside this folder will be made available to JMeter test. You should the file names mentioned here in the property file and use that variable in JMX. You can select the folder path and update the input here. This is visible in case you select 'SourceCode' in the `JMX Input File Source(s) Source` step.\n\n`JMX Input File(s) Source URL Comma Separated` : This provides you the option to provide your JMeter Input File(s) via a direct external URL(s). You can specify multiple URLs comma separated. All files mentioned will be downloaded sequentially and made available to JMeter for test run. One or many, comma Separated, https URLs are expected here.\n\n`Publish Logs and Test Results to Pipeline Artifacts`: This is a boolean flag that if set to true will publish the test results and logs to pipeline artifacts. This should not be enabled if this task is used in a release pipeline. This is default set to true and will yield the result on task completion.\n\n`Artifact Name for Apache JMeter Reports`: This is the output artifact name for the test results. Only visible if the `Publish Logs and Test Results to Pipeline Artifacts` is set to true.\n\n`Artifact Name for Apache JMeter Logs`: This is the output artifact name for the test logs. Only visible if the `Publish Logs and Test Results to Pipeline Artifacts` is set to true.\n\n`Fail Task if JMeter test fails`: This would fail the pipeline task in case any test fails. This is useful if your use-case is to identify any regression failure due to new deployment. Enabling this in CICD would help you monitor failures in jmx by failing the task itself.\n\n`Fail Task if test failure count is more than`: In case your JMeter test failure count is more than the number specified, then the pipeline will fail.\n\n`Add Custom Plugin(jars) To Jmeter Lib/ext Folder`: This would allow you to copy your custom plugins to Jmeter Lib/Ext Folder. Might be used in custom implementation and 3rd Party jars used to run JMX\n\n`Source Folder for Custom Plugins`: Source Type from where you want to copy custom plugins to JMeter's lib/ext folder.\n\n`Folder Path For JMeter Plugins`: This is the path of the Plugins Input source code Folder. All files inside this folder will be made available to Jmeter via copying to ext folder. You can select the folder path and update the input here. This is visible in case you select 'SourceCode' in the `Source Folder for Custom Plugins` step.\n\n`Custom Plugin File(s) Or Jar(s) Source URL Comma Separated`: This provides you the option to provide your Plugin File(s) via a direct external URL(s). You can specify multiple URLs comma separated. All files mentioned will be downloaded sequentially and made available to JMeter for test run via copying to ext folder. One or many, comma Separated, https URLs are expected here.This is visible in case you select 'urls' in the `Source Folder for Custom Plugins` step.\n\n`Additional Command Line Arguments`: The arguments specified here will be appened to the Jmeter run command. The run script looks like jmeter -q {propertFileIfAny} -n -t {JMXFile}  -l {LogFile} {Your Arguments go here}. Hence this can be '-JUser=admin -dSomeProperty=Value'. Please note that this is appened as it is hence User must ensure correctness of the command.\n\n`Copy Performance Test Result to Azure Blob Storage`: This is a boolean flag that if set to true enabling this would help to copy Apache JMeter's Performance Test Report, Log File and JTL File to be copied to Azure Blob Storage. Using Static website hosting turned on you can access any run report later as well via a direct link.\"\n\n`ConnectedServiceNameSelector`: This is a picklist option to specify where to make azure connection. Currently this is set to Azure Resource Manager only.\n\n`Azure Subscription`: This option lets you select the subscription in which you have created your storage container.\n\n`Destination Type`: This is where your output result and logs would be copied to. Currently only Azure Blob Storage is supported.\n\n`RM Storage Account`: This is the storage account name inside the above mentioned subscription. You need to `Enable Static Storage Hosting` option here and create a container named $web inside. For this container $web, you will need to change the access to publish in order to view html reports. You can either do this or create a CDN to expose data from here More details available here: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website.\n\n`Container Name`: This is the container name inside the storage account where your results and logs will be copied. This is defaulted to $web. Note that in case you want continuous reporting available on html webpage, keep it to $web only. Files in $web will be available to view as html files.\n\n`Blob Prefix`: This is the prefix inside container name where each run's report will be copied. It is essential to note that for continuous integration and testing it's best to prefix it will build number. This way all continuous build's result will be made available at all times as a http exposed UTL. If used in Build pipeline, it can be `Releases/Release_DEV_$(Build.BuildNumber)`, if in release then it can be `Release_DEV_$(Release.ReleaseName)_$(Release.AttemptNumber)`\n\n`Storage Container URI`: This is the Primary endpoint URI for your container. On Enabling static web hosting, this URL is made available. If not specified, the task will not be able to create a direct html link to the test results.\n\n#### Advance Variables\nThese variables are available under the `advanced` section of the pipeline. These are default to some values and do not require change unless you want to customize it.\n\n`JMeter Download URL`: This is the https link to JMeter binary. A TGZ binary file https URL is expected here. It is set to download 5.5 version of Apache JMeter by default.\n\n`Extracted folder name for JMeter binary`: In case you modify the JMeter Download URL, to some other version then this variable also needs to change. This is the folder name of the JMeter binary file post extraction/unzip. Also update this if you modify the above jmeter url to the extracted folder name\n\n`JMeter Custom Unzipped Folder Name`: In case you wish to unzip the Jmeter binary to a custom folder, you can update the path name here. This is useful if you want to have seperate Jmeter instances task in same pipeline. If this value is same as the value provided for `Extracted folder name for JMeter binary` then a hierarchy won't be created and jmeter binary would be directly extracted into `Extracted folder name for JMeter binary`. If this value is different than the value provided for `Extracted folder name for JMeter binary`, then a hierarchy would be created for extracting binary `JMeter Custom Unzipped Folder Name`/`Extracted folder name for JMeter binary`\n\n`Add random suffix to JMeter Folders`: This would enable you to create a unique jmeter folder by adding a random suffix at the end of download folder. There are at time possibility that multiple task run in same pipeline and hence would overwrite each other by the end of the pipeline run. Enabling this will ensure each task in a pipeline run would execute smoothly.\n\n`Add random suffix to JMeter Artifacts`: This would enable you to create a unique published folder(s) by adding a random suffix at the end of artifacts(log and report). There are at time possibility that multiple task run in same pipeline and hence would overwrite the logs and result of each other by the end of the pipeline run. Enabling this will ensure each task in a pipeline run would execute smoothly and generate build artifacts with unique names.\n\n`JMeter Log Folder Name`: This is the name of Log folder that is created as a result of test run. This is defaulted to `CurrentLog_$(System.StageName)`. This is one of the artifacts that is later published as well.\n\n`JMeter Report Folder Name\"`: This is the name of Result folder that is created as a result of test run. This is defaulted to `CurrentReport_$(System.StageName)`. This is one of the artifacts that is later published as well.\n\n`Telemetry Data Collection`: This flag enables the task to send telemetry data for improving the product over time. The task does not log any personal information. The task logs runs and errors for developers to analyze and provide a better version of the task. Turning this off would not allow the task to send logs to Microsoft.\n\n\n## Run Locally\n\nClone the project\n\n```bash\n  git clone https://github.com/microsoft/jmeter-performance-analyzer-devops-extension\n```\n\nGo to the project directory task\n\n```bash\n  cd perfanalyzer\n```\n\nInstall dependencies\n\n```bash\n  npm install\n```\n\nCompile the typescript file\n\n```bash\n  tsc index.ts\n```\n\nRun and debug locally using VS Code or Compiled Java-script file.\n\n\n## Packaging\n\nOnce Compiled using tsc and the dependencies are downloaded using `npm i` you can package the project to create a `vsix`\n\nUpdate the publisher name in vss-extension.json\n```bash\n  touch vss-extension.json\n```\n\nPackage the dependecies\n\n```bash\n  npx tfx extension create --manifest-globs vss-extension.json\n```\n\nYou might get an error with the following text\n\n```bash\n   Error: Part Name 'perfanalyzer/node_modules/azure-pipelines-tasks-azure-arm-rest-v2/openssl/OpenSSL License.txt'\n```\nThis is because of one of the dependencies  from azure-pipelines-tasks-azure-arm-rest-v2. There is a file named OpenSSL License.txt. You need to remove\nany blank space from this. Hence rename this file to OpenSSL_License.txt and rerun the command. You should get a successful output vsix file.\n\n```bash\n  === Completed operation: create extension ===\n - VSIX: C:\\R\\personal_projects\\PerfAnalyzer\\uat-id-az-pipeline.********-****-****-****-************-*.*.**.vsix\n - Extension ID: ********-****-****-****-************-*.*.**.vsix\n - Extension Version: 0.1.33\n - Publisher: uat-id-az-pipeline\n```\nYou can now publish this extension to your own organization and share it with them.\n\nFollow the link for more Update: [AZURE WEB Extension](https://docs.microsoft.com/en-us/azure/devops/extend/get-started/node?view=azure-devops)\n\n\n## Note\n\n### Data Collection.\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft\u2019s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n# Support\n\n## How to file issues and get help  \n\nThis project uses [GitHub Issues](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/issues) to track bugs and feature requests. Please search the existing\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or\nfeature request as a new Issue.\n\nFor help and questions about using this project, please create a Issue [Here](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/issues)\n\n## Microsoft Support Policy  \n\nSupport for this **PROJECT or PRODUCT** is limited to the resources listed above.\n\n\n## License\n\n[MIT License](https://github.com/microsoft/jmeter-performance-analyzer-devops-extension/blob/main/LICENSE.txt)\n", "repo_name": "jmeter-performance-analyzer-devops-extension", "org_name": "microsoft", "org_repo": "microsoft/jmeter-performance-analyzer-devops-extension", "platform_org_repo": "github+microsoft/jmeter-performance-analyzer-devops-extension", "link_to_repo": "https://github.com/microsoft/jmeter-performance-analyzer-devops-extension", "platform": "github", "language": "TypeScript", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Microsoft GPS CSA Tech Stack FY23\n\n## Azure Core\n### [Azure\u4e0a\u7684\u53cc\u673a\u9ad8\u53ef\u7528\u96c6\u7fa4\u642d\u5efa](./HACluster/readme.md)\n\n\n### [Azure\u4e0a\u901a\u8fc7Functions\u5b9e\u73b0httpdns\u7684\u7b80\u6613\u601d\u8def](./AzureHttpdns/Azure\u4e0a\u901a\u8fc7Functions\u5b9e\u73b0httpdns\u7684\u7b80\u6613\u601d\u8def.md)\n\n\n## App Innovation\n### [\u4f7f\u7528Azure\u6258\u7ba1Prometheus\u548cGrafana\u76d1\u63a7AKS](./\u4f7f\u7528Auzre\u6258\u7ba1Prometheus\u548cGrafana\u76d1\u63a7AKS)\n### [\u4f7f\u7528 ChatGPT API \u6784\u5efa\u667a\u80fd\u804a\u5929\u673a\u5668\u4eba\u5e76\u5c06\u5e76\u90e8\u7f72\u5230 Azure App Service \u548cMicrosoft Teams App](./Create-A-ChatGPT-Bot-APP-and-Deploy-To-Azure-APP-Service-or-Teams-APP/)\n&nbsp; \n\n## Data&AI\n### [\u4f7f\u7528Terraform/Ansible\u5728Azure\u4e0a\u90e8\u7f72Hadoop](https://github.com/microsoft/gps-csa-tech-stack/tree/main/HadoopWithTerraform#%E4%BD%BF%E7%94%A8terraformansible%E5%9C%A8azure%E4%B8%8A%E9%83%A8%E7%BD%B2hadoop)\n\n### [Azure Database for PostgreSQL\u8fc1\u79fb\u90e8\u7f72\u548c\u7279\u6027\u63a2\u7d22\u5b9e\u9a8c](./PostgresqlWorkshop/)\n\n\n### [\u4f7f\u7528Azure Machine Learning\u7684MLOps\u5b9e\u73b0\u5b8c\u6574\u7684\u673a\u5668\u5b66\u4e60\u9879\u76ee](./MLopsInADay/)\n\n\n&nbsp; \n## Hybrid Cloud\n&nbsp; \n## SAP on MS Cloud\n&nbsp; \n## Security\n&nbsp; \n## Biz Apps\n### [D365 CE/PP User Onboarding Automation Tool](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/D365%20Users%20Onboarding%20Automation#d365-cepp-user-onboarding-automation-tool)\n### [\u57fa\u4e8e21v PP\u73af\u5883\u90e8\u7f72CoE Starter Kit](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/PP%20COE%20Starter%20Kit%20Deployment%20in%2021V)\n### [DevOps\u81ea\u52a8\u5316\u90e8\u7f72PP\u53caD365 CE Solution](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/PP%26D365%20CE%20Solution%20Deployment%20with%20Azure%20DevOps)\n### [\u501f\u52a9Teams+PVA+OCS+Bot Framework Composer \u642d\u5efa\u4f01\u4e1a\u7ea7IT Helpdesk](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/Build%20IT%20HelpDesk%20with%20BizApp%20Platform)\n### [Power Platform\u642d\u5efa\u4f01\u4e1a\u5185\u90e8\u65b0\u95fb\u793e\u533a](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/Build%20News%20Community%20with%20Power%20Platform)\n### [\u4f7f\u7528dataflow\u5c06\u5916\u90e8\u6570\u636e\u52a0\u8f7d\u5230dataverse\u4e2d](https://github.com/microsoft/gps-csa-tech-stack/tree/main/BizApp-Lab/%E4%BD%BF%E7%94%A8dataflow%E5%B0%86%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0dataverse%E4%B8%AD)\n\n## Microsoft 365\n&nbsp; \n\n\n\n&nbsp; \n&nbsp; \n&nbsp; \n&nbsp; \n&nbsp; \n&nbsp; \n&nbsp; \n&nbsp; \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "gps-csa-tech-stack", "org_name": "microsoft", "org_repo": "microsoft/gps-csa-tech-stack", "platform_org_repo": "github+microsoft/gps-csa-tech-stack", "link_to_repo": "https://github.com/microsoft/gps-csa-tech-stack", "platform": "github", "language": "HCL", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "This is a combination of the following repos\n\nCBL-Mariner https://github.com/microsoft/CBL-Mariner/ \n\nMellanox/bfb-mariner https://github.com/Mellanox/bfb-mariner\n\n## Build BFB\n\nThe following steps will pull RPMs from Nvidia servers. These RPMs will be tied to Nvidia's licensing and user agreements.\n\n`$ sudo ./bfb-build`\n\nYou may need to run the following to configure qemu\n`docker run --rm --privileged multiarch/qemu-user-static --reset -p yes`\n\n## Flash BFB \n\nOn the linux machine connected via USB to the bf2, \nIf you do not have RSHIM installed:\n RSHIM package is available under: https://developer.nvidia.com/networking/doca\n https://www.mellanox.com/downloads/BlueField/RSHIM/rshim_2.0.6-3.ge329c69_amd64.deb\n\n\nDeploy the image to BF2 device via the rshim interface (assuming rshim0).\n\n`$ sudo bfb-install -r rshim0 -b your-bfb`\n\nYou can watch the process using a serial monitor like minicom on port /dev/rshim0/console\n\n`$ sudo minicom`\n\nConfigure the Linux rshim network interface static IP address to 192.168.100.3/24 (basically 192.168.100.x).\nThe BF2 device is set to static IP 192.168.100.2.\nIf you are not using the BF2 management network interface, but rather the rshim network interface, you will need to configure your Linux machine as a gateway/router, so the device can use it for internet access.\n\nMake sure ipv4 forward is enabled\n ```\n $ cat /proc/sys/net/ipv4/ip_forward\n 1\n ```\n\nNAT (MASQUERADE) is enabled on the external network interface, in this example eno1:\n`$ sudo iptables -t nat -A POSTROUTING -o eno1 -j MASQUERADE`\n\nOnce the bfb is flashed, log in using minicom and the credentials \nuser: mariner\npw: mariner\n\nSome network configuration may be needed. Make the following changes:\n\n```\n$ sudo cat > /etc/resolv.conf << EOF\nnameserver 127.0.0.53\nEOF\n```\n```\n$ sudo cat >  /etc/netplan/60-mlnx.yaml << EOF\nnetwork:\n    ethernets:\n        oob_net0:\n            renderer: networkd\n            dhcp4: true\n        tmfifo_net0:\n            renderer: networkd\n            addresses:\n            - 192.168.100.2/24\n            dhcp4: false\n            gateway4: 192.168.100.3\n            nameservers:\n               addresses: [10.50.10.50, 10.50.50.50]\n               search: [corp.microsoft.com]\n        enp3s0f0s0:\n            renderer: networkd\n            dhcp4: true\n        enp3s0f1s0:\n            renderer: networkd\n            dhcp4: true\n    version: 2\nEOF\n```\n`$ sudo netplan apply`\n\nNow you should be able to ssh using the rshim device\n`$ ssh mariner@192.168.100.2`\n", "repo_name": "CBL-Mariner-SmartNIC", "org_name": "microsoft", "org_repo": "microsoft/CBL-Mariner-SmartNIC", "platform_org_repo": "github+microsoft/CBL-Mariner-SmartNIC", "link_to_repo": "https://github.com/microsoft/CBL-Mariner-SmartNIC", "platform": "github", "language": "Shell", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Nuget Ninja (A Hackthon project)\n\n![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)\n![Build Status](https://github.com/microsoft/NugetNinja/actions/workflows/build.yml/badge.svg)\n\n(Nuget Ninjia was not built or released as a production product. Instead it was our hackthon project while we prefer opensource. It was not officialy release by Microsoft as a product.)\n\n(Non-production! This project is still working in progress...)\n\nNuget Ninjia is a tool for detecting dependencies of .NET projects. It analyzes the dependency structure of .NET projects in a directory and builds a directed acyclic graph. And will give some modification suggestions for Nuget packages, so that the dependencies of the project are as concise and up-to-date as possible.\n\n## Usage\n\nAfter getting the binary, run it directly in the terminal.\n\n```cmd\nC:\\workspace> ninja.exe\n\nDescription:\n  Nuget Ninja, a tool for detecting dependencies of .NET projects.\n\nUsage:\n  Microsoft.NugetNinja [command] [options]\n\nOptions:\n  -p, --path <path> (REQUIRED)   Path of the projects to be changed.\n  --nuget-server <nuget-server>  If you want to use a customized nuget server instead of the official nuget.org, \n  --token <token>                The PAT token which has privilege to access the nuget server.\n  -d, --dry-run                  Preview changes without actually making them\n  -v, --verbose                  Show detailed log\n  -?, -h, --help                 Show help and usage information\n\nCommands:\n  all, all-officials  The command to run all officially supported features.\n  remove-deprecated   The command to replace all deprecated packages to new packages.\n  upgrade-pkg         The command to upgrade all package references to possible latest and avoid conflicts.\n  clean-pkg           The command to clean up possible useless package references.\n  clean-prj           The command to clean up possible useless project references.\n```\n\n## How to build and run locally\n\nRequirements about how to develop.\n\n* [.NET SDK 6.0](https://github.com/dotnet/core/tree/master/release-notes)\n\n1. Execute `dotnet restore` to restore all .NET dependencies.\n2. Execute the following command to build the app:\n   * `dotnet publish -c Release -r win-x64   --self-contained` on Windows.\n   * `dotnet publish -c Release -r linux-x64 --self-contained` on Linux.\n   * `dotnet publish -c Release -r osx-x64   --self-contained` on Mac OS.\n3. Execute `dotnet run` to run the app\n\n## Run in Microsoft Visual Studio\n\n1. Open the `.sln` file in the project path.\n2. Press `F5`.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "NugetNinja", "org_name": "microsoft", "org_repo": "microsoft/NugetNinja", "platform_org_repo": "github+microsoft/NugetNinja", "link_to_repo": "https://github.com/microsoft/NugetNinja", "platform": "github", "language": "C#", "stargazers_count": 81, "watchers_count": 81}, {"README_text": "# AzureTRE-Deployment Repo\n\nThis project is intended to assist the deployment of the Azure TRE project in real world environments. This includes deploying using a dev container from your local machine, deploying using GitHub Actions, and publishing custom templates.\n\nSee the [Azure TRE documentation](https://microsoft.github.io/AzureTRE/) which includes detailed documentation and best practices to ensure a successful deployment and to assist you with customizing your own templates using this repository.\n\n## Contents\n\nIn this project you will find:\n\n- Github Actions implementing AzureTRE automation, including running deployments to Azure\n- Configuration specific to deployment\n- Workspace template definitions\n- User resource template definitions\n- Devcontainer setup\n\n### Prerequisites\n\nTo work with devcontainers you will need:\n\n- [Visual Studio Code](https://code.visualstudio.com)\n- [Remote containers extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureTRE-Deployment", "org_name": "microsoft", "org_repo": "microsoft/AzureTRE-Deployment", "platform_org_repo": "github+microsoft/AzureTRE-Deployment", "link_to_repo": "https://github.com/microsoft/AzureTRE-Deployment", "platform": "github", "language": "Shell", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "## Azure Synapse 1-click POC environment with pre-populated dataset, pipeline, notebook\nThis 1-click deployment allows the user to deploy a Proof-of-Concept environment of Azure Synapse Analytics with dataset (New York Taxi Trips & Fares data), pipeline to (ingest, merge, aggregate), \tnotebook (Spark ML prediction)\n\n## Prerequisites\n\nOwner role (or Contributor roles) for the Azure Subscription the template being deployed in. This is for creation of a separate Proof-of-Concept Resource Group and to delegate roles necessary for this proof of concept. Refer to this [official documentation](https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-steps) for RBAC role-assignments.\n\n## Deployment Steps\n1. Fork out [this github repository](https://github.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC) into your github account. \n    \n   **If you don't fork repo:** \n   + **The pre-populated dataset, pipeline and notebook will not be deployed**\n   + **You will get a Github publishing error**\n   \n   \n  <!--  ![Fork](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/4.gif) -->\n \n2. Click 'Deploy To Azure' button given below to deploy all the resources.\n\n    [![Deploy To Azure](https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/1-CONTRIBUTION-GUIDE/images/deploytoazure.svg?sanitize=true)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2FAzure%2FTest-Drive-Azure-Synapse-with-a-1-click-POC%2Fmain%2Fazuredeploy.json)\n\n   - Provide the values for:\n\n     - Resource group (create new)\n     - Region\n     - Company Tla\n     - Option (true or false) for Allow All Connections\n     - Option (true or false) for Spark Deployment\n     - Spark Node Size (Small, Medium, large) if Spark Deployment is set to true\n     - Sql Administrator Login\n     - Sql Administrator Login Password\n     - Sku\n     - Option (true or false) for Metadata Sync\n     - Frequency\n     - Time Zone\n     - Resume Time\n     - Pause Time\n     - Option (Enabled or Disabled) for Transparent Data Encryption\n     - Github Username (username for the account where [this github repository](https://github.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC) was forked out into)\n\n   - Click 'Review + Create'.\n   - On successful validation, click 'Create'.\n\n## Azure Services being deployed\nThis template deploys necessary resources to run an Azure Synapse Proof-of-Concept. \nFollowing resources are deployed with this template along with some RBAC role assignments:\n\n- An Azure Synapse Workspace \n- An Azure Synapse SQL Pool\n- An optional Apache Spark Pool\n- Azure Data Lake Storage Gen2 account\n- A new File System inside the Storage Account to be used by Azure Synapse\n- A Logic App to Pause the SQL Pool at defined schedule\n- A Logic App to Resume the SQL Pool at defined schedule\n- A key vault to store the secrets\n\n<!-- The data pipeline inside the Synapse Workspace gets New York Taxi trip and fare data, joins them and perform aggregations on them to give the final aggregated results. Other resources include datasets, linked services and dataflows. All resources are completely parameterized and all the secrets are stored in the key vault. These secrets are fetched inside the linked services using key vault linked service. The Logic App will check for Active Queries. If there are active queries, it will wait 5 minutes and check again until there are none before pausing -->\n\n## Post Deployment\n- Current Azure user needs to have [\"Storage Blob Data Contributor\" role access](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-add-admin#azure-rbac-role-assignments-on-the-workspaces-primary-storage-account) to recently created Azure Data Lake Storage Gen2 account to avoid 403 type permission errors.\n- After the deployment is complete, click 'Go to resource group'.\n- You'll see all the resources deployed in the resource group.\n- Click on the newly deployed Synapse workspace.\n- Click on link 'Open' inside the box labelled as 'Open Synapse Studio'.\n- Click on 'Log into Github' after workspace is opened. Provide your credentials for the github account holding the forked out repository.\n- After logging in into your github account, click on 'Integrate' icon in the left panel. A blade will appear from right side of the screen.\n- Make sure that 'main' branch is selected as 'Working branch' and click 'Save'.\n\n![PostDeployment-1](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/1.gif)\n\n- Now open the pipeline named 'TripFaresDataPipeline'.\n- Click on 'Parameters' tab at bottom of the window.\n- Update the following parameter values. ___(You can copy the resource names from the resource group recently deployed.)___\n    - SynapseWorkspaceName  (Make sure workspace name is fully qualified domain name, i.e. workspaceName.database.windows.net)\n    - SQLDedicatedPoolName\n    - SQLLoginUsername\n    - KeyVaultName\n    - DatalakeAccountName\n\n![PostDeployment-2](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/2.gif)\n\n- After the parameters are updated, click on 'Commit all'.\n- After successful commit, click 'Publish'. A blade will appear from right side of the window.\n- Click 'Ok'.\n\n![PostDeployment-3](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/3.gif)\n\n- Now to trigger the pipeline, click 'Add trigger' at the top panel and click 'Trigger now'.\n- Confirm the pipeline parameters' values and click 'Ok'.\n- You can check the pipeline status under 'Pipeline runs' in the 'Monitor' tab on the left panel.\n\n![PostDeployment-4](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/5.gif)\n\n- To run the notebook (if spark pool is deployed), click on 'Develop' tab on the left panel.\n- Now under 'Notebooks' dropdown on left side of screen, click the notebook named 'Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib'.\n- Click 'Run all' to run the notebook. (It might take a few minutes to start the session)\n\n![PostDeployment-5](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/6.gif)\n\n- Once published all the resources will now be available in the live mode.\n- To switch to the live mode from git mode, click the drop down at top left corner and select 'Switch to live mode'.\n\n![PostDeployment-6](https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/images/liveMode.PNG)\n\n## Steps for PowerBI integration\n\n**Pre-requisites**\n\nPowerBI workspace created. Please note that you can\u2019t use default workspace (\u2018My workspace\u2019). create a new PBI workspace or use any other workspace other than \u2018My workspace\u2019.\n\nCreate PowerBI workspace --> https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-create-the-new-workspaces\n\n**Link Azure Synapse workspace to PowerBI workspace**\n\n- In Synapse workspace, go to Manage --> Linked Services.\n- Click on PowerBIWorkspaceTripsFares linked service\n- From the drop down list, select your PowerBI workspace and Save and publish.\n\n![20211014134407](https://user-images.githubusercontent.com/88354448/137524650-9d066921-d057-4a08-8d55-4f8c02eb3690.gif)\n\n- Download [NYCTaxiCabTripAndFare.pbit] (https://github.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/tree/main/synapsepoc/PowerBITemplate/NYCTaxiCabTripAndFare.pbit) from PowerBITemplate folder\n- Provide ServerName, DatabaseName and login credentials. ServerName and DatabaseName can be found in connection strings.\n- To get the connection string, click on Dedicated SQL Pool\n- On the left hand side menu, click on connection strings\n- Copy ServerName and DatabaseName from connection string, paste them in PowerBI and click on 'Load'.\n- Select 'Database' (instead of default 'Windows') and provide User name,  Password and click on 'Connect'\n\n![20211014140340](https://user-images.githubusercontent.com/88354448/137524802-c720137f-9f9c-4c84-93b9-35c5ef0ce759.gif)\n\n- Change the sensitivity level to 'Public' and **save** the dashboard. \n- Publish the dashboard to the PowerBI workspace you have created by clicking on 'Publish' and selecting the workspace.\n- In Synapse workspage navigate to Develop --> PowerBI --> Refresh.\n- You see the PowerBI report in Synapse you had published in PowerBI workspace.\n\n![20211014144422](https://user-images.githubusercontent.com/88354448/137524861-ac32c4dc-856f-41e9-8f01-8dfa0cc7baae.gif)\n\n", "repo_name": "Synapse-Data-exploration", "org_name": "microsoft", "org_repo": "microsoft/Synapse-Data-exploration", "platform_org_repo": "github+microsoft/Synapse-Data-exploration", "link_to_repo": "https://github.com/microsoft/Synapse-Data-exploration", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Microsoft Store Developer Command Line Interface (CLI)\n\n[![CI](https://github.com/microsoft/msstore-cli/actions/workflows/build.yml/badge.svg)](https://github.com/microsoft/msstore-cli/actions/workflows/build.yml)\n\n## About\nThe Microsoft Store Developer Command Line Interface is a cross-platform (Windows, MacOS, Linux) CLI that helps developers access the Microsoft Store APIs, for both managed (MSIX), as well as unmanaged (MSI/EXE) applications. It helps developers by creating required online resources (credentials), as well as later setting up their application projects (UWPs, Win32s, Flutter, PWAs, Electron, React-Native, as well as many other types of Windows applications) to be ready to ship to the Microsoft Store, going from the initial steps of configuring the application's manifest, as well as the actual publishing of an MSIX or MSI/EXE.\n\n## Helpful links\n* [Documentation](https://aka.ms/msstoredevcli/docs) - Microsoft's official documentation on regards to available commands, installation steps, how to properly setup CI/CD environments, and general guidance.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Data/Telemetry\n\nThe `msstore.exe` client is instrumented to collect usage and diagnostic (error) data and sends it to Microsoft to help improve the product.\n\nIf you build the client yourself the instrumentation will not be enabled and no data will be sent to Microsoft.\n\nSee the [privacy statement](privacy.md) for more details.\n\n### Telemetry Configuration\n\nTelemetry collection is on by default. To opt out, please run `msstore settings --enableTelemetry false` to turn it off.\n", "repo_name": "msstore-cli", "org_name": "microsoft", "org_repo": "microsoft/msstore-cli", "platform_org_repo": "github+microsoft/msstore-cli", "link_to_repo": "https://github.com/microsoft/msstore-cli", "platform": "github", "language": "C#", "stargazers_count": 48, "watchers_count": 48}, {"README_text": "# Offline Learner Simulation\n**Offline Simulations for Online Reinforcement Learning**\n\nThis repository contains code used in the NeurIPS 2022 Offline RL workshop paper [Towards Data-Driven Offline Simulations for Online Reinforcement Learning](https://arxiv.org/abs/2211.07614) by Shengpu Tang, Felipe Vieira Frujeri, Dipendra Misra, Alex Lamb, John Langford, Paul Mineiro, Sebastian Kochman.\n\nAs part of this research project, we have started creation of the ``offsim4rl`` library, containing algorithms for Offline Learner Simulation (OLS), like Per-State Rejection Sampling (originally proposed by [Mandel et al, 2016](https://grail.cs.washington.edu/projects/nonstationaryeval/)), as well as tools for evaluating different OLS methods. We share it in the early stage of development and are looking forward to seeing how the RL research community takes it further, either via contributions or project forks.\n\nTo [reproduce the experiments](https://github.com/microsoft/rl-offline-simulation/wiki/Reproducing-experiments) from the paper, please use the [neurips_workshop_2022](https://github.com/microsoft/rl-offline-simulation/tree/neurips_workshop_2022) branch, which includes experimental notebooks. The notebooks will be removed from the main branch, such that it is easier to continue developing the offsim4rl library without breaking the workshop experiments. However, if you intend to use the ``offsim4rl`` library (e.g., to run offline learner simulation using your own dataset), you're better off using the [main](https://github.com/microsoft/rl-offline-simulation/tree/main) branch.\n\n## Getting Started\n\nTo install the ``offsim4rl`` library, run the following steps.\n\nNote: currently, we support Linux Ubuntu (can be via Windows Subsystem for Linux). Other platforms should work, but haven't been tested.\n\n1. Make sure the native dependencies are installed:\n\n```console\nsudo apt update\nsudo apt install libopenmpi-dev\n```\n\n2. [Optional] Create a virtual environment, e.g., using conda:\n\n```console\nconda create -n offsim4rl python=3.7\nconda activate offsim4rl\n```\n\n3. Clone this repository and install the library in the development mode:\n\n```console\ngit clone https://github.com/microsoft/rl-offline-simulation.git\ncd rl-offline-simulation\npip install -r requirements.txt\npip install -e .\n```\n\n4. [Optional] Run unit tests.\n\n```console\npytest\n```\n\nNote: many dependencies are listed in both \"requirements.txt\" and in \"setup.py\".\n * The \"requirements.txt\" file contains exact versions of the dependencies, some of which are required for our test pipeline to pass. If you'd like to make sure everything is running correctly, would like to reproduce our results, or would like to contribute to the project, we recommend installing the dependecies via \"pip install -r requirements.txt\", before installing the library.\n * The \"setup.py\" offers more flexibility in terms of versioning dependencies. If you intend to use offsim4rl as a library in your own project, and you'd like to use different versions of some dependencies than the ones we specified in \"requirements.txt\", you may skip \"pip install -r requirements.txt\" and run \"pip install -e .\" directly.\n\n## Citation\n\nIf you found this repository useful in your research, please cite our work using the following BibTeX:\n\n```\n@inproceedings{\n    tang2022towards,\n    title={Towards Data-Driven Offline Simulations for Online Reinforcement Learning},\n    author={Tang, Shengpu and Frujeri, Felipe Vieira and Misra, Dipendra and Lamb, Alex and Langford, John and Mineiro, Paul and Kochman, Sebastian},\n    booktitle={3rd Offline RL Workshop: Offline RL as a ''Launchpad''},\n    year={2022},\n    url={https://arxiv.org/abs/2211.07614}\n}\n```\n\n## Contribute\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "rl-offline-simulation", "org_name": "microsoft", "org_repo": "microsoft/rl-offline-simulation", "platform_org_repo": "github+microsoft/rl-offline-simulation", "link_to_repo": "https://github.com/microsoft/rl-offline-simulation", "platform": "github", "language": "Python", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Building the business migration case with Linux and OSS DB to Azure\n\nTerra Firm Laboratories is a global bioengineering company that\u2019s the leading researcher and innovator in genetic and biological science technology. The company was founded in 1975 with its corporate headquarters in Palo Alto, CA. Their mission-critical workloads are currently hosted in an on-premises datacenter and are beginning a journey to modernize and migrate into the cloud using Microsoft Azure.\n\nThe CTO, Dennis Nedry, has kicked off an initiative for the organization to begin adopting the Microsoft Azure cloud and modernize its infrastructure. He has already had his team begin assessing their current environment and what it will take to migrate to the cloud. They are looking to optimize their technology investments by reducing technical debt and streamlining operations using Azure Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) cloud services.\n\nDecember 2022\n\n## Target audience\n\n- Technical Architects\n- Infrastructure Architects\n- Data Architects\n\n## Abstracts\n\n### Workshop\n\nIn this workshop, you will gain experience designing and implementing a strategy for migrating Red Hat Enterprise Linux (RHEL) and MySQL database workloads to Azure.\n\nAt the end of the workshop, you will be better able to design and implement an Azure migration strategy for Red Hat Enterprise Linux and MySQL database workloads using Azure Virtual Machines, and Azure Database for MySQL.\n\n### Whiteboard design session\n\nIn this whiteboard design session, you will learn to design a strategy for migrating existing on-premises Red Hat Enterprise Linux (RHEL) and MySQL database workloads to Azure. Throughout the whiteboard design session, you will look at the virtual machine (VM) pricing and migrating MySQL database workloads to Azure.\n\nAt the end of the workshop, you will be better able to design a migration strategy for Red Hat Enterprise Linux (RHEL) workloads to Azure VMs and MySQL database workloads to Azure Database for MySQL.\n\nContinue to the [Whiteboard design session](Whiteboard%20design%20session) documents folder.\n\n### Hands-on Lab\n\nIn this hands-on lab, you will perform steps to migrate Red Hat Enterprise Linux (RHEL) and MySQL database workloads to Azure. You will go through provisioning a Red Hat Enterprise Linux VM, and migrate MySQL database to Azure Database for MySQL.\n\nAt the end of this hands-on lab, you will be better able to set up a Red Hat Enterprise Linux (RHEL) VM for application migration to Azure and migrate an on-premises MySQL database to Azure Database for MySQL.\n\nContinue to the [Hands-on lab](Hands-on%20lab) documents folder.\n\n## Azure services and related products\n\n- RedHat Enterprise Linux\n- Azure VMs\n- Azure Hybrid Benefit (AHB)\n- Azure Database for MySQL\n- Azure Networking\n- Azure Database Migration Service\n- Azure Bastion\n\n## Azure solutions\n\nDC Migration\n\n## Related references\n\n- [MCW](https://microsoftcloudworkshop.com)\n- [Hub-spoke network topology in Azure](https://learn.microsoft.com/azure/architecture/reference-architectures/hybrid-networking/hub-spoke)\n- [What is Azure Database for MySQL?](https://learn.microsoft.com/azure/mysql/single-server/overview)\n- [Azure Database Migration Service](https://azure.microsoft.com/products/database-migration/#overview)\n\n## Help & Support\n\nWe welcome feedback and comments from Microsoft SMEs & learning partners who deliver MCWs.  \n\n***Having trouble?***\n- First, verify you have followed all written lab instructions (including the Before the Hands-on lab document).\n- Next, submit an issue with a detailed description of the problem.\n- Do not submit pull requests. Our content authors will make all changes and submit pull requests for approval.  \n\nIf you are planning to present a workshop, *review and test the materials early*! We recommend at least two weeks prior.\n\n### Please allow 5 - 10 business days for review and resolution of issues.\n", "repo_name": "MCW-Building-the-business-migration-case-with-Linux-and-OSS-DB-to-Azure", "org_name": "microsoft", "org_repo": "microsoft/MCW-Building-the-business-migration-case-with-Linux-and-OSS-DB-to-Azure", "platform_org_repo": "github+microsoft/MCW-Building-the-business-migration-case-with-Linux-and-OSS-DB-to-Azure", "link_to_repo": "https://github.com/microsoft/MCW-Building-the-business-migration-case-with-Linux-and-OSS-DB-to-Azure", "platform": "github", "language": "Bicep", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Building the business migration case with Windows Server and SQL Server\n\nTailspin Toys is a global manufacturer of children\u2019s toys that was founded in 1957 with its global headquarters located in Milwaukee, WI. Their mission-critical workloads are currently hosted in an on-premises datacenter and are beginning a journey to modernize and migrate into the cloud using Microsoft Azure.\n\nDuring the Envision Workshop, Kaylee Frye, CTO of Tailspin Toys, saw the value of digital transformation, adopting the Microsoft Azure cloud, and modernizing their infrastructure. She has already had the Technical Architects at Tailspin Toys begin assessing their current environment and what it will take to migrate to the cloud. They are looking to optimize their technology investments by reducing technical debt, streamlining operations, and simplifying their DevOps workflow. According to Kaylee Frye, \"Our development teams have already begun adopting DevOps strategies and implemented CI/CD (continuous integration and continuous delivery) pipelines with Azure DevOps. We really look forward to better streamlining IT operations as we adopt Microsoft Azure for the infrastructure too.\"\n\nOctober 2022\n\n## Target audience\n\n- Enterprise Architects\n- Infrastructure Engineers\n\n## Abstracts\n\n### Workshop\n\nIn this workshop, you will gain experience designing and implementing a strategy for migrating Windows Server and SQL Server workloads to Azure, and enabling on-premises virtual machines management by using Azure Arc.\n\nAt the end of the workshop, you will be better able to design and implement an Azure migration strategy for Windows Server and SQL Server workloads using Azure Virtual Machines, Azure SQL Managed Instance (SQL MI), and Azure Arc.\n\n### Whiteboard Design Session\n\nIn this whiteboard design session, you will learn to design a strategy for migrating existing on-premises Windows Server and SQL Server workloads to Azure. Throughout the whiteboard design session, you will look at the virtual machine (VM) pricing, integrated management of cloud, and on-premises workloads using Azure Arc, and migrating SQL Server workloads to Azure.\n\nAt the end of the whiteboard design session, you will be better able to design a migration strategy for Windows Server workloads to Azure VMs, SQL Server workloads to Azure SQL Managed Instance (SQL MI), and simplify workload management using Azure Arc.\n\nContinue to the [Whiteboard design session](Whiteboard%20design%20session) documents folder.\n\n### Hands-on Lab\n\nIn this lab, attendees will perform steps toward migrating Tailspin Toy's on-premises Windows Server and SQL Server workloads to Azure. Tailspin needs a new Windows Server VM created in Azure for hosting their Web application, an on-premises SQL Server database migrated to Azure SQL Managed Instance, and an on-premises Windows Server VM to be Azure Arc-enabled.\n\nTailspin already has a Hub and Spoke network setup in Azure with Azure Bastion for enabling remote management of Azure VM using Azure Bastion. The Azure resources provisioned throughout this lab will be deployed into this environment.\n\nAt the end of this hands-on lab, you will be better able to set up a Windows Server for application migration to Azure, migrate an on-premises SQL Database to Azure SQL Managed Instance, and Azure Arc-enable an on-premises virtual machine so it can be managed from Azure.\n\nContinue to the [Hands-on lab](Hands-on%20lab) documents folder.\n\n## Azure services and related products\n\n- Azure VMs\n- Azure Arc\n- Azure SQL Managed Instance\n- Azure Networking\n- Microsoft Data Migration Assistant\n- Azure Data Studio - Azure SQL Migration Extension\n\n## Azure solutions\n\nDC Migration\n\n## Related references\n- [MCW](https://github.com/Microsoft/MCW)\n- [Azure Arc](https://docs.microsoft.com/azure/azure-arc/overview)\n- [Azure SQL Managed Instance](https://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview)\n\n## Help & Support\n\nWe welcome feedback and comments from Microsoft SMEs & learning partners who deliver MCWs.  \n\n***Having trouble?***\n- First, verify you have followed all written lab instructions (including the Before the Hands-on lab document).\n- Next, submit an issue with a detailed description of the problem.\n- Do not submit pull requests. Our content authors will make all changes and submit pull requests for approval.\n\nIf you are planning to present a workshop, *review and test the materials early*! We recommend at least two weeks prior.\n\n### Please allow 5 - 10 business days for review and resolution of issues.\n", "repo_name": "MCW-Building-the-business-migration-case-with-Windows-Server-and-SQL-Server", "org_name": "microsoft", "org_repo": "microsoft/MCW-Building-the-business-migration-case-with-Windows-Server-and-SQL-Server", "platform_org_repo": "github+microsoft/MCW-Building-the-business-migration-case-with-Windows-Server-and-SQL-Server", "link_to_repo": "https://github.com/microsoft/MCW-Building-the-business-migration-case-with-Windows-Server-and-SQL-Server", "platform": "github", "language": "Bicep", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Healthcare Characteristics\n\nExtending Dataverse bookable resource characteristics table as a foundation for configuring common healthcare use cases with low-code.\n\n![Sample screenshot of skills management app and mobile concept.](./images/HealthcareSkillsDemo.png)\n\n## Features\n\n- Use healthcare-specific criteria to route chats, calls, and records to the appropriate person, team, or queue.\n- Search for available resources based on credentials and specialties.\n- Supports additional resource categories\n- Import your own skills list from .csv, Excel, or through secure integration with a practice management system\n\n## Dependencies\n\n- Dataverse with Bookable Resources (e.g. resources from Dynamics 365 Customer Service or Field Service)\n\n## Deployment\n\n1. Ensure that dependencies (above) are met and that you have an Environment Maker role (or equivalent) in the Environment\n1. Deploy the current version of the managed solution to a Dataverse environment\n    1. Publish the *Healthcare Skills Admin* Power App\n1. Deploy sample data (or create your own)\n1. Update Resources with their associated healthcare-related Characteristics via the **Healthcare Skills Admin app**\n\n*The packaged solution .zip files included in this repository are intended for testing and educational purposes.*\n\n## Major components\n\n- Bookable resource characteristic type of State Licenses\n- Bookable resource characteristic type of Specialties\n- Table views configured for State Licenses and Specialties\n- Application UI to manage these\n\n## Design Decisions\n\n- Configured **bookableresourcecharacteristictype** choice field to include *Specialty* and *State License* in the option set when adding Characteristics.\n  - This introduces a dependency on the Rescource data model\n  - This dependency mimimizes future configuration and potential duplication of data when setting up routing rules.\n\n- Leveraging Dataverse tables for defining Resources and their associated characteristics.\n  - Customers who are not already using Dataverse to represent relationships with these resources (e.g. Practitioners) will need to import/integrate provider information\n  - This approach brings support for non-clinical and clinical characteristics to be searched in a unified manner.\n  - There is an additoinal opportunity to expose a portal for practitioners and their staff to keep data up to date (e.g. preferred contact details, clinic associations, etc.)\n\n- Note: Sample code for **bookableresourcecharacteristictype** global Choice field includes out of the box values as well, which will reset any environment-specific configurations. Alternatively, the choice field could be configured directly in the customer's own dev environment.\n\n## Sample Data\n\nThis repository includes sample data that allows testing/modification without requiring a live FHIR integration.\n\n1. Import the *csv* file found in the solutions folder of this repository\n    1. Not sure how? Navigate to *Power Apps Maker Portal->Data->Patient Stays->Data* then use the **Import from CSV** action\n\nfile: characteristics-v2.0360.2.7.csv\nsource: https://www.hl7.org/fhir/v2/0360/2.7/index.html\nName: v2.0360.2.7\nTitle: v2 table 0360, Version 2.7\nDefinition: FHIR Value set/code system definition for HL7 v2 table 0360 ver 2.9 ( Degree/License/Certificate)\n\nfile: characteristics-specialties.csv\nsource: https://www.hl7.org/fhir/valueset-c80-practice-codes.html\nName: PracticeSettingCodeValueSet\nTitle: Practice Setting Code Value Set\nDefinition: This is the code representing the clinical specialty of the clinician or provider who interacted with, treated, or provided a service to/for the patient. The value set used for clinical specialty has been limited by HITSP to the value set reproduced from HITSP C80 Table 2-149 Clinical Specialty Value Set Definition.\nCopyright: This resource includes content from SNOMED Clinical Terms\u00ae (SNOMED CT\u00ae) which is copyright of the International Health Terminology Standards Development Organisation (IHTSDO). Implementers of these specifications must have the appropriate SNOMED CT Affiliate license - for more information contact http://www.snomed.org/snomed-ct/get-snomed-ct or info@snomed.org\n\n## Solution History\n\n### Healthcare Skills 1.0.3\n\nInitial release of the sample code, low-code app, and sample data. Source included in the /src folder and deployable solution files included in the /solutions folder.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "resource-characteristics-health", "org_name": "microsoft", "org_repo": "microsoft/resource-characteristics-health", "platform_org_repo": "github+microsoft/resource-characteristics-health", "link_to_repo": "https://github.com/microsoft/resource-characteristics-health", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "reacteverywhere", "org_name": "microsoft", "org_repo": "microsoft/reacteverywhere", "platform_org_repo": "github+microsoft/reacteverywhere", "link_to_repo": "https://github.com/microsoft/reacteverywhere", "platform": "github", "language": null, "stargazers_count": 11, "watchers_count": 11}, {"README_text": "View the Azure Threat Research Matrix https://aka.ms/ATRM", "repo_name": "Azure-Threat-Research-Matrix", "org_name": "microsoft", "org_repo": "microsoft/Azure-Threat-Research-Matrix", "platform_org_repo": "github+microsoft/Azure-Threat-Research-Matrix", "link_to_repo": "https://github.com/microsoft/Azure-Threat-Research-Matrix", "platform": "github", "language": "HTML", "stargazers_count": 49, "watchers_count": 49}, {"README_text": "![microsoft_security.png](assets/images/microsoft_security.png)\n\n# [Microsoft ICS Forensics Framework](https://azure.microsoft.com/en-us/products/iot-defender/)\n\nMicrosoft ICS Forensics Tools is an open source forensic framework for analyzing Industrial PLC metadata and project files.  \nit enables investigators to identify suspicious artifacts on ICS environment for detection of\ncompromised devices during incident response or manual check.  \nopen source framework, which allows investigators to verify the actions of the tool or customize it to specific needs.\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n```\ngit clone https://github.com/microsoft/ics-forensics-tools.git\n```\n\n### Prerequisites\n\n- Install Python >= 3.8: https://www.python.org/downloads\n\n\n### Installing\n\n- Install python requirements\n\n    ``` \n    pip install -r requirements.txt\n    ```\n## Usage\n\n### General application arguments:\n|          Args          |                             Description                             | Required / Optional |\n|:----------------------:|:-------------------------------------------------------------------:|:-------------------:|\n|     `-h`, `--help`     |                   show this help message and exit                   |      Optional       |\n| `-s`, `--save-config`  |               Save config file for easy future usage                |      Optional       |\n|    `-c`, `--config`    |              Config file path, default is config.json               |      Optional       |\n|  `-o`, `--output-dir`  | Directory in which to output any generated files, default is output |      Optional       |\n|   `-v`, `--verbose`    |             Log output to a file as well as the console             |      Optional       |\n| `-p`, `--multiprocess` |       Run in multiprocess mode by number of plugins/analyzers       |      Optional       |\n\n### Specific plugin arguments:\n|      Args      |                        Description                        | Required / Optional |\n|:--------------:|:---------------------------------------------------------:|:-------------------:|\n| `-h`, `--help` |              show this help message and exit              |      Optional       |\n|     `--ip`     | Addresses file path, CIDR or IP addresses comma seperated |      Required       |\n|    `--port`    |                        Port number                        |      Optional       |\n| `--transport`  |                          tcp/udp                          |      Optional       |\n|  `--analyzer`  |                   Analyzer name to run                    |      Optional       |\n\n### Executing examples in the command line\n\t python driver.py -s -v PluginName --ip ips.txt\n     python driver.py -s -v PluginName --ip ips.txt --analyzer AnalyzerName\n\t python driver.py -s -v -c config.json --multiprocess\n\n\n### Import as library example\n```python\nfrom forensic.client.forensic_client import ForensicClient\nfrom forensic.interfaces.plugin import PluginConfig\nforensic = ForensicClient()\nplugin = PluginConfig.from_json({\n    \"name\": \"PluginName\",\n    \"port\": 123,\n    \"transport\": \"tcp\",\n    \"addresses\": [\"192.168.1.0/24\", \"10.10.10.10\"],\n    \"parameters\": {\n    },\n    \"analyzers\": []\n})\nforensic.scan([plugin])\n```\n## Architecture\n![architecture.png](assets/images/architecture.png)\n\n## Adding Plugins\nWhen developing locally make sure to mark src folder as \"Sources root\"\n\n- Create new directory under plugins folder with your plugin name\n- Create new Python file with your plugin name\n- Use the following template to write your plugin and replace 'General' with your plugin name\n\n```python\nfrom pathlib import Path\nfrom forensic.interfaces.plugin import PluginInterface, PluginConfig, PluginCLI\nfrom forensic.common.constants.constants import Transport\n\n\nclass GeneralCLI(PluginCLI):\n    def __init__(self, folder_name):\n        super().__init__(folder_name)\n        self.name = \"General\"\n        self.description = \"General Plugin Description\"\n        self.port = 123\n        self.transport = Transport.TCP\n\n    def flags(self, parser):\n        self.base_flags(parser, self.port, self.transport)\n        parser.add_argument('--general', help='General additional argument', metavar=\"\")\n\n\nclass General(PluginInterface):\n    def __init__(self, config: PluginConfig, output_dir: Path, verbose: bool):\n        super().__init__(config, output_dir, verbose)\n\n    def connect(self, address):\n        self.logger.info(f\"{self.config.name} connect\")\n\n    def export(self, extracted):\n        self.logger.info(f\"{self.config.name} export\")\n\n\n```\n- Make sure to import your new plugin in the `__init__.py` file under the plugins folder\n- In the PluginInterface inherited class there is 'config' parameters, you can use this to access any data that's available in the PluginConfig object (plugin name, addresses, port, transport, parameters).  \nthere are 2 mandatory functions (connect, export).  \nthe connect function receives single ip address and extracts any relevant information from the device and return it.  \nthe export function receives the information that was extracted from __all__ the devices and there you can export it to file.\n- In the PluginCLI inherited class you need to specify in the init function the default information related to this plugin.  \nthere is a single mandatory function (flags).  \nIn which you must call base_flags, and you can add any additional flags that you want to have.\n\n## Adding Analyzers\n- Create new directory under analyzers folder with the plugin name that related to your analyzer.\n- Create new Python file with your analyzer name\n- Use the following template to write your plugin and replace 'General' with your plugin name\n```python\nfrom pathlib import Path\nfrom forensic.interfaces.analyzer import AnalyzerInterface, AnalyzerConfig\n\n\nclass General(AnalyzerInterface):\n    def __init__(self, config: AnalyzerConfig, output_dir: Path, verbose: bool):\n        super().__init__(config, output_dir, verbose)\n        self.plugin_name = 'General'\n        self.create_output_dir(self.plugin_name)\n\n    def analyze(self):\n      pass\n\n```\n\n- Make sure to import your new analyzer in the `__init__.py` file under the analyzers folder\n\n## Resources and Technical data & solution:\n[Microsoft Defender for IoT](https://azure.microsoft.com/en-us/services/iot-defender/#overview) is an agentless network-layer security solution that allows\norganizations to continuously monitor and discover assets, detect threats, and manage vulnerabilities in their IoT/OT\nand Industrial Control Systems (ICS) devices, on-premises and in Azure-connected environments.\n\n[Section 52 under MSRC blog](https://msrc-blog.microsoft.com/?s=section+52)    <br/>\n[ICS Lecture given about the tool](https://ics2022.sched.com/event/15DB2/deep-dive-into-plc-ladder-logic-forensics)    <br/>\n[Section 52 - Investigating Malicious Ladder Logic | Microsoft Defender for IoT Webinar - YouTube](https://www.youtube.com/watch?v=g3KLq_IHId4&ab_channel=MicrosoftSecurityCommunity)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Legal Disclaimer\n\nCopyright (c) 2018 Microsoft Corporation. All rights reserved.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n", "repo_name": "ics-forensics-tools", "org_name": "microsoft", "org_repo": "microsoft/ics-forensics-tools", "platform_org_repo": "github+microsoft/ics-forensics-tools", "link_to_repo": "https://github.com/microsoft/ics-forensics-tools", "platform": "github", "language": "Python", "stargazers_count": 96, "watchers_count": 96}, {"README_text": "# dp-transformers\r\n\r\n:warning: This repo is intended for research projects and prototypes.\r\nWhile we try to provide tests for all the functionality, the repo has not (yet) undergone the detailed review process that is necessary for deploying a system of critical nature such as privacy.\r\n\r\n## Introduction \r\n\r\nSee [dp-transformers](https://www.microsoft.com/en-us/research/project/dp-transformers) for a brief introduction to our repository.\r\n\r\n## Installation\r\n\r\nFor installing the `dp-transformers` package, you can just type\r\n\r\n```\r\npip install .\r\n```\r\n\r\n## Examples\r\n\r\nSee `./examples` for end to end examples of how to use the library.\r\n\r\nA basic example can be found in `examples/nlg-reddit/sample-level-dp/fine-tune-dp.py`.\r\nFirst, create an Anaconda environment by doing `conda env create -f examples/nlg-reddit/sample-level-dp/environment.yml`.\r\nThen, you can run the example using the following command (here we assume there are 16 GPUs in the machine, and thus set `--nproc_per_node 16`):\r\n\r\n```\r\npython -m torch.distributed.run --nproc_per_node 16 examples/nlg-reddit/sample-level-dp/fine-tune-dp.py \\\r\n--output_dir scratch \\\r\n--model_name gpt2 \\\r\n--sequence_len 128 \\\r\n--per_device_train_batch_size 32 \\\r\n--gradient_accumulation_steps 2 \\\r\n--evaluation_strategy steps \\\r\n--eval_steps 45 \\\r\n--log_level info \\\r\n--per_device_eval_batch_size 64 \\\r\n--eval_accumulation_steps 1 \\\r\n--seed 42 \\\r\n--target_epsilon 8 \\\r\n--per_sample_max_grad_norm 1.0 \\\r\n--prediction_loss_only \\\r\n--weight_decay 0.01 \\\r\n--remove_unused_columns False \\\r\n--num_train_epochs 3 \\\r\n--logging_steps 5 \\\r\n--max_grad_norm 0 \\\r\n--lr_scheduler_type constant \\\r\n--learning_rate 1e-4 \\\r\n--disable_tqdm True \\\r\n--dataloader_num_workers 2\r\n```\r\n\r\n## \ud83e\udd17 Transformers with Opacus\r\n\r\n### Trainer\r\n\r\nHuggingface's trainer provides callback hooks which we can use to make sure the required methods in the privacy engine are called.\r\n\r\nYou can use the callback as demonstrated in the example below\r\n\r\n``` python\r\nprivacy_engine = opacus.PrivacyEngine(module=model, ...)\r\n\r\n# No need to attach the privacy engine to the optimizer. The callback will automatically attach the optimizer.\r\n\r\ntrainer = transformers.Trainer(\r\n    model = model,\r\n    [...],\r\n    callbacks = [dp_transformers.PrivacyEngineCallback(privacy_engine)]  # <-- Add this line to make sure the privacy engine is used in the trainer\r\n    [...]\r\n)\r\n```\r\n\r\n### Data Collation\r\n\r\n\ud83e\udd17 Transformers library often provides sensible default arguments.\r\nFor example, when no `position_ids` are provided, the library automatically will use incrementing integers.\r\nThe way this is implemented is by first creating a tensor of shape `[1, sequence_length]` filled with increasing integers.\r\nDuring a second step that tensor is replicated for the whole batch.\r\nHowever, the replication is part of the computational graph and hence Opacus cannot infer the batch size from this input tensor.\r\n\r\nWe have therefore implemented a custom data collator (see `dp_transformers.DataCollatorForPrivateCausalLanguageModeling`) which automatically creates the `position_ids` input tensor by using `torch.repeat`.\r\nThis works with opacus since the `position_ids` tensor appears as batch size different inputs in the computation graph.\r\n\r\n### GPT2\r\n\r\nThe \ud83e\udd17 Transformers implementation for GPT2 uses a custom layer type namely `Conv1D`.\r\nIt is not quite clear why this was introduced since it is essentially a regular linear layer.\r\nThis causes problems with Opacus though since it is not sure how to apply the backward hooks for this layer.\r\n\r\nIn this repo we provide an implementation for handling this type of layer.\r\nSee `dp_transformers.grad_sample.transformers.conv_1d`\r\n\r\nAll necessary grad samplers can be registered by merely importing `conv_1d` before the model training.\r\nSee the Known Issues section below for more details.\r\n\r\n## General tips for DP training\r\n\r\nIn this section, we collect a few helpful strategies for training models with DP.\r\nAlso Opacus's FAQs have a few tips on how to get started with DP training (see [Opacus FAQ](https://opacus.ai/docs/faq))\r\n\r\n### Hyper-parameters\r\n\r\nLarger batch sizes help DP training.\r\nAs a general rule, try starting with $\\sqrt{|D|}$ where $D$ is the training dataset.\r\nSince Opacus increases memory consumption significantly, this is only possible using gradient accumulation.\r\n\r\nWe have found a surprisingly small dependence on the clipping norm.\r\nAs a general rule of thumb start with a clipping parameter of 0.1\r\n\r\nFine-tuning the model longer is also helpful.\r\n\r\n\r\n### Deploying DP trained models\r\n\r\nPay attention which pseudo random number generator (PRNG) was used.\r\nPytorch's default (Mersenne Twister) might be attackable.\r\nSee [Opacus FAQ](https://opacus.ai/docs/faq#what-is-the-secure_rng-argument-in-privacyengine)\r\nMake sure to use a better PRNG before deploying models.\r\n\r\n## Known issues\r\n\r\n### Register custom grad samplers late in the training process\r\n\r\nWhen registering custom grad sampler like `dp_transformers.grad_sample.transformers.conv_1d`, functions are added to a global dictionary that Opacus handles.\r\nThis global dictionary is used to establish whether models are compatible with Opacus and how to handle the per-sample gradient computation.\r\nAll grad samplers need to be registered as early as possible in the training process.\r\nDefinitely, before the model is wrapped with `GradSampleModule`.\r\n\r\n## How to Cite\r\n\r\n```\r\n@misc{dp-transformers,\r\n  author        = {Lukas Wutschitz and Huseyin A. Inan and Andre Manoel},\r\n  title         = {dp-transformers: Training transformer models with differential privacy},\r\n  year          = {2022},\r\n  month         = {August},\r\n  howpublished  = {\\url{https://www.microsoft.com/en-us/research/project/dp-transformers}}\r\n}\r\n```\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions. Most contributions require you to\r\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\r\nand actually do, grant us the rights to use your contribution. For details, visit\r\nhttps://cla.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\r\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\r\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\r\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\nFor any other questions, feel free to open an issue on GitHub.\r\n", "repo_name": "dp-transformers", "org_name": "microsoft", "org_repo": "microsoft/dp-transformers", "platform_org_repo": "github+microsoft/dp-transformers", "link_to_repo": "https://github.com/microsoft/dp-transformers", "platform": "github", "language": "Python", "stargazers_count": 41, "watchers_count": 41}, {"README_text": "# @typescript/server-replay\nTool for replaying TypeScript server requests from a file in order to reproduce bugs.\nA convenient wrapper around [@typescript/server-harness](https://www.npmjs.com/package/@typescript/server-harness).\n\n## Format\n\n> :warning: **Subject to change**\n\nNewline-delimited JSON with one request per line.\nAfter the first line, which provides configuration information, each line describes a request.\nEach request has be modified in two notable ways for configurability.\n\n1. All paths have had the root path replaced by a placeholder (value specified when the script is run).\n2. Requests that would contain the contents of an entire file (`updateOpen` or `applyChangedToOpenFiles`) omit those contents.\n\n## Usage\n\n`npx tsreplay project_root replay_script server_path`\n\n- `project_root` is the directory that would be open in the editor if this were a manual scenario\n- `replay_script` is the path to a newline-delimited JSON file, as described above\n- `server_path` is the path to a copy of `tsserver.js` to be tested\n\nTo help with debugging, you can pass `-l`, `-t`, or `-i` to enable logging, tracing, or inspecting/debugging, respectively.\n\nFuture: a subsequent version is expected to have a switch for automatically reducing the script.\n\n## Deployment\n\nTo publish a new version of this package, change the version in `package.json` and push to main.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "typescript-server-replay", "org_name": "microsoft", "org_repo": "microsoft/typescript-server-replay", "platform_org_repo": "github+microsoft/typescript-server-replay", "link_to_repo": "https://github.com/microsoft/typescript-server-replay", "platform": "github", "language": "JavaScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# `Systematic Testing for C++`\n\n![Windows CI](https://github.com/microsoft/cpp-systematic-testing/workflows/Windows%20CI/badge.svg)\n![Linux CI](https://github.com/microsoft/cpp-systematic-testing/workflows/Linux%20CI/badge.svg)\n\n**Note: This is still work-in-progress (WIP) and provided here as-is. If you are a Microsoft\nemployee interested in using this library please get in touch over Teams or email to discuss.**\n\nA library for **systematically testing** concurrent C++ code and **deterministically reproducing**\nbugs.\n\nUsing this library, you get access to a `SystematicTesting::TestEngine` that can be used to (1)\ninstrument your code for taking control of sources of concurrency and nondeterminism in your C++\ncode, and (2) for writing and running what we call concurrency unit tests. These look like your\nregular unit tests, but can reliably test concurrent workloads (such as tasks and threads). In\nregular unit tests, you would typically avoid concurrency due to flakiness, but with this library\nyou are encouraged to embrace concurrency in your tests to find bugs.\n\nThis library is part of the [Coyote](https://microsoft.github.io/coyote/) project by [Microsoft\nResearch](https://www.microsoft.com/en-us/research/). To learn more about the research behind our\ntechnology, check out our published papers\n[here](https://microsoft.github.io/coyote/learn/resources/publications).\n\n## How to build\n\nOn Windows, run the following script for a VS 2019 developer command prompt:\n```bat\nscripts\\build.bat\n```\n\nOn Linux, run the following bash script from the root directory:\n```bash\n./scripts/build.sh\n```\n\nAfter building the project, you can find a static and shared library in `bin`.\n\nFor more detailed building instructions (e.g. if you want to build without the scripts), read\n[here](./docs/building.md).\n\n*Note: the build/ci scripts do not currently work on macOS, feel free to contribute!*\n\n## How to use\n\nTo use the systematic testing engine in a C++ project, link the static or shared library to your\nproject, and include the following header file (from the [`include`](./include) directory):\n```c++\n#include \"systematic_testing.h\"\n```\n\nThen use the `SystematicTesting::TestEngine` APIs to instrument your code similar to our examples\n[here](./test/integration).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a\nCLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repositories using our CLA.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of\nConduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "cpp-systematic-testing", "org_name": "microsoft", "org_repo": "microsoft/cpp-systematic-testing", "platform_org_repo": "github+microsoft/cpp-systematic-testing", "link_to_repo": "https://github.com/microsoft/cpp-systematic-testing", "platform": "github", "language": "C++", "stargazers_count": 32, "watchers_count": 32}, {"README_text": "# AI4IndustrySimulations\n\n## Overview\n\n**AI for Industry Simulations** is a project for training large-scale surrogate models for solving partial differential equations (PDEs) with deep learning. We target large-scale three-dimensional applications as common in industrial applications such as reservoir simulation. The current repository contains two example applications:\n\n- Simulating two-phase CO2 flow in porous media.\n\n- Solving the 3D Navier Stokes to simulate flow around a sphere.\n\nFor each example, we provide the code to simulate the training data and to train a neural surrogate model using a [model-parallel implementation of Fourier Neural Operators](https://arxiv.org/abs/2204.01205). We train our deep surrogate model using supervised training, so simulating the training data by solving the underlying PDE for different inputs is the first step of the workflow. For industry-sized applications, training data simulation is time consuming, as we need to solve 3D PDEs for a large number of samples. We provide examples for simulating training data in parallel on Azure using the AzureClusterlessHPC package and for storing the data in Azure's cloud object store (Blob Storage).\n\nFor training, we use a model-parallel version of [Fourier Neural Operators](https://arxiv.org/pdf/2010.08895.pdf). The model-parallel FNO uses domain decomposition, which enables a higher level of concurrently than model sharding or pipeline parallelism. The model-parallel FNO is based on distributed programming with [DistDL](https://github.com/distdl/distdl), a Python package with distributed communication primitives for implementing model-parallel neural networks.\n\n\n## Quickstart for parallel FNO training\n\nClone this repository:\n\n```\ngit clone https://github.com/microsoft/AI4FluidSimulations\n```\n\nGo to the training directory and pull our docker image for training:\n\n```\n# Go to examples directory\ncd AI4FluidSimulations/training\n\n# Start docker container\ndocker run --gpus all \\\n    -v $(pwd):/workspace/home \\\n    -e OMPI_ALLOW_RUN_AS_ROOT=\"1\" \\\n    -e OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=\"1\" \\\n    -it philippwitte/ai4fluidsimulations-training:v1.0\n```\n\nRun the example script (e.g., on 4 GPUs):\n\n```\nmpiexec -n 4 python3 example_pfno.py\n```\n\n## Data simulation on Azure\n\nFollow the instructions [here](https://github.com/microsoft/AI4FluidSimulations/tree/main/simulation). An Azure subscription is required for data generation.\n\n## Credits\n\nThis repository is developed and maintained by the [Microsoft Research for Industry](https://www.microsoft.com/en-us/research/group/research-for-industry/) (RFI) team.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AI4FluidSimulations", "org_name": "microsoft", "org_repo": "microsoft/AI4FluidSimulations", "platform_org_repo": "github+microsoft/AI4FluidSimulations", "link_to_repo": "https://github.com/microsoft/AI4FluidSimulations", "platform": "github", "language": "C++", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Topological gap protocol: `azure-quantum-tgp`\n<img width=\"308\" alt=\"tgp\" align=\"left\" src=\"https://user-images.githubusercontent.com/6897215/196533626-f573acab-15d3-4fe9-932e-12cae7cc251f.png\">\n\nThis code performs the analysis as reported in _\"InAs-Al Hybrid Devices Passing the Topological Gap Protocol\"_ by Microsoft Azure Quantum.\n\nThe paper is available as a pre-print on [arXiv:2207.02472](https://arxiv.org/abs/2207.02472).\n\nSee the Jupyter notebooks\n* [notebooks/stage-one-analysis.ipynb](notebooks/stage-one-analysis.ipynb) as a step-by-step example of the ***Stage 1*** analysis,\n* [notebooks/stage-two-analysis.ipynb](notebooks/stage-two-analysis.ipynb) as a step-by-step example of the ***Stage 2*** analysis,\n* [notebooks/yield-analysis.ipynb](notebooks/yield-analysis.ipynb) which performs the yield analysis on a large set of simulation data,\n* [notebooks/fridge-calibration.ipynb](notebooks/fridge-calibration.ipynb) which shows the fridge calibration data used for high-frequency corrections,\n* and [notebooks/paper-figures.ipynb](notebooks/paper-figures.ipynb) which performs _all_ the analysis and generates the plots that appear in the paper.\n\nView the executed notebooks with nbviewer: [`paper-figures.ipynb`](https://nbviewer.org/github/microsoft/azure-quantum-tgp/blob/executed_notebooks/notebooks/paper-figures.ipynb), [`stage-two-analysis.ipynb`](https://nbviewer.org/github/microsoft/azure-quantum-tgp/blob/executed_notebooks/notebooks/stage-two-analysis.ipynb), [`stage-one-analysis.ipynb`](https://nbviewer.org/github/microsoft/azure-quantum-tgp/blob/executed_notebooks/notebooks/stage-one-analysis.ipynb), [`yield-analysis.ipynb`](https://nbviewer.org/github/microsoft/azure-quantum-tgp/blob/executed_notebooks/notebooks/yield-analysis.ipynb), [`fridge-calibration.ipynb`](https://nbviewer.org/github/microsoft/azure-quantum-tgp/blob/executed_notebooks/notebooks/fridge-calibration.ipynb).\n\n## Data\n\nWe store the raw data in this repository using Git LFS in the [`data/`](data) folder.\nInstall [Git LFS](https://git-lfs.github.com/) before cloning this repository.\n\nThe `data/simulated/yield` folder is 17 GB and is only used in the [`notebooks/yield-analysis.ipynb` Jupyter notebook](notebooks/yield-analysis.ipynb).\n\nTo clone the repo *without* this data, use (with `ssh`):\n```bash\ngit lfs clone git@github.com:microsoft/azure-quantum-tgp.git --exclude=\"data/simulated/yield\"\n```\nor with `https`:\n```bash\ngit lfs clone https://github.com/microsoft/azure-quantum-tgp.git --exclude=\"data/simulated/yield\"\n```\n\n## Installation\n\nInstall `azure-quantum-tgp` from PyPI with\n```bash\npip install azure-quantum-tgp\n```\n\nor clone this repository and do a developer install with\n```\nconda create --name tgp python=3.10  # create a new conda env\nconda activate tgp\npip install -e \".[test]\"\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-quantum-tgp", "org_name": "microsoft", "org_repo": "microsoft/azure-quantum-tgp", "platform_org_repo": "github+microsoft/azure-quantum-tgp", "link_to_repo": "https://github.com/microsoft/azure-quantum-tgp", "platform": "github", "language": "Python", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "rpagels-azuremapsdemo", "org_name": "microsoft", "org_repo": "microsoft/rpagels-azuremapsdemo", "platform_org_repo": "github+microsoft/rpagels-azuremapsdemo", "link_to_repo": "https://github.com/microsoft/rpagels-azuremapsdemo", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Perception-Action Causal Transformer\n\nThis repo contains the code associated with the paper *PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-training*. For more information, please check the [project webpage](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=868116) .\n\n<img src=\"./docs/pact_main.png\" width=\"600\" >\n<img src=\"./docs/arch.png\" width=\"600\" >\n\n## Paper, video and datasets\n\nIf you use this code in an academic context, please cite the following publication:\n\nPaper: [PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-training](https://arxiv.org/abs/2209.11133)\n\nVideo: [YouTube](https://youtu.be/mNQvQu_atuw)\n\nCitation:\n\n```\n@article{bonatti2022pact,\n  title={Pact: Perception-action causal transformer for autoregressive robotics pre-training},\n  author={Bonatti, Rogerio and Vemprala, Sai and Ma, Shuang and Frujeri, Felipe and Chen, Shuhang and Kapoor, Ashish},\n  journal={arXiv preprint arXiv:2209.11133},\n  year={2022}\n}\n```\n\n## Setting up locally\n\n- Using conda\n\n  ```\n  # clone this repository\n  git clone git@github.com:microsoft/PACT.git\n\n  # create env\n  cd PACT\n  conda env create --file docker/environment.yml\n\n  # activate it\n  conda activate pact\n\n  # install this repo\n  (pact) $ pip install -e .\n  ```\n\n## Data and models download from Azure\n\nYou can download pre-generated datasets for MuSHR and Habitat, or feel free to generate your own with the instructions that you will find later in this README.\n\n```\n# download the MuSHR dataset (approx 327 GB total including pointclouds and birds-eye-view LiDAR pseudo-images)\ncd YOUR_MUSHR_DATA_DIR # the directory where you want to store the dataset\n# install az copy for data download\nwget https://aka.ms/downloadazcopy-v10-linux\ntar -xvf downloadazcopy-v10-linux\n# finally download the data\n./azcopy_linux_amd64_*/azcopy copy 'https://pactrelease.blob.core.windows.net/datasets/mushr-dataset' '.' --recursive\n\n# download the Habitat dataset (approx 4.4TB total including FPV images, depth, semantic observations, and top-down maps)\ncd YOUR_HABITAT_DATA_DIR # the directory where you want to store the dataset\n# install az copy for data download\nwget https://aka.ms/downloadazcopy-v10-linux\ntar -xvf downloadazcopy-v10-linux\n# finally download the data\n./azcopy_linux_amd64_*/azcopy copy 'https://pactrelease.blob.core.windows.net/datasets/habitat-dataset' '.' --recursive\n\n# download the pre-trained models (approx 2 GB total)\ncd YOUR_MODELS_DIR # the directory where you want to store the models\n# install az copy for data download\nwget https://aka.ms/downloadazcopy-v10-linux\ntar -xvf downloadazcopy-v10-linux\n# finally download the data\n./azcopy_linux_amd64_*/azcopy copy 'https://pactrelease.blob.core.windows.net/datasets/models' '.' --recursive\n```\n\n# Simulators\n\n## MuSHR Dataset\n\nFor more information on the MuSHR simulator, data generation procedures and model deployment please check the [MuSHR sim section](./mushr_sim/README.md).\n\n<img src=\"./docs/lidargif.gif\" width=\"200\" >\n<img src=\"./docs/trajgif.gif\" width=\"300\" >\n\n## Habitat Dataset\n\nWe also provide the dataset collected from the [Habitat simulator](https://aihabitat.org/docs/habitat-sim/). We use 10 environments in Habitat from the [HM3D dataset](https://aihabitat.org/datasets/hm3d/), and sample random valid start and goal locations for the agent. Using Habitat\u2019s built-in shortest path function to generate the agent\u2019s actions, we record a total of 800K perception-action pairs consisting of RGB images of size 224x224 with their respective discrete actions (left turn, right turn, go forward, stop). We also record the local occupancy maps in the neighborhood of the agent at every step.\n\n# Training and fine-tuning PACT\n\n## Running the code\n\n- Mount the data data storage at your desired directory and then change the `dataset_dir_entry` parameter within `configs/<dataset>_<task>.yaml` to mounted one. Dataset can be either mushr/habitat, and the task can be pretrain/localization/mapping. For example, to pretrain a PACT model on the MuSHR dataset, choose `mushr_pretrain.yaml`\n\n- Set the right JSON filenames for `train_ann_file_name` and `val_ann_file_name` in the configuration YAML.\n\n- Adjust any other hyper-parameters for training or model size in the same YAML file. You can also adjust the dataset size for training and validation with `train_dataset_fraction`.\n\n- Run the code as\n\n  `python src/train.py base=configs/mushr_pretrain.yaml`\n\n## Usage Notes\n\nThe code for training PACT is powered by PyTorch Lightning. The codebase is organized as follows:\n\n```\n\u251c\u2500\u2500 configs\n|   \u251c\u2500\u2500 *.yaml\n\u251c\u2500\u2500 src\n|   \u251c\u2500\u2500 train.py\n|   \u251c\u2500\u2500 datamodules\n|   \u251c\u2500\u2500 models\n|   |   \u251c\u2500\u2500 localization.py\n|   |   \u251c\u2500\u2500 mapping.py\n|   |   \u251c\u2500\u2500 pretrain.py\n|   |   \u251c\u2500\u2500 modules\n|   |   |    \u251c\u2500\u2500 decoder_utils.py\n|   |   |    \u251c\u2500\u2500 minGPT.py\n|   |   |    \u251c\u2500\u2500 modeling_pact.py\n|   |   |    \u251c\u2500\u2500 tokenizer_pact.py\n|   |   |    \u251c\u2500\u2500 tokenizer_utils.py\n```\nThe configuration of the dataset/dataloader, model, and the training strategy are set up through YAML files located in `configs/`.\n\nEach task (pretraining, localization, mapping) is defined through a separate Python script which creates a LightningModule. The modules share a common underlying structure:\n\n<img src=\"./docs/pacttask.png\" width=\"600\" >\n\n1. PACTBase(nn.Module): This module consists of a tokenizer and GPT (minGPT) module for the causal Transformer. The input to this module is raw sequences of states (observations) and actions. The data is tokenized through the appropriate tokenizers, and the final output is sequence embedding vectors. As the name indicates, this is a universal submodule of every PACT task.\n   - PACT tokenizer (nn.Module): this is a module that automatically tokenizes each type of input according to the config file (currently configs/*.yaml). The `model.input_config` section in the YAML indicates the state and action data configurations.\n   \n2. Head (nn.Module): Given output embeddings, it performs task-specifc operations. For example, the pretraining module contains state embedding and action prediction heads, whereas the localization module contains a pose prediction head.\n\nThese core modules, along with other functionality such as optimizer configuration, can be seen in `src/models/modules/modeling_pact.py`).\nTo define a new downstream task, you would need to specify a new head module and its corresponding loss function(s).\n\n### Use Pretrained PACT Model\n\nTo initialize the PACT model (tokenizer + GPT) from pretrained weights, set `from_pretrained` to `True` in `model.pretrain_config`\nin the configuration yaml, and `load_ckpt_path` to the path of the desired checkpoint. Optionally, `freeze_base` can be set to `True`\nto fix the PACT base weights when finetuning.\n\n```\n      from_pretrained: True\n      load_ckpt_path: \"{path_to_checkpoint_saved_by_lightning}\"\n      # e.g. \"${trainer.default_root_dir}/checkpoints/last.ckpt\"\n      freeze_base: True\n      # True if you want loaded pretrained weights fixed in later use (e.g., finetuning)\n```\n\nThe parts loaded with pretrained weights are set to `eval()` mode by default.\n\n### Other notes\n\n- The configuration of the GPT model (such as embedding size, number of blocks) resides in the `model.gpt_config` section.\n- The length of the input sequence can be set through `data.train_params.clip_len`.\n- We provide a set of basic Lightning callbacks for use during training such as early stopping, model checkpointing etc. See `trainer.callbacks`.\n- By default, the configuration attempts to scale the training to the number of GPUs available on your machine. If you run it on a multi-GPU machine, training is done using the DistributedDataParallel strategy.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PACT", "org_name": "microsoft", "org_repo": "microsoft/PACT", "platform_org_repo": "github+microsoft/PACT", "link_to_repo": "https://github.com/microsoft/PACT", "platform": "github", "language": "Python", "stargazers_count": 46, "watchers_count": 46}, {"README_text": "# Auto-retinoscopy\n\nThis repo contains the code for the video analysis pipeline for the paper *Towards Automating Retinoscopy for Refractive Error Diagnosis* accepted at the IMWUT 2022. The input to the system is a *retinoscopic video*, with the patient wearing a custom pair of paper frames, and the output is the net refractive power of the eye along the scoped meridian.\n\nThe aim of this README is to describe in detail the video analysis pipeline for ease of understanding and usage by a beginner user. The readme describes in detail the different functions and parameters of the pipeline along with the code snippets.\n\nThe figure below illustrates the (a) Proposed setup consisting of a retinoscope attached to a smartphone, (b) the setup from the patient\u2019s viewpoint facing the logMAR chart, (c) retinoscope and its vergence sleeve, and (d) a single frame of the digital retinoscopy video with automatic detection of fiducials, pupil, reflex edges, and beam edges, using our video processing pipeline.\n\n<p align='center'>\n      <img src=\"./readme_images/summary.jpg\" width=\"720\" height=\"350\" title=\"Overview\">\n</p>\n\n## Dependencies\n\n```\n* Python3.7.XX\n* numpy==1.18.5\n* opencv-contrib-python==4.4.0.42\n* opencv-python==4.4.0.42\n* Pillow==7.2.0\n* scikit-image==0.15.0\n* scikit-learn\n* scipy\n* matplotlib==3.0.3\n* PyYAML==5.3.1\n```\n\nThese are all easily installable via, e.g., `pip install numpy`. Any reasonably recent version of these packages should work. It is recommended to use a python `virtual` environment to setup the dependencies and the code.\n\n## Image Processing Pipeline Overview\n\n<p align='center'>\n      <img src=\"./readme_images/processing_pipeline.jpg\" title=\"Video Analysis Pipeline\">\n</p>\n\n\nThe video analysis pipeline takes as input the video collected via a smartphone attached to the retinoscope, and outputs the net refractive power of the eye along the scoped horizontal meridian. The analysis pipeline first performs **pre-processing** (*image cropping, and tracking*) on the input image. The next step is to detect the **fiducial markers**, and correct for the perspective distortions. This is followed by **beam and pupil detection and reflex edge localization** in the respective search spaces. Finally the refractive power is estimated based on the mathematical model explained in the paper.\n\n## Running Script\n\n`python annotate_init_bbox.py --input_dir <input_dir> --video <video_name.mp4> --paper_frame_size 2`\n\nThis command will prompt the user to draw a rough bounding box across the paper frame. The frame size and input bounding box will be saved in the *init_bbox.csv* file.\n\n`python velocity_pipeline.py --video <video_name.mp4>`\n\nThis command will load the default parameters from the *input_params.yaml* and save the intermediate frames with detected fiducials, beam, pupil and reflex in the output directory. The final predicted power will be stored in the output csv file in the same directory.\n\n## About the code:\n\n* `velocity_pipeline.py` : This is the main file which processes the video based upon input parameters.\n* `annotate_init_bbox.py` : Prompts the user to draw initial rough bounding box across the paper frame.\n* `beam.py` : Detects the pixel locations of the retinoscopic beam given the location of the fiducial markers.\n* `eyes.py`: Detects the pupil and pixel locations for the left and right reflex edge given the cropped region of interest.\n* `fiducials.py`: Detects the square fiducials in the input video.\n* `glasses.py` : Selects the type and size of paper frame design.\n* `utils.py`: Contains supporting utility functions for tracking, cropping, etc.\n* `calculate_power.py`: Contains the functions to calculate the net refractive power from the numpy arrays.\n\n### Additional Modules\n\n* `FSRCNN_x4.pb` : Contains the weights of model used for super resolving the image.\n* `template_2_curr.png` : image of the current frame design used for perspective correction.\n* `input_params.yaml` : default input parameters used in the video analysis pipeline.\n\n## Input Parameters\n\n* `start_frame_index`: Starting frame in the video. Keep 0 by default\n* `end_frame_index`: Ending frame in the video. Keep -1 for processing entire video\n* `scaling_factor`: Default scaling factor: 1 [DEPRECATED]\n\n* `input`:\n    * `directory_path`: path to the input directory\n    * `init_bbox_file`: name of the csv file containing init bbox\n\n* `output_path`:\n    * `original_frames`: path to the directory saving original video frames.\n    * `tracking_frames`: path to the directory saving final annotated frames.\n    * `raw_warped_frames`: path to the directory saving perspective corrected frames.\n    * `eyes_frames`: path to the directory saving intermediate frames of pupil and reflex detection.\n    * `beam_frames`: path to the directory saving intermediate frames of beam detection.\n    * `glasses_frames`: path to the directory saving intermediate frames of fiducial detection.\n    * `numpy_path`: path to the directory saving numpy output of all the detected entities.\n\n* `output`:\n    * `directory_path`: output directory path.\n    * `save_original_frames`: Boolean to save original frames.\n    * `save_tracking_frames`: Boolean to save annotated output frames.\n    * `run_glasses_detection`: Boolean to run fiducial detection. This is required for beam/reflex detection.\n    * `run_beam_segmentation`: Boolean to run beam detection.\n    * `run_reflex_segmentation`: Boolean to run pupil and reflex edge detection.\n    * `save_numpy_output`: Boolean to save detected entities in the form of numpy array.\n    * `power_prediction`: Boolean to predict power using proposed mathematical formulation.\n\n* `eyes`:\n    * `scaling_factor`: Scaling factor for reflex detection. Default: 4.\n    * `pupillary_margin`: Margin around detected pupil.\n    * `pupil_hough_param2_max_value`: Hough parameter for pupil detection. [PUPIL_DETECTION]\n    * `pupil_hough_param2_min_value`: Hough parameter for pupil detection. [PUPIL_DETECTION]\n    * `pupil_min_radius`: Minimum radius of pupil. [PUPIL_DETECTION]\n    * `pupil_max_radius`: Maximum radius of pupil. [PUPIL_DETECTION]\n    * `averaging_window`: Window size for gradient based reflex edge calculation. [REFLEX_EDGE_AVERAGING]\n    * `histogram_bar_size`: Histogram width for finding center coordinates of pupil. [PUPIL_TIMESTAMP_DETECTION]\n    * `pupil_pass_separator`: Maximum number of allowed frames where pupil is not detected within the single pass. [PUPIL_DETECTION, DEPRECATED]\n    * `median_pupil_radius_margin`: Allowed margin for pupil radius around median pupil radius. [PUPIL_DETECTION]\n    * `reflex_vertical_column_percent`: Percentage of reflex along the column so that it is not considered part of specular reflex [SPECULAR_REFLEX_REMOVAL]\n\n* `glasses`:\n    * `square_tolerance`: Allowed tolerance for fiducial square [SQUARE_DETECTION]\n    * `fiducials_min_area`: Minimum area of detected fiducial\n    * `fiducials_extent`: Extent of detected contour with square. Controls squareness of detected quadrilaterals. [SQUARE_DETECTION]\n    * `fiducials_aspect`: Allowed aspect ratio of detected contours to be called as square. Controls squareness of detected quadrilaterals. [SQUARE_DETECTION]\n    * `fiducials_side`: Square fiducials, Default: 4 [DEPRECATED]\n    * `number_of_fids`: Number of fiducials in the frame. For current pattern, its 5\n    * `fiducial_real_size_cm`: Size of fiducial in real: 0.5 cm\n    * `fid_centers_right_curr_2`: List of centers of fiducial squares in the template image (Right eye) [FITTING_PAPER_FRAME]\n    * `fid_centers_left_curr_2`: List of centers of fiducial squares in the template image (Left eye) [FITTING_PAPER_FRAME]\n    * `fid_bbox_size`: Size of fiducials in template image (in pixels) [HOMOGRAPHY_SCALE_RECOVERY]\n\n* `power_calculation`:\n    * `minimum_passes_reqd`: Minimum passes required for power calculation [NEUTRALIZATION]\n    * `number_lines`: Number of lines to scope within pupil\n    * `pupil_vertical_allowed_range`: Allowed central region in the pupil for line plotting along the y-axis\n    * `pupil_horizontal_allowed_range`: Allowed central region in the pupil for timestamp selection along the x-axis\n    * `horizontal_line_width_reflex`: Width of the plotting line\n    * `start_per1`: Start point for reflex edge 1\n    * `start_per2`: Start point for reflex edge 2\n    * `end_per1`: End point for reflex edge 1\n    * `end_per2`: End point for reflex edge 2\n    * `effective_light_distance`: Effective light source distance\n\n* `device`:\n    * `camera_sensor_height`: Camera sensor height used for working distance estimate.\n    * `camera_focal_length`: Camera sensor focal length used for working distance estimate.\n\n**Note** : The video processing pipeline for now only supports retinoscopic beam scoped along horizontal meridian at 0 degree, with video resolution at `4096 x 2160` pixels. Although, the working distance is calculated automatically using size of detected fiducials, capture videos from 30-40 cm from the eyes to maintain the trade-off between resolution and decreasing pupil size due to accommodation.\n\n## Disclaimer\n`This is a research project and not an approved medical software and should not be used for diagnostic purposes.`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Auto-retinoscopy", "org_name": "microsoft", "org_repo": "microsoft/Auto-retinoscopy", "platform_org_repo": "github+microsoft/Auto-retinoscopy", "link_to_repo": "https://github.com/microsoft/Auto-retinoscopy", "platform": "github", "language": "Python", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Empirical Estimation of Differential Privacy\r\n\r\nThis repository provides utilities for estimating DP-$\\varepsilon$ from the confusion matrix of a membership inference attack based on the paper <a href=\"https://arxiv.org/abs/2206.05199\">Bayesian Estimation of Differential Privacy</a>.\r\n\r\n## Installation\r\n\r\nSimply run the following command to install the privacy-estimates python package. It should install all the relevant dependencies as well.\r\n\r\n``` bash\r\npip install privacy-estimates\r\n```\r\n\r\n\r\n## Example\r\n\r\nThe following command takes the output of a membership inference attack on a target model or multiples models in the form of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN). It also requires the value for  $\\alpha$ which states the significance level of the estimate for two sided intervals of the estimated $\\varepsilon$ value.\r\n\r\nFor example, we can post-proces the attack outputs of a CNN trained on CIFAR10 with $(\\varepsilon = 10, \\delta = 10^{-5})$ by running\r\n\r\n``` bash\r\npython scripts/estimate-epsilon.py --alpha 0.1 --delta 1e-5 --TP 487 --TN 1 --FP 512 --FN 0 \r\n```\r\n\r\nThis should take approximately 5 minutes and produce the following output\r\n\r\n``` bash\r\nMethod             Interval                Significance level  eps_lo  eps_hi\r\nJoint beta (ours)  two-sided equal-tailed  0.100               0.145   6.399\r\nJoint beta (ours)  one-sided               0.050               0.145   inf\r\nClopper Pearson    two-sided equal-tailed  0.100               0.000   inf\r\nClopper Pearson    one-sided               0.050               0.000   inf\r\nJeffreys           two-sided equal-tailed  0.100               0.000   inf\r\nJeffreys           one-sided               0.050               0.000   inf\r\n```\r\n\r\n\r\n## Tests\r\n\r\nWe provide a few test cases which can be run by\r\n\r\n``` bash\r\npytest .\r\n```\r\n\r\n# Contributing\r\n\r\nThis project welcomes contributions and suggestions. Most contributions require you to\r\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\r\nand actually do, grant us the rights to use your contribution. For details, visit\r\nhttps://cla.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\r\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\r\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\r\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n", "repo_name": "responsible-ai-toolbox-privacy", "org_name": "microsoft", "org_repo": "microsoft/responsible-ai-toolbox-privacy", "platform_org_repo": "github+microsoft/responsible-ai-toolbox-privacy", "link_to_repo": "https://github.com/microsoft/responsible-ai-toolbox-privacy", "platform": "github", "language": "Python", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Viva Learning SharePoint Starter Kit\n\n[Vis\u00e3o Geral](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/) | [Arquitetura](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/1.-Arquitetura) | [Guia de instala\u00e7\u00e3o](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/2.-Guia-de-Instala%C3%A7%C3%A3o) | [Usando a solu\u00e7\u00e3o](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/3.-Usando-a-Solu%C3%A7%C3%A3o)\n\nO Viva Learning SharePoint Starter Kit \u00e9 um aplicativo customizado desenvolvido usando a Power Platform que faz uso de um site no SharePoint Online permitindo aos colaboradores da organiza\u00e7\u00e3o sugerirem seus pr\u00f3prios conte\u00fados de treinamento com a finalidade de agregar valor \u00e0 organiza\u00e7\u00e3o, acelerar a ado\u00e7\u00e3o do conte\u00fado baseado em SharePoint, compartilhar conhecimento e integrar as \u00e1reas.\n\nA aplicativo fornece uma interface simples para organizar os conte\u00fados do Viva Learning provenientes do SharePoint. O Viva Learning SharePoint Starter Kit prov\u00ea a aprova\u00e7\u00e3o do conte\u00fado antes de serem publicados dentro da biblioteca de cat\u00e1logo da organiza\u00e7\u00e3o, posteriormente estando dispon\u00edvel no Viva Learning.\n\n**Aten\u00e7\u00e3o:** estes recursos de implementa\u00e7\u00e3o s\u00e3o destinados a profissionais de TI com experi\u00eancia em SharePoint.\n\n![Tela principal do Vival Learning SharePoint Starter Kit](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/images/solutionOverview.png)\n\n## Recursos principais\n\n- **Sugest\u00e3o de conte\u00fado:** os usu\u00e1rios selecionados da organiza\u00e7\u00e3o facilmente podem sugerir um conte\u00fado dentro do Viva Learning SharePoint Starter Kit.\n\n- **Aprova\u00e7\u00e3o de conte\u00fado:** os administradores de conte\u00fado da organiza\u00e7\u00e3o recebem as comunica\u00e7\u00f5es dos conte\u00fados que existem para serem aprovados, analisam aprovam ou rejeitam de acordo com a pol\u00edtica de cada organiza\u00e7\u00e3o.\n\n- **Publica\u00e7\u00e3o na biblioteca:** ap\u00f3s a aprova\u00e7\u00e3o a solu\u00e7\u00e3o se encarrega de realizar a publica\u00e7\u00e3o dentro da biblioteca de conte\u00fado do SharePoint mapeada com o Viva Learning.\n\n## Iniciando\n\nComece lendo a **[Vis\u00e3o Geral](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/)** da solu\u00e7\u00e3o para saber mais do que a solu\u00e7\u00e3o n\u00e3o faz.\n\nQuando voc\u00ea estiver confort\u00e1vel para realizar a implementa\u00e7\u00e3o do Viva Learning SharePoint Starter Kit em sua organiza\u00e7\u00e3o, voc\u00ea deve seguir os passos abaixo:\n\n- **[Conhe\u00e7a um pouco da arquitetura](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/1.-Arquitetura)**\n\n- **[Siga as etapas do guia de instala\u00e7\u00e3o](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/2.-Guia-de-Instala%C3%A7%C3%A3o)**\n\n- **[Aprenda a usar a solu\u00e7\u00e3o com o guia de uso](https://github.com/microsoft/vivalearningsharepointstarterkit/wiki/3.-Usando-a-Solu%C3%A7%C3%A3o)**\n\nAproveitem essa solu\u00e7\u00e3o de controle de conte\u00fado do cat\u00e1logo do Viva Learning proveniente de bibliotecas do SharePoint.\n\nEssa \u00e9 uma solu\u00e7\u00e3o modelo, e pode ser adaptada conforme a necessidade da organiza\u00e7\u00e3o, seu c\u00f3digo n\u00e3o est\u00e1 bloqueado ou mesmo criptografado.\n\nCaso queira conhecer mais sobre o Microsoft Viva acesse: https://www.microsoft.com/microsoft-viva\n\n## Aviso legal\n\n### Sobre este reposit\u00f3rio do GitHub\n\nO reposit\u00f3rio do GitHub do Viva Learning SharePoint Starter Kit cont\u00e9m a origem, os problemas e os itens de backlog de todos os componentes que fazem parte da solu\u00e7\u00e3o.\n\n### Contribuindo\n\nEste projeto aceita contribui\u00e7\u00f5es e sugest\u00f5es. A maioria das contribui\u00e7\u00f5es exige que voc\u00ea concorde com um \"Contributor License Agreement (CLA)\" declarando que voc\u00ea tem o direito de, e realmente tem, de nos conceder os direitos de usar sua contribui\u00e7\u00e3o. Para detalhes, visite https://cla.opensource.microsoft.com.\n\nQuando voc\u00ea enviar uma solicita\u00e7\u00e3o, um bot CLA determinar\u00e1 automaticamente se voc\u00ea precisa fornecer um CLA e um PR adequadamente (por exemplo, verifica\u00e7\u00e3o de status, coment\u00e1rio). Basta seguir as instru\u00e7\u00f5es fornecidas pelo Bot. Voc\u00ea s\u00f3 precisar\u00e1 fazer isso uma vez em todos os reposit\u00f3rios usando nosso CLA.\n\nEste projeto adotou o [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nPara obter maiores informa\u00e7\u00f5es, consulte o [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) ou\nentre em contato com [opencode@microsoft.com](mailto:opencode@microsoft.com) com quaisquer perguntas ou coment\u00e1rios adicionais.\n\n### Marcas registradas\n\nEste projeto pode conter marcas registradas ou logotipos de projetos, produtos ou servi\u00e7os. Uso autorizado da Microsoft marcas e/ou logotipos est\u00e3o sujeitos e devem seguir [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). O uso de marcas registradas e/ou logotipos da Microsoft em vers\u00f5es modficadas deste projeto n\u00e3o deve causar confus\u00e3o ou implicar patroc\u00ednio da Microsoft. Qualquer uso de marcas e/ou logotipos de terceiros est\u00e1 sujeito a pol\u00edticas desses terceiros.", "repo_name": "vivalearningsharepointstarterkit", "org_name": "microsoft", "org_repo": "microsoft/vivalearningsharepointstarterkit", "platform_org_repo": "github+microsoft/vivalearningsharepointstarterkit", "link_to_repo": "https://github.com/microsoft/vivalearningsharepointstarterkit", "platform": "github", "language": null, "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Generate Teams backgrounds from the Planetary Computer\n\nThis repository contains a script to generate Teams backgrounds from the Planetary Computer.\n\n__Note__: This project is still under development, and utilizes a unreleased test endpoint from the Planetary Computer. Expect breaking changes until a full release!\n\n## Requirements\n\n- Python >= 3.6\n\n## Usage\n\nTo use this project, first install the dependencies.\n\nIdeally in a [virtual environment](https://docs.python.org/3/tutorial/venv.html). For example, using a bash shell:\n\n```\n> python -m venv venv\n> source venv/bin/activate\n```\n\nTo install dependencies:\n\n```\n> pip install -r requirements.txt\n```\n\n## Running\n\nRun the script via\n\n```\n> python pc_teams_background.py\n```\n\nThis will generate a new background image based on the settings if it detects that a new image should be generated. A new image is generated if:\n- There is no existing background\n- If the last image was generated longer than the setting \"force_regen_after\" ago.\n- It detects that the previous background has been used (using the last access time), and the background image does not come from an AOI (described below)\n- If the background image is from an AOI, then regenerate if it's been \"aois.refresh_days\" days from the settings.\n\nA new background image will be selected as follows:\n- Construct a set of target\n\n## Settings\n\nYou need to edit the settings for your environment. Copy the `settings.template.yaml` file to `settings.yaml`, and edit any required settings.\n\nSee the settings file for the meaning of the various settings.\n\n## Image location\n\nThe image location is set in the settings file, and should be the folder that Teams saves uploaded images to. This should be something like `c:\\Users\\${USER}\\AppData\\Roaming\\Microsoft\\Teams\\Backgrounds\\Uploads`.\n If using in WSL 2 (recommended), you can access this through the path `/mnt/c/Users/${USER}/AppData/Roaming/Microsoft/Teams/Backgrounds/Uploads`.\n\n ## AOIs\n\n You can provide a GeoJSON feature collection of AOIs that a preferenced for showing by the script. Uncomment the section in the settiongs named `aois` to enable this. You can generate the GeoJSON FeatureCollection however you'd like, but one suggestion is to draw AOIs using the tool [geojson.io](https://geojson.io), and saving off the FeatureCollection that is generated. The script will assign IDs to the features and edit the properties of the collection to keep track of the images used over those areas. If you want to add features to the file, you can copy the contents back into geojson.io, add features, and copy the contents back into the file.\n\n## Setting up a cron job\n\nYou can set up a cron job to run this script at a regular interval in order to automatically mix up your Teams background. If you're running Ubuntu in WSL 2, run crontab to create the cron job to activate the Python virtual environment and run the script.\n\nE.g.\n```\n> crontab -e\n(Add this line via the editor:)\n\n*/15 * * * *  /bin/bash -c \"cd ~/proj/pc/pc-teams-background && source venv/bin/activate && python pc_teams_background.py\"\n```\n\n(The above assumes that you cloned the repository to ~/proj/pc-teams-background and followed the above instructions to create a virtualenv)\n\nThe above cron job will run the script every 15 minutes.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "planetary-computer-teams-background", "org_name": "microsoft", "org_repo": "microsoft/planetary-computer-teams-background", "platform_org_repo": "github+microsoft/planetary-computer-teams-background", "link_to_repo": "https://github.com/microsoft/planetary-computer-teams-background", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# HostProcess container base image\n\n## Overview\n\nThis project produces a minimal base image that can be used with [HostProcess containers](https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/).\n\nThis image *cannot* be used with any other type of Windows container (process isolated, Hyper-V isolated, etc...)\n\n### Benefits\n\nUsing this image as a base for HostProcess containers has a few advantages over using other base images for Windows containers including:\n\n- Size - This image is a few KB. Even the smallest official base image (NanoServer) is still a few hundred MB is size.\n- OS compatibility - HostProcess containers do not inherit the same [compatibility requirements](https://docs.microsoft.com/virtualization/windowscontainers/deploy-containers/version-compatibility) as Windows server containers and because of this it does not make sense to include all of the runtime / system binaries that make up the different base layers. Using this image allows for a single container image to be used on any Windows Server version which can greatly simplify container build processes.\n\n## Usage\n\nBuild your container from `mcr.microsoft.com/oss/kubernetes/windows-host-process-containers-base-image:v1.0.0`.\n\n### Dockerfile example\n\nCreate `hello-world.ps1` with the following content:\n\n```powershell\nWrite-output \"Hello World!\"\n```\n\nand `Dockerfile.windows` with the following content:\n\n```Dockerfile\nFROM mcr.microsoft.com/oss/kubernetes/windows-host-process-containers-base-image:v1.0.0\n\nADD hello-world.ps1 .\n\nENV PATH=\"C:\\Windows\\system32;C:\\Windows;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;\"\nENTRYPOINT [\"powershell.exe\", \"./hello-world.ps1\"]\n```\n\n### Build with BuildKit\n\nContainers based on this image cannot currently be built with Docker Desktop.\nInstead use BuildKit or other tools.\n\nExample:\n\n#### Create a builder\n\nOne time step\n\n```cmd\ndocker buildx create --name img-builder --use --platform windows/amd64\n```\n\n#### Build your image\n\nUse the following command to build and push to a container repository\n\n```cmd\n docker buildx build --platform windows/amd64 --output=type=registry -f {Dockerfile} -t {ImageTag} .\n```\n\n### Container Manifests\n\nAs mentioned in [Benefits](#benefits) above, HostProcess containers can run on any Windows Server version however\nthere is currently logic in containerd to only pull Windows container images if the `OSVersion` defined in the\ncontainer manifest matches the `OSVersion` of the node.\n\nWhen building container images from this base image it is recommended to not include this image in a manifest-list\nand also not include *any* platform information in the manifest for now.\n\nPlease see https://github.com/containerd/containerd/issues/7431 for more information.\n\n## Licensing\n\nCode is the repository is released under the `MIT` [license](/LICENSE).\n\nThe container images produced by this repository are distributed under the `CC0` license.\n\n- [CC0 license](/cc0-license.txt)\n- [CC0 legacode](/cc0-legalcode.txt)\n", "repo_name": "windows-host-process-containers-base-image", "org_name": "microsoft", "org_repo": "microsoft/windows-host-process-containers-base-image", "platform_org_repo": "github+microsoft/windows-host-process-containers-base-image", "link_to_repo": "https://github.com/microsoft/windows-host-process-containers-base-image", "platform": "github", "language": "PowerShell", "stargazers_count": 29, "watchers_count": 29}, {"README_text": "# Open Source Software (OSS) Secure Supply Chain (SSC) Framework\n\nTHIS REPO HAS BEEN CONTRIBUTED TO THE OPENSSF. THE NEW REPO IS HERE [https://github.com/ossf/s2c2f/](https://github.com/ossf/s2c2f/).\n\n<img alt=\"secure package icon\" src=\"images/secure-package-icon.png\" width=15%>\n\n## Overview\nThis guide outlines and defines how to securely consume Open Source Software (OSS) dependencies into the developer\u2019s workflow. This paper is split into two parts: a solution-agonistic set of practices and a maturity model-based implementation guide. The Framework is targeted toward organizations that do software development, that take a dependency on open source software, and that seek to improve the security of their software supply chain. \n\nThe OSS SSC Framework is complete with: \n\n* A high-level solution-agnostic set of practices \n* A detailed list of requirements \n* A list of real-world supply chain threats specific to OSS, and how our Framework requirements mitigates them \n* A maturity model-based implementation guide, with links to tools from across the industry \n* A process for assessing your organization\u2019s maturity \n* A mapping of the Framework requirements to 6 other supply chain specifications \n\n## View or Download the OSS SSC Framework Specification\n\n> \u2b50: **Click\n> _[here](./specification/Open_Source_Software_(OSS)_Secure_Supply_Chain_(SSC)_Framework.pdf)_ for the PDF of the specification**\n> \n> :atom:: **Click _[here](./specification/framework.md)_ to view the specification in markdown** \n\n## Contributing\n\nThe general Community Specification Contributing Policy is captured on the [Contributing](Contributing.md) section. Specific guidelines based on the policy for how best to contribute to the OSS SSC Framework specification is [here](./specification/README.md). The living OSS SSC Framework is captured in [markdown](./specification/framework.md) and is where all updates will take place.\n\n*SLA to Triage Issues*:\n- The OSS SSC Framework working group will review, triage, and respond to issues during each Community Meeting.\n\n## Meeting Times\n\n*Community and Technical Meetings*:\n- <a href=https://calendar.google.com/calendar/ical/s63voefhp5i9pfltb5q67ngpes%40group.calendar.google.com/public/basic.ics>iCal Subscription Link</a>\n\n- OSS SSC Framework community meetings are held the 3rd Tuesday of every month @ 12:00 PM Pacific. Please click the iCal Subscription link above or email adrian.diglio@microsoft.com to be added to the meeting invitation.\n\n*Technical Meetings*:\n- OSS SSC Framework technical meetings are held the last Monday of every month @ 2:00 PM Pacific. Please click the iCal Subscription link above or email adrian.diglio@microsoft.com to be added to the meeting invitation.\n\n[Meeting minutes and agenda](https://docs.google.com/document/d/1YG-CVbKa7pVlNNkLAOV8O7kiY5mBoFXpehc1VOW0MW4)\n\n*Chat channels*:\n\n- We have a Slack channel on the OpenSSF Slack instance: <a href=https://openssf.slack.com/archives/C03THTH3RSM><img src=\"https://img.shields.io/badge/Slack:-%23oss_ssc_framework%20on%20OpenSSF%20Slack-blue.svg?style=plastic&logo=slack\" alt=\"Slack Channel\"></a>\n<a href=https://slack.openssf.org/><img src=\"https://img.shields.io/badge/Slack-OpenSSF%20Slack%20Invite-blue.svg?style=plastic&logo=slack\" alt=\"Slack Invite\"></a>\n", "repo_name": "oss-ssc-framework", "org_name": "microsoft", "org_repo": "microsoft/oss-ssc-framework", "platform_org_repo": "github+microsoft/oss-ssc-framework", "link_to_repo": "https://github.com/microsoft/oss-ssc-framework", "platform": "github", "language": null, "stargazers_count": 231, "watchers_count": 231}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kvs-rc-migration-web-orchestrator", "org_name": "microsoft", "org_repo": "microsoft/kvs-rc-migration-web-orchestrator", "platform_org_repo": "github+microsoft/kvs-rc-migration-web-orchestrator", "link_to_repo": "https://github.com/microsoft/kvs-rc-migration-web-orchestrator", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# pulse\n[![Build Status](https://github.com/microsoft/pulse/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/microsoft/pulse/actions/workflows/build.yml)\n[![codecov](https://codecov.io/gh/microsoft/pulse/branch/main/graph/badge.svg?token=BJH0u9ZUcC)](https://codecov.io/gh/microsoft/pulse)\n[![Nuget](https://img.shields.io/nuget/v/pulse.svg)](https://www.nuget.org/packages/pulse/)\n[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/microsoft/pulse/blob/main/LICENSE)\n\nPulse is a .NET library defining a common interface and metric configurations. Pulse's common interface allows you to develop metrics libraries that abstract an underlying monitoring system in a test-driven way.\n\nAbstracted metrics libraries are helpful in the event the underlying monitoring system changes. Whether the underlying monitoring library experiences breaking changes or [you decide to do a complete swap of the underlying monitoring library](#switching-to-a-different-metrics-library), rest assured that you will only have to update the abstracted library and not your service code.\n\nThe library targets net5, net6, and net7.\n\n## Download\n\n[pulse](https://www.nuget.org/packages/pulse/) is distributed via the NuGet gallery.\n\n## Projects Using pulse\n\n* [pulse-prometheus](https://github.com/microsoft/pulse-prometheus)\n\n## Switching to a Different Metrics Library?\n\n* All [pulse-projects](#projects-using-pulse) implement the [pulse](#pulse) interface, meaning all [pulse-projects](#projects-using-pulse) are interchangable. \n* If you need to change monitoring systems in the future, you can do so without having to change your projects code!\n* If a [pulse-project](#projects-using-pulse) does not exist for the metric monitoring system you need to use, you can easily create one by implementing the [pulse](#pulse) common interface.\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "pulse", "org_name": "microsoft", "org_repo": "microsoft/pulse", "platform_org_repo": "github+microsoft/pulse", "link_to_repo": "https://github.com/microsoft/pulse", "platform": "github", "language": "C#", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "<p align=\"left\"> \n  <img src=\"https://github.com/microsoft/contributorlicenseagreement/actions/workflows/build_net_core.yml/badge.svg?branch=main&event=push\"></a>\n  <img src=\"https://github.com/microsoft/contributorlicenseagreement/actions/workflows/publish_all.yml/badge.svg?branch=main&event=push\"></a>\n  <img src=\"https://github.com/microsoft/contributorlicenseagreement/blob/coverage/docs/images/linecoverage.svg\"></a>\n</p>\n\n [Count Lines Of Code](https://github.com/microsoft/ContributorLicenseAgreement/blob/cloc/docs/cloc/cloc.txt)\n\n# Contributor License Agreement - CLA \n\n## What is CLA?\n\nCLA is a tool that allows outside contributors to sign a contribution license agreement (cla), an important license protection for Microsoft and our contributors. Signing this agreement allows external contributors to contribute code to Microsoft open-source repos. It is built on the Microsoft GitHub Policy Service platform.\n\n## Installation\n\n- Install [Microsoft GitHub Policy Service](https://github.com/apps/microsoft-github-policy-service)\n- If you run on GH Enterprise Cloud, you have to give us(by creating an issue on this repo) the name of your enterprise.\n- Create a .github repo.\n- Add [platformcontext.yml](https://github.com/microsoft/.github/blob/main/policies/platformcontext.yml) under policies folder. You can push this directly.\n- Add [cla.yml](https://github.com/microsoft/.github/blob/main/policies/cla.yml) under policies folder. Create a seperate PR for this, the policy service will create a comment example bellow. After you merge the PR, CLA policy will be activated across the entire org.\n![image](https://user-images.githubusercontent.com/19934057/197821627-3933c109-bbba-4714-b16c-8b457ad2084d.png)\n- For checks on branch protection make sure you select \"any source\" or \"Microsoft GitHub Policy Service\".\n![image](https://user-images.githubusercontent.com/19934057/198332238-66781732-8b4c-4b04-8f05-e7571caec999.png)\n\n\n## Usage\n\nTo use CLA, you need to define a [cla.yml](src/ContributorLicenseAgreement.Core.Tests/Data/cla.yml)/[Microsoft GitHub CLA](https://github.com/microsoft/.github/blob/main/policies/cla.yml) file on org level (example YAML file). This YAML file should define how the CLA should act, the content of the license agreement, and which accounts are exempt from signing.\nIn addition, the *Microsoft GitHub Policy Service* needs to be installed for your organization.\n\n### cla.yml - required properties\n- **content**: the contribution licence agreement the author should sign.\n- **minimalChangeRequired**: defines the minumum changes in files or codelines required to make the policy enforce signing a cla first.\n- **--files**: defines the minimum number of files changed for cla to act.\n- **--codeLines**: defines the minimum number of code lines changed for cla to act.\n\n### cla.yml - optional properties\n- **bypassUsers**: defines the users for which the cla check is omitted.\n- **bypassOrgs**: defines the orgs for which the cla check is omitted.\n- **prohibitedCompanies**: defines the companies for which users cannot sign a cla.\n- **autoSignMsftEmployee**: if set to true, Microsoft employees will not be asked to sign a cla.\n- **checkSummary**: defines the check summary text shown.\n- **signRepos**:\trepoName, companyName, & fileName (this section is relevant only for the list of partners that have signed the CLA for their employees)\n- **--repoName**:\trepository that lives in same organization as the policy and contains approvedUsers.csv\n- **--companyName**:\tname of the company the CLA is for (stored in our CLA database)\n- **--fileName**: approvedUsers.csv\t(links to list of users allowed to use CLA, more info below)\n\n### List of Approved Users\nIf your company has an agreement with Microsoft where only certain users are allowed to make contributions on behalf of your company, then you can specify the users via a CSV file titled approvedUsers.csv which should be located inside the company's repo. The list is global per CLA content link and has to be specified only once, [example here](https://github.com/microsoft/.github/blob/main/policies/cla.yml).\n\nFor each user that you want to allow making contributions, add the github username as a line in the csv file (no commas).\n\n### List of Approved Bots\nIn order to allow bots to create and merge pull requests, they must be pre-approved. Pre-approving bots is done by adding the bot name to the *approvedBos.csv* file located in the [cla-approved-bots](https://github.com/microsoft/cla-approved-bots) repo.\n\n\n## Commands\n\nWhenever a pull request is created, the CLA check will confirm whether or not the user who opened the PR has \nalready signed an agreement. If not, it will output a comment prompting the user to accept the agreement and the CLA check on the PR will not pass until that is done.\n\n### Accepting\n\nTo accept the agreement, the user can issue one of the following two commands as a comment on the pull request.\n\n```\nIf you are contributing on behalf of yourself:\n@microsoft-github-policy-service agree\n\nIf you are contributing on behalf of a company:\n@microsoft-github-policy-service agree company=\"your company\"\n```\n\n### Terminating\n\nA user can choose to terminate the signed agreement by issuing the following command by commenting under a pull\nrequest that was opened by the same user.\n\n```\n@microsoft-github-policy-service terminate\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ContributorLicenseAgreement", "org_name": "microsoft", "org_repo": "microsoft/ContributorLicenseAgreement", "platform_org_repo": "github+microsoft/ContributorLicenseAgreement", "link_to_repo": "https://github.com/microsoft/ContributorLicenseAgreement", "platform": "github", "language": "C#", "stargazers_count": 37, "watchers_count": 37}, {"README_text": "# pulse-prometheus\n[![Build Status](https://github.com/microsoft/pulse-prometheus/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/microsoft/pulse-prometheus/actions/workflows/build.yml)\n[![codecov](https://codecov.io/gh/microsoft/pulse-prometheus/branch/main/graph/badge.svg?token=q1quhRyQgj)](https://codecov.io/gh/microsoft/pulse-prometheus)\n[![Nuget](https://img.shields.io/nuget/v/pulse.prometheus.svg)](https://www.nuget.org/packages/pulse.prometheus/)\n[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/microsoft/pulse-prometheus/blob/main/LICENSE.txt)\n\npulse-prometheus is a .NET library that implements the [pulse](https://github.com/microsoft/pulse) interface and abstracts the C# [prometheus-net](https://github.com/prometheus-net/prometheus-net) library.\n\nAbstracted metrics libraries are helpful in the event the underlying monitoring system changes. Whether the underlying monitoring library experiences breaking changes or [you decide to do a complete swap of the underlying monitoring library](#switching-to-a-different-metrics-library), rest assured that you will only have to update the abstracted library and not your service code.\n\n# Table of Contents\n* [Requirements](#requirements)\n* [Download](#download)\n* [Best Practices and Usage](#best-practices-and-usage)\n* [Quick Start](#quick-start)\n* [Middleware Extensions](#middleware-extensions)\n* [Depedency Injection](#dependency-injection)\n* [Metric Factory](#metric-factory)\n* [Counter](#counter)\n* [Gauge](#gauge)\n* [Histogram](#histogram)\n* [Summary](#summary)\n* [Tracking Operation Duration](#tracking-operation-duration)\n* [Counting In-Progress Operations](#counting-in-progress-operations)\n* [Counting Exceptions](#counting-exceptions)\n* [Mutable Labels](#mutable-labels)\n* [Immutable Labels](#immutable-labels)\n* [Switching to a Different Metrics Library?](#switching-to-a-different-metrics-library)\n* [Contributing](CONTRIBUTING.md)\n* [Security](SECURITY.md)\n* [Support](SUPPORT.md)\n* [License](LICENSE.txt)\n\n## Requirements\n\n* [.NET 5.0](https://dotnet.microsoft.com/en-us/download/dotnet/5.0), [.NET 6.0](https://dotnet.microsoft.com/en-us/download/dotnet/6.0), or [.NET 7.0](https://dotnet.microsoft.com/en-us/download/dotnet/7.0)\n\n## Download\n\n[pulse-prometheus](https://www.nuget.org/packages/pulse.prometheus/) is distributed via the NuGet gallery.\n\n## Best Practices and Usage\n\nThis library allows you to instrument your code with custom metrics.\nYou are expected to be familiar with:\n* [Prometheus user guide](https://prometheus.io/docs/introduction/overview/)\n* [Prometheus metric types](http://prometheus.io/docs/concepts/metric_types/)\n* [Prometheus metric best practices](http://prometheus.io/docs/practices/instrumentation/#counter-vs.-gauge-vs.-summary)\n\n## Quick Start\n\n1. Configure the endpoint. See [Middleware Extensions](#middleware-extensions).\n1. Register the [IMetricFactory](#metric-factory). See [Dependency Injection](#dependency-injection). Optionally, [create an IMetricFactory](#metric-factory) instead of injecting it.\n1. Use your [IMetricFactory](#metric-factory) to create [counters](#counter), [gauges](#gauge), [histograms](#histogram), and [summaries](#summary).\n1. Use your metrics to do other cool things like [track operation duration](#tracking-operation-duration), [count in-progress operations](#counting-in-progress-operations), and [count exceptions](#counting-exceptions). Also check out how to use [mutable labels](#mutable-labels) and [immutable labels](#immutable-labels)\n\n## Middleware Extensions\n\nUse metric middleware extensions to output metrics to an endpoint.\n\nThe default is `/metrics`.\n\n```csharp\npublic class Startup\n{\n    ...\n\n    public void Configure(IApplicationBuilder app, IWebHostEnvironment env)\n    {\n        app.UseEndpoints(endpoints =>\n        {\n            ...\n            endpoints.MapMetrics();\n        });\n    }\n    \n    ...\n}\n```\n\n## Dependency Injection\n\nUse [IServiceCollection](https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection.iservicecollection?view=dotnet-plat-ext-6.0) extensions to make it easy for consumers to register implementations for the included [IMetricFactory](#metric-factory).\n\n```csharp\npublic class Startup\n{\n    ...\n    \n    public void ConfigureServices(IServiceCollection services)\n    {\n        ...\n        services.AddMetricFactory();\n    }\n    \n    ...\n}\n```\n\nIn subsequent code, request an implementation for the [IMetricFactory](#metric-factory) by including it in the constructor of the classes which require it.\n\n```csharp\npublic class Example\n{\n    private readonly pulseMetricFactory;\n\n    public Example(IMetricFactory metricFactory)\n    {\n        pulseMetricFactory = metricFactory;\n    }\n\n    public void TrackSomething()\n    {\n        var counter = pulseMetricFactory.CreateCounter(\"counter\", \"this is a counter metric\");\n        using (counter.NewTimer())\n        {\n            Thread.Sleep(1000);\n        }\n    }\n}\n```\n\n## Metric Factory\n\nCreate a metric factory as an entry point into the library, or use [dependency injection](#dependency-injection).\n\n```csharp\nprivate static readonly IMetricFactory MyAppMetricFactory = new PulseMetricFactory(new PrometheusMetricFactory());\n```\n\n## Counter\n\nCounters only increase in value and reset to zero when the process restarts.\n\n```csharp\nprivate static readonly ICounter LogCounter = \n    MyAppMetricFactory.CreateCounter(\"myapp_number_of_logs_emitted\", \"Number of logs emitted.\");\n\n...\n\nLog();\nLogCounter.Increment();\n```\n\n## Gauge\n\nGauges can have any numeric value and change arbitrarily.\n\n```csharp\nprivate static readonly IGauge JobQueueGauge = \n    MyAppMetricFactory.CreateGauge(\"myapp_jobs_queued\", \"Number of jobs queued.\");\n\n...\n\njobs.Enqueue(job);\nJobQueueGauge.Increment();\n\n...\n\njobs.Dequeue(job);\nJobQueueGauge.Decrement();\n```\n\n## Histogram\n\nHistograms track the size and number of events in buckets. This allows for aggregatable calculation over a set of buckets.\n\n```csharp\ndouble[] buckets = new double[] { 100.0, 200.0, 300.0, 400.0, 500.0 } \n\nprivate static readonly IHistogram OrderValueHistogram = \n    MyAppMetricFactory.CreateHistogram(\n        \"myapp_order_value_usd\", \n        \"Histogram of received order values (in USD).\", \n        new HistogramConfiguration()\n        {\n            Buckets = buckets\n        });\n\n...\n\nOrderValueHistogram.Observe(order.TotalValueUsd);\n```\n\n## Summary\n\nSummaries track events over time, with a default of 10 minutes.\n\n```csharp\nprivate static readonly ISummary UploadSizeSummary = \n    MyAppMetricFactory.CreateSummary(\n        \"myapp_upload_size_bytes\", \n        \"Summary of upload sizes (in bytes) over last 10 minutes.\");\n\n...\n\nUploadSizeSummary.Observe(file.Length);\n```\n\n## Tracking Operation Duration\n\nTimers can be used to report the duration of an operation (in seconds) to a Summary, Histogram, Gauge or Counter. Wrap the operation you want to measure in a using block.\n\n```csharp\ndouble[] buckets = new double[] { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0 };\n\nprivate static readonly IHistogram UploadDuration = \n    MyAppMetricFactory.CreateHistogram(\n        \"myapp_upload_duration_seconds\", \n        \"Histogram of file upload durations.\", \n        new HistogramConfiguration()\n        {\n            Buckets = buckets\n        });\n\n...\n\nusing (UploadDuration.NewTimer())\n{\n    Scheduler.Upload(file);\n}\n```\n\n## Counting In-Progress Operations\n\nYou can use `Gauge.TrackInProgress()` to track how many concurrent operations are taking place. Wrap the operation you want to track in a using block.\n\n```csharp\nprivate static readonly IGauge UploadsInProgress = \n    MyAppMetricFactory.CreateGauge(\n        \"myapp_uploads_in_progress\", \n        \"Number of upload operations occuring.\");\n\n...\n\nusing (UploadsInProgress.TrackInProgress())\n{\n    Scheduler.Upload(file);\n}\n```\n\n## Counting Exceptions\n\nYou can use `Counter.CountExceptions()` to count the number of exceptions that occur while executing some code.\n\n```csharp\nprivate static readonly ICounter FailedExtractions =\n    MyAppMetricFactory.CreateCounter(\n        \"myapp_extractions_failed_total\", \n        \"Number of extraction operations that failed.\");\n\n...\n\nFailedExtractions.CountExceptions(() => Extractor.Extract(file));\n```\n\nYou can also filter the exception types to observe:\n\n```csharp\nFailedExtractions.CountExceptions(() => Extractor.Extract(file), IsExtractionRelatedException);\n\nbool IsExtractionRelatedException(Exception ex)\n{\n    return ex is ExtractionException; // Only count ExtractionExceptions.\n}\n```\n\n## Mutable Labels\n\nAll metrics can have mutable labels, allowing grouping of related time series.\n\nSee the best practices on [naming](http://prometheus.io/docs/practices/naming/)\nand [labels](http://prometheus.io/docs/practices/instrumentation/#use-labels).\n* Labels should contain a limited set of label values.\n    * URLs would be a bad choice. There are infinite options.\n    * HTTP response codes would be a good choice. There is a finite set of options.\n\nTaking a counter as an example:\n\n```csharp\nprivate static readonly ICounter HttpResponseCounter = \n    MyAppMetricFactory.CreateCounter(\n        \"myapp_http_responses_total\", \n        \"Number of responses received by http method and response code.\", \n        new CounterConfiguration()\n        {\n            MutableLabelNames = new string[] { \"http_method\", \"http_response_code\" }\n        });\n\n...\n\n// Specify the value(s) for the label(s) when you want to call a metric operation.\nHttpResponseCounter.WithLabels(\"GET\", \"200\").Inc();\n```\n\n## Immutable Labels\n\nYou can add immutable labels that always have fixed values.\n\nTaking a counter as an example with immutable labels and mutable labels:\n\n```csharp\nDictionary<string, string> immutableLabels = new Dictionary<string, string>() { { \"service_name\", \"scheduler\" } };\n...\nprivate static readonly ICounter HttpResponseCounter = \n    MyAppMetricFactory.CreateCounterWithStaticLabels(\n        \"myapp_http_responses_received\", \n        \"Count of responses received, labelled by response code.\", \n        new CounterConfiguration()\n        {\n            ImmutableLabels = immutableLabels\n            MutableLabelNames = new string[] { \"http_response_code\" }\n        });\n\n...\n\n// Labels applied to individual instances of the metric.\nHttpResponseCounter.WithLabels(\"404\").Inc();\nHttpResponseCounter.WithLabels(\"200\").Inc();\n```\n\n## Switching to a Different Metrics Library?\n\n* All pulse-projects implement the [pulse](https://github.com/microsoft/pulse) interface, meaning all pulse-projects are interchangable. \n* If you need to change monitoring systems in the future, you can do so without having to change your projects code!\n* If a pulse-project does not exist for the metric monitoring system you need to use, you can easily create one by implementing the [pulse](https://github.com/microsoft/pulse) common interface.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "pulse-prometheus", "org_name": "microsoft", "org_repo": "microsoft/pulse-prometheus", "platform_org_repo": "github+microsoft/pulse-prometheus", "link_to_repo": "https://github.com/microsoft/pulse-prometheus", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-netapp-files", "org_name": "microsoft", "org_repo": "microsoft/azure-netapp-files", "platform_org_repo": "github+microsoft/azure-netapp-files", "link_to_repo": "https://github.com/microsoft/azure-netapp-files", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Microsoft Kiota Abstractions Library for Python\n[![PyPI version](https://badge.fury.io/py/microsoft-kiota-abstractions.svg)](https://badge.fury.io/py/microsoft-kiota-abstractions)\n[![CI Actions Status](https://github.com/microsoft/kiota-abstractions-python/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-abstractions-python/actions)\n[![Downloads](https://pepy.tech/badge/microsoft-kiota-abstractions)](https://pepy.tech/project/microsoft-kiota-abstractions)\n\nThe Microsoft Kiota abstractions library for Python is the python package defining the basic constructs Kiota projects need once an SDK has been generated from an OpenAPI definition.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to the abstraction package to build and run.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Abstractions Library\n\nIn order to use this library, install the package by running:\n\n```cmd\npip install microsoft-kiota-abstractions\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-abstractions-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-abstractions-python", "platform_org_repo": "github+microsoft/kiota-abstractions-python", "link_to_repo": "https://github.com/microsoft/kiota-abstractions-python", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Microsoft Kiota HTTP library\n[![PyPI version](https://badge.fury.io/py/microsoft-kiota-http.svg)](https://badge.fury.io/py/microsoft-kiota-http)\n[![CI Actions Status](https://github.com/microsoft/kiota-http-python/actions/workflows/build_publish.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-http-python/actions)\n[![Downloads](https://pepy.tech/badge/microsoft-kiota-http)](https://pepy.tech/project/microsoft-kiota-http)\n\nThe Microsoft Kiota HTTP Library is a python HTTP implementation with HTTPX library.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a http package to to make HTTP requests to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Microsoft Kiota HTTP library\n\nIn order to use this library, install the package by running:\n\n```cmd\npip install microsoft-kiota-http\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-http-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-http-python", "platform_org_repo": "github+microsoft/kiota-http-python", "link_to_repo": "https://github.com/microsoft/kiota-http-python", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Microsoft Kiota Serialization library for JSON\n[![PyPI version](https://badge.fury.io/py/microsoft-kiota-serialization-json.svg)](https://badge.fury.io/py/microsoft-kiota-serialization-json)\n[![CI Actions Status](https://github.com/microsoft/kiota-serialization-json-python/actions/workflows/build_publish.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-serialization-json-python/actions)\n[![Downloads](https://pepy.tech/badge/microsoft-kiota-serialization-json)](https://pepy.tech/project/microsoft-kiota-serialization-json)\n\nThe Microsoft Kiota Serialization Library for JSON is a python implementation to serialize/deserialize JSON.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a json serialization package to handle json payloads from an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Microsoft Kiota Serialization JSON library\n\nIn order to use this library, install the package by running:\n\n```cmd\npip install microsoft-kiota-serialization-json\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-json-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-json-python", "platform_org_repo": "github+microsoft/kiota-serialization-json-python", "link_to_repo": "https://github.com/microsoft/kiota-serialization-json-python", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Microsoft Kiota Text Serialization library\n[![PyPI version](https://badge.fury.io/py/microsoft-kiota-serialization-text.svg)](https://badge.fury.io/py/microsoft-kiota-serialization-text)\n[![CI Actions Status](https://github.com/microsoft/kiota-serialization-text-python/actions/workflows/build_publish.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-serialization-text-python/actions)\n[![Downloads](https://pepy.tech/badge/microsoft-kiota-serialization-text)](https://pepy.tech/project/microsoft-kiota-serialization-text)\n\nThe Microsoft Kiota Text Serialization Library is a python implementation to serialize/deserialize text/plain.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a json serialization package to handle json payloads from an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Microsoft Kiota Text Serialization library\n\nIn order to use this library, install the package by running:\n\n```cmd\npip install microsoft-kiota-serialization-text\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-text-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-text-python", "platform_org_repo": "github+microsoft/kiota-serialization-text-python", "link_to_repo": "https://github.com/microsoft/kiota-serialization-text-python", "platform": "github", "language": "Python", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Microsoft Kiota Authentication Azure Library for Python\n[![PyPI version](https://badge.fury.io/py/microsoft-kiota-authentication-azure.svg)](https://badge.fury.io/py/microsoft-kiota-authentication-azure)\n[![CI Actions Status](https://github.com/microsoft/kiota-authentication-azure-python/actions/workflows/build_publish.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-authentication-azure-python/actions)\n[![Downloads](https://pepy.tech/badge/microsoft-kiota-authentication-azure)](https://pepy.tech/project/microsoft-kiota-authentication-azure)\n\nThe Microsoft Kiota Authentication Azure Library is an implementation to authenticate HTTP requests using @azure/identity.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to an authentication provider to make calls to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Microsoft Kiota Authentication Azure library\n\nIn order to use this library, install the package by running:\n\n```cmd\npip install microsoft-kiota-authentication-azure\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-authentication-azure-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-authentication-azure-python", "platform_org_repo": "github+microsoft/kiota-authentication-azure-python", "link_to_repo": "https://github.com/microsoft/kiota-authentication-azure-python", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "[![Go Reference](https://pkg.go.dev/badge/github.com/microsoft/go-winmd.svg)](https://pkg.go.dev/github.com/microsoft/go-winmd)\n\n# Go winmd parser\n\nA Windows Metadata (a.k.a. winmd) parser written in Go and based on the ECMA-335 6th edition standard.\n\n## Development References\n\nThese resources are useful as reference while working on the go-winmd module:\n\n* [ECMA-335 specification](https://www.ecma-international.org/wp-content/uploads/ECMA-335_6th_edition_june_2012.pdf)\n* [microsoft/win32metadata](https://github.com/microsoft/win32metadata) GitHub repository\n* [Windows Runtime (WinRT) document](https://learn.microsoft.com/en-us/uwp/winrt-cref/winmd-files) on Windows Metadata\n* [.NET System.Reflection.Metadata implementation](https://github.com/dotnet/runtime/tree/main/src/libraries/System.Reflection.Metadata)\n* [go.dev/issue/43838 \"x/sys/windows: use win32metadata?\"](http://go.dev/issue/43838)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "go-winmd", "org_name": "microsoft", "org_repo": "microsoft/go-winmd", "platform_org_repo": "github+microsoft/go-winmd", "link_to_repo": "https://github.com/microsoft/go-winmd", "platform": "github", "language": "Go", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Azure Dev Day\n\nThis repo contains all the information for Azure Dev Day labs.\n\n## Prerequisites\n\nWe encourage you to follow along the hands-on labs during lab sessions.\n\n* If you don't have an Azure Subscription to use for these labs, please create a free subscription at https://azure.microsoft.com/free/.\n\n## Labs\n\n1. [Web Solutions Lab](./1-web-lab)\n1. [Serverless Solutions Lab](./2-serverless-lab)\n1. [Microservice Solutions Lab](./3-microservice-lab)\n1. [DevOps with GitHub Lab](./4-devops-lab)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-dev-day-java", "org_name": "microsoft", "org_repo": "microsoft/azure-dev-day-java", "platform_org_repo": "github+microsoft/azure-dev-day-java", "link_to_repo": "https://github.com/microsoft/azure-dev-day-java", "platform": "github", "language": "HTML", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "<table>\n<tr>\n<th>Client App</th>\n<th>API</th>\n</tr>\n<tr>\n<td>\n    <a href=\"https://github.com/microsoft/fhir-watch/actions/workflows/azure-static-web-apps-calm-wave-01c2b9f10.yml\"><img src=\"https://github.com/microsoft/fhir-watch/actions/workflows/azure-static-web-apps-calm-wave-01c2b9f10.yml/badge.svg\" alt=\"Azure Static Web Apps CI/CD\"></a>\n</td>\n<td>\n    <a href=\"https://github.com/microsoft/fhir-watch/actions/workflows/FhirWatchApi20220815130348.yml\"><img src=\"https://github.com/microsoft/fhir-watch/actions/workflows/FhirWatchApi20220815130348.yml/badge.svg\" alt=\"Build and deploy .NET Core application to Function App FhirWatchApi20220815130348\"></a>\n</td>\n</tr>\n</table>\n\n![fhir-watch-banner](./docs/img/fhir-watch-banner.png)\n\n# FHIRWatch\nWelcome to FHIRWatch!\n\n## Overview\nFHIRWatch is an MVP Blazor Webassembly static web app for comparing data synchronized between FHIR API and Microsoft Cloud for Health.\n\n## Architecture\nA Blazor WebAssembly (wasm) application using the pattern of micro-frontends implemented by lazy-loading UI \"modules\" only when requested by navigation events from the client.\n\n## FhirWatch Web app\n\n### Patient List\n![fhirwatchPatientList](https://user-images.githubusercontent.com/69863736/186772782-86f7a070-0667-44fc-a8c9-e0fcc9ae21f6.png)\n\n### Compare Patient table\n![fhirwatchComparePatient](https://user-images.githubusercontent.com/69863736/186772861-d795e47c-b4ed-4f52-8678-e327a0c00699.png)\n\n\n## Get Started\nHere's what you'll need to run this application in your own environment.\n\n### FHIR API (R4)\nYou will need access to a FHIR API that meets the HL7 R4 spec requirements. \nHost your own with one of these popular offerings:\n* [Azure API for FHIR](https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir/fhir-portal-quickstart)\n* [Azure FHIR Server (OSS)](https://github.com/microsoft/fhir-server/blob/main/docs/QuickstartDeployPortal.md)\n* [Firely Server](https://fire.ly/products/firely-server/)\n\nOR, use one of these [publicly available test servers](https://wiki.hl7.org/index.php?title=Publicly_Available_FHIR_Servers_for_testing).\n\n### Web Host\nThere are many options for hosting a Blazor WebAssembly application. This repo contains a Github Action workflow to automatically deploy the site to Azure Static Web Apps. \nFollow [this tutorial](https://docs.microsoft.com/en-us/azure/static-web-apps/deploy-blazor) to learn more.\n\nOther options include:\n* Azure App Service\n* Azure Blob Storage static web\n* [and more...](https://docs.microsoft.com/en-us/aspnet/core/blazor/host-and-deploy/webassembly?view=aspnetcore-5.0)\n\n### Next Steps\n[Fork this repo](https://github.com/microsoft/fhir-watch/fork) to modify or add your own functionality.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Community Discord\nIf you are interested in projects like this or connecting with the health and life sciences developer community, please join our Discord server at https://aka.ms/HLS-Discord. We're a technology agnostic community seeking to share and collaborate on all things related to developing healthcare solutions. For in-depth questions specific to this project, please use the \"Discussions\" tab on GitHub. We welcome your thoughts and feedback.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fhir-watch", "org_name": "microsoft", "org_repo": "microsoft/fhir-watch", "platform_org_repo": "github+microsoft/fhir-watch", "link_to_repo": "https://github.com/microsoft/fhir-watch", "platform": "github", "language": "C#", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Azure Health Data Services Toolkit\n\nThe Azure Health Data Services Toolkit helps you extend the functionality of Azure Health Data Services by providing a consistent toolset to build custom operations to modify the core service behavior.\nWith the growth of health data workloads on Azure, we\u2019ve found that developers need custom behavior on top of our services. This toolkit abstracts common patterns so you can focus on delivering your use cases.\n\n## NuGet Packages\n\n| Package Name | Description |\n| --- | --- |\n| [Microsoft.AzureHealth.DataServices.Core](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Core/)<br/>[![NuGet](https://img.shields.io/nuget/v/Microsoft.AzureHealth.DataServices.Core.svg?label=NuGet)](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Core)| .NET 6 toolkit for creating custom operations when using Azure Health Data Services. |\n| [Microsoft.AzureHealth.DataServices.Channels.Extensions](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Channels.Extensions/)<br/>[![NuGet](https://img.shields.io/nuget/v/Microsoft.AzureHealth.DataServices.Channels.Extensions.svg?label=NuGet)](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Channels.Extensions) | .NET 6 toolkit for extending channels using Azure Health Data Services. |\n| [Microsoft.AzureHealth.DataServices.Caching](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Caching/)<br/>[![NuGet](https://img.shields.io/nuget/v/Microsoft.AzureHealth.DataServices.Caching.svg?label=NuGet)](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Caching) | .NET 6 toolkit for adding caching using Azure Health Data Services. |\n| [Microsoft.AzureHealth.DataServices.Storage](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Storage/)<br/>[![NuGet](https://img.shields.io/nuget/v/Microsoft.AzureHealth.DataServices.Storage.svg?label=NuGet)](https://www.nuget.org/packages/Microsoft.AzureHealth.DataServices.Storage)| .NET 6 toolkit to simplify Azure storage operations when using Azure Health Data Services. |\n\n## Getting started\n\nThe fastest way to test out the toolkit and see it in action is through our [Quickstart sample](./samples/Quickstart/). This sample will walk you through some common patterns that you'll need to create custom operations for Azure Health Data Services.\n\nRead the [developer guide](./docs/dev_setup.md) for help setting up your local and cloud environment for developing custom behaviors for Azure Health Data Services.\n\nAlso check out our full list of [samples on how to use the toolkit here](./samples/README.md) for even more inspiration on how to create your own custom operations.\n\n## Common Fast Healthcare Interoperability Resources (FHIR\u00ae) use cases\n\nSome FHIR service use cases that this toolkit can help you implement are:\n\n- FHIR operations not [supported by the FHIR Service](https://docs.microsoft.com/azure/healthcare-apis/fhir/fhir-features-supported#extended-operations) yet.\n  - Trial implementation guides.\n  - Organization-specific operations.\n  - Less widely adopted operations.\n- Implementation guide development.\n- Transforming request and/or response payloads.\n- Custom authorization logic (like consent, etc.).\n\n## Key Concepts\n\nFor detailed information, read [the concept guide here](./docs/concepts.md).\n\nWhen we say \u201ccustom operations\u201d we are talking about a purpose-built solution which acts as a proxy for a single or small set of HTTP endpoints. This toolkit is here to simplify developing such solutions. It\u2019s recommended to use Azure API Management or similar for routing certain requests to these custom operations so that the client only sees one endpoint. Azure API Management can also present a unified authorization experience to your clients. This is why our samples don\u2019t have authorization on the endpoints. \n\nWhen building custom operations, you\u2019ll come across these concepts related to the toolkit.\n\n- **Operation Context**: Common object passed between components of a pipeline containing the request and response.\n- **Pipeline**: Container for the actions of custom operations with filters, channels, and bindings executed in the order shown below.\n  - **Filter:** A unit of action that modifies the request and/or result via the Operation Context. Filters can be chained together in a single input/output section of a pipeline.\n  - **Channel:** Used to output data in a pipeline to an external system actor (ESA). This is usually an Azure service (like Storage, Event Hub, and/or Service Bus).\n  - **Binding:** The target service for a custom operation (usually a FHIR service). This can be null for custom operations that don't need to have a destination.\n\n### What about the FHIR Proxy? \n\n[FHIR Proxy](https://github.com/microsoft/fhir-proxy) was created in response to customer requests for customizing the Azure API for FHIR. With the release of Azure Health Data Services, we\u2019ve come up with a new approach to customization.\n\n- This toolkit lets you go beyond the proxy pattern and gives you tools for more extensive customization with programmatic components that flexibly connect to the broader Azure ecosystem.\n- This toolkit is designed to be used in smaller operation-specific modules. If you are customizing a certain behavior, you don\u2019t need to proxy the rest of your API calls.\n- This toolkit is compute-agnostic and can be deployed on any .NET 6.0 server like Azure Functions, Azure App Service, Azure Kubernetes Service, etc.\n- This toolkit is released and versioned via NuGet packages.\n- We have designed this toolkit with coding best practices in mind, like object-oriented pipelines and extended testing.\n\n*If there is functionality in the FHIR Proxy that is not covered by the Health Data Services toolkit, please submit an issue and we will look into adding a sample!*\n\n## Resources\n\n### Links\n\n- [FHIR Service Documentation](https://docs.microsoft.com/azure/healthcare-apis/fhir/overview)\n- [FHIR Server OSS Repository](https://github.com/microsoft/fhir-server)\n \n### Sample production architecture\n\nThis architecture is a sample of how you could deploy and integrate custom operations built with the Azure Health Data Services toolkit in a production environment with Azure Health Data Services.\n\n![Example architecture diagram](./docs/images/HealthcareAPIInfastructure20220929.png)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Disclaimers\n\nThe Azure Health Data Services toolkit is an open-source project. It is not a managed service, and it is not part of Microsoft Azure Health Data Services. You bear sole responsibility for compliance with local law and for any data you use with this open-source toolkit. Please review the information and licensing terms on this GitHub website before using the Azure Health Data Services toolkit.\n\nThe Azure Health Data Services toolkit GitHub is intended only for use in transferring and formatting data. It is not intended for use as a medical device or to perform any analysis or any medical function and the performance of the software for such purposes has not been established. You bear sole responsibility for any use of this software, including incorporation into any product intended for a medical purpose.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\nFHIR\u00ae is the registered trademark of HL7 and is used with the permission of HL7. \n", "repo_name": "azure-health-data-services-toolkit", "org_name": "microsoft", "org_repo": "microsoft/azure-health-data-services-toolkit", "platform_org_repo": "github+microsoft/azure-health-data-services-toolkit", "link_to_repo": "https://github.com/microsoft/azure-health-data-services-toolkit", "platform": "github", "language": "C#", "stargazers_count": 27, "watchers_count": 27}, {"README_text": "# Russh\n[![Rust](https://github.com/warp-tech/russh/actions/workflows/rust.yml/badge.svg)](https://github.com/warp-tech/russh/actions/workflows/rust.yml)  <!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-16-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\nLow-level Tokio SSH2 client and server implementation.\n\nThis is a fork of [Thrussh](https://nest.pijul.com/pijul/thrussh) by Pierre-\u00c9tienne Meunier.\n\n> \u2728 = added in Russh\n\n* [More panic safety](https://github.com/warp-tech/russh#safety) \u2728\n* `async_trait` support \u2728\n* `direct-tcpip` (local port forwarding)\n* `forward-tcpip` (remote port forwarding) \u2728\n* `direct-streamlocal` (local UNIX socket forwarding, client only) \u2728\n* Ciphers:\n  * `chacha20-poly1305@openssh.com`\n  * `aes256-gcm@openssh.com` \u2728\n  * `aes256-ctr` \u2728\n  * `aes192-ctr` \u2728\n  * `aes128-ctr` \u2728\n* Key exchanges:\n  * `curve25519-sha256@libssh.org`\n  * `diffie-hellman-group1-sha1` \u2728\n  * `diffie-hellman-group14-sha1` \u2728\n  * `diffie-hellman-group14-sha256` \u2728\n* MACs:\n  * `hmac-sha1` \u2728\n  * `hmac-sha2-256` \u2728\n  * `hmac-sha2-512` \u2728\n  * `hmac-sha1-etm@openssh.com` \u2728\n  * `hmac-sha2-256-etm@openssh.com` \u2728\n  * `hmac-sha2-512-etm@openssh.com` \u2728\n* Host keys:\n  * `ssh-ed25519`\n  * `rsa-sha2-256`\n  * `rsa-sha2-512`\n  * `ssh-rsa` \u2728\n* Dependency updates\n* OpenSSH keepalive request handling \u2728\n* OpenSSH agent forwarding channels \u2728\n* OpenSSH `server-sig-algs` extension \u2728\n\n## Safety\n\n* `deny(clippy::unwrap_used)`\n* `deny(clippy::expect_used)`\n* `deny(clippy::indexing_slicing)`\n* `deny(clippy::panic)`\n* Exceptions are checked manually\n\n### Panics\n\n* When the Rust allocator fails to allocate memory during a CryptoVec being resized.\n\n### Unsafe code\n\n* `cryptovec` uses `unsafe` for faster copying, initialization and binding to native API.\n\n## Ecosystem\n\n* [russh-sftp](https://crates.io/crates/russh-sftp) - server-side SFTP subsystem support for `russh` - see `russh/examples/sftp_server.rs`.\n* [async-ssh2-tokio](https://crates.io/crates/async-ssh2-tokio) - simple high-level API for running commands over SSH.\n\n## Contributors \u2728\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mihirsamdarshi\"><img src=\"https://avatars.githubusercontent.com/u/5462077?v=4?s=100\" width=\"100px;\" alt=\"Mihir Samdarshi\"/><br /><sub><b>Mihir Samdarshi</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=mihirsamdarshi\" title=\"Documentation\">\ud83d\udcd6</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://peet.io/\"><img src=\"https://avatars.githubusercontent.com/u/2230985?v=4?s=100\" width=\"100px;\" alt=\"Connor Peet\"/><br /><sub><b>Connor Peet</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=connor4312\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kvzn\"><img src=\"https://avatars.githubusercontent.com/u/313271?v=4?s=100\" width=\"100px;\" alt=\"KVZN\"/><br /><sub><b>KVZN</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=kvzn\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.telekom.de\"><img src=\"https://avatars.githubusercontent.com/u/21334898?v=4?s=100\" width=\"100px;\" alt=\"Adrian M\u00fcller (DTT)\"/><br /><sub><b>Adrian M\u00fcller (DTT)</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=amtelekom\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.evilsocket.net\"><img src=\"https://avatars.githubusercontent.com/u/86922?v=4?s=100\" width=\"100px;\" alt=\"Simone Margaritelli\"/><br /><sub><b>Simone Margaritelli</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=evilsocket\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://joegrund.com\"><img src=\"https://avatars.githubusercontent.com/u/458717?v=4?s=100\" width=\"100px;\" alt=\"Joe Grund\"/><br /><sub><b>Joe Grund</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=jgrund\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AspectUnk\"><img src=\"https://avatars.githubusercontent.com/u/59799956?v=4?s=100\" width=\"100px;\" alt=\"AspectUnk\"/><br /><sub><b>AspectUnk</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=AspectUnk\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://0io.eu\"><img src=\"https://avatars.githubusercontent.com/u/203575?v=4?s=100\" width=\"100px;\" alt=\"Sim\u00e3o Mata\"/><br /><sub><b>Sim\u00e3o Mata</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=simao\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://mariotaku.org\"><img src=\"https://avatars.githubusercontent.com/u/830358?v=4?s=100\" width=\"100px;\" alt=\"Mariotaku\"/><br /><sub><b>Mariotaku</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=mariotaku\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yorkz1994\"><img src=\"https://avatars.githubusercontent.com/u/16678950?v=4?s=100\" width=\"100px;\" alt=\"yorkz1994\"/><br /><sub><b>yorkz1994</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=yorkz1994\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://volution.ro/\"><img src=\"https://avatars.githubusercontent.com/u/29785?v=4?s=100\" width=\"100px;\" alt=\"Ciprian Dorin Craciun\"/><br /><sub><b>Ciprian Dorin Craciun</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=cipriancraciun\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mllken\"><img src=\"https://avatars.githubusercontent.com/u/11590808?v=4?s=100\" width=\"100px;\" alt=\"Eric Milliken\"/><br /><sub><b>Eric Milliken</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=mllken\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Swelio\"><img src=\"https://avatars.githubusercontent.com/u/24651896?v=4?s=100\" width=\"100px;\" alt=\"Swelio\"/><br /><sub><b>Swelio</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=Swelio\" title=\"Code\">\ud83d\udcbb</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/joshbenz\"><img src=\"https://avatars.githubusercontent.com/u/94999261?v=4?s=100\" width=\"100px;\" alt=\"Joshua Benz\"/><br /><sub><b>Joshua Benz</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=joshbenz\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://homepage.ruhr-uni-bochum.de/Jan.Holthuis/\"><img src=\"https://avatars.githubusercontent.com/u/1834516?v=4?s=100\" width=\"100px;\" alt=\"Jan Holthuis\"/><br /><sub><b>Jan Holthuis</b></sub></a><br /><a href=\"#security-Holzhaus\" title=\"Security\">\ud83d\udee1\ufe0f</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mateuszkj\"><img src=\"https://avatars.githubusercontent.com/u/2494082?v=4?s=100\" width=\"100px;\" alt=\"mateuszkj\"/><br /><sub><b>mateuszkj</b></sub></a><br /><a href=\"https://github.com/warp-tech/russh/commits?author=mateuszkj\" title=\"Code\">\ud83d\udcbb</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n", "repo_name": "vscode-russh", "org_name": "microsoft", "org_repo": "microsoft/vscode-russh", "platform_org_repo": "github+microsoft/vscode-russh", "link_to_repo": "https://github.com/microsoft/vscode-russh", "platform": "github", "language": "Rust", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "---\nArtifactType: executable\nDocumentation: N/A\nLanguage: csharp\nPlatform: windows, linux\nStackoverflow: N/A\nTags: roslyn,analyzers,precompiled\n---\n\n# Peeker\n\nPeeker is a tool that runs Roslyn code analyzers on precompiled .NET binaries. If source mappings are provided in the form of a PDB file, the inspection results are associated with the original source locations. Peeker outputs its results in the SARIF file format.\n\n## Getting Started\n\nClone Peeker and build with `dotnet build`. In use, you will need to supply libraries that provide Roslyn analyzers (i.e. Microsoft.CodeQuality.Analyzers.dll) and their dependencies.\n\n### Prerequisites\n\n.NET 6.0\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Versioning and changelog\n\nWe use [SemVer](http://semver.org/) for versioning. For the versions available, see the tags on this repository.\n\n## Acknowledgments\n\n* This project makes great use of [ILSpy](https://github.com/ICSharpCode/ILSpy), a third party .NET decompilation library.\n\nSee also [NOTICE.md](https://github.com/microsoft/peeker/blob/main/NOTICE.md).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "peeker", "org_name": "microsoft", "org_repo": "microsoft/peeker", "platform_org_repo": "github+microsoft/peeker", "link_to_repo": "https://github.com/microsoft/peeker", "platform": "github", "language": "C#", "stargazers_count": 37, "watchers_count": 37}, {"README_text": "# Flake8 extension for Visual Studio Code\n\nA Visual Studio Code extension with support for the `flake8` linter. The extension ships with `flake8=5.0.4`.\n\nNote:\n\n-   This extension is supported for all [actively supported versions](https://devguide.python.org/#status-of-python-branches) of the `python` language (i.e., python >= 3.7).\n-   The bundled `flake8` is only used if there is no installed version of `flake8` found in the selected `python` environment.\n-   Minimum supported version of `flake8` is `5.0.0`.\n\n## Usage\n\nOnce installed in Visual Studio Code, flake8 will be automatically executed when you open a Python file.\n\nIf you want to disable flake8, you can [disable this extension](https://code.visualstudio.com/docs/editor/extension-marketplace#_disable-an-extension) per workspace in Visual Studio Code.\n\n## Settings\n\n| Settings                | Default                                                                                                                                | Description                                                                                                                                                                                                                                                                                                              |\n| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| flake8.args             | `[]`                                                                                                                                   | Custom arguments passed to `flake8`. E.g `\"flake8.args\" = [\"--config=<file>\"]`                                                                                                                                                                                                                                           |\n| flake8.severity         | `{ \"convention\": \"Information\", \"error\": \"Error\", \"fatal\": \"Error\", \"refactor\": \"Hint\", \"warning\": \"Warning\", \"info\": \"Information\" }` | Controls mapping of severity from `flake8` to VS Code severity when displaying in the problems window. You can override specific `flake8` error codes `{ \"convention\": \"Information\", \"error\": \"Error\", \"fatal\": \"Error\", \"refactor\": \"Hint\", \"warning\": \"Warning\", \"W0611\": \"Error\", \"undefined-variable\": \"Warning\" }` |\n| flake8.logLevel         | `error`                                                                                                                                | Sets the tracing level for the extension.                                                                                                                                                                                                                                                                                |\n| flake8.path             | `[]`                                                                                                                                   | Setting to provide custom `flake8` executable. This will slow down linting, since we will have to run `flake8` executable every time or file save or open. Example 1: `[\"~/global_env/flake8\"]` Example 2: `[\"conda\", \"run\", \"-n\", \"lint_env\", \"python\", \"-m\", \"flake8\"]`                                                |\n| flake8.interpreter      | `[]`                                                                                                                                   | Path to a python interpreter to use to run the linter server.                                                                                                                                                                                                                                                            |\n| flake8.importStrategy   | `useBundled`                                                                                                                           | Setting to choose where to load `flake8` from. `useBundled` picks flake8 bundled with the extension. `fromEnvironment` uses `flake8` available in the environment.                                                                                                                                                       |\n| flake8.showNotification | `off`                                                                                                                                  | Setting to control when a notification is shown.                                                                                                                                                                                                                                                                         |\n\n## Commands\n\n| Command                | Description                       |\n| ---------------------- | --------------------------------- |\n| Flake8: Restart Server | Force re-start the linter server. |\n", "repo_name": "vscode-flake8", "org_name": "microsoft", "org_repo": "microsoft/vscode-flake8", "platform_org_repo": "github+microsoft/vscode-flake8", "link_to_repo": "https://github.com/microsoft/vscode-flake8", "platform": "github", "language": "Python", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# Project\n\nScripts and Files to support the Compliance Partner Build Intent Workshops (https://aka.ms/mci/workshops)\n\n\n## ComplianceActivationAssessment\nUse the ComplianceActivationAssesment.ps1 file as part of the Protect and Govern Sensitive Data Activator\nFollow the instructions in the workshop guide run the script and include the output of the report as part of your final results for your customer\n\n### Current Issues and Limitations\n1) The ComplianceActivationAssessment Report has only been tested against Commercial Office 365 Tenants.  If you need to connect to a GCC or Regional(China / Germany) Tenant, please update the powershell connection strings inside the code\n2) Scripts have only been tested against English/Unicode lanuguages\n3) License Friendly Names MAY not exist for non commercial license SKUs\n\n## WorkshopPOEReport\nUse the workshoppoereport.ps1 file as part of the Protect and Govern Sensitive Data Activator\nFollow the instructions in the workshop guide run the script and include the output of the report as part of your final results for your customer\n\n### Current Issues and Limitations:\n1) The WorkshopPOE Report only works against Commercial Office 365 Tenants.  If you need to connect to a GCC or Regional(China / Germany) Tenant, please update the powershell connection strings inside the code\n2) The WorkshopPOE Report currently uses the AzureAD powershell Module.  It will be updated to GraphAPI in a future version\n3) Scripts have only been tested against English/Unicode lanuguages\n\n## ComplianceEnvrionmentPrep\nUse the complianceenvriomentprep.ps1 file as part of the Mitigate Complinace and Prviacy Risks Activator\nFollow the instructions in the workshop guide and run the script to prepare the isolated Microsoft 365 Developer Tenant.\n\n### Current Issues and Limitations:\n1) The ComplianceEnvriomentPrep script is designed to be used against tenants that are provisioned as part of the Microsoft 365 Developer Subscription. It has not been tested against other Microsoft 365 envrioments\n2) use the startup switch '-debug' to enable basic logging and get an output of information logged to the screen\n3) Scripts have only been tested against English/Unicode lanuguages\n\n### Other Files\nThe additional files in this repository are developed for the Mitigate Compliance and Privacy Risks Activator. Please refer to the engagement master delivery guide on how to leverage them\n1) Rulepack.xml - Custom sensitive information type rule pack\n2) DeleteFileFlow.zip - Power Automate Flow\n3) FileCopyFlow.zip - Power Automate Flow\n4) FileCreationFlow.zip - Power Automate Flow\n5) Mark8.zip - Sample files\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CompliancePartnerWorkshops", "org_name": "microsoft", "org_repo": "microsoft/CompliancePartnerWorkshops", "platform_org_repo": "github+microsoft/CompliancePartnerWorkshops", "link_to_repo": "https://github.com/microsoft/CompliancePartnerWorkshops", "platform": "github", "language": "PowerShell", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project ALTA\nALTA is a load testing helper tool that allows you to execute MSTests through Azure Load Testing with minimal additional latency. ALTA provides end-to-end solutions for tests written in .NET Core and .NET framework.\n\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ALTA", "org_name": "microsoft", "org_repo": "microsoft/ALTA", "platform_org_repo": "github+microsoft/ALTA", "link_to_repo": "https://github.com/microsoft/ALTA", "platform": "github", "language": "C#", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nAll code is licensed under the MIT license and we triage actively on GitHub. We enthusiastically welcome contributions and feedback. Please read the [contributing guide](./contributing.md) before starting.\n\n## Security Reporting\n\nIf you find a security issue with our libraries or services please report it to [secure@microsoft.com](mailto:secure@microsoft.com) with as much detail as possible. Your submission may be eligible for a bounty through the [Microsoft Bounty](http://aka.ms/bugbounty) program. Please do not post security issues to GitHub Issues or any other public site. We will contact you shortly upon receiving the information. We encourage you to get notifications of when security incidents occur by visiting [this page](https://technet.microsoft.com/en-us/security/dd252948) and subscribing to Security Advisory Alerts.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "quick-authentication-mobile", "org_name": "microsoft", "org_repo": "microsoft/quick-authentication-mobile", "platform_org_repo": "github+microsoft/quick-authentication-mobile", "link_to_repo": "https://github.com/microsoft/quick-authentication-mobile", "platform": "github", "language": "Java", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Power Platform Advocates\n\nWelcome to Power Platform Advocates GitHub repository. In this repo, the Power Platform Advocates team will add presentations and workshops.\n\n## Presentations\n\n- [Exam Readiness Live (PL-100)](/Presentations/ExamReadinessLivePL100.pdf)\n\n## Workshops\n\n- [Power Platform & Mixed Reality](/Workshops/MR/README.md)\n- [Power Apps & Java](/Workshops/JavaAndPowerApps/README.md)\n- [Power BI & Synapse](/Workshops/SynapsePowerBI/README.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PowerPlatformAdvocates", "org_name": "microsoft", "org_repo": "microsoft/PowerPlatformAdvocates", "platform_org_repo": "github+microsoft/PowerPlatformAdvocates", "link_to_repo": "https://github.com/microsoft/PowerPlatformAdvocates", "platform": "github", "language": "Bicep", "stargazers_count": 54, "watchers_count": 54}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "monubamb", "org_name": "microsoft", "org_repo": "microsoft/monubamb", "platform_org_repo": "github+microsoft/monubamb", "link_to_repo": "https://github.com/microsoft/monubamb", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "## W3C DID for Confidential Consortium Framework ![CCF](https://img.shields.io/badge/CCF-3.0.3-green)\r\n\r\nRepository specifying a [W3C DID-Core](https://www.w3.org/TR/did-core/) method for building decentralized identifier networks on top of CCF. This repositorty also includes\r\na CCF application (Typescript) implementation for generating new identifiers, resolving identifiers and maintaining identifiers and their associated cryptographic keys.\r\n\r\n## Third-party components\r\n\r\nWe rely on several open source third-party components, attributed under [THIRD_PARTY_NOTICES](THIRD_PARTY_NOTICES.txt).\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions. Please see the [Contribution guidelines](.github/CONTRIBUTING.md).", "repo_name": "did-ccf", "org_name": "microsoft", "org_repo": "microsoft/did-ccf", "platform_org_repo": "github+microsoft/did-ccf", "link_to_repo": "https://github.com/microsoft/did-ccf", "platform": "github", "language": "TypeScript", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "# BioGPT\nThis repository contains the implementation of [BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9), by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.\n\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version == 1.12.0\n* Python version == 3.10\n* fairseq version == 0.12.0:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\ngit checkout v0.12.0\npip install .\npython setup.py build_ext --inplace\ncd ..\n```\n* Moses\n``` bash\ngit clone https://github.com/moses-smt/mosesdecoder.git\nexport MOSES=${PWD}/mosesdecoder\n```\n* fastBPE\n``` bash\ngit clone https://github.com/glample/fastBPE.git\nexport FASTBPE=${PWD}/fastBPE\ncd fastBPE\ng++ -std=c++11 -pthread -O3 fastBPE/main.cc -IfastBPE -o fast\n```\n* sacremoses\n``` bash\npip install sacremoses\n```\n* sklearn\n``` bash\npip install scikit-learn\n```\n\nRemember to set the environment variables `MOSES` and `FASTBPE` to the path of Moses and fastBPE respetively, as they will be required later.\n\n# Getting Started\n## Pre-trained models\nWe provide our pre-trained BioGPT model checkpoints along with fine-tuned checkpoints for downstream tasks, available both through URL download as well as through the Hugging Face \ud83e\udd17 Hub. \n\n|Model|Description|URL|\ud83e\udd17 Hub|\n|----|----|---|---|\n|BioGPT|Pre-trained BioGPT model checkpoint|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/Pre-trained-BioGPT.tgz)|[link](https://huggingface.co/microsoft/biogpt)|\n|BioGPT-Large|Pre-trained BioGPT-Large model checkpoint|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/Pre-trained-BioGPT-Large.tgz)|[link](https://huggingface.co/microsoft/biogpt-large)|\n|BioGPT-QA-PubMedQA-BioGPT|Fine-tuned BioGPT for question answering task on PubMedQA|[link](https://msralaphilly2.blob.core.windows.net/release/BioGPT/checkpoints/QA-PubMedQA-BioGPT.tgz)| |\n|BioGPT-QA-PubMedQA-BioGPT-Large|Fine-tuned BioGPT-Large for question answering task on PubMedQA|[link](https://msralaphilly2.blob.core.windows.net/release/BioGPT/checkpoints/QA-PubMedQA-BioGPT-Large.tgz)||\n|BioGPT-RE-BC5CDR|Fine-tuned BioGPT for relation extraction task on BC5CDR|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/RE-BC5CDR-BioGPT.tgz)| |\n|BioGPT-RE-DDI|Fine-tuned BioGPT for relation extraction task on DDI|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/RE-DDI-BioGPT.tgz)| |\n|BioGPT-RE-DTI|Fine-tuned BioGPT for relation extraction task on KD-DTI|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/RE-DTI-BioGPT.tgz)| |\n|BioGPT-DC-HoC|Fine-tuned BioGPT for document classification task on HoC|[link](https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/DC-HoC-BioGPT.tgz)| |\n\nDownload them and extract them to the `checkpoints` folder of this project.\n\nFor example:\n``` bash\nmkdir checkpoints\ncd checkpoints\nwget https://msramllasc.blob.core.windows.net/modelrelease/BioGPT/checkpoints/Pre-trained-BioGPT.tgz\ntar -zxvf Pre-trained-BioGPT.tgz\n```\n\n## Example Usage\nUse pre-trained BioGPT model in your code:\n```python\nimport torch\nfrom fairseq.models.transformer_lm import TransformerLanguageModel\nm = TransformerLanguageModel.from_pretrained(\n        \"checkpoints/Pre-trained-BioGPT\", \n        \"checkpoint.pt\", \n        \"data\",\n        tokenizer='moses', \n        bpe='fastbpe', \n        bpe_codes=\"data/bpecodes\",\n        min_len=100,\n        max_len_b=1024)\nm.cuda()\nsrc_tokens = m.encode(\"COVID-19 is\")\ngenerate = m.generate([src_tokens], beam=5)[0]\noutput = m.decode(generate[0][\"tokens\"])\nprint(output)\n```\n\nUse fine-tuned BioGPT model on KD-DTI for drug-target-interaction in your code:\n```python\nimport torch\nfrom src.transformer_lm_prompt import TransformerLanguageModelPrompt\nm = TransformerLanguageModelPrompt.from_pretrained(\n        \"checkpoints/RE-DTI-BioGPT\", \n        \"checkpoint_avg.pt\", \n        \"data/KD-DTI/relis-bin\",\n        tokenizer='moses', \n        bpe='fastbpe', \n        bpe_codes=\"data/bpecodes\",\n        max_len_b=1024,\n        beam=1)\nm.cuda()\nsrc_text=\"\" # input text, e.g., a PubMed abstract\nsrc_tokens = m.encode(src_text)\ngenerate = m.generate([src_tokens], beam=args.beam)[0]\noutput = m.decode(generate[0][\"tokens\"])\nprint(output)\n```\n\nFor more downstream tasks, please see below.\n\n## Downstream tasks\nSee corresponding folder in [examples](examples):\n### [Relation Extraction on BC5CDR](examples/RE-BC5CDR)\n### [Relation Extraction on KD-DTI](examples/RE-DTI/)\n### [Relation Extraction on DDI](examples/RE-DDI)\n### [Document Classification on HoC](examples/DC-HoC/)\n### [Question Answering on PubMedQA](examples/QA-PubMedQA/)\n### [Text Generation](examples/text-generation/)\n\n## Hugging Face \ud83e\udd17 Usage\n\nBioGPT has also been integrated into the Hugging Face `transformers` library, and model checkpoints are available on the Hugging Face Hub.\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\n```python\nfrom transformers import pipeline, set_seed\nfrom transformers import BioGptTokenizer, BioGptForCausalLM\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\nset_seed(42)\ngenerator(\"COVID-19 is\", max_length=20, num_return_sequences=5, do_sample=True)\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BioGptTokenizer, BioGptForCausalLM\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nBeam-search decoding:\n\n```python\nimport torch\nfrom transformers import BioGptTokenizer, BioGptForCausalLM, set_seed\n\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n\nsentence = \"COVID-19 is\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\n\nset_seed(42)\n\nwith torch.no_grad():\n    beam_output = model.generate(**inputs,\n                                 min_length=100,\n                                 max_length=1024,\n                                 num_beams=5,\n                                 early_stopping=True\n                                )\ntokenizer.decode(beam_output[0], skip_special_tokens=True)\n```\n\nFor more information, please see the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/biogpt) on the Hugging Face website.\n\n## Demos\n\nCheck out these demos on Hugging Face Spaces:\n* [Text Generation with BioGPT-Large](https://huggingface.co/spaces/katielink/biogpt-large-demo)\n* [Question Answering with BioGPT-Large-PubMedQA](https://huggingface.co/spaces/katielink/biogpt-qa-demo)\n\n# License\n\nBioGPT is MIT-licensed.\nThe license applies to the pre-trained models as well.\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "BioGPT", "org_name": "microsoft", "org_repo": "microsoft/BioGPT", "platform_org_repo": "github+microsoft/BioGPT", "link_to_repo": "https://github.com/microsoft/BioGPT", "platform": "github", "language": "Python", "stargazers_count": 3931, "watchers_count": 3931}, {"README_text": "# \ud83c\udf33 The Mother Of All Workshops (MOAW)\n\n[![Open in Visual Studio Code](https://img.shields.io/static/v1?logo=visualstudiocode&label=&message=Open%20in%20VS%20Code&labelColor=2c2c32&color=007acc&logoColor=007acc)](https://github.dev/microsoft/moaw)\n[![Deploy website to GitHub Pages](https://github.com/microsoft/moaw/actions/workflows/deploy.yml/badge.svg)](https://github.com/microsoft/moaw/actions/workflows/deploy.yml)\n[![License: CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA-222.svg)](https://creativecommons.org/licenses/by-sa/4.0/)\n\nGrab-and-go resources to help you learn new skills, with all the tools you need to create, host and share your own workshop.\n\n\ud83d\udc49 https://aka.ms/moaw\n\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/593151/185623023-1175ab1f-9f55-461c-884f-9ed9723edaf3.jpg\" alt=\"logo\" />\n</div>\n\n## Usage\n\n### \ud83d\udcda Want to learn something new?\n\nBrowse the available content on the [website](https://aka.ms/moaw/catalog).\n\n### \ud83d\udc69\u200d\ud83c\udfeb Want to conduct a workshop?\n\nYou can link to any workshop from this repository using the following URL format:\n`https://aka.ms/ws?src=<workshop_folder>/`\n\n**Example:** [https://aka.ms/ws?src=create-workshop/](https://aka.ms/ws?src=create-workshop/)\n\nFor more details, see the [how to use](HOW_TO_USE.md#-conduct-a-workshop) guide.\n\n### \ud83d\ude80 Want to create a new workshop?\n\nFollow this step-by-step tutorial for detailed instructions: [Create a workshop](https://microsoft.github.io/moaw/workshop/create-workshop/).\n\n## Contributing\n\nAll contributions are welcome, including translations, new workshops, bug fixes, etc.\nIf you want to contribute to this repository, please read the [contributing guidelines](CONTRIBUTING.md).\n\n## License\n\n[![Creative Commons License](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n\nAll workshop content is available under the [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/), meaning that you can share and modify it any way you want, as long as you credit the original authors and keep the modified content under the same license.\n\nThe source code of the various tools is available under the [MIT License](packages/website/LICENSE).\n", "repo_name": "moaw", "org_name": "microsoft", "org_repo": "microsoft/moaw", "platform_org_repo": "github+microsoft/moaw", "link_to_repo": "https://github.com/microsoft/moaw", "platform": "github", "language": "TypeScript", "stargazers_count": 34, "watchers_count": 34}, {"README_text": "\n| :warning: WARNING          |\n|:---------------------------|\n| This module is currently in development and is considered pre-release software. Please use with caution!      |\n\n\n\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SMBSecurity", "org_name": "microsoft", "org_repo": "microsoft/SMBSecurity", "platform_org_repo": "github+microsoft/SMBSecurity", "link_to_repo": "https://github.com/microsoft/SMBSecurity", "platform": "github", "language": "PowerShell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Introduction \nCode supplementing AI for Good Lab's work on long COVID sequelae using EHR data\n\nThis project aimed to discover long COVID sequelae (symptoms) from Electronic Health Records (EHR) data using causal impact analysis of time series and analyze its association with social determinants of health\nThe goal is to understand symptoms of long COVID and how it affects underprivileged communities for a better healthcare. This aligns with AI for Health initiative pillar of Microsoft.\n\n# Getting Started\nPlease use environment.yml file with conda for installing the required python packages\n\n# Notebooks\nCausalImpactLongCOVID: shows the steps to discover long COVID sequelae from aggregrated time series data of symptoms of EHR\n\nDescribeDemographics: shows the analysis of demographics (social determinants of health, SDOH) in association with long COVID\n\nIn addition to the code snippets, the notebooks also contain additional results, such as long COVID sequelae with False Discovery Rate (FDR) adjustment\n\nThis repository accompanies the paper titled \"Using data science and a health equity lens to identify long-COVID sequelae among medically underserved populations.\"\n", "repo_name": "causal-impact-long-covid", "org_name": "microsoft", "org_repo": "microsoft/causal-impact-long-covid", "platform_org_repo": "github+microsoft/causal-impact-long-covid", "link_to_repo": "https://github.com/microsoft/causal-impact-long-covid", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "<p align=\"center\">\n<img src=\".github/selftune_banner.png\" width=500 alt=\"SelfTune: An RL framework to tune configuration parameters\">\n</p>\n\n<p align=\"center\">\n<a href=\"https://github.com/microsoft/SelfTune/blob/main/LICENSE\"><strong>License</strong></a> \u2022\n<a href=\"https://github.com/microsoft/SelfTune/blob/main/SECURITY.md\"><strong>Security</strong></a> \u2022\n<a href=\"https://github.com/microsoft/SelfTune/blob/main/SUPPORT.md\"><strong>Support</strong></a> \u2022\n<a href=\"https://github.com/microsoft/SelfTune/blob/main/CODE_OF_CONDUCT.md\"><strong>Code of Conduct</strong></a>\n</p>\n\n# SelfTune\n\nSelfTune is an RL framework that enables systems and service developers to automatically tune various configuration parameters and other heuristics in their codebase, rather than manually-tweaking, over time in deployment. It provides easy-to-use API (Python, C# bindings) and is driven by bandit-style RL & online gradient-descent algorithms.\n\n## Installation and Usage\n\nRefer to the [python README](python/README.md) and [C# README](c%23/README.md).\n\n## Basic tour of the SelfTune package\nIn this section, we present the syntax and semantics of SelfTune's python bindings. These ideas apply to the C# bindings too. However, do note that some of the features in the python bindings(normalization, step size, optimizers, ...) may not be available in the C# bindings.\n\n### 1. Identifying the reward function\n\nSelfTune's optimization algorithm(e.g., Bluefin) uses a reward to compute a gradient-ascent style update to the parameter values. This reward can be any health or utilization metric of the current state of the system (e.g., throughput, latency, ...).\n\n### 2. Defining the parameters to be tuned\nWe define the parameters of the system to be tuned to optimize the supplied reward. Our implementation currently supports tuning only *numerical* parameters (support for *categorical* parameters will soon be made available). The library allows optional arguments that encode domain knowledge for tuning the parameters:\n* `type` - Type of the parameter. Can be `discrete`(only integer values) or `continuous`.\n* `name` - The name of the parameter. The prediction returned by SelfTune will be a python dict with the name as the key and prediction as the value.\n* `initial_value` - The initial value of the parameter</li>\n* `lb` (optional) - The lower bound value that the parameter can take. (Equivalent to c_min in version 1.0.0)\n* `ub` (optional) - The upper bound value that the parameter can take. (Equivalent to c_max in version 1.0.0)\n* `step_size` (optional) - If a step_size is provided, the parameter moves in steps of the step_size. For example, in the below example, the valid `p2`\nvalues will be `(100.0, 200.0, 300.0, ... 900.0)`.\n\n```python\nimport numpy as np\n\nfrom selftune_core import SelfTune\n\nparameters = (\n    {\n        \"type\": \"discrete\",\n        \"name\": \"p1\",\n        \"initial_value\": 5,\n        \"lb\": 0,\n        \"ub\": 10,\n    },\n    {\n        \"type\": \"continuous\",\n        \"name\": \"p2\",\n        \"initial_value\": 100.0,\n        \"lb\": 100.0,\n        \"ub\": 900.0,\n        \"step_size\": 100.0,\n    },\n)\n```\n\n### 3. Create an instance of SelfTune\nOnce we define the parameters to be tuned, we can create an instance of the parameter learning problem for SelfTune. Below is a description of the model hyperparameters.\n\n- <strong>algorithm</strong> - The optimization algorithm to use. Currently, we only support the `bluefin` algorithm.\n- <strong>parameters</strong> - The parameters to tune.\n- <strong>feedback</strong> - The type of feedback update. The feedback can be either `onepoint` or `twopoint`. `onepoint` is recommended when the reward function changes with time i.e., it is not possible to query the reward function at the same set of parameters twice and expect the same reward. In settings (e.g., simulations) where it is possible to obtain the reward at two different sets of parameters, `twopoint` is prefered since it is more sample-efficient and converges faster.\n\nAlong with the above arguments, the user can also optionally provide\n- <strong>eta</strong> - The learning rate. \n- <strong>delta</strong> - The exploration radius.\n- <strong>optimizer</strong> - The optimizer to use. Can be (\"sgd\", \"rmsprop\").\n- <strong>optimizer_kwargs</strong> - Optimizer specific arguments. For example, for the rmsprop optimizer, optimizer_kwargs can be `{\"alpha\": 0.99, \"momentum\": 0, \"eps\": 1e-8}`\n- <strong>random_seed</strong> - The random seed used to initialize the numpy pseudo-random number generator.\n- <strong>eta_decay_rate</strong> - The decay rate of eta. \n- <strong>normalize</strong> - Specifies whether the parameter values have to be normalized. Uses min-max normalization.\n\n\n```python\nst = SelfTune(\n    algorithm=\"bluefin\",\n    parameters=parameters,\n    algorithm_args=dict(\n        feedback=\"twopoint\",\n        eta=0.01,\n        delta=0.1,\n        random_seed=4,\n    ),\n)\n```\n\nThe user can also modify the value of eta after each round. For example,\n```python\ndef eta_decay(inital_eta, curr_round, decay_rate):\n    return (decay_rate**curr_round)*initial_eta\n\nnum_rounds = 100\ndecay_rate = 0.95\ninitial_eta = 0.01\n\nst = SelfTune(\n    algorithm=\"bluefin\",\n    parameters=parameters,\n    algorithm_args=dict(\n        feedback=\"onepoint\"\n    ),\n)\n\nfor round in range(1, num_rounds):\n    model.eta = eta_decay(initial_eta, round, decay_rate)\n    model.delta = model.eta**0.5\n```\n\nIn cases where both `onepoint` and `twopoint` feedback are applicable, it is recommended to use `twopoint`. `twopoint` provides more stable convergence and more accurate gradient estimates compared to `onepoint`.\n\n<table>\n    <tr>\n        <td>\n            <p align=\"center\">\n                <img src=\".github/onepoint_twopoint_2parameter.png\" alt=\"Onepoint vs Twopoint loss decay for a 2 parameter learning problem\" width=700/>\n            </p>\n        </td>\n        <td>\n            <p align=\"center\">\n                <img src=\".github/onepoint_twopoint_5parameter.png\" alt=\"Onepoint vs Twopoint loss decay for a 5 parameter learning problem\" width=700/></td>\n            </p>\n    </tr>\n</table>\n\n### 4. Predict, Set Reward\nNow that we have set up an instance of SelfTune, we can call `model.predict` to get the current set of parameters. Once the reward is available, it can be sent back to SelfTune using `model.set_reward`. In the plots comparing `onepoint` and `twopoint` feedback, we use the negative squared loss as the reward function.\n\n```python\nnum_rounds = 100\nfor i in range(num_rounds):\n    # Get the current set of parameters\n    pred = st.predict() # pred = {\"p1\": 6, \"p2\": 200.0}\n\n    # Receive feedback\n    reward = black_box_reward(pred)\n\n    # Send the feedback to SelfTune for the gradient update\n    st.set_reward(reward)\n    \n    if i % 5 == 0:\n        print(\n            f'Round={i}, Reward={reward}, Pred=({pred[0]:.4f}, {pred[1]:.4f}), Best=({st.center[0]}, {st.center[1]})'\n        )\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SelfTune", "org_name": "microsoft", "org_repo": "microsoft/SelfTune", "platform_org_repo": "github+microsoft/SelfTune", "link_to_repo": "https://github.com/microsoft/SelfTune", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "## Dense Gradient Tree\nThis repository houses the supporting code for the paper [Learning Accurate Decision Trees with Bandit Feedback via Quantized Gradient Descent](https://arxiv.org/abs/2102.07567). \n\nThe Dense Gradient Tree(DGT) technique supports learning decision trees of a given height for (a) multi-class classification, (b) regression settings, with both (a) standard supervised, and (b) bandit feedback. In the bandit feedback setting, the true loss function is unknown to the learning algorithm; the learner can only query the loss for a given prediction. The goal then is to learn decision trees in an online manner, where at each round the learner maintains a tree model, makes prediction for the presented features, receives a loss, and updates the tree model.\n\n## Setup\n\n1. Install necessary packages\n\nCreate a new conda environment named `dgt_env` with `python==3.6.8`, `pytorch==1.7.0` and install all dependencies inside:\n\n```\n$ conda env create -f dgt_env.yml\n$ conda activate dgt_env\n```\n\n2. Change working directory to `src`:\n\n```\n$ cd src\n```\n\n3. Run the algorithm\n\nTo reproduce some of our results, please run `bash run.sh`.\n- The script by default runs our algorithm with height 6 on `ailerons`. Commands for `abalone`, `satimage`, and `pendigits` are commented out.\n- To change height of the tree learnt, change the argument corresponding to `--height` flag.\n- The `--proc_per_gpu` option denotes how many processes to run per GPU. It defaults to 4 which is ideal for a typical GPU but on a GPU with small memory, reducing it from 4 might be required.\n- The `--num_gpu` option denotes how many GPUs to parallelize over (and assumes device ordinal of GPUs start with 0). It defaults to 1.\n\nNote: For `abalone` dataset we report the final performance across 5 different shuffles.\n\n4. Check Results\n\nFinal scores, i.e. mean test RMSE/Accuracy and standard deviation, can be found in the file `./out/exp@{dataset}_{height}@{start_time}/meanstd-exps/meanstd-run-summary.csv` under the columns `test_acc_mean` and `test_acc_std`.\n\n## Code Contributors\n\n[Ajaykrishna Karthikeyan](https://github.com/ajay0)  \n[Naman Jain](https://github.com/Naman-ntc)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n", "repo_name": "DGT", "org_name": "microsoft", "org_repo": "microsoft/DGT", "platform_org_repo": "github+microsoft/DGT", "link_to_repo": "https://github.com/microsoft/DGT", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Flink on Azure\n\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\nThis repo provides examples of Flink integration with Azure, like Azure Kubernetes, Azure SQL Server, Azure Data Factory, etc.\n\n## Examples\n\nOutline the examples in the repository.\n\n| Example | Description | Pipeline Status |\n|-|-|-|\n| [Flink Streaming Examples](flink-streaming-example) |  Examples for Flink Streaming, including custom source & sink |  |\n| [Flink Stream Batch Unified Examples](flink-stream-batch-unified-example) |  Examples for Flink Stream Batch Unified Connector |  |\n| [Flink History Server](flink-history-server) |  Examples for Flink History Server |  |\n| [Flink CDC SQL Server Examples](flink-cdc-sql-server-example) |  Examples for Flink CDC SQL Server Connector |  |\n| [Flink on Native Azure Kubernetes](flink-on-native-azure-kubernetes) |  Examples for Flink Job on Native Azure Kubernetes |  |\n| [Flink Azure Data Factory Cloud Native Extension](flink-adf-cloud-native-extension) |  Flink Azure Data Factory Cloud Native Extension |  |\n| [Flink Deep Learning Tensorflow](flink-dl-tensorflow) |  Flink Online & Offline Training, Tensorflow Integration |  |\n\n## Prerequisites\n\nBasic:\n\n* [Git](https://www.git-scm.com/downloads)\n* [Java Development Kit (JDK) 1.8](https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html)\n* [Apache Maven](http://maven.apache.org/download.cgi) and [install](http://maven.apache.org/install.html) a Maven binary archive\n* [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\nAzure:\n\n* [Azure Container Registry](https://azure.microsoft.com/en-us/services/container-registry/)\n* [Azure Kubernetes](https://azure.microsoft.com/en-us/services/kubernetes-service/)\n* [Azure SQL Server](https://azure.microsoft.com/en-us/services/sql-database/)\n* [Azure Data Factory](https://azure.microsoft.com/en-us/services/data-factory/)\n* [Azure Data Lake Storage Gen2](https://azure.microsoft.com/en-us/services/storage/data-lake-storage/#overview)\n* [Azure Blob Storage NFS Support](https://learn.microsoft.com/en-us/azure/storage/blobs/network-file-system-protocol-support)\n* [Azure Storage Fuse](https://github.com/Azure/azure-storage-fuse)\n\nFlink:\n\n* [Flink](https://downloads.apache.org/flink)\n\nDeep Learning:\n\n* [Tensorflow](https://www.tensorflow.org/)\n* [Kubeflow](https://www.kubeflow.org/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "flink-on-azure", "org_name": "microsoft", "org_repo": "microsoft/flink-on-azure", "platform_org_repo": "github+microsoft/flink-on-azure", "link_to_repo": "https://github.com/microsoft/flink-on-azure", "platform": "github", "language": "Java", "stargazers_count": 42, "watchers_count": 42}, {"README_text": "# Anyone can Code! \u2013 Light Your Fire for Coding\n\n![FemaleTechGenLogo](./img/MSFT_AnyonecanCode_Banner_small.png)\n\n  <p>\n    <sub>Built with \u2764 by everybody who wants to make the Microsoft App Dev Innovation world a little bit more diverse! </sub>\n  </p>\n\n</div>\n\n<hr>\n\n## [Click here to take a look at the sample app](https://microsoft.github.io/anyonecancode/)\n\n## Agenda for the Anyone can Code workshop\n\n### Day 1\n- Create your own GitHub Account\n- Use this repository as our baseline\n- Use Azure Web Apps to host your own application\n- Play around with the frontend to customize your App and to understand the basic underlying concept of Azure\n- Use Azure Functions to automate your tasks\n- Use storages and databases to store images and data\n\nAll these Azure Services are used behind the scenes and can be consumed by our application aka personal App.\n\n### Day 2\n- Make your existing personal app smart with pre-trained machine learning models. For this we will focus on our managed services like Azure Cognitive Services: Computer Vision API and Speech API.\n- Connect your application with the services. This shows the entire lifecycle and brings together best of both worlds.\n- Trainers will suggest a long list of online courses to get started and to get a deeper dive into the technologies.\n\nFurthermore, all Anyone Can Code graduates are invited to join our [AI Developer College](https://github.com/azuredevcollege/aidevcollege) or\n[Azure Developer College](https://github.com/azuredevcollege/trainingdays).\n\n<div align=\"center\">\n  <p> Female Tech \u2764\ufe0e All Generations</p>\n</div>\n\n<br>\n\nThis workshop consists of multiple challenges, which you have to complete. Your trainers will guide you through the workshop by giving you introduction talks to each of the topics/challenges you have to complete.\n\nHere's the overview of the training week. Happy hacking!\n\n## Hands on Guide\n\n- [Day 1 - GitHub - Create new dreams](instructions/day1/GitHub/README.md)\n- [Day 1 - Application on our Phone ](instructions/day1/Application/README.md)\n- [Day 2 - Make our application recognizer objects in images](instructions/day2/Vision/README.md)\n- [Day 2 - Make our application understand speech](instructions/day2/Speech/README.md)\n\n## Goal of the Anyone can Code Program for women in the ENTIRE ECOSYSTEM over Generations\n\nThe goal is to show all women of any generation, that anything is possible also in the area of tech and to light the fire for coding. We aim to connect all generations of women whether they are pupils, university students or professionals already performing in jobs. Furthermore, we aim to connect those generations which haven't gotten in touch with tech yet since we noticed that there is a huge gap between these communities. We believe there aren't enough female tech heroes yet.\n\nHowever, it is possible to dive into tech at any age. Therefore, we thought of starting with something that we are all familiar with - applications on our phones. Together we will build an app on our phones which can take pictures and which we can talk to. And to put it in other (tech) words: _how to build a progressive web app on our phones which uses pre-trained Machine Learning models as restful endpoints_. The application uses so-called REST APIs (don't worry we will explain this term later) and we will use Azure to meet the challenges to get it running.\n\nTo close the gap, we will first start with getting the application up and running and then secondly integrate the Machine Learning REST APIs. As a place where we can create our tech dreams we will use GitHub to create our space of collaboration as our motto always is: **Sharing is Caring!**\n\n<div align=\"center\">\n  <p></p>\n  <img src=\"./img/microgram-mock.png\" alt=\"Microgram App Mockup\" />\n</div>\n\n## Contributing\n\nThis project welcomes contributions and suggestions.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n[![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)  \nThis work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n", "repo_name": "anyonecancode", "org_name": "microsoft", "org_repo": "microsoft/anyonecancode", "platform_org_repo": "github+microsoft/anyonecancode", "link_to_repo": "https://github.com/microsoft/anyonecancode", "platform": "github", "language": "Vue", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Localization tooling for Visual Studio Code\n\nThis repository contains tooling for localizing Visual Studio Code extensions. Localization for VS Code extension's source code has 4 important parts:\n\n* [`vscode.l10n.t`](#vscodel10nt) - The API for translating strings in your extension's code\n* [`@vscode/l10n-dev`](#vscodel10n-dev) - The tooling used for extracting l10n strings from vscode extensions and working with XLF files\n* [`@vscode/l10n`](#vscodel10n) - The library used for loading the translations into subprocesses of your extension\n* [`package.nls.json`](#packagenlsjson) - The file used for translating static contributions in your extension's `package.json`\n\nAdditionally, for a sample of how to use these tools, see the [l10n-sample](https://github.com/microsoft/vscode-extension-samples/tree/main/l10n-sample) in the vscode-extension-samples repo.\n\n## `vscode.l10n.t`\n\nThis API, introduced in VS Code 1.73, is used for translating strings in your extension's code. It is a part of the main VS Code extension API and is further documented [here](https://code.visualstudio.com/api/references/vscode-api#l10n).\n\n> **Note**\n>\n> Make sure you your VS Code engine and `@types/vscode` version in your extension manifest is at least `^1.73.0`.\n\n## `@vscode/l10n-dev`\n\nTooling used for extracting `l10n` strings from vscode extensions and working with XLF files. See it's dedicated [README](./l10n-dev) for usage instructions.\n\n## `@vscode/l10n`\n\nLibrary used for loading the translations into subprocesses of your extension. See it's dedicated [README](./l10n) for usage instructions.\n\n> **Note**\n>\n> You should _NOT_ use this library in your extension's main process. The translations are loaded into the main process by VS Code itself.\n\n## `package.nls.json`\n\nThis file, along with `package.nls.{locale}.json` files, are used for translating static contributions in your extension's `package.json`. Here's an example:\n\nYour `./package.json`:\n\n```jsonc\n{\n  \"name\": \"my-extension\",\n  \"version\": \"0.0.1\",\n  \"main\": \"./out/extension.js\",\n  \"l10n\": \"./l10n\",\n  //...\n  \"contributes\": {\n    \"commands\": [\n      {\n        \"command\": \"my-extension.helloWorld\",\n        // The key is surrounded by % characters\n        \"title\": \"%my-extension.helloWorld.title%\"\n      }\n    ]\n  }\n}\n```\n\nYour `./package.nls.json`:\n\n```jsonc\n{\n  // That same key from the package.json\n  \"my-extension.helloWorld.title\": \"Hello World\"\n}\n```\n\nYour `./package.nls.de.json`:\n\n```jsonc\n{\n  // That same key from the package.json\n  \"my-extension.helloWorld.title\": \"Hallo Welt\"\n}\n```\n\nVS Code will automatically load the correct `package.nls.{locale}.json` (or `package.nls.json` for English) file based on the locale of the user. If no translation is available for a given key, VS Code will fall back to the English translation.\n\n> **Note**\n>\n> [@vscode/l10n-dev](#vscodel10n-dev) has some tooling around these files (converting them to XLIFF files, generating Pseudo-Localization files, etc.) that you can use.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Build steps\n\nFirst, install all of the dependencies using `npm install`.\n\nIf you plan on working with `l10n-dev` it has one additional step. This package requires building the tree-sitter WASM files for the two grammars that we consume. To do this, you can run the following commands:\n\n```\ncd l10n-dev\nnpm run build-wasm\n```\n\n> **Note**\n>\n> On macOS or Windows, you will need to have Docker running in order to build the WASM files. The CLI runs a linux container to build the WASM files.\n\nIf you've done this correctly, you should see two `.wasm` files in the `l10n-dev` folder.\n\nAt this point you can run the build task in the repo to build in the background and run the tests with `npm test`.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-l10n", "org_name": "microsoft", "org_repo": "microsoft/vscode-l10n", "platform_org_repo": "github+microsoft/vscode-l10n", "link_to_repo": "https://github.com/microsoft/vscode-l10n", "platform": "github", "language": "TypeScript", "stargazers_count": 24, "watchers_count": 24}, {"README_text": "# m365-renovate-config\n\nShared Renovate presets for use in M365 projects.\n\n## Useful links\n\n- [All configuration options](https://docs.renovatebot.com/configuration-options/)\n- [`packageRules` configuration](https://docs.renovatebot.com/configuration-options/#packagerules) for applying custom options to individual packages or groups\n- [Built-in presets](https://docs.renovatebot.com/presets-default/) (see also other pages in the same section)\n- [Preset definitions](https://github.com/renovatebot/renovate/tree/main/lib/config/presets/internal)\n\n## Using presets\n\nThere are a few different ways to reference presets from this repo in your Renovate config:\n\n```jsonc\n{\n  \"extends\": [\n    // Use the default preset\n    \"github>microsoft/m365-renovate-config\",\n\n    // Use a specific preset\n    \"github>microsoft/m365-renovate-config:somePreset\",\n\n    // Use a specific version of a preset\n    \"github>microsoft/m365-renovate-config:somePreset#v2.1.0\",\n\n    // Use a major version of a preset (see note below)\n    \"github>microsoft/m365-renovate-config:somePreset#v2\"\n  ]\n}\n```\n\nNote that **semver ranges are not supported for preset versions** because Renovate only supports resolving presets to specific refs (tags or branches). The supported refs are:\n\n- Major version branches: `v1`, `v2`\n- Tags listed on the [Releases page](https://github.com/microsoft/m365-renovate-config/releases)\n\n## Version 2 breaking changes\n\n`m365-renovate-config` version 2 makes the default preset a bit more opinionated based on testing, streamlines preset naming, and updates settings to better reflect recent improvements in Renovate.\n\n**These changes have been picked up automatically** unless you specified a ref (e.g. `#v1`) as part of the preset names in your `extends` config.\n\nNote: `<m365>` in preset names referenced below is a shorthand for `github>microsoft/m365-renovate-config`. This is just for readability of the readme and will _**not**_ work in actual configs (you must use the full repo prefix).\n\n### Default preset changes\n\nThe default preset (`github>microsoft/m365-renovate-config`) is now a bit more \"opinionated\" and includes most settings that were previously defined in `<m365>:libraryRecommended`. These settings can be disabled either individually or using the `excludePresets` option.\n\nThe dependency version update strategy (`rangeStrategy`) has also changed as described below.\n\n### Major preset changes and deprecations\n\nDeprecated presets still exist for now to avoid immediate breaks in consuming repos, but will be removed in version 3.\n\n- `<m365>:libraryRecommended` is deprecated in favor of this repo's default preset.\n- `<m365>:beachballLibraryRecommended` is renamed to `<m365>:beachball`.\n\n### Dependency version update strategy\n\nPreviously, Renovate's [`config:base`](https://docs.renovatebot.com/presets-config/#configbase) would pin `devDependencies` and possibly also `dependencies` to exact versions. Pinning `dependencies` is usually not desirable for libraries, so `v1` of `m365-renovate-config` omitted any pinning behavior in its default preset, and enabled pinning _only_ `devDependencies` in its `<m365>:libraryRecommended` preset.\n\nA [recent Renovate update](https://docs.renovatebot.com/release-notes-for-major-versions/#version-35) included greatly expanded support for doing in-range updates (e.g. updating the installed version for `\"foo\": \"^1.0.0\"` from `1.1.0` to `1.2.0`) by changing only the lockfile. Therefore, Renovate's default [`rangeStrategy: \"auto\"`](https://docs.renovatebot.com/configuration-options/#rangestrategy) was changed to do lockfile-only updates when possible (instead of pinning or replacing versions), and `config:base` no longer includes any pinning of versions.\n\nSince the lockfile-only updates are likely a good strategy in many cases, `m365-renovate-config`'s default preset (which supersedes `<m365>:libraryRecommended`) has been updated to remove `rangeStrategy` overrides and extend `config:base`.\n\nNotes on pinning behavior:\n\n- For any versions that are currently pinned or that you manually pin, Renovate updates will bump to a new pinned version.\n  - If you'd like to unpin your dev deps, use [`better-deps`](https://www.npmjs.com/package/better-deps): `npx better-deps unpin-dev-deps`\n- If you prefer to restore the previous behavior of pinning _all_ `devDependencies`, extend the Renovate preset [`:pinOnlyDevDependencies`](https://docs.renovatebot.com/presets-default/#pinonlydevdependencies).\n\n## Presets in this repo\n\n<!--\nMost content in this section is generated by scripts/updateReadme.js.\n\nThe preset's primary description should be defined and edited in its .json file.\n\nIn this section, ONLY edit between \"extra content\" marker comments!\n-->\n\n<!-- start presets TOC -->\n\n- [Full config presets](#full-config-presets)\n  - [default](#default)\n  - [beachball](#beachball)\n- [Grouping presets](#grouping-presets)\n  - [groupMore](#groupmore)\n  - [groupD3](#groupd3)\n  - [groupEslint](#groupeslint)\n  - [groupFixtureUpdates](#groupfixtureupdates)\n  - [groupFluent](#groupfluent)\n  - [groupJest](#groupjest)\n  - [groupLageBackfill](#grouplagebackfill)\n  - [groupNodeMajor](#groupnodemajor)\n  - [groupReact](#groupreact)\n  - [groupRollup](#grouprollup)\n  - [groupTypes](#grouptypes)\n  - [groupYargs](#groupyargs)\n- [Compatibility presets](#compatibility-presets)\n  - [disableEsmVersions](#disableesmversions)\n  - [restrictNode](#restrictnodearg0)\n- [Other presets](#other-presets)\n  - [automergeDevLock](#automergedevlock)\n  - [automergeTypes](#automergetypes)\n  - [beachballPostUpgrade](#beachballpostupgrade)\n  - [dependencyDashboardMajor](#dependencydashboardmajor)\n  - [keepFresh](#keepfresh)\n  - [newConfigWarningIssue](#newconfigwarningissue)\n  - [scheduleNoisy](#schedulenoisy)\n  <!-- end presets TOC -->\n\n<!-- start presets -->\n\n### Full config presets\n\n#### `default`\n\nRecommended config which is intended to be appropriate for most projects.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"extends\": [\n    \"config:base\",\n    \"github>microsoft/m365-renovate-config:groupReact\",\n    \"github>microsoft/m365-renovate-config:newConfigWarningIssue\",\n    \"github>microsoft/m365-renovate-config:dependencyDashboardMajor\"\n  ],\n  \"prConcurrentLimit\": 10,\n  \"prHourlyLimit\": 2,\n  \"printConfig\": true,\n  \"timezone\": \"America/Los_Angeles\",\n  \"vulnerabilityAlerts\": {\n    \"enabled\": true\n  },\n  \"packageRules\": [\n    {\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"commitMessageTopic\": \"devDependency {{{depName}}}\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis preset extends Renovate's [`config:base`](https://docs.renovatebot.com/presets-config/#configbase), which enables the following:\n\n- [`:ignoreModulesAndTests`](https://docs.renovatebot.com/presets-default/#ignoremodulesandtests): Ignore packages under `node_modules` or common test/fixture directory names\n- [`:semanticPrefixFixDepsChoreOthers`](https://docs.renovatebot.com/presets-default/#semanticprefixfixdepschoreothers): If the repo uses semantic commits, Renovate will use `fix` for dependencies and `chore` for others\n- [`group:monorepos`](https://docs.renovatebot.com/presets-group/#groupmonorepos): Group known monorepos\n- [`group:recommended`](https://docs.renovatebot.com/presets-group/#grouprecommended): Other known groupings (mostly not relevant for node)\n- [`replacements:all`](https://docs.renovatebot.com/presets-replacements/): Replace renamed packages\n- [`workarounds:all`](https://docs.renovatebot.com/presets-workarounds/#workaroundsall): Workarounds for known problems with packages\n\nExtended presets from this repo:\n\n- [`groupReact`](#groupreact): Group React-related packages and types\n- [`newConfigWarningIssue`](#newconfigwarningissue): Create a new issue every time there's a config warning (not supported for Azure DevOps)\n- [`dependencyDashboardMajor`](#dependencydashboardmajor): Require dependency dashboard approval for major upgrades (not supported for Azure DevOps)\n\nOther settings:\n\n- PR limits (`prHourlyLimit` and `prConcurrentLimit`): Prevent Renovate from creating an overwhelming number of PRs all at once. It's _highly encouraged_ to adjust these in your repo to fit your team's needs!\n- `printConfig`: Log the final resolved config to make debugging easier\n- `timezone`: Run schedules relative to Pacific time, since many M365 repos are based in that time zone. See the [time zone list](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones) for other options.\n- `vulnerabilityAlerts`: Enable PRs to address security vulnerabilities. Note that this **only** works for GitHub and currently is **only** able to update direct dependencies (except in repos using `npm` 6 or older).\n- For `devDependencies`: Use \"devDependencies\" in commit messages (instead of the default \"dependencies\") to be clearer about what is being modified\n<!-- end extra content -->\n\n---\n\n#### `beachball`\n\nRecommended config for library repos which use Beachball for publishing.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"extends\": [\n    \"github>microsoft/m365-renovate-config\",\n    \"github>microsoft/m365-renovate-config:beachballPostUpgrade\"\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis is a full config preset which extends the [default preset](#default) and adds [`beachballPostUpgrade`](#beachballpostupgrade) to generate appropriate change files after upgrades:\n\n- Change type `none` for updating `devDependencies`\n- Change type `patch` for all other changes\n\nThese change types will be correct the majority of the time, but if a different change type is appropriate, you can always edit the change file in the PR before it merges.\n\n<!-- end extra content -->\n\n---\n\n### Grouping presets\n\n#### `groupMore`\n\nApply all the groupings from this repo (except groupTypes).\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"extends\": [\n    \"group:monorepos\",\n    \"group:recommended\",\n    \"github>microsoft/m365-renovate-config:groupD3\",\n    \"github>microsoft/m365-renovate-config:groupEslint\",\n    \"github>microsoft/m365-renovate-config:groupFixtureUpdates\",\n    \"github>microsoft/m365-renovate-config:groupFluent\",\n    \"github>microsoft/m365-renovate-config:groupJest\",\n    \"github>microsoft/m365-renovate-config:groupLageBackfill\",\n    \"github>microsoft/m365-renovate-config:groupNodeMajor\",\n    \"github>microsoft/m365-renovate-config:groupReact\",\n    \"github>microsoft/m365-renovate-config:groupRollup\",\n    \"github>microsoft/m365-renovate-config:groupYargs\"\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nTo use this preset but disable an individual grouping, add its name to the `ignorePresets` array.\n\n<!-- end extra content -->\n\n---\n\n#### `groupD3`\n\nGroup D3 updates.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"D3 packages\",\n      \"matchPackagePrefixes\": [\"d3-\", \"@types/d3-\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n<!-- end extra content -->\n\n---\n\n#### `groupEslint`\n\nGroup and schedule all eslint-related updates.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"eslint packages\",\n      \"matchPackagePatterns\": [\"eslint\"],\n      \"schedule\": [\"before 5am on the 8th and 22nd day of the month\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n<!-- end extra content -->\n\n---\n\n#### `groupFixtureUpdates`\n\nGroup, schedule, and auto-merge all dependency updates in `__fixtures__` sub-folders.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"ignorePresets\": [\":ignoreModulesAndTests\"],\n  \"ignorePaths\": [\n    \"**/node_modules/**\",\n    \"**/bower_components/**\",\n    \"**/vendor/**\",\n    \"**/examples/**\",\n    \"**/__tests__/**\",\n    \"**/test/**\",\n    \"**/tests/**\"\n  ],\n  \"packageRules\": [\n    {\n      \"groupName\": \"fixture dependencies\",\n      \"schedule\": [\"before 5am on the 1st and 15th day of the month\"],\n      \"matchPaths\": [\"**/__fixtures__/**\"],\n      \"matchPackagePatterns\": [\"*\"],\n      \"matchDepTypes\": [\n        \"dependencies\",\n        \"devDependencies\",\n        \"engines\",\n        \"optionalDependencies\",\n        \"overrides\",\n        \"packageManager\",\n        \"peerDependencies\",\n        \"resolutions\"\n      ],\n      \"major\": {\n        \"dependencyDashboardApproval\": false\n      },\n      \"commitMessagePrefix\": \"[fixtures]\",\n      \"commitMessageExtra\": \"\",\n      \"automerge\": true,\n      \"platformAutomerge\": true\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThe motivation for this is to reduce false positive Dependabot vulnerability alerts coming from test fixtures, since [Dependabot has no way to ignore folders](https://github.com/dependabot/dependabot-core/issues/4364). The alerts are likely not meaningful for test fixtures, but dismissing them is extra work, and leaving them sitting creates noise and makes it harder to notice meaningful alerts.\n\nThis preset works by disabling Renovate's [`:ignoreModulesAndTests` preset](https://docs.renovatebot.com/presets-default/#ignoremodulesandtests), ignoring most of the same folders, and then creating a group encompassing all updates to all deps from `package.json` files within the `__fixtures__` folder.\n\nUpdates will occur once daily (to reduce noise) and will only be auto-merged if status checks pass. For updates only affecting fixtures, it should be safe to assume that if the tests using the fixture pass, it should be safe to auto-merge. However, in repos requiring a minimum number of reviews, the PR will still need to be manually reviewed.\n\nNote that this will still make separate PRs for major and non-major updates unless `separateMajorMinor` is set to false. (You can use a custom `packageRules` entry with the same `groupName` to do this.) Vulnerability update PRs may also be created separately and immediately.\n\nTo customize this rule's behavior for individual packages, you can add entries to `packageRules` in your repo. For example:\n\n- Exclude individual packages: `{ \"groupName\": \"fixture dependencies\", \"excludePackageNames\": [\"foo\"] }` (or other [exclusion options](https://docs.renovatebot.com/configuration-options/#excludepackagenames))\n- Limit the allowed versions for a specific package: `{ \"matchPackageNames\": [\"foo\"], \"allowedVersions\": \"<6.0.0 }`\n<!-- end extra content -->\n\n---\n\n#### `groupFluent`\n\nGroup Fluent UI and related package updates.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"Fluent UI React v9 packages\",\n      \"matchPackagePrefixes\": [\"@fluentui/\"],\n      \"matchCurrentVersion\": \">=9.0.0-alpha.0\"\n    },\n    {\n      \"groupName\": \"Fluent UI React v9 packages\",\n      \"matchPackagePrefixes\": [\"@griffel/\"]\n    },\n    {\n      \"groupName\": \"Fluent UI React v8 packages\",\n      \"matchPackagePrefixes\": [\"@fluentui/\"],\n      \"matchCurrentVersion\": \"/^[1234568]\\\\./\",\n      \"excludePackageNames\": [\n        \"@fluentui/eslint-plugin\",\n        \"@fluentui/react-conformance\",\n        \"@fluentui/react-teams\"\n      ]\n    },\n    {\n      \"groupName\": \"Fluent UI React v8 packages\",\n      \"matchPackageNames\": [\"@fluentui/react-cards\"]\n    },\n    {\n      \"groupName\": \"Fluent UI React Northstar packages\",\n      \"matchPackagePrefixes\": [\"@fluentui/\"],\n      \"matchCurrentVersion\": \"0.x\",\n      \"excludePackageNames\": [\n        \"@fluentui/eslint-plugin\",\n        \"@fluentui/react-cards\",\n        \"@fluentui/react-conformance\",\n        \"@fluentui/public-docsite-setup\"\n      ]\n    },\n    {\n      \"groupName\": \"Fabric packages\",\n      \"matchPackageNames\": [\"office-ui-fabric-react\"],\n      \"matchPackagePrefixes\": [\"@uifabric/\"]\n    },\n    {\n      \"groupName\": \"Fabric packages\",\n      \"matchPackageNames\": [\"@fluentui/react\"],\n      \"matchCurrentVersion\": \"^7.0.0\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis config creates the following groups:\n\n- `Fluent UI React v8 packages`: Any packages corresponding to `@fluentui/react` version 8\n- `Fluent UI React v9 packages`: Any packages corresponding to `@fluentui/react-components` version 9\n- `Fluent UI React Northstar packages`: Any packages corresponding to `@fluentui/react-northstar`\n- `Fabric packages`: Any packages corresponding to `office-ui-fabric-react` or `@fluentui/react` 7 or earlier\n\nIf any packages are mis-categorized, please file an issue.\n\n<!-- end extra content -->\n\n---\n\n#### `groupJest`\n\nGroup and schedule jest, ts-jest, jest types, and related packages.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"ignorePresets\": [\n    \"monorepo:jest\",\n    \"group:jestMonorepo\",\n    \"group:jestPlusTypes\",\n    \"group:jestPlusTsJest\"\n  ],\n  \"packageRules\": [\n    {\n      \"groupName\": \"Jest packages\",\n      \"matchSourceUrls\": [\"https://github.com/facebook/jest\"],\n      \"schedule\": [\"before 5am on the 8th and 22nd day of the month\"]\n    },\n    {\n      \"groupName\": \"Jest packages\",\n      \"matchPackagePrefixes\": [\"@types/jest-\", \"jest-\"],\n      \"matchPackageNames\": [\"@types/jest\", \"ts-jest\"],\n      \"schedule\": [\"before 5am on the 8th and 22nd day of the month\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis replaces the built-in presets [`group:jestMonorepo`](https://docs.renovatebot.com/presets-group/#groupjestmonorepo), [`group:jestPlusTSJest`](https://docs.renovatebot.com/presets-group/#groupjestplustsjest), and [`group:jestPlusTypes`](https://docs.renovatebot.com/presets-group/#groupjestplustypes) with a group which catches more related dependencies.\n\n<!-- end extra content -->\n\n---\n\n#### `groupLageBackfill`\n\nGroup Lage and Backfill packages (separate group for each).\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"lage monorepo\",\n      \"matchSourceUrls\": [\"https://github.com/microsoft/lage\"]\n    },\n    {\n      \"groupName\": \"backfill monorepo\",\n      \"matchSourceUrls\": [\"https://github.com/microsoft/backfill\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n<!-- end extra content -->\n\n---\n\n#### `groupNodeMajor`\n\nGroup major updates of Node and its types.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"Node\",\n      \"matchPackageNames\": [\"@types/node\", \"node\", \"nodejs/node\"],\n      \"matchUpdateTypes\": [\"major\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis preset should work for the Node version as defined by `@types/node` dependency, `engines.node` in `package.json`, `.nvmrc`, or `.node-version`.\n\nIt does NOT work for `actions/setup-node` (GitHub workflows) or `NodeTool` (Azure Pipelines). For a workaround for GitHub workflows, see the notes on [`restrictNode`](#restrictnodearg0).\n\n<!-- end extra content -->\n\n---\n\n#### `groupReact`\n\nGroup React packages and types.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"react monorepo\",\n      \"matchPackageNames\": [\n        \"@types/react\",\n        \"@types/react-dom\",\n        \"@types/react-test-renderer\",\n        \"@types/react-is\",\n        \"@types/scheduler\"\n      ]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis uses the same name as (and therefore extends) the built-in config [`group:reactMonorepo`](https://docs.renovatebot.com/presets-group/#groupreactmonorepo).\n\n<!-- end extra content -->\n\n---\n\n#### `groupRollup`\n\nGroup all Rollup-related updates.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"rollup packages\",\n      \"matchPackagePrefixes\": [\"@rollup\"],\n      \"matchPackagePatterns\": [\"^rollup\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n<!-- end extra content -->\n\n---\n\n#### `groupTypes`\n\nGroup minor and patch updates to `@types` `devDependencies`.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"@types devDependencies\",\n      \"schedule\": [\"before 5am on the 1st and 15th day of the month\"],\n      \"matchPackagePrefixes\": [\"@types/\"],\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"matchUpdateTypes\": [\"minor\", \"patch\"],\n      \"matchCurrentVersion\": \">1.0.0\",\n      \"excludePackagePrefixes\": [\"@types/d3-\", \"@types/jest-\"],\n      \"excludePackageNames\": [\n        \"@types/jest\",\n        \"@types/react\",\n        \"@types/react-dom\",\n        \"@types/react-is\",\n        \"@types/react-test-renderer\",\n        \"@types/scheduler\",\n        \"@types/yargs\",\n        \"@types/yargs-parser\"\n      ]\n    },\n    {\n      \"groupName\": \"@types devDependencies\",\n      \"matchPackagePrefixes\": [\"@types/\"],\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"matchUpdateTypes\": [\"patch\"],\n      \"matchCurrentVersion\": \"0.x\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n`@types` packages can update frequently, and used as `devDependencies`, they're generally low risk/effort to update. So this preset groups them together to reduce noise. It also uses an early-morning schedule since updates to `@types` packages may be released throughout the day, and probably at most one PR per day is desired.\n\nThis group excludes updates to `@types` packages with `0.x` versions since those could technically be breaking changes (and to avoid conflicting with the `dependencyDashboardMajor` preset's `0.x` rule). It also excludes things such as `@types/react` which are often included with other groups.\n\nIf you want to exclude a package from this group, add a new `packageRules` entry as follows:\n\n```json\n{\n  \"groupName\": \"@types devDependencies\",\n  \"excludePackageNames\": [\"some-package\"]\n}\n```\n\n<!-- end extra content -->\n\n---\n\n#### `groupYargs`\n\nGroup yargs, yargs-parser, and their types.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"groupName\": \"yargs packages\",\n      \"matchPackageNames\": [\"yargs\", \"yargs-parser\", \"@types/yargs\", \"@types/yargs-parser\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nNote that if you depend on both `yargs` and `yargs-parser`, and they have different major versions, this preset can cause an immortal PR (so you may want to the `ignorePresets` array).\n\n<!-- end extra content -->\n\n---\n\n### Compatibility presets\n\n#### `disableEsmVersions`\n\nDisable upgrades to package versions that have been converted to ES modules.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"matchPackageNames\": [\"p-limit\"],\n      \"allowedVersions\": \"<4.0.0\"\n    },\n    {\n      \"matchPackageNames\": [\"chalk\"],\n      \"allowedVersions\": \"<5.0.0\"\n    },\n    {\n      \"matchPackageNames\": [\"ansi-regex\", \"execa\", \"find-up\", \"pretty-bytes\"],\n      \"allowedVersions\": \"<6.0.0\"\n    },\n    {\n      \"matchPackageNames\": [\"supports-color\"],\n      \"allowedVersions\": \"<9.0.0\"\n    },\n    {\n      \"matchPackageNames\": [\"globby\"],\n      \"allowedVersions\": \"<12.0.0\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nWhile ES modules are the new standard, migrating immediately may not be practical, in particular for libraries whose main consumers can't immediately migrate. This preset is a stopgap to prevent having to verify that every major update does not include an ESM conversion.\n\n<!-- end extra content -->\n\n---\n\n#### `restrictNode(<arg0>)`\n\nRestrict Node version to the range `arg0`.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"matchPackageNames\": [\"@types/node\", \"node\", \"nodejs/node\"],\n      \"allowedVersions\": \"{{arg0}}\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nThis preset should work for the Node version as defined by `@types/node` dependency, `engines.node` in `package.json`, `.nvmrc`, or `.node-version`.\n\nIt does NOT work for `actions/setup-node` (GitHub workflows) or `NodeTool` (Azure Pipelines). To ensure the Node version stays in sync for GitHub actions, it's recommended to either:\n\n- Specify `engines.node` in `package.json` and specify `node-version-file: package.json` in the action, or\n- Create a `.nvmrc` file and specify `node-version-file: .nvmrc` in the action\n<!-- end extra content -->\n\n---\n\n### Other presets\n\n#### `automergeDevLock`\n\nAuto-merge minor and patch updates to `devDependencies` and lock file maintenance (if the build passes).\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"lockFileMaintenance\": {\n    \"automerge\": true,\n    \"platformAutomerge\": true\n  },\n  \"packageRules\": [\n    {\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"matchUpdateTypes\": [\"minor\", \"patch\"],\n      \"automerge\": true,\n      \"platformAutomerge\": true,\n      \"internalChecksFilter\": \"strict\",\n      \"excludePackageNames\": [\"typescript\"],\n      \"minimumReleaseAge\": \"2 days\"\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nAny branch policies will be respected, including required status checks and required reviewers. If you have a required reviewers policy, this will prevent the PRs from merging in an entirely automated manner.\n\nExperimentally, this preset has `stabilityDays` set to 2 to reduce the chance of automerging malicious dependencies. You can force creation of the PR earlier using the dependency dashboard.\n\n<!-- end extra content -->\n\n---\n\n#### `automergeTypes`\n\nAuto-merge minor and patch updates to `@types` `devDependencies` (if the build passes).\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"matchPackagePrefixes\": [\"@types/\"],\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"matchUpdateTypes\": [\"minor\", \"patch\"],\n      \"excludePackageNames\": [\"@types/react\", \"@types/react-dom\"],\n      \"automerge\": true,\n      \"platformAutomerge\": true\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nAny branch policies will be respected, including required status checks and required reviewers. If you have a required reviewers policy, this will prevent the PRs from merging in an entirely automated manner.\n\n<!-- end extra content -->\n\n---\n\n#### `beachballPostUpgrade`\n\nRun `beachball change` as a post-upgrade task.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"gitAuthor\": \"Renovate Bot <renovate@whitesourcesoftware.com>\",\n  \"postUpgradeTasks\": {\n    \"commands\": [\n      \"git add --all\",\n      \"npx beachball change --no-fetch --no-commit --type patch --message '{{{commitMessage}}}'\",\n      \"git reset\"\n    ],\n    \"fileFilters\": [\"**/*\"],\n    \"executionMode\": \"branch\"\n  },\n  \"lockFileMaintenance\": {\n    \"postUpgradeTasks\": {\n      \"commands\": [\n        \"git add --all\",\n        \"npx beachball change --no-fetch --no-commit --type none --message '{{{commitMessage}}}'\",\n        \"git reset\"\n      ],\n      \"fileFilters\": [\"**/*\"],\n      \"executionMode\": \"branch\"\n    }\n  },\n  \"packageRules\": [\n    {\n      \"matchDepTypes\": [\"devDependencies\"],\n      \"postUpgradeTasks\": {\n        \"commands\": [\n          \"git add --all\",\n          \"npx beachball change --no-fetch --no-commit --type none --message '{{{commitMessage}}}'\",\n          \"git reset\"\n        ],\n        \"fileFilters\": [\"**/*\"],\n        \"executionMode\": \"branch\"\n      }\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nGenerate appropriate change files after upgrades:\n\n- Change type `none` for updating `devDependencies` or `lockFileMaintenance`\n- Change type `patch` for all other changes\n\nThese change types will be correct the majority of the time, but if a different change type is appropriate, you can always edit the change file in the PR before it merges.\n\nNote that in the GitHub app, commands in [`postUpgradeTasks`](https://docs.renovatebot.com/configuration-options/#postupgradetasks) are limited to a specific set of strings for security reasons. As of writing, only the specific commands used in this config are allowed (though `--no-fetch` can optionally be removed).\n\n<!-- end extra content -->\n\n---\n\n#### `dependencyDashboardMajor`\n\nRequire dependency dashboard approval for major upgrades, 0.x upgrades, and minor upgrades of deps known not to follow semver.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"dependencyDashboard\": true,\n  \"major\": {\n    \"dependencyDashboardApproval\": true\n  },\n  \"packageRules\": [\n    {\n      \"matchPackageNames\": [\"go\", \"typescript\"],\n      \"matchUpdateTypes\": [\"minor\"],\n      \"dependencyDashboardApproval\": true\n    },\n    {\n      \"matchCurrentVersion\": \">=0.1.0 <1.0.0-0\",\n      \"matchUpdateTypes\": [\"minor\"],\n      \"dependencyDashboardApproval\": true\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nNote: The dependency dashboard feature doesn't work in Azure DevOps as of writing due to [lack of issue creation support](https://github.com/renovatebot/renovate/issues/9592) in Renovate (and lack of markdown checkbox support in Azure DevOps).\n\nMajor upgrades of certain dependencies may be disruptive or require extra validation, so to avoid the PRs sitting for a long time, it may be desirable to manually approve upgrades.\n\nThis policy is also applied for certain _minor_ upgrades that may contain breaking changes:\n\n- Packages with `0.x` versions (allowed per semver)\n- Packages that don't respect major/minor semver: so far, `typescript` and `go`\n\nThe downside of setting this policy for all major upgrades is that it reduces the visibility of available upgrades, if nobody is checking the dashboard regularly.\n\nSome alternative strategies which would need to be configured per repo (see [Renovate docs](https://docs.renovatebot.com/configuration-options/#dependencydashboardapproval) for examples):\n\n- Create a `packageRules` group which requires dependency dashboard approval for only major upgrades of specific packages that are known to be high risk/effort.\n- Set [schedules](https://docs.renovatebot.com/configuration-options/#schedule) for individual `packageRules` groups to avoid the upgrades being forgotten.\n<!-- end extra content -->\n\n---\n\n#### `keepFresh`\n\nKeep locally-used dependency versions deduplicated and updated.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"lockFileMaintenance\": {\n    \"enabled\": true,\n    \"rebaseWhen\": \"behind-base-branch\",\n    \"schedule\": [\"before 5am on the 1st and 15th day of the month\"]\n  },\n  \"postUpdateOptions\": [\"yarnDedupeFewer\", \"npmDedupe\"]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\n- [`lockFileMaintenance`](https://docs.renovatebot.com/configuration-options/#lockfilemaintenance): Completely re-create lock files twice a month. This will update direct and indirect dependency versions used _only within the repo_ to the latest versions that satisfy semver.\n  - [`rebaseWhen`](https://docs.renovatebot.com/configuration-options/#rebasewhen): If the lock file maintenance PR gets out of date, rebase it even if there aren't conflicts.\n- [`postUpdateOptions`](https://docs.renovatebot.com/configuration-options/#postupdateoptions):\n  - `yarnDedupeFewer`: If using yarn, run `yarn-deduplicate --strategy fewer` after updates.\n  - `npmDedupe`: If using npm, run `npm dedupe` after updates. WARNING: This may slow down Renovate runs significantly.\n\nIt's **highly recommended** to manually run the deduplication command before enabling this preset. In a large repo that hasn't been regularly deduplicated (or had its lock file refreshed), it's likely that initial deduplication will cause build breaks due to implicit reliance on subtle interactions between particular old versions.\n\n<!-- end extra content -->\n\n---\n\n#### `newConfigWarningIssue`\n\nAlways create a new issue if there's a config problem (for visibility).\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"configWarningReuseIssue\": false\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nNote that issue creation is not supported in Azure DevOps as of writing.\n\n<!-- end extra content -->\n\n---\n\n#### `scheduleNoisy`\n\nUpdate \"noisy\" (frequently-updating) packages once every other week.\n\n<details><summary><b>Show config JSON</b></summary>\n\n```json\n{\n  \"packageRules\": [\n    {\n      \"matchPackagePrefixes\": [\"@microsoft/api-extractor\"],\n      \"schedule\": [\"before 5am on the 8th and 22nd day of the month\"]\n    }\n  ]\n}\n```\n\n</details>\n\n<!-- start extra content (EDITABLE between these comments) -->\n\nCertain packages tend to publish updates multiple times per week, and getting a PR for every one of those updates can be annoying. This rule includes a list of packages known to update frequently and spreads them out throughout the week (to avoid attempting to create too many PRs at once and being rate limited).\n\n<!-- end extra content -->\n\n---\n\n<!-- end presets -->\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "m365-renovate-config", "org_name": "microsoft", "org_repo": "microsoft/m365-renovate-config", "platform_org_repo": "github+microsoft/m365-renovate-config", "link_to_repo": "https://github.com/microsoft/m365-renovate-config", "platform": "github", "language": "JavaScript", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# vscode-azure-functions-remote-web-extension README\n\nThis is the README for your extension \"vscode-azure-functions-remote-web-extension\". After writing up a brief description, we recommend including the following sections.\n\n## Features\n\nDescribe specific features of your extension including screenshots of your extension in action. Image paths are relative to this README file.\n\nFor example if there is an image subfolder under your extension project workspace:\n\n\\!\\[feature X\\]\\(images/feature-x.png\\)\n\n> Tip: Many popular extensions utilize animations. This is an excellent way to show off your extension! We recommend short, focused animations that are easy to follow.\n\n## Requirements\n\nIf you have any requirements or dependencies, add a section describing those and how to install and configure them.\n\n## Extension Settings\n\nInclude if your extension adds any VS Code settings through the `contributes.configuration` extension point.\n\nFor example:\n\nThis extension contributes the following settings:\n\n* `myExtension.enable`: enable/disable this extension\n* `myExtension.thing`: set to `blah` to do something\n\n## Known Issues\n\nCalling out known issues can help limit users opening duplicate issues against your extension.\n\n## Release Notes\n\nUsers appreciate release notes as you update your extension.\n\n### 1.0.0\n\nInitial release of ...\n\n### 1.0.1\n\nFixed issue #.\n\n### 1.1.0\n\nAdded features X, Y, and Z.\n\n-----------------------------------------------------------------------------------------------------------\n\n## Working with Markdown\n\n**Note:** You can author your README using Visual Studio Code.  Here are some useful editor keyboard shortcuts:\n\n* Split the editor (`Cmd+\\` on macOS or `Ctrl+\\` on Windows and Linux)\n* Toggle preview (`Shift+CMD+V` on macOS or `Shift+Ctrl+V` on Windows and Linux)\n* Press `Ctrl+Space` (Windows, Linux) or `Cmd+Space` (macOS) to see a list of Markdown snippets\n\n### For more information\n\n* [Visual Studio Code's Markdown Support](http://code.visualstudio.com/docs/languages/markdown)\n* [Markdown Syntax Reference](https://help.github.com/articles/markdown-basics/)\n\n**Enjoy!**\n", "repo_name": "vscode-azure-functions-remote-web-extension", "org_name": "microsoft", "org_repo": "microsoft/vscode-azure-functions-remote-web-extension", "platform_org_repo": "github+microsoft/vscode-azure-functions-remote-web-extension", "link_to_repo": "https://github.com/microsoft/vscode-azure-functions-remote-web-extension", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# AGIC Installation assistant\nThis is a PowerShell script that will guide you through the installation of the AGIC controller for AKS (Brownfield deployment only)\n\nInstalling the AGIC Controller, specially using Helm, can be hard work! The AKS AGIC Installation assistant focuses on expediting customers onboarding of Azure Kubernetes Service Ingres Controller based on the Application Gateway.\n\nThis project unifies guidance provided by the AGIC controller deployment guide here: https://azure.github.io/application-gateway-kubernetes-ingress/setup/install-existing/\n\n## About the Project\nKeep in mind that this is a work-in-progress, I will continue to contribute to it when I can.\n\nAll constructive feedback is welcomed \ud83d\ude4f\n\n## Getting Started\n\n### Prerequisites\nThe AGIC installation assistant assumes you already have the following tools and infrastructure installed:\n- AKS with Advanced Networking enabled (CNI). The script has been tested only with CNI networking configuration and it and will not let you continue if you have Kubenet.\n- App Gateway v2 in the same virtual network as AKS (App Gateway in different VNET has not been tested using this script yet).\n- AAD Pod Identity installed on your AKS cluster. The assitant will check this and enable it.\n- The script will create a User Mangaged Identity and it requires the ability to do so in your selected AD Tenant.\n- Make sure your Azure Cloud Shell has the below elements installed:\n  -   Az CLI,\n  -   kubectl,\n  -   Helm 3.0\n\n\nBefore running the powershell script, we recommend that you peform the following actions:\n  - This is a PowerShell script. It will only run on the Azure Cloud Powershell shell:\n![image](https://user-images.githubusercontent.com/41587804/185116965-95541326-1cc0-4527-9d88-20f3060152ec.png)\n\n  - Az Login with a user that has Owner/Contributor rights on the subscription/RG of the AKS cluster and App Gateway\n  - Although the powershell script will get credentials to your cluster, it is recommended that you do it before (just to make sure you are on the right context!)\n  - Run Kubectl get pods -A this will make sure you have access to your cluster.\n\n\n### IMPORTANT! Please backup your App Gateway's configuration before installing AGIC:\nThis script will ask you if you want to export (ARM) your application gateway configuration. Still we recommend you to export the configuration manually:\n1. Using Azure Portal navigate to your App Gateway instance\n2. From Export template click Download\n\n### VERY IMPORTANT!\nIf you are facing the Identity not found issue, that is documented here:\nhttps://azure.github.io/application-gateway-kubernetes-ingress/troubleshootings/troubleshooting-agic-addon-identity-not-found/, this program offers you an option to fix this problem (OPTION 3 IN THE MENU). However, It is recommended that you open a support case with Microsoft as changing the VM scalesets directly can leave your cluster as non-supported.\nPerforming this action, without the approval of the Microsoft Support team, is not supported.\n\n\n### Basic\nIf this is the first time you're using the project, follow these steps:\n\n1. Download the agicInstaller.ps1 file from this repo (or clone the repo)\n2. Upload the agicInstaller.ps1 file to the Azure Powershell Cloud Shell:\n\n![image](https://user-images.githubusercontent.com/41587804/185117995-09237cb4-a2fa-43b0-bef1-fdc8fe82f80d.png)\n\n3. Execute the ./agicInstaller.ps1\n4. The assistant will guide your through the rest of the process\n\nThis is how the main menu looks like:\n\n![image](https://user-images.githubusercontent.com/41587804/185781425-8c0bda8e-13a5-48a7-b06f-c9ebeb56a2c1.png)\n\n\n### Troubleshooting\nPlease, refer to  : https://azure.github.io/application-gateway-kubernetes-ingress/troubleshootings/ if you find issues once the AGIC Controller is installed.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AGIC-Installer-assistant", "org_name": "microsoft", "org_repo": "microsoft/AGIC-Installer-assistant", "platform_org_repo": "github+microsoft/AGIC-Installer-assistant", "link_to_repo": "https://github.com/microsoft/AGIC-Installer-assistant", "platform": "github", "language": "PowerShell", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "![image](https://user-images.githubusercontent.com/64599697/191536989-b798cdc8-6c96-4e8c-88b2-5422c033882c.png)\n\n# Large-Scale Route Optimization Accelerator\n\nThis accelerator provides the code template for solving large-scale route optimization problems. The accelerator implements a scalable optimization approach that partitions a large optimization problem into smaller problems, and solves those reduced optimization problems in parallel by leveraging Azure Machine Learning. The solutions to the smaller problems are then consolidated to form a feasible solution to the original optimization problem. A real-world route optimization scenario is used as an example to demonstrate the use of the accelerator.\n\n![image](https://user-images.githubusercontent.com/64599697/191935065-15316f45-5905-4c24-a533-658c610f8c48.png)\n\n\n\n<!-- ## Challenges for Optimization Application\n\nThere are some common challenges for creating a production-grade optimization application:\n1. Most optimization problems are [NP-hard](https://en.wikipedia.org/wiki/NP-hardness) (route optimization falls into this category). When the scale of the problem becomes large, it is impossible to find any good solution in a reasonable time.\n2. The constraints in an optimization problem may change over time as the customer's business evolves. This creates the burden for maintaining the optimization application. \n3. Deploying the optimization application in a way that can be easy to consume by other applications is also crucial in practice.\n\nTo tackle challenge 1 and 3, in this accelerator, we will demonstrate an optimization framework using Azure ML that applies partitioning strategy to partition the large-scale route optimization problem into many smaller ones and then solve them individually. This is a practical way to solve any real-world large-scale optimization problem. We will also leverage Azure ML to deploy the optimization application as a REST API such that it can be easy to consume by other applications. \n\nA pure rule-based optimization application is difficult to maintain as the constraints of the problem change. A better way to tackle challenge 2 is to leverage [optimization solver](https://en.wikipedia.org/wiki/List_of_optimization_software) to model the problem and then let the solver search the solution automatically. There are many optimization techniques, e.g., Linear Programming (LP), Mixed Integer Programming (MIP), [Constraint Programming](https://en.wikipedia.org/wiki/Constraint_programming), etc. One can choose the best fit for their own problem since the framework introduced in this accelerator is optimization technique agnostic. In this accelerator, we demonstrate how to use Constraint Programming to model the route optimization problem.   \n\nComparing to mathematical optimization techniques (e.g., LP, MIP), Constraint Programming is more expressive since it allows us to express a larger collection of problems. For example, a constraint programming model has no limitation on the arithmetic constraints that can be set on decision variables, while a mathematical programming engine is specific to a class of problems whose formulation satisfies certain mathematical properties (for example: quadratic, MIQCP, and convex vs non-convex).\n\nBesides, Constraint programming is also an efficient approach to solve and optimize problems that are too irregular for mathematical optimization. This includes time tabling problems, sequencing problems, and allocation or rostering problems.\n\nThe reasons for these irregularities that make the problem difficult to solve for mathematical optimization can be:\n* Constraints that are nonlinear in nature\n* A non convex solution space that contains many locally optimal solutions\n* Multiple disjunctions, which result in poor information returned by a linear relaxation of the problem\n\nTo who is interested in the detailed comparison, one can refer to this [link](https://www.ibm.com/docs/en/icos/12.8.0.0?topic=overview-constraint-programming-versus-mathematical-programming).  -->\n\n## Route Optimization - A Real World Scenario\n\nThe example demonstrated in this solution accelerator is inspired by a real-world optimization scenario. The customer is a manufacturing company. They have warehouses in different locations. When they receive orders from their clients, a planner need to plan the item-to-truck assignment for order delivery. The planner also need to decide the route of each delivery truck, namely, the sequence of the stops to deliver orders to different destinations. A delivery assignment has its associated cost determined by the type of the assigned delivery truck and the corresponding travelling distance. The optimization objective here is to minimize the overall delivery cost. \n\nThis is a variant of the [vehicle routing problem (VRP)](https://en.wikipedia.org/wiki/Vehicle_routing_problem). The constraints modeled in our example are:\n1. There are different types of trucks we can choose from. A truck has capacity limit on both area and weight. (We assume that there is no limit on the number of trucks for each type)\n2. An item is only available by a specific time. A truck can start only when all items assigned to it are available.\n3. The available time difference between the earliest and last available items in the same truck should be less than a user defined limit (e.g., 4 hours).  \n4. All items need to be delivered to their destinations before their deadlines.\n5. Depending on the properties of products, some items can put in the same truck, but some cannot.\n6. A truck can have at most N stops, where N is a user defined number.\n7. A truck need to stay at each stop for M hours to unload the items, where M is a user defined number. Each stop will incur a fixed amount of cost in addition to the delivery cost. \n\n### Example Input\n\nBelow shows an example input of the route optimization problem. It is a set of items to be delivered, where the Item_ID uniquely defines an item.\n | Order_ID | Material_ID | Item_ID | Source | Destination | Available_Time | Deadline | Danger_Type | Area (m^2) | Weight (kg) |\n | ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- | --------------| --------------|\n | A140109 | B-6128 | P01-79c46a02-e12f-41c4-9ec9-25e48597ebfe | City_61 | City_54 | 2022-04-05 23:59:59 | 2022-04-11 23:59:59 | type_1 | 3.888 | 3092 | \n | A140112 | B-6128 | P01-84ac394c-9f34-48e7-bd15-76f92120b624 | City_61 | City_54 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_1 | 3.888 | 3092 | \n | A140112 | B-6128 | P01-b70c94db-630a-497b-bb63-b0ad86a7dce6 | City_61 | City_54 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_1 | 3.888 | 3092 | \n<!--  | A140112 | B-6128 | P01-4534a7e8-6d73-4a2e-8363-a6645d9bc345 | City_61 | City_54 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_1 | 38880 | 30920000 | \n | A140112 | B-6128 | P01-7208eb61-2cc1-4e7c-b698-e1ab2327b658 | City_61 | City_54 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_1 | 38880 | 30920000 | \n | A190223 | B-6155 | nan-4ac2f30e-bc0a-4415-8612-a6b38d833317 | City_61 | City_53 | 2022-04-06 23:59:59 | 2022-04-12 23:59:59 | type_2 | 9840 | 7640000 | \n | A190225 | B-6155 | nan-5ae70ea9-a28e-4107-b267-5a6c84d4a3c7 | City_61 | City_53 | 2022-04-05 23:59:59 | 2022-04-11 23:59:59 | type_2 | 9840 | 7640000 | \n | A190226 | B-6155 | nan-c9658637-b5f1-433d-885e-b3008612a73d | City_61 | City_53 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_2 | 9840 | 7640000 | \n | A190226 | B-6155 | nan-75768ff3-3dde-4952-9aa0-594c373421d1 | City_61 | City_53 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_2 | 9840 | 7640000 | \n | A190226 | B-6155 | nan-39cdd29b-baee-4ed6-bec0-33227cc8608d | City_61 | City_53 | 2022-04-07 23:59:59 | 2022-04-13 23:59:59 | type_2 | 9840 | 7640000 |  -->\n\n \nAlso assume there are 3 types of trucks available for assignment:\n | Truck Type (length in m) | Inner Size (m^2) | Weight Capacity (kg) | Cost Per KM | Speed (km/h) |\n | ----------- | ----------- | --------------|----------- |----------- |\n | 16.5 | 16.1x2.5 | 10000 | 3 | 40 |  \n | 12.5 | 12.1x2.5 | 5000 | 2 | 40 | \n | 9.6 | 9.1x2.3 | 2000 | 1 | 40 | \n \n ### Example Output\n\nBelow is an example output of the route assignment, where Truck_ID uniquely defines a truck. The column Shared_Truck indicates if there are items from different orders sharing the same truck.\n| Truck_ID  |  Truck_Route  |  Order_ID  |  Material_ID  |  Item_ID  |  Danger_Type  |  Source  |  Destination  |  Start_Time  |  Arrival_Time  |  Deadline  |  Shared_Truck  |  Truck_Type  | \n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- | --------------| --------------| ----------- | --------------| --------------|\n| d27e70e3-e143-4419-8c4a-2faf130e29b3  |  City_61->City_54  |  A140109  |  B-6128  |  P01-79c46a02-e12f-41c4-9ec9-25e48597ebfe  |  type_1  |  City_61  |  City_54  |  2022-04-05 23:59:59  |  2022-04-08 13:11:46  |  2022-04-11 23:59:59  |  N  |  9.6  | \n| 7fb70614-64c5-4d40-a8a2-2f6e39205a67  |  City_61->City_54  |  A140112  |  B-6128  |  P01-84ac394c-9f34-48e7-bd15-76f92120b624  |  type_1  |  City_61  |  City_54  |  2022-04-07 23:59:59  |  2022-04-10 13:11:46  |  2022-04-13 23:59:59  |  N  |  9.6  | \n| 7fb70614-64c5-4d40-a8a2-2f6e39205a67  |  City_61->City_54  |  A140112  |  B-6128  |  P01-b70c94db-630a-497b-bb63-b0ad86a7dce6  |  type_1  |  City_61  |  City_54  |  2022-04-07 23:59:59  |  2022-04-10 13:11:46  |  2022-04-13 23:59:59  |  N  |  9.6  | \n\n\n\n<!-- With the help of Constraint Programming, we can model all these constraints programmatically. There are a lot of CP solvers we can pick. In this accelerator, we use [Google OR-Tools](https://developers.google.com/optimization). It is open-sourced and its performance [surpasses many other solvers](https://www.minizinc.org/challenge2022/results2022.html). To learn about how to model a problem using CP in OR-Tools, one can refer to its [API documents](https://developers.google.com/optimization/reference/python/sat/python/cp_model). -->\n\n# Solution Design\n\nThe key idea of this accelerator is to implement a general framework (illustrated by the below figure) for solving large-scale route optimization problems. The end-to-end pipeline is implemented using Azure ML pipeline consisting of 4 key steps. The complete definition of the pipeline can be found in this [notebook](./notebook/aml_pipeline.ipynb).\n\n![image](docs/media/pipeline.png)\n\nWe use the following simplified example to illustrate the above steps.\nAssume we have a set of orders as below, where we group same type (namely, having same Material_ID) of items from the same order as a single record for ease of discussion.\n\n| Order_ID | Material_ID | Number_of_items | Weight_Per_Item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 1 | A | 8 | 2t | S1 | D1 | 2022-08-01 7AM | 2022-08-02 |\n| 2 | B | 15 | 1t | S1 | D2 | 2022-08-01 9AM | 2022-08-03 |\n| 3 | C | 18 | 1t | S1 | D3 | 2022-08-01 10AM | 2022-08-04 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n| 300 | AA | 33 | 1t | S1 | D1 | 2022-08-02 10AM | 2022-08-04 |\n\n## Step 1: Reduce the Search Space\n\nFor large-scale route optimization problems, it is often possible to apply human heuristics to pre-assign a subset of the items, effectively reducing the problem size. In some cases, we can easily find an optimal/near-optimal assignment based on simple heuristics. For example, in the above route optimization scenario, there are different types of trucks we can choose from. Among them, the biggest truck (i.e., the 10t one) is most cost effective. A simple heuristic is to fill up the biggest truck with items of the same destination. This assignment incurs the lowest delivery cost for those items. \n\nFor example, we can apply the above heuristic to our original input and obtain the following two outcomes:\n* a partial result that contains the heuristic assignment:\n\n| Schedule_ID | Order_ID | Material_ID | Number_of_Items | Source | Destination | Truck_Type |\n| --------------|----------- | ----------- | --------------| ----------- | ----------- | ----------- |\n| 1 | 1 | A | 5 | S1 | D1 | 10t |\n| 2 | 2 | B | 10 | S1 | D2 | 10t |\n| 3 | 3 | C | 10 | S2 | D3 | 10t |\n| ... | ... | ... | ... | ... | ... | ... |\n| 100 | 300 | AA | 10 | S1 | D1 | 10t |\n| 101 | 300 | AA | 10 | S1 | D1 | 10t |\n| 102 | 300 | AA | 10 | S1 | D1 | 10t |\n\n* the remaining unassigned items as the input for the partition step (you may compare it with the original input of the reduce step):\n\n| Order_ID | Material_ID | Number_of_Items | Weight_Per_Item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 1 | A | 3 | 2t | S1 | D1 | 2022-08-01 7AM | 2022-08-02 |\n| 2 | B | 5 | 1t | S1 | D2 | 2022-08-01 9AM | 2022-08-03 |\n| 3 | C | 8 | 1t | S1 | D3 | 2022-08-01 10AM | 2022-08-04 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n| 300 | AA | 3 | 1t | S1 | D1 | 2022-08-02 10AM | 2022-08-04 |\n\n## Step 2: Partition the Problem\n\nGiven the reduced problem from step 1, we can apply different partition strategies to further reduce the problem space. The objective is to ensure each single partition is small enough to solve within a user defined time limit. In an ideal case, the chosen partitioning strategy should not change the optimum of the original problem. Using the above route optimization problem as an example, partitioning the items by the delivery source as below will not change the optimum of the original problem: \n\n* items starting from Source S1:\n\n| Order_ID | Material_ID | Number_of_Items | Weight_Per_item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 1 | A | 3 | 2t | S1 | D1 | 2022-08-01 7AM | 2022-08-02 |\n| 2 | B | 5 | 1t | S1 | D2 | 2022-08-01 9AM | 2022-08-03 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n| 300 | AA | 3 | 1t | S1 | D1 | 2022-08-02 10AM | 2022-08-04 |\n\n* items starting from Source S2:\n\n| Order_ID | Material_ID | Number_of_Items | Weight_Per_Item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 3 | C | 8 | 1t | S1 | D3 | 2022-08-01 10AM | 2022-08-04 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n\nIn the case where there are a lot of items from the same source, you may need to further partition those items such that individual problems can be solved within a given time limit. The optimality may not be guaranteed in this case. \n\nFor example, we can further partition items from source S1 by Available_Time. The intuition is that the business constraint #3 of our problem restricts the time span of all the items in the same truck. So, grouping items with similar available time will be easier to satisfy this constraint. Below are two example partitions to illustrate the idea.\n\n* Orders that are available on 2022-08-01:\n\n| Order_ID | Material_ID | Number_of_Items | Weight_Per_Item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 1 | A | 3 | 2t | S1 | D1 | 2022-08-01 7AM | 2022-08-02 |\n| 2 | B | 5 | 1t | S1 | D2 | 2022-08-01 9AM | 2022-08-03 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n\n* Orders that are available on 2022-08-02:\n\n| Order_ID | Material_ID | Number_of_Items | Weight_Per_Item | Source | Destination | Available_Time | Deadline |\n| ----------- | ----------- | --------------|----------- | ----------- | --------------| ----------- | ----------- |\n| 300 | AA | 3 | 1t | S1 | D1 | 2022-08-02 10AM | 2022-08-04 |\n| ... | ... | ... | ... | ... | ... | ... | ... |\n\n## Step 3: Solve the Smaller Problem\n\nThis step is achieved by the ParallelRunStep function provided by Azure Machine Learning. The ParallelRunStep function can be configured to solve partitioned optimization problems in parallel with a chosen optimization solver.\n\nThere are many optimization techniques, e.g., Linear Programming (LP), Mixed Integer Programming (MIP), [Constraint Programming](https://en.wikipedia.org/wiki/Constraint_programming), etc. that one can use to solve the optimization problems. The framework introduced in this accelerator is solver-agnostic. In this accelerator, we demonstrate the use of Constraint Programming for route optimization.\n\nComparing to mathematical optimization techniques (e.g., LP, MIP), Constraint Programming is more expressive in that it can be used to model optimization problems in forms of arbitrary constraints; for more details see [this article](https://www.ibm.com/docs/en/icos/12.8.0.0?topic=overview-constraint-programming-versus-mathematical-programming).\n## Step 4: Merge the Results\n\nOnce all the smaller problems are solved, we can merge their corresponding solutions with the partial result produced in step 1 to form a feasible solution to the original optimization problem. Assume we have solved two smaller problems with their individual results as follows:\n\n* Result for partition 1:\n\n| Schedule_ID | Order_ID | Material_ID | Number_of_Items | Source | Destination | Truck_Type |\n| --------------|----------- | ----------- | --------------| ----------- | ----------- | ----------- |\n| 103 | 1 | A | 3 | S1 | D1 | 10t |\n| 104 | 2 | B | 5 | S1 | D2 | 5t |\n| ... | ... | ... | ... | ... | ... | ... |\n\n* Result for partition 2:\n\n| Schedule_ID | Order_ID | Material_ID | Number_of_Items | Source | Destination | Truck_Type |\n| --------------|----------- | ----------- | --------------| ----------- | ----------- | ----------- |\n| 201 | 300 | AA | 3 | S1 | D1 | 5t |\n| ... | ... | ... | ... | ... | ... | ... |\n\nWe can simply concatenate the above results to form the final solution:\n| Schedule_ID | Order_ID | Material_ID | Number_of_Items | Source | Destination | Truck_Type |\n| --------------|----------- | ----------- | --------------| ----------- | ----------- | ----------- |\n| 103 | 1 | A | 3 | S1 | D1 | 10t |\n| 104 | 2 | B | 5 | S1 | D2 | 5t |\n| ... | ... | ... | ... | ... | ... | ... |\n| 201 | 300 | AA | 3 | S1 | D1 | 5t |\n| ... | ... | ... | ... | ... | ... | ... |\n     \n\n<!-- 1. **Reduce Search Space**: Given the problem space is huge, it could be a good idea to adopt some human heuristics to assign part of the items first. There two reasons: (1) For a large-scale problem, it could end up with a lot of partitions after the second step, which means we need to launch many machines to parallel the job and it will cost a lot of money; (2) For some special cases, we may easily find an optimal/near-optimal assignment based on some simple heuristics. For example, in our route optimization scenario, there are different kind of trucks we can choose from. Among them, the biggest truck is the most cost efficient. A simple heuristic is to fill up the biggest truck by items having same destination. This heuristic gives us the lowest delivery cost for those items. After applying this heuristic, we will have (i) a partial result that contains the heuristic assignment, (ii) the remaining unassigned items as the input for the partition step.\n2. **Partition Problem**: Given the reduced problem from step 1, we can apply different partition strategies to cut down the problem space. The objective here is to make sure each single partition is small enough to solve within a user defined time limit. In an ideal case, we hope the partition strategy will not hurt the optimality of the original problem. For example, in our route optimization scenario, partitioning the items by the delivery source will keep the optimality of the original problem. However, in the case that there are a lot of items are from the same source, we need to further partition those such that we can solve the problem within the time limit. The optimality may lose after that. There will be a trade-off between optimality and running time. Usually, shortening running time is more preferred.  \n3. **Solve Individual Partition**: This step is achieved by ParallelRunStep in Azure ML. The ParallelRunStep will make sure items from the same partition will be assigned to the same process. Within each partition, we will input the corresponding items to our optimization program, which models the problem using our desired optimization solver (it is OR-Tools in our case). The optimization program will solve the problem and output the result to the next step. \n4. **Merge Result**: Once all the smaller problems are solved, we can merge them with the partial result produced in step 1 as the final result. There is still chance that we can further optimize the result using some simple heuristic in this final step. For example, within each partition, some items may be assigned to a smallest truck since there are no other items can be delivered together with them. However, when considering items from other partitions in the merge step, we may have chance to further combine those into a bigger truck if they all share the same destination.   \n\nThe whole Azure ML pipeline will be published as a REST API such that it can be reused by specifying different input and parameters.  -->\n\n# Try the Accelerator\n\nThe below sections describe the detailed steps to run the accelerator.\n\n## Prerequisite\n\nYou need to have an Azure subscription with the access to the following resources: \n\n| Azure Resources      | Description | Note |\n| ----------- | ----------- | --------------|\n| Azure Machine Leaning | To run the end-2-end pipeline   | Refer to the [instructions](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=azure-portal#create-a-workspace) to provision an Azure ML service |\n\n## Getting Started\n\n1. Create a virtual environment. The solution accelerator is tested with Python 3.8. \n    ```\n    conda create -n route-optimization python=3.8\n    conda activate route-optimization\n    ```\n\n2. Clone the repo and install python dependencies:\n    ```\n    git clone https://github.com/microsoft/dstoolkit-route-optimization.git\n    cd dstoolkit-route-optimization\n    pip install -r requirements.txt\n    ```\n    \n    If you are using Visual Studio Code, you will find a kernel named route-optimization in the kernel list. If you want to use Jupyter Notebook instead, create a kernel explicitly:\n    ```\n    python -m ipykernel install --user --name route-optimization --display-name \"route-optimization\"\n    ```\n\n3. Upload sample data\n\n    We have prepared some sample input data in the [sample_data](./sample_data) directory. You need to upload all the data to the default Datastore in your Azure ML workspace. The [order_small.csv](./sample_data/order_small.csv) under this directory is a small sample, best for testing. You can alternatively use [order_large.csv](./sample_data/order_large.csv) to test parallel run. In addition, there is another file named [distance.csv](./sample_data/distance.csv) that stores the pair-wise distance between different locations. \n    \n    To find your default Datastore, you can login your Azure ML studio, and click on the Datastores icon:\n    \n    ![image](docs/media/default-datastore.png)\n    \n    In the Overview page of the default Datastore, you can find the Blob container linking to this Datastore. Follow the link, you can go to the portal of the container, where you can upload the sample data.\n    \n    ![image](docs/media/default-container.png)\n    \n    For example, below we create a folder named model_input and upload all the sample data into this folder. \n    \n    ![image](docs/media/upload-file.png)\n    \n4. Configure Environment Variables\n\n    Create a `.env` file in the root directory of the repository, and fill in the values for the following variables (**Note**: `.env` is meant to be used in local mode. It is already added to the `.gitignore` file to avoid accidental commit of credentials to a repo):\n\n    ```\n    # Azure ML configuration\n    AML_WORKSPACE_NAME=    # The name of the Azure ML workspace\n    AML_SUBSCRIPTION_ID=  # The Azure subscription ID related to the above Azure ML workspace\n    AML_RESOURCE_GROUP=     # The resource group of the Azure ML workspace\n    ```\n\n5. Run the optimization pipeline\n\n    You can now create and run the whole pipeline using [the notebook for pipeline definition](./notebook/aml_pipeline.ipynb). Once the pipeline run is completed, it will output the final route assignment as a csv file to the Azure ML default Datastore under the output path you specified in the notebook (e.g., model_output in our above example). \n\n## Code structure\n\n```sh\n\u251c\u2500\u2500 ./notebook\n\u2502   \u251c\u2500\u2500 ./notebook/aml_pipeline.ipynb     # Notebook for optimization pipeline\n\u251c\u2500\u2500 ./requirements.txt                    # Defines required libraries in Python\n\u251c\u2500\u2500 ./sample_data\n\u2502   \u251c\u2500\u2500 ./sample_data/distance.csv        # Sample data defining distances between locations\n\u2502   \u251c\u2500\u2500 ./sample_data/order_large.csv     # Large example of customers' orders\n\u2502   \u2514\u2500\u2500 ./sample_data/order_small.csv     # Small example of customers' orders\n\u251c\u2500\u2500 ./src\n\u2502   \u251c\u2500\u2500 ./src/core\n\u2502   \u2502   \u251c\u2500\u2500 ./src/core/logger.py          # Defines logging features\n\u2502   \u2502   \u251c\u2500\u2500 ./src/core/merger.py          # Defines logic for merging the partitioned problem result\n\u2502   \u2502   \u251c\u2500\u2500 ./src/core/model.py           # Defines the modelling logic and the core optimizaiton problem\n\u2502   \u2502   \u251c\u2500\u2500 ./src/core/partitioner.py     # Defines the partition strategy\n\u2502   \u2502   \u251c\u2500\u2500 ./src/core/reducer.py         # Defines any heuristic for search space reduction\n\u2502   \u2502   \u2514\u2500\u2500 ./src/core/structure.py       # Defines basic data structure\n\u2502   \u251c\u2500\u2500 ./src/merge.py                    # Wrapping script for merge process\n\u2502   \u251c\u2500\u2500 ./src/partition.py                # Wrapping script for partition process\n\u2502   \u251c\u2500\u2500 ./src/reduce.py                   # Wrapping script for reduce process\n\u2502   \u2514\u2500\u2500 ./src/solve.py                    # Wrapping script for solve process\n\u2514\u2500\u2500 ./tests\n    \u2514\u2500\u2500 ./tests/core                      # Test codes for each process\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-route-optimization", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-route-optimization", "platform_org_repo": "github+microsoft/dstoolkit-route-optimization", "link_to_repo": "https://github.com/microsoft/dstoolkit-route-optimization", "platform": "github", "language": "Python", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Blob Inventory report analytics\n\nThe [Azure Storage blob inventory](https://docs.microsoft.com/azure/storage/blobs/blob-inventory) feature provides an overview of your containers, blobs, snapshots, and blob versions within a storage account. The inventory report can be used to understand various attributes of blobs and containers such as total data size, age, encryption status, immutability policy, legal hold and so on.  \n\nInventory provides information on Containers and blobs within the storage account. This project demonstrates how to bring up Inventory reports in Azure Synapse and run queries to derive insights. Additionally, the project also describes how to visualize the analytics data in Power BI. Such an analysis will be helpful in understanding the characteristics of Storage Accounts in terms of their usage, the data stored in them etc. Further, the information can be used to optimize for cost using Lifecycle policies.\n\n## List of Insights\n\n1. Overview\n    1. Overall growth in data over time\n    1. Amount of data added to the storage account over time\n    1. Number of files modified\n    1. Blob snapshot size [Blob snapshots - Azure Storage | Microsoft Docs](https://learn.microsoft.com/azure/storage/blobs/snapshots-overview)\n1. Detailed analysis\n    1. Data access patterns in different tiers of Storage (Hot, Cool, Archive)\n    1. Distribution of data across tiers\n    1. Distribution of data across blob types\n1. Data profile\n    1. Distribution of blob in blob types\n    1. Distribution of data in containers\n    1. Distribution of blobs in access tiers\n    1. Distribution of data by file types\n\n## Sample visualizations\n\n![PowerBI report overview](./media/power-bi-report-overview.png)\n\n![PowerBI report detailed analysis](./media/power-bi-report-detailed-analysis.png)\n\n![PowerBI report breakdown](./media/power-bi-report-breakdown.png)\n\n## Steps for generating insights and visualizations\n\n1. [Enable inventory reports](#1-enable-inventory-reports)\n1. Create resources\n    1. [Setup Azure Synapse workspace - via manual steps or via Arm templates provided](#2-set-up-azure-synapse-workspace-\u2013-via-manual-steps-or-via-the-arm-templates)\n    1. [Configure permissions to allow Azure Synapse to store files in a storage account](#3-configure-permissions-to-allow-azure-synapse-to-store-files-in-a-storage-account)\n    1. [Provide the configuration file to read the blob inventory report files](#4-provide-the-configuration-file-to-read-the-blob-inventory-report-files)\n1. [Import the PySpark notebook that has analytics queries](#5-import-the-pyspark-notebook-that-has-analytics-queries)\n1. [Run the PySpark notebook](#6-run-the-pyspark-notebook)\n1. [Visualize the results of the above queries in PowerBI reports](#7-visualizing-the-data)\n\n### 1. Enable inventory reports\nThe first step is to enable blob inventory reports on your storage account. After the inventory reports are generated, please process to the next steps. You may have to wait upto 24 hours after enabling inventory reports for your first report to be generated.\n\n### 2. Set up Azure Synapse workspace \u2013 via manual steps or via the ARM templates\n\nAn Azyre Synapse workspace can be created either via manual steps or via the ARM templates provided.\n\n#### Manual steps\n\n1. Create an Azure Synapse workspace\n    1. [Create an Azure Synapse workspace](https://learn.microsoft.com/azure/synapse-analytics/get-started-create-workspace) where you will execute a PySpark notebook to analyze the inventory report files.\n    1. Grant Storage Blob Data Contributor access to Synapse workspace\u2019s Managed Service Identity (MSI).\n1. [Create Apache Spark pool](https://learn.microsoft.com/azure/synapse-analytics/get-started-analyze-spark) in Synapse workspace created above. This Apache Spark pool will be used to execute PySpark notebook that will process the reports generated by Azure Blob Storage Inventory.\n\n#### Via ARM templates\n\nDeploy [SynapseArmTemplate.json](https://github.com/microsoft/Blob-Inventory-Report-Analytics/blob/main/src/arm-templates/SynapseArmTemplate.json) from Azure portal using [Deploy a custom template](https://learn.microsoft.comazure/azure-resource-manager/templates/quickstart-create-templates-use-the-portal).\n\n### 3. Configure permissions to allow Azure Synapse to store files in a storage account\n\nGrant yourself and other users who will require access to data visualization the following roles:\n1. [Storage Blob Data Contributor role on Storage account used while creating the Synapse workspace](https://learn.microsoft.com/azure/synapse-analytics/get-started-add-admin#azure-rbac-role-assignments-on-the-workspaces-primary-storage-account)\n1. [Contributor role on Synapse workspace](https://learn.microsoft.com/azure/synapse-analytics/get-started-add-admin#azure-rbac-owner-role-for-the-workspace)\n1. [Synapse Administrator role on Synapse studio](https://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-add-admin#synapse-rbac-synapse-administrator-role-for-the-workspace)\n\nPlease note that this step needs to be repeated for each user that will run the PySpark notebook used later in this documentation and also for users who will visualize the data using PowerBI\n\n### 4. Provide the configuration file to read the Blob Inventory report files\n\nDownload [BlobInventoryStorageAccountConfiguration.json](https://github.com/microsoft/Blob-Inventory-Report-Analytics/blob/main/src/BlobInventoryStorageAccountConfiguration.json) and update the placeholders inside it in following manner:\n1. *storageAccountName*: Name of the storage account for which inventory has been run\n1. *destinationContainer*: Name of the container where inventory reports are stored\n1. *blobInventoryRuleName*: Name of the inventory rule whose results should be analyzed\n1. *accessKey*: [Access keys for the storage account](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal#view-account-access-keys) in which inventory reports are stored.\n\nAfter updating the above placeholders, upload the configuration file in the storage container used while creating Synapse workspace.\n\n### 5. Import the PySpark Notebook that has analytics queries\n\nAfter you create the above resources and assign the permissions, perform the following steps:\n1. Download [ReportAnalysis.ipynb](https://github.com/microsoft/Blob-Inventory-Report-Analytics/blob/main/src/ReportAnalysis.ipynb)\n1. Navigate to [https://web.azuresynapse.net](https://web.azuresynapse.net)\n1. Select the **Develop** tab on the left edge\n1. Select the large plus sign (+) to add an item\n1. Select Import\n1. This will open a select file dialog. Upload the [ReportAnalysis.ipynb](https://github.com/microsoft/Blob-Inventory-Report-Analytics/blob/main/src/ReportAnalysis.ipynb) downloaded above\n    ![Select Import to import the PySpark notebook](./media/synapse-studio-import-dialog.png)\n1. This will open a Properties tab on the right. Select **Configure Session**\n    ![Select Configure Session to configure spark pool](./media/pyspark-notebook-configure-session.png)\n1. Select the Apache Spark pool created earlier in the **Attach to** dropdown and click **Apply**\n1. In the first cell of Python notebook, update the value of **storage_account** and **container_name** variables to the names of the storage account and container specified while creating the synapse workspace\n1. Click on \"Publish\" button in order to save the notebook in Azure Synapse studio and to avoid uploading it next time you run the notebook\n\n### 6. Run the PySpark notebook\n\n1. In the PySpark notebook imported above and click on **Run all**\n1. It will take 3-4 minutes to start the Spark session and another couple of minutes to process the Inventory reports\n1. Do remember to publish the notebook again if you made any changes in the notebook while running it\n\nPlease note that the first run could take a while if there are a lot of Inventory reports to process. Subsequent runs will only process the new inventory reports created since the last run\n\n### 7. Visualizing the data\n\nOpen [ReportAnalysis.pbit](https://github.com/microsoft/Blob-Inventory-Report-Analytics/blob/main/src/ReportAnalysis.ipynb) file using [PowerBI desktop application](https://powerbi.microsoft.com/downloads/). Please note that this file may not render correctly with the web version of PowerBI application and hence we recommend that you use the Desktop version of PowerBI. When the report is opened, a popup will open like the one shown below. In this popup, enter the name of the Synapse workspace in **synapse_workspace_name** and **database_name** as reportdata.\n\n![Enter the name of synapse workspace in synapse_workspace_name and reportdata in database_name](./media/power-bi-report-popup.png)\n\n## Common errors\n\nIf you see any error/warning in opening the PowerBI report, including but not limited to:\n1. The key did not match any row in the table\n1. Access to the resource is forbidden\n\nThe above errors are generic errors displayed by PowerBI. Please ensure that you have provided the input correctly to PowerBI template. Additionally, the following actions can be taken to handle the most frequently occurring errors:\n1. *Missing permissions*: Please make sure the user logged into PowerBI has the required access mentioned in [Configure permissions to allow Azure Synapse to store files in a Storage Account](#3-configure-permissions-to-allow-azure-synapse-to-store-files-in-a-storage-account) step.\n1. *Inventory reports not yet generated*: Verify that the inventory run has succeeded at least once for the rule provided to PySpark notebook. Also verify the destination container provided to the PySpark notebook. An alternate way to also verify this is by going to Azure synapse studio in which you ran the PySpark notebook and ensure that a database named \"reportdata\" is present in Data -> Workspace -> Lake Database.\n\n## Next steps\n\n* [Use Azure Storage blob inventory to manage blob data](https://learn.microsoft.com/azure/storage/blobs/blob-inventory)\n* [Optimize costs by automatically managing the data lifecycle](https://learn.microsoft.com/azure/storage/blobs/lifecycle-management-overview)\n* [Created data trigger to automatically process inventory reports](https://learn.microsoft.com/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Blob-Inventory-Report-Analytics", "org_name": "microsoft", "org_repo": "microsoft/Blob-Inventory-Report-Analytics", "platform_org_repo": "github+microsoft/Blob-Inventory-Report-Analytics", "link_to_repo": "https://github.com/microsoft/Blob-Inventory-Report-Analytics", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Virtual Training Day - Azure for Developers\n\nSample feedback form app using Azure App Service and Azure Cosmos DB.\n\n## Prerequisites\n\n-  Have an Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/?ocid=VTD_AzureDev).\n- Install [Node.js and npm](https://nodejs.org). Run the command `node --version` to verify that Node.js is installed.\n- Install [Visual Studio Code](https://code.visualstudio.com?ocid=VTD_AzureDev).\n- The [Azure App Service extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azureappservice&ocid=VTD_AzureDev) for Visual Studio Code. \n- Have a [GitHub account](https://github.com/)\n- Install [GitHub Desktop](https://desktop.github.com/)\n\n---\n\n## Inital setup\n\n#### Install Azure App Service Extentsion for Visual Studio Code\n\nIn Visual Studio Code, in the [Activity Bar](https://code.visualstudio.com/docs/getstarted/userinterface?ocid=VTD_AzureDev), select the **Azure** logo.\n\n1. In the **App Service** explorer, select **Sign in to Azure...** and follow the instructions.\n\n    In Visual Studio Code, you should see your Azure email address in the Status Bar and your subscription in the **AZURE APP SERVICE** explorer.\n\n\n#### Configure Azure Cosmos DB instance\n\n1. Sign in to the [Azure portal](https://portal.azure.com?ocid=VTD_AzureDev).\n\n1. From the Azure portal menu or the **Home page**, select **Create a resource**.\n\n1. On the **New** page, search for and select **Azure Cosmos DB**.\n\n1. On the **Select API option** page, select the **Create** option within the **MongoDB** section. Azure Cosmos DB has five APIs: SQL, MongoDB, Gremlin, Table, and Cassandra.\n\n1. On the **Create Azure Cosmos DB Account** page, enter the following information:\n\n   | Setting | Value | Description |\n   | --- | --- | --- |\n   | Subscription | Subscription name | Select the Azure subscription that you wish to use for this Azure Cosmos DB account. |\n   | Resource Group | Resource group name | Select a resource group, or select **Create new**, then enter a unique name for the new resource group. |\n   | Account Name | A unique name | Enter a name to identify your Azure Cosmos DB account. The name will be used as part of a fully qualified domain name (FQDN) with a suffix of *documents.azure.com*, so the name must be globally unique. The name can only contain lowercase letters, numbers, and the hyphen (-) character. The name must also be between 3-44 characters in length. |\n   | Location | The region closest to your users | Select a geographic location to host your Azure Cosmos DB account. Use the location that is closest to your users to give them the fastest access to the data. |\n   | Capacity mode |Provisioned throughput or Serverless|Select **Provisioned throughput** to create an account in [provisioned throughput](../../set-throughput.md) mode. Select **Serverless** to create an account in [serverless](../../serverless.md) mode. |\n   | Apply Azure Cosmos DB free tier discount | **Apply** or **Do not apply** |With Azure Cosmos DB free tier, you'll get the first 1000 RU/s and 25 GB of storage for free in an account. Learn more about [free tier](https://azure.microsoft.com/pricing/details/cosmos-db?ocid=VTD_AzureDev). |\n   | Version | MongoDB version  | Select the MongoDB server version that matches your application requirements.\n\n   > [!NOTE]\n   > You can have up to one free tier Azure Cosmos DB account per Azure subscription and must opt-in when creating the account. If you do not see the option to apply the free tier discount, this means another account in the subscription has already been enabled with free tier.\n   :::image type=\"content\" source=\"../media/quickstart-nodejs/new-cosmos-account-page.png\" lightbox=\"../media/quickstart-nodejs/new-cosmos-account-page.png\" alt-text=\"Screenshot of new account page for Azure Cosmos DB DB SQL API.\":::\n\n1. Select **Review + create**.\n\n1. Review the settings you provide, and then select **Create**. It takes a few minutes to create the account. Wait for the portal page to display **Your deployment is complete** before moving on.\n\n1. Select **Go to resource** to go to the Azure Cosmos DB account page. \n\n\n\n1. From the Azure Cosmos DB for NoSQL account page, select the **Connection String** navigation menu option.\n\n1. Record the values for the **PRIMARY CONNECTION STRING** field. You'll use this value in a later step.\n\n<!-- > Grab setup instructions from quickstart https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/cosmos-db/mongodb/quickstart-nodejs.md -->\n\n\n---\n\n## Configure and deploy app\n\n### Edit configuration\n\n1. Fork [repo](https://github.com/microsoft/AzureForDevelopers)\n1. Open forked repo with GitHub Desktop\n1. In GitHub Desktop, select \"Open in Visual Studio Code\"\n1. In Visual Studio Code, open the .env file. Replace the MongoDB  Connection String\n\n   ```.env\n   PORT=3001\n   MONGODB_URL=\"mongodb://<connection string>\"\n   ```\n\n### Run locally\n\n1. In the menu bar, expand **Terminal** > Select **New Terminal**\n1. Install the node modules\n\n   ```cli\n    npm install\n   ```\n\n1. Start the app\n\n   ```cli\n   node app.js\n   ```\n\n1. In a browser, navigate to http://localhost:3001. You should see something like this:\n\n### Deploy app to Azure\n\n1. In Visual Studio Code, in the Activity Bar, select the Azure logo.\n1. Right-click on App Services and select **Create new Web App**. (A Linux container is used by default.)\n1. Type a globally unique name for your web app and press **Enter**. The name must be unique across all of Azure and use only alphanumeric characters ('A-Z', 'a-z', and '0-9') and hyphens ('-').\n1. In Select a runtime stack, select the Node.js version you want. An **LTS** version is recommended.\n1. In Select a pricing tier, select **Free (F1)** and wait for the resources to be provisioned in Azure.\n1. In the popup **Always deploy the workspace \"AZUREFORDEVELOPERS\" to \\<app-name>\"**, select **Yes**. This way, as long as you're in the same workspace, Visual Studio Code deploys to the same App Service app each time.\n\n    While Visual Studio Code provisions the Azure resources and deploys the code, it shows [progress notifications](https://code.visualstudio.com/api/references/extension-guidelines#notifications).\n\n1. Once deployment completes, select **Browse Website** in the notification popup. The browser should display the feedback form page.\n\n### Redeploy updates\n\nYou can deploy changes to this app by making edits in Visual Studio Code, saving your files, and then redeploy to your Azure app. For example:\n\n1. Open *public/index.html* and change the Title color\n\n    ```html\n    <!-- Title -->\n      <div class=\"row mb-3\">\n        <h1 style=\"color:blue;\">Customer Feedback Form</h1>\n      </div>\n    ```\n\n    to\n\n    ```html\n    <!-- Title -->\n      <div class=\"row mb-3\">\n        <h1 style=\"color:orange;\">Customer Feedback Form</h1>\n      </div>\n    ```\n\n2. In the **App Service** explorer, select the **Deploy to Web App** icon again, confirm by clicking **Deploy** again.\n1. Wait for deployment to complete, then select **Browse Website** in the notification popup. You should see that the title is now colored orange.\n\n---\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureForDevelopers", "org_name": "microsoft", "org_repo": "microsoft/AzureForDevelopers", "platform_org_repo": "github+microsoft/AzureForDevelopers", "link_to_repo": "https://github.com/microsoft/AzureForDevelopers", "platform": "github", "language": "HTML", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Remote Lab\n\n## Table of Contents\n1. [About](#about)\n2. [Installation](#installation)\n3. [Usage](#usage)\n    1. [Initial Setup](#initial_setup)\n    2. [OBS Integration](#obs)\n    3. [Recording Transform Data](#recording_transform)\n        1. [Recording Non-Instantiated GameObjects](#recording_noninst)\n        2. [Recording Instantiated GameObjects](#recording_inst)\n    4. [Interactable UI](#interactable_ui)\n    5. [Tracking Custom Variables](#custom_var)\n    6. [Questionnaire](#custom_questionnaire)\n    7. [Replay System](#replay_system)\n4. [Example Study](#example)\n5. [Contributors](#contributors)\n6. [Contributing To This Project](#contributing)\n\n\n## About <a name=\"about\"></a>\nThe XR Remote study toolkit is a toolkit for Unity that allows users to record and replay gameplay from the Unity editor window. The toolkit currently allows users to track the changes in Transform for each GameObject in the scene, record UI events (Toggle, Button, and Slider events), note changes in user-defined variables, and implement custom Likert scale surveys and the raw NASA-TLX survey. \n\nThis was a collaborative project between researchers from Microsoft Research (Payod Panda, Eyal Ofek), University of Washington (Jaewook Lee), Vanderbuilt University (Raahul Natarrajan) and UIUC (Sebastian Rodriguez). The work is described in a paper presented at UIST 2022. Please use the following references for citing this work:\n\n#### ACM Ref:\n\nJaewook Lee*, Raahul Natarrajan*, Sebastian S. Rodriguez, Payod Panda, and Eyal Ofek. 2022. RemoteLab: A VR Remote Study Toolkit. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22). Association for Computing Machinery, New York, NY, USA, Article 51, 1\u20139. https://doi.org/10.1145/3526113.3545679\n\n#### BibTex ref:\n\n```@inproceedings{lee2022remotelab,\nauthor = {Lee, Jaewook and Natarrajan, Raahul and Rodriguez, Sebastian S. and Panda, Payod and Ofek, Eyal},\ntitle = {RemoteLab: Virtual Reality Remote Study ToolKit},\norganization = {ACM},\nbooktitle = {UIST},\nyear = {2022},\nmonth = {October},\npublisher = {ACM},\nurl = {https://www.microsoft.com/en-us/research/publication/remotelab-virtual-reality-remote-study-toolkit/},\n}\n```\n\n## Installation <a name=\"installation\"></a>\nDownload the latest Unity package from the Releases section. Then, import the package into Unity by selecting `Assets > Import Package > Custom Package` from the Unity menu bar. The package contents will be imported into a folder called Replay System in the Unity Assets folder for your project.\n\n## Usage <a name=\"usage\"></a>\n\n### Initial Setup <a name=\"initial_setup\"></a>\n\n![RemoteLab panel](docs/imgs/RemoteLabPanel.png \"RemoteLab Panel\")\n\nFirst, open the RemoteLab panel by selecting `RemoteLab > Control Panel` from the Unity menu bar. The RemoteLab panel contains useful actions needed to successfully execute a study using RemoteLab's features.\n\nTo set up the recording system, add the `Replay Manager` prefab by clicking the `Replay` button located under `Add RemoteLab Manager to Scene`. \n\n![Replay Manager Component](docs/imgs/ReplayManager.PNG \"Replay Manager Component\")\n\nThe `Replay Manager` prefab contains the `ReplayManager` script, which controls the scene recording functionality. The `Transform Data File Prefix` determines the name of the CSV file containing the tracked Transform information. `Controller Data File Prefix` determines the name of the CSV file containing the tracked controller data. `UI Event Data File Prefix` determines the name of the CSV file containing the logged UI events. `Custom Variable Data File Prefix` determines the name of the CSV file containing the logged data for user-defined variables. \n\nThe `Session Id` and `Participant Id` values determine where the data will be stored. If the `Session Id` is `session_1` and `Participant Id` is `participant_1`, then the recordings will be stored in `Assets/Recordings/session_1/participant_1`. Within the `Assets/Recordings/session_1/participant_1` folder, there will be one or multiple folders with time stamps as the name of the folder in the format of `MM-DD-YYYY_HH-MM-SS_XM`. Each folder will contain the recording files for recordings started at the time stamp indicated by the folder name.\n\n![Starting and stopping recording and/or OBS](docs/imgs/RecordingWorkFlow.png \"Starting and stopping recording and/or OBS\")\n\nIn order to start recording, run the scene in the editor or click the `Start Experiment` button located under `Experiment Actions`. Then click the `Start Recording` button located under `Experiment Actions`. To stop recording, click the `Stop Recording` button located under `Experiment Actions`. To use OBS, toggle `Use OBS` on before clicking the `Start Experiment` button. More details will be provided in the [OBS section](#obs). The `Frame Rate` field in the `ReplayManager` script determines at what frame rate the system will record the gameplay. The `Recordables` list lets you visualize at runtime what objects are being tracked.\n\n## OBS Integration <a name=\"obs\"></a>\nOBS Studio lets you record video through screen captures. If you are interested in recording your screen during the recording session, you can integrate OBS with this toolkit. First, you will need to install OBS Studio by downloading the appropriate package from [https://obsproject.com/](https://obsproject.com/). The rest of the instructions assumes that you are familiar with the OBS interface.\n\nNext, you will need to install `obs-websocket` in order to allow the toolkit to interface with the OBS software. The software can be found at [https://github.com/obsproject/obs-websocket](https://github.com/obsproject/obs-websocket). Install the appropriate version for your OBS client. \n\nThe toolkit interacts with the OBS software through the `OBS Manager`script located in the `ReplayManager` prefab. \n\n![OBSManager.cs](docs/imgs/OBSManager.PNG \"OBS Manager Component\")\n\nThe `Obs Path` parameter sets the path to the directory that contains the OBS Studio executable. The `Url` paramter sets the endpoint that the toolkit should connect to in order to attach itself to the OBS software. This path is something you will set inside of the OBS Studio setting before running the toolkit. Similarly, the `Password` parameter is the password to connect to the URL endpoint for the OBS client. The `Wait For OBS Delay` setting lets you determine how long to wait for the OBS software to boot up in seconds before retrying. Adjust this setting based on how long software takes to boot up on your computer. The `RetryLimit` setting determines how many times the `OBS Manager` will retry a connection to the OBS software. If the `OBS Manager` reaches the retry limit, then the recording process will be aborted and nothing will be recorded. \n\nTo record the screen using OBS alongside the toolkit, first set up the OBS displays beforehand in the OBS software. Then, verify that the `Use OBS` option is selected in the RemoteLab panel, as described in a previous paragraph. Make sure to select these settings before running the scene. Afterwards, run the scene and click the `Start Recording` button to start recording. Once you are finished, click the `Stop Recording` button to stop recording. \n\n## Recording Transform data for a GameObject <a name=\"recording_transform\"></a>\n\n![Recordable Button](docs/imgs/RecordableButton.PNG \"Recordable Button\")\n\nIn order to record the transform information for a GameObject during runtime, you need to add a `Recordable` component to that GameObject. To do so, click on the desired gameobject, then click the `Recordable` button located under `Add RemoteLab component to GameObject`. This will also automatically add the gameobject to the `Recordables` list.\n\n### Recording Component for non-instantiated GameObject <a name=\"recording_noninst\"></a>\n![Recordable.cs](docs/imgs/Recordable.png \"Recordable Component\")\n\nThe above picture shows the Recordable component for a non-instantiated GameObject (GameObject not created during runtime using a method call like `Instantiate()`). To finish the setup for a non-instantiated GameObject, select `Generate GUID` in order to generate a GUID for the GameObject. This is a unique ID for the GameObject used during the replay of a recording.\n\nIf the GameObject will be instantiated at runtime, you need to turn on `Is Instantiated At Runtime`. This selection will cause another section called `Resource Path` to pop up. \n\n### Recordable Component for instantiated GameObject <a name=\"recording_inst\"></a>\n![Recordable.cs Instanced](docs/imgs/RecordableInstanced.PNG \"Recordable Component for Instanced Objects\")\n\nThis selection will also remove the `Generate GUID` button. If the GameObject is instantiated at runtime, make sure that the `Guid String` field is empty. The GUID is not set for instantiated objects for the recording and replay functions.\n\nThe `Resource Path` field is used to determine where the instantiated object is located in the Assets folder for the replay system so that the object can be instantiated accordingly. For the replay system to correctly find the instantiated object, put the GameObject in a folder labeled Resources such as `Assets/Resources`. \n\nNow, the GameObject with the Recordable component will have its Transform information recorded and ready for replay.\n\n## Interactable UI <a name=\"interactable_ui\"></a>\nThe XR Remote Study Toolkit can track UI events for buttons, toggles, and sliders. To track a UI event, add a `Interactable UI` component to the UI object in the scene through drag-and-drop or adding the component through the `Inspector`. \n\n![InteractableUI.cs](docs/imgs/InteractableUI.PNG \"Interactable UI Component\")\n\nThe `Interactable Type` field is a drop-down menu, which lets you select one of the three types of UI events you want to track (button, toggle, slider). The `Guid String` field is filled by selecting the `Generate GUID` button. Generate the GUID for the component if the object containing the UI element will be instantiated at runtime. Otherwise, make sure to leave the `Guid String` field blank. \n\nThe UI element with the `Interactable UI` component is ready for recording UI events.\n\n## Tracking Custom Variables <a name=\"custom_var\"></a>\nIn order to track user-defined variables like a member variable in a user-defined class, you need to set up the tracking mechanism through additional lines of code. For instance, suppose you have the following class.\n\n```C#\nusing UnityEngine;\n\npublic class CustomVariableExample : MonoBehaviour\n{\n    private string m_someString;\n\n    private void Start()\n    {\n        m_someString = \"\";\n    }\n\n    private void Update()\n    {\n        if (Input.GetKeyDown(KeyCode.Alpha1))\n        {\n            m_someString += \"a\";\n            print(\"m_someString: \" + m_someString);\n        }\n    }\n}\n```\n\nIf you want to track the change in value for `m_someString` during runtime, we need to make some modifications.\n\n```diff\nusing UnityEngine;\n\npublic class CustomVariableExample : MonoBehaviour\n{\n    private string m_someString;\n+   public string SomeString\n+   {\n+       get\n+       {\n+           return m_someString;\n+       }\n+       set\n+       {\n+           m_someString = value;\n+           ReplayManager.Instance.WriteCustomVariableDataEntry(\"CustomVariableExample\"\n+               \"m_someString\", m_someString);\n+        }\n+   }\n\n    private void Start()\n    {\n-       m_someString = \"\";\n+       SomeString = \"\";\n    }\n\n    private void Update()\n    {\n        if (Input.GetKeyDown(KeyCode.Alpha1))\n        {\n-           m_someString += \"a\";\n-           print(\"m_someString: \" + m_someString);\n+           SomeString += \"a\";\n+           print(\"m_someString: \" + SomeString);\n        }\n    }\n}\n```\n\nTo track the variable, we are creating a C# property that wraps around the variable we want to track so that we can add in the additional line that logs data anytime the variable is changed. So, we create an additional field called `SomeString`, a property that manipulates `m_someString`, where the `set` method is set up to log any changes in the variable. Within the `set` method, we call `ReplayManager.Instance.WriteCustomVariableDataEntry(\"CustomVariableExample\", \"m_someString\", m_someString)`. The first parameter for this method call is the name of the script containing the tracked variable as a string. Next, the second parameter is the name of the tracked variable as a string. Finally, the last parameter is the value of the variable itself. Now, by calling this method whenever we set the variable, we can log changes to the variable during runtime. Now, we need to replace every instance where the codebase uses `m_someString` with `SomeString` so that the `set` method with the logging feature is activated correctly. So, we replace the uses of `m_someString` in the `Start()` and `Update()` methods with `SomeString`.\n\nNow, this example code tracks the variable changes for `m_someString`. Here is the completed code.\n\n```C#\nusing UnityEngine;\n\npublic class CustomVariableExample : MonoBehaviour\n{\n    private string m_someString;\n    public string SomeString\n    {\n        get\n        {\n            return m_someString;\n        }\n        set\n        {\n            m_someString = value;\n            ReplayManager.Instance.WriteCustomVariableDataEntry(\"CustomVariableExample\",       \"m_someString\", m_someString);\n        }\n    }\n\n    private void Start()\n    {\n        SomeString = \"\";\n    }\n\n    private void Update()\n    {\n        if (Input.GetKeyDown(KeyCode.Alpha1))\n        {\n            SomeString += \"a\";\n            print(\"m_someString: \" + SomeString);\n        }\n    }\n}\n```\n\n## Questionnaire <a name=\"custom_questionnaire\"></a>\nWe can easily create questionnaires using the XR Remote Study Toolkit. In order to make a custom questionnaire, first drag and drop the `Net Questionnaire` prefab from the `RemoteLab/Resources/Prefabs/Questionnaires/Resources` folder into the scene. Now, we must generate a `Questionnaire Content` object and drag and drop it into the `Questionnaire Content` parameter of `Net Questionnaire Manager` component and `Questionnaire Manager` component attached to the `Net Questionnaire` prefab.\n\n![Questionnaire.cs](docs/imgs/Quetsionnaire_Manager.png \"Questionnaire Manager Component\")\n\nIn order to create a custom `Questionnaire Content` object for your questionnaire, click the `New Questionnaire` button located under `Add Questionnaire`. You should see an empty `Questionnaire Content` scriptable object.\n\n![Empty Questionnaire Content](docs/imgs/QuestionnaireContent.png \"Empty Questionnaire Content\")\n\nYou can populate the list by clicking on the `+` button or increasing the number of questionnaire questions to more then zero and filling in the corresponding details.\n\nFirst, fill in the question.\n\n![Questionnaire Question Entry](docs/imgs/QuestionnaireQuestion.png \"Questionnaire Question Entry\")\n\nThen, select a choice type. `Single` represents a multiple choice question where participants can only select one answer. `Multiple` represents a multiple choice question where participants can select multiple answers. Finally, `Slider` represents questions where participants can select a value from a range of values. Each choice type requires different parameters, which researchers can specify.\n\n![Single_Choice](docs/imgs/Single.png \"Single Choice Option\")\n\n![Multiple_Choice](docs/imgs/Multiple.png \"Multiple Choice Option\")\n\n![Slider](docs/imgs/Slider.png \"Slider Option\")\n\nFinally, researchers can specify whether a question can be skipped (i.e., not provide an answer) by the participants or not. Simply toggle the `Can Skip Question` parameter to do so.\n\nThe toolkit provides you with a pre-made `LikertScale` `SUS` and `NASA TLX` Questionnaire Content objects for use, which can be found in the `RemoteLab/Resources/Prefabs/Questionnaires/Examples` folder. Please check them out to see how Questionnaire Content object can be expanded to fit different questionnaires.\n\n![SUS Questionnaire Content](docs/imgs/SUS.png \"SUS Questionnaire Content\")\n\n![NASA TLX Questionnaire Content](docs/imgs/NASA_TLX.png \"NATA TLX Questionnaire Content\")\n\n![Likert Questionnaire Content](docs/imgs/LikertScale.png \"Likert Scale Questionnaire Content\")\n\nBefore using the questionnaire, make sure to generate the GUIDs for the previous, next, and skip button UI elements, which are called `PrevButton` `NextButton` and `SkipButton` respectively.\n\nTo use your created Questionnaire Content for the questionnaire, select the appropriate Questionnaire Content file as the parameter for the `Questionnaire Content` field in the `Net Questionnaire Manager` and `Questionnaire Manager` components.\n\nFinally, drag and drop the questionnaire prefab as a new prefab into the `RemoteLab/Resources/Prefabs/Questionnaires/Resources` folder. This prefab is the networked questionnaire. In the `QuestionnaireManager` prefab, add the name of this prefab into the `Questionnaires to Spawn` list. Duplicate this prefab and rename it by adding `_local` at the end. Additionally, enable the `Questionnaire Manager` script, disable the `Net Questionnaire Manager` script, and uncheck `Is Instantiated at Runtime`. This is the local version of the questionnaire, which will be used by the replay system.\n\nWhen a questionnaire is filled and submitted by a participant, the corresponding data is written to a CSV file with the following naming convention: nameOfTheQuestionnaireGameObject_questionnaire_data.csv.\n\n## Setting up questionnaire and UI\nIn order to interact with the questionnaire objects and any other UI objects, you need to use or implement a form of UI interactor for your XR system. For instance, if you are using the Oculus (OVR) Integration, you need to add the `OVR Raycaster` to the top-level GameObject for any survey that you are using. Furthermore, you need to use some kind of UI input selector for your scene. For Oculus, we set up the UI interaction system using the `UIHelpers` prefab from the Oculus Integration package.\n\n## Replay System <a name=\"replay_system\"></a>\nThe replay system implemented in the toolkit replays whatever information was recorded in the CSV files for UI events and Transform changes. So, the replay does include the replaying of animations and in-game events that cannot be fully captured through changes in transform, instantiation, object destruction, activation, deactivation, and UI click, slide, and toggle events.\n\nTo set up the system for replay, first duplicate the scene to be replayed. Then, with the duplicated scene loaded, click on the `Set Up Replay` button located under `Experiment Actions` to transform the scene into a replay scene.\n\nNow, go to the `Replay System` prefab and verify some parameters.\n\n![ReplaySystem.cs](docs/imgs/ReplaySystem.PNG \"Replay System Component\")\n\nThe `Transform Data Path` is used to determine what transform file to replay. The `Ui Data Path` is used to determine what UI data file to replay. `Transform Data File Prefix` is the prefix of the CSV file that contains the Transform data to replay. `Ui Data File Prefix` is the prefix of the CSV file that contains the UI event data to replay. Make sure that these values are set to replay the appropriate files. The default values for the prefixes assume that you did not modify the file prefixes in the `Replay Manager` component in the original scene. You need to verify that the absolute paths to the transform and UI data are valid for your file system. The `Frame Rate` parameter is used to determine at what frame rate to replay the recordings. This should be the same as the value you used to record the scene. The `Video Player` paramemeter is used to determine what video player to use in the scene to replay any video you want to play alongside the 3D replay in a second display. \n\nTo replay the recording, first run the replay scene. Then, in the game view, you can see that there is a video player like interface with a `Play/Pause` button, `Stop` button, and a video scrubbing slider. Click the `Play/Pause` button to start the replay. When you want to pause or resume the replay, press the `Play/Pause` button. To stop the replay and reset the recording to the beginning, press the `Stop` button. The slider at the bottom will track the progress of the replay. You can jump between points of the replay by selecting any point on the slider to scrub to that location in the replay.\n\n## Example Study <a name=\"example\"></a>\nTo understand how everything in the toolkit works, we have provided an example user study designed using the toolkit. The example study also uses `Photon PUN` for networking. So, this example will also provide some guidance on how to design a networked study using the toolkit. To use `Photon PUN`, first go to [https://www.photonengine.com/pun](https://www.photonengine.com/pun) to understand how to install and use the package. The toolkit provides some template scripts to use for networking, but it will be helpful to understand how the code works behind the scenes. \n\nWe provide one scene as part of the example study, `[NET] Sample Experiment`. The `[NET] Sample Experiment` scene contains a basic user study scene set up using the toolkit. You can use this scene as a template for your own studies. \n\nThe example study uses the Oculus SDK for the VR system.\n\n### [NET] Sample Experiment\nIn this scene, there are various networking components that you can reuse for your own projects.\n\nThe scene is set up so that a participant can enter the scene using an Oculus headset and interact with the objects in the scene. The researcher will observe the participant through a fixed perspective camera.\n\nSince the study is set up using the Oculus SDK, we modify the OVR Camera Rig and use the OVR UI Helpers from the SDK to handle the VR interaction. This example serves as a guide as to how to implement multiplayer mechanisms within your own user studies since there are multiple available networking solutions and multiple ways to code XR interactions in Unity. So, we are providing a base project to show you how to use this toolkit to make remote studies alongside an example project to give you some basic scripts and templates to create your own studies.\n\n#### NetworkManager\nThe `Network Manager` object in the hierarchy contains the scripts necessary to run the networked user study. It contains the `Launcher` and `Ownership Manager` components. \n\nThe `Launcher` is used to set up the Photon room in the user study for each of the clients. The `Game Version` parameter is for your own consistency purposes. The `Player Name` is the name of the player that will run the scene using the `Launcher`. If you want to change the name for each user, be sure to modify this parameter using your own functions either through runtime editing or pre-execution routines. The `Room Name` parameter is the name of the shared room in Photon. Make sure that every client that should join the same room have the same `Room Name`. The `Is Participant` parameter is used to determine what objects are activated in the scene at runtime. These objects all exist in the hierarchy before running the scene. If the parameter is selected, then the objects in `Participant Objs` will be activated. Otherwise, the objects in `Researcher Objs` will be activated. Make sure to edit these approriately to your needs. For the template, we have the scene set up such that the participant can manipulate the scene through a VR headset, but the researcher is limited to a fixed perspective camera in the scene to demonstrate that you can have different types of clients join the lobby.\n\nThe `Ownership Manager` handles the ownership of networked objects in the Photon code. There are no editable parameters.\n\n#### Transfer Manager\nThe `Transfer Manager` object in the hierarchy is set up to transfer the data recorded from the participants' perspective to the researchers. The object containts the `Net Transfer` component. The `Launcher` field points to the `Launcher` component in the `Network Manager`. The `Replay Manager` will be automatically handled by the script. The `Trigger Transfer` function triggers the data transfer from the participants to the researchers when clicked. The `Batch Size` parameter is used to determine how much data to send at a time in terms of line count.\n\n#### PlayerController\nThe `PlayerController` object in the hierarchy serves as the main Player object for the participant. In this object, we use scripts to set up how an avatar is rendered in the scene to view for everyone in the scene. You can try to use this object as a template for creating your own multiplayer VR controller. In the object, there is a `Net VR Player` component, which handles the instantiation for the player. \n\nIn the `Net VR Player` component, there are various parameters. `Left Hand Root` and `Right Hand Root` determine where to instantiate the left and right hands for the VR player. The `Left Hand Offset` and `Right Hand Offset` handle the offset for where the hand should be placed within the root objects. `Left Hand Avatar Name` and `Right Hand Avatar Name` determine how to instantiate the left hand and right hand prefabs. Set these values to the names of the left hand and right hand prefabs for your avatar hands visual. `Left Net Hand` and `Right Net Hand` refer to the networked left and right hand scripts within the `PlayerController` that handle how the network processes the user's left and right hands.\n\nWithin the `PlayerController`, we add various `Trackable` objects to essential parts of the VR Player so that the necessary parts are recorded for review and replay in the toolkit. Within the hand anchors for the `PlayerController`, we modified the OVR Distance Grabbing Hands to be networked using Photon. We called these scripts `Net Hand` and `Net Distance Grabber`, and created a new `[NET] DistanceGrabHand` prefab based on that.\n\nThe `GrabManager` object within the controller is a modified version of the `Grab Manager` component called `Net Grab Manager` so that the grabbing is networked.\n\n#### General Networking\nTo generally make a user study networked, the approach is to convert all scripts to accomodate the networking package. We created various scripts for this purpose by creating a modified version of scripts for the Oculus SDK with the prefix `Net`. Please take a look at these scripts to understand how to adapt these solutions to your own project. \n\nAn example conversion is `Net Physics` script for any objects that need to have their physics functions networked. This would include objects like projectiles that need to be physically moved for all clients in the room or if someone grabs an object, everybody else in the room also needs to observe that the person grabbed an object. This script handles those sort of interactions using the Photon toolkit. \n\n### REPLAY - Sample Experiment\nThis scene contains the converted version of the `[NET] SampleExperiment` scene. In this scene the `[NET] SampleExperiment` scene has been converted following the instruction in the [Replay System](#replay_system) section. This scene additionally removes any Photon script and Net scripts from all object so that the replay system does not fail due to faulty networking in a scene that is not supposed to be networked. It contains the main `Replay System` component in the `Replay Manager` object to handle the replay of recordings. Please follow the Replay System instructions to set up recordings to replay. \n\n\n# Contributors <a name=\"contributors\"></a>\n\nJaewook Lee - University of Washington \n\nRaahul Natarrajan - Vanderbilt University\n\nSebastian S. Rodriguez - University of Illinois at Urbana-Champagne\n\nPayod Panda - Microsoft Research, Cambridge\n\nEyal Ofek - Microsoft Research, Redmond\n\n# Contributing <a name=\"contributing\"></a>\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n", "repo_name": "Remote-Lab", "org_name": "microsoft", "org_repo": "microsoft/Remote-Lab", "platform_org_repo": "github+microsoft/Remote-Lab", "link_to_repo": "https://github.com/microsoft/Remote-Lab", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mixedreality.dmx.agent", "org_name": "microsoft", "org_repo": "microsoft/mixedreality.dmx.agent", "platform_org_repo": "github+microsoft/mixedreality.dmx.agent", "link_to_repo": "https://github.com/microsoft/mixedreality.dmx.agent", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Azure Synapse Analytics Support\n\nThis project intends to host notebooks and scripts which supplement public documentation, troubleshooters, and how-to documents for user consumption.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "synapse-support", "org_name": "microsoft", "org_repo": "microsoft/synapse-support", "platform_org_repo": "github+microsoft/synapse-support", "link_to_repo": "https://github.com/microsoft/synapse-support", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "===================================================\nProject Mu Developer Operations (DevOps) Repository\n===================================================\n\n|Latest Mu DevOps Release Version (latest SemVer)| |Commits Since Last Release| |Sync Mu DevOps Files to Mu Repos| |Containers Build|\n\n.. |Latest Mu DevOps Release Version (latest SemVer)| image:: https://img.shields.io/github/v/release/microsoft/mu_devops?label=Latest%20Release\n   :target: https://github.com/microsoft/mu_devops/releases/latest\n\n.. |Commits Since Last Release| image:: https://img.shields.io/github/commits-since/microsoft/mu_devops/latest/main?include_prereleases\n   :target: https://github.com/microsoft/mu_devops/releases\n\n.. |Sync Mu DevOps Files to Mu Repos| image:: https://github.com/microsoft/mu_devops/actions/workflows/FileSyncer.yml/badge.svg\n   :target: https://github.com/microsoft/mu_devops/actions/workflows/FileSyncer.yml\n\n.. |Containers Build| image:: https://github.com/microsoft/mu_devops/actions/workflows/Build-Containers.yml/badge.svg?branch=main\n   :target: https://github.com/microsoft/mu_devops/actions/workflows/Build-Containers.yml\n\nThis repository is part of Project Mu.  Please see Project Mu for details https://microsoft.github.io/mu\n\nThis repository is used to manage files related to build, continuous integration (CI), and continuous deployment (CD)\nfor other Project Mu repositories.\n\nMany of these files are generic YAML templates that can be combined together to compose a fully functional pipeline.\n\nPython based code leverages `edk2-pytools` to support cross platform building and execution.\n\nYou can find a high-level summary of the latest changes since the last release by viewing the `latest draft release`_.\n\n.. _`latest draft release`: https://github.com/microsoft/mu_devops/releases\n\nTable of Contents\n=================\n\n1. `Project Mu Developer Operations (DevOps) Repository`_\n\n2. `Table of Contents`_\n\n3. `Continuous Integration (CI)`_\n\n4. `Conventions`_\n\n5. `Containers`_\n\n6. `GitHub Automation Workflow Overview`_\n\n   - `Leaf Workflows & Reusable Workflows`_\n\n   - `Reusable Workflows`_\n\n   - `Leaf Workflows`_\n\n7. `GitHub Automation Workflow Summary`_\n\n   - `Auto Merge`_\n\n   - `Auto Approver`_\n\n   - `File Synchronization`_\n\n   - `Initial Issue Triage`_\n\n   - `Issue Assignment`_\n\n   - `Label Automation`_\n\n   - `Pull Request Validator`_\n\n   - `Release Drafting`_\n\n   - `Scheduled Maintenance`_\n\n   - `Stale Detection`_\n\n8. `GitHub Action Summary`_\n\n   - `Submodule Release Updater`_\n\n9.  `Links`_\n\nContinuous Integration (CI)\n===========================\n\nThere are two broad categories of CI - Core CI and Platform CI. You may see these terms used in the repo.\n  - **Core CI** - Focused on building and testing all packages in Edk2 without an actual target platform.\n  - **Platform CI** - Focused on building a single target platform and confirming functionality on that platform.\n\nConventions\n===========\n\n- Shared templates in Project Mu repos are encouraged to be maintained in this repository.\n\n- The `.github` directory contains GitHub collateral for this repository.\n\n  - Some of the files are shared GitHub actions or workflows used (referenced) by other repositories as well.\n\n- Files that are synced to other repositories should be placed in the `.sync` folder.\n\n  - Some files are synced back to this repository (`mu_devops`).\n\n- Azure Pipelines job and step templates should respectively be placed in the `Jobs` and `Steps` directories.\n\n- YAML files should have the extensions `*.yml`.\n\n  - An exception is the markdown configuration file (`.markdownlint.yaml`) that uses `.yaml` for consistency with\n    pre-existing conventions across Mu repos.\n\nContainers\n==========\n\nThis repo maintains containers used throughout Project Mu projects. Containers provide well-defined, ready-to-go\nimages and result in improved performance, portability, and consistency of build environments. Project Mu leverages\ncontainers for both server-side builds (e.g. pull requests and continuous integration) and for local developer builds.\n\nAt this time, containers are only offered for Linux. If you want to get started quickly and receive the smoothest\nbuild experience, it is recommended to use containers where available.\n\nThe `Containers` directory contains the actual dockerfiles for building the containers. The containers are actually\nbuilt (in pull requests to dockerfiles and merges to the `main` branch) in the `.github/workflows/Build-Containers.yml`\nworkflow. On any change to a dockerfile a new container is built and pushed to the Microsoft GitHub container registry\nas a container package associated with this repo. The latest Project Mu container builds are available in the\n`Packages - Mu DevOps`_ container feed and more information is available in the `Container Readme file`_.\n\n.. _`Container Readme file`: https://github.com/microsoft/mu_devops/blob/main/Containers/Readme.md\n.. _`Packages - Mu DevOps`: https://github.com/orgs/microsoft/packages?repo_name=mu_devops\n\nGitHub Automation Workflow Overview\n===================================\n\nThis repository also drives automation of Project Mu GitHub repositories.\n\nLeaf Workflows & Reusable Workflows\n-----------------------------------\n\nWorkflows are split into two categories **(1) leaf** and **(2) reusable**.\n\nThe main reason for reusable workflows is to consolidate the main logic for the workflow to a single file and allow\nthe leaf workflow to be present in repositories that opt into what the reusable workflow provides. Leaf workflows can\nprovide any repo-specific input to a reusable workflow (if necessary). Leaf workflows can be considered minimal\nwrappers around reusable workflows.\n\nReusable Workflows\n------------------\n\nThese workflow are only designed to be called from other workflows. The files are maintained in the `.github/workflows`\ndirectory of this repository. This is mandatory as GitHub only allows leaf workflows to call reusable workflows\nlocated in this directory.\n\nReview reusable workflow files to understand what they do and what input parameters are available.\n\nLeaf Workflows\n------------------\n\nThese workflow are only designed to call reusable workflows. They should not directly invoke GitHub Actions. The\nactual GitHub Actions used by Project Mu are centrally tracked/updated in the single-copy reusable workflow files\nin the Mu DevOps repo. This allows dependabot to update the actions here at once.\n\nGitHub Automation Workflow Summary\n==================================\n\nFollowing is a brief summary of the actual workflows in the repository.\n\nAuto Merge\n----------\n\nAs automated bots pick up mundane tasks like syncing PIP module updates, submodules, files, and so on, an increasing\nnumber of pull requests can accumulate that essentially update dependencies we expect to be updated over time. In most\ncases, we simply care that the new update passes CI checks.\n\nTherefore, Project Mu repos auto merge certain pull requests to reduce human burden of approving these requests in all\nof the Project Mu repos. Individual repos can opt out of this functionality by removing the leaf workflow sync to their\nrepo, however, it is recommended to keep this flow enabled for consistency across all repos.\n\nTo see more about this flow look in these files:\n\n- The main reusable workflow file:\n\n  - `.github/workflows/AutoMerger.yml`\n\n- The leaf workflow\n\n  - `.sync/workflows/leaf/auto-merge.yml`\n\nA Project Mu repo simply needs to sync `.sync/workflows/leaf/auto-merge.yml` to their repo in `Files.yml` and the\nauto merge workflow will run in the repo.\n\nAuto Approver\n-------------\n\nAuto approves pull requests from allowed bot accounts. As part of reducing dependency overhead, this workflow first\napproves pull requests that are then auto merged after CI status checks complete. If a CI status check (e.g. build)\nfails, the pull request will not be merged.\n\nNote: This is currently disabled in most Project Mu repos.\n\nTo see more about this flow look in these files:\n\n- The main reusable workflow file:\n\n  - `.github/workflows/AutoApprover.yml`\n\n- The leaf workflow\n\n  - `.sync/workflows/leaf/auto-approve.yml`\n\nA Project Mu repo simply needs to sync `.sync/workflows/leaf/auto-approve.yml` to their repo in `Files.yml` and the\nauto approve workflow will run in the repo.\n\nFile Synchronization\n--------------------\n\nBecause Project Mu is distributed over many repositories, a need arises to sync common files across all of the repos.\nThis is done via the `.github/workflows/FileSyncer.yml` workflow in Mu DevOps. It determines how to map files from\nMu DevOps to any repo with the configuration file `.sync/Files.yml`.\n\nThe configuration file can map any file in Mu DevOps to any file path in a destination repo. Flexibility is provided\nto map the same file to different file paths in different repos, not map the file to some repos, etc. Whole directories\ncan also be synced as well.\n\nThe file sync operation automatically runs anytime a file in the `.sync/` directory of Mu DevOps is updated.\n\nThe file modification flow should be as follows:\n\n1. Developer updates a synced file in Mu DevOps\n2. Once PR for (1) is merged all mapped repos get a PR with the change\n3. Reviewers in each repo review and approve the PR\n4. The file is now in sync across all repos\n\nFile synchronization PRs are created by the `Project Mu UEFI Bot`_ account.\n\nThe file synchronization process will use the original commit title and message when syncing the change if it is\ntriggered on a single commit. Therefore, it is recommended to make changes to sync files one file per commit at a\ntime. If more than one file is modified, the PR is simply a single commit with a generic message containing both\nchanges.\n\n.. _`Project Mu UEFI Bot`: https://github.com/uefibot\n\nInitial Issue Triage\n--------------------\n\nThis repo syncs `GitHub issue form templates`_ to many Project Mu repos. Part of initial triage for incoming issues\ninvolves parsing data in the issue form to apply the appropriate labels so the issue is ready for triage by a human.\n\nIssues need to be triaged by a human when the `state:needs-triage` label is present. This workflow can parse details\nprovided in issue forms to apply additional labels. For example, the `state:needs-owner` label is applied if the user\nindicates they are not fixing the issue, the `urgency:<level>` label is applied based on user selection in the urgency\ndropdown, etc.\n\nA Project Mu repo simply needs to sync `.sync/workflows/leaf/triage-issues.yml` to their repo and the issue triage\nworkflow will run in the repo.\n\n.. _`GitHub issue form templates`: https://github.com/microsoft/mu_devops/tree/main/.sync/github_templates/ISSUE_TEMPLATE\n\nThis workflow works in concert with other issue workflows such as `.sync/workflows/leaf/issue-assignment.yml` to\nautomate labels in issues based on the state of the issue.\n\nIssue Assignment\n----------------\n\nA generic workflow that contains actions applied when GitHub issues are assigned. Currently, the workflow removes\nlabels from the issue that are no longer relevant after it is assigned.\n\nTo see more about this flow look in these files:\n\n- The main reusable workflow file:\n\n  - `.github/workflows/IssueAssignment.yml`\n\n- The leaf workflow\n\n  - `.sync/workflows/leaf/issue-assignment.yml`\n\nLabel Automation\n----------------\n\nLabels are automated from this repo in two main ways:\n\n1. Automatically synchronize labels across all Project Mu repos\n2. Automatically apply labels to issues and PRs\n\n(1) is provided via the `.github/workflows/LabelSyncer.yml` reusable workflow with the labels defined in the file\n`.github/Labels.yml`.\n\n(2) is provided via the `.github/workflows/Labeler.yml` reusable workflow with the labeling configuration defined in\n`.sync/workflows/config/label-issues`.\n\nLabels are synced to all repos on a regular schedule that is the same for all repos.\n\nLabels are automatically applied to issues and pull request on creation/modification and can be applied based on file\npaths modified a pull request or content in the body of the issue or pull request.\n\nPull Request Validator\n----------------------\n\nValidates pull request formatting against requirements defined in the workflow. This workflow is not intended to\nstrictly validate exact formatting details but provide hints when simple, broad changes are needed to enhance the\nquality of pull request verbiage.\n\n- The leaf workflow\n\n  - `.sync/workflows/leaf/pull-request-formatting-validator.yml`\n\nRelease Drafting\n----------------\n\nIn order to ensure semantic versioning is followed based on well-defined labels used in Project Mu pull requests, the\nrelease drafting process is automated. On every PR merge, a draft release is updated that contains the PR change entry\ncategorized according to the labels with the semantic version of the draft release updated according to the semantic\nversion specification.\n\nThis means, that the details for an upcoming release are always available, the release format is consistent across\nProject Mu repos, and semantic versioning is followed consistently.\n\nThe draft release should be converted to an actual release any time the minor or major version is updated by a change.\n\nTo see more about this flow look in these files:\n\n- The main reusable workflow file:\n\n  - .github/workflows/ReleaseDrafter.yml\n\n- The configuration file for the reusable workflow:\n\n  - .sync/workflows/config/release-draft/release-draft-config.yml\n\n    - This will be synced to .github/release-draft-config.yml in repos using release drafter\n\nA Project Mu repo simply needs to sync `.sync/workflows/leaf/release-draft.yml` and the config file\n`.sync/workflows/config/release-draft/release-draft-config.yml` to their repo and adjust any parameters needed in the\nsync process (like repo default branch name) and the release draft workflow will run in the repo.\n\nScheduled Maintenance\n---------------------\n\nPerforms regularly scheduled maintenance-related tasks such as closing pull requests and issues marked stale. Similar\ntasks can be added to the workflow over time.\n\nThe leaf workflow contains the primary implementation and is directly synced to subscribed repos:\n\n- `.sync/workflows/leaf/scheduled-maintenance.yml`\n\nStale Detection\n---------------\n\nStale issues and pull requests are automatically labeled and closed after a configured amount of time.\n\nThis is provided by the `.github/workflows/Stale.yml` reusable workflow.\n\nIndividual repositories can control the label and time settings but it is strongly recommended to use the default\nvalues defined in the reusable workflow for consistency.\n\nGitHub Action Summary\n=====================\n\nFollowing is a brief summary of the GitHub Actions maintained in the repository.\n\nSubmodule Release Updater\n-------------------------\n\nA GitHub Action and leaf workflow that automatically create a pull request for any submodule in a repo\nthat has a new GitHub release available. The leaf workflow can easily be synced to repos and wraps around\nthe GitHub action.\n\n- The GitHub action\n\n  - `.github/actions/submodule-release-updater`\n\n- The leaf workflow\n\n  - `.sync/workflows/leaf/submodule-release-update.yml`\n\nLinks\n=====\n- `Basic Azure Landing Site <https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops>`_\n- `Pipeline jobs <https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml>`_\n- `Pipeline YAML scheme <https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema%2Cparameter-schema>`_\n- `Pipeline Expressions <https://docs.microsoft.com/en-us/azure/devops/pipelines/process/expressions?view=azure-devops>`_\n- `PyTool Extensions <https://github.com/tianocore/edk2-pytool-extensions>`_\n- `PyTool Library <https://github.com/tianocore/edk2-pytool-library>`_\n", "repo_name": "mu_devops", "org_name": "microsoft", "org_repo": "microsoft/mu_devops", "platform_org_repo": "github+microsoft/mu_devops", "link_to_repo": "https://github.com/microsoft/mu_devops", "platform": "github", "language": "Dockerfile", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Inclusiveness Analyzer\n\n> Make your code inclusive!\n\nThe Inclusiveness Analyzer is a GitHub action that checks your repository for offensive / exclusive terms.\n\nIt also provides context on why a word is exclusive and suggests alternate terms that can be used instead.\n\n## Install Guide\n\n### Add Inclusive Analyzer action to your build workflow\n\n* In your GitHub repository, select the **Actions** tab and either add or edit a workflow.\n* Search for **Inclusiveness Analyzer** from the **Marketplace** tab on the right.\n* Copy and paste the yaml into your workflow.\n![Screenshot showing Inclusiveness Analyzer being added to a build.](docs/images/ghscreenshot-1.png)\n\nCopy paste the following workflow definition into your project `.github/workflows/inclusiveness-analyzer.yml`\n\n```yaml\n# This workflow checks out code and scans the content changed or added in the \n# last commit for offensive / exclusive terms.\n# The scan will provide context on the found terms and alternatvies that can be\n# used instead.\n\nname: Inclusiveness Analyser scan\n\non:\n  push:\n  workflow_dispatch:\n\njobs:\n  Inclusiveness-Analyser-scan:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v3\n\n    - name: Inclusiveness Analyzer\n      uses: microsoft/InclusivenessAnalyzer@v1.0.1\n```\n\n* Commit your changes to trigger the workflow or run the workflow manually\n* The Annotations view will show the first ten non-inclusive terms that are found.\n* You can select the Jobs detail log to view all the instances of non-inclusive terms.\n![Screenshot showing Inclusiveness Analyzer warning of the work blacklist being used.](docs/images/ghscreenshot-2.png)\n\n### Action configuration options\n\nUse the options below to configure exclusions and build state when non-inclusive terms are found in the repository.\n\n|          option          |            example             |  description  |\n|--------------------------|--------------------------------|---------------|\n| `failOnNonInclusiveTerm` |                                | If `false` (Default) the build completes successfully and warnings are provided in the logs.<br/>If `true` the build is failed if non-inclusive terms are found. |\n| `excludeUnchangedFiles`  |                                | If `true` (Default) limits the scan to files changed in the latest commit.<br/>If `false` a full scan is run on each commit. |\n| `excludeFiles   `        | `**/skipme.txt,**/donotscan/*` | Comma separated list of file patterns to exclude from analysis. [Glob patterns](https://github.com/isaacs/node-glob#glob-primer) are supported with a prefix of `**/` |\n| `excludeTerms`           | `he,she`                       | Comma separated list of non-inclusive terms to exclude from analysis. |\n| `maxLineLength`          | 2000                           | Maximum line length to inspect. Defaults to 1000. |\n\n## Inclusiveness Analyzer for other Platforms\n\n* [Inclusiveness Analyzer Visual Studio Extension](https://github.com/microsoft/InclusivenessAnalyzerVisualStudio)\n* [Inclusiveness Analyzer Azure DevOps Extension](https://github.com/microsoft/InclusivenessAnalyzerAzureDevOps)\n\n## About the project\n\nAs humans, we hold many unconscious and implicit biases that we rely on to react quickly to our environment and any novel stimuli. However, since the unconscious brain processes and reacts with speed, we sometimes speak quickly without thinking, which may cause us to slip offensive terms and stereotypes although we mean no malice.\n\nIn order to confront these biases that we see in ourselves and others, we must rewire ourselves to regularly use inclusive practices (such as the words we speak). If you don't intentionally and proactively include, you will unintentionally exclude. \n\n> Join our effort to push out exclusive terms and make inclusive terms a part of our everyday vocabulary!\n\nHelp us confront these biases by pushing out exclusive terms and making inclusive terms a part of our everyday vocabulary!\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "InclusivenessAnalyzer", "org_name": "microsoft", "org_repo": "microsoft/InclusivenessAnalyzer", "platform_org_repo": "github+microsoft/InclusivenessAnalyzer", "link_to_repo": "https://github.com/microsoft/InclusivenessAnalyzer", "platform": "github", "language": "JavaScript", "stargazers_count": 29, "watchers_count": 29}, {"README_text": "# React Component Toolkit\n\n\n### What is the React Component Toolkit?\nThe react component toolkit is a toolkit for building and testing react components and includes a set of unique features including AI component generation and automatic conversion to Azure APIM Widgets.\n\n---\n\n#### Table of Contents\n\n1. [Features](#features)\n2. [Install with Azure CLI Dev Tools](#install-via-azdev)\n3. [Install manually (no backend)](#manual-install)\n4. [Quick Start](#quick-start)\n5. [Notes](#notes)\n6. [Contributing](#contributing)\n7. [Trademarks](#trademarks)\n\n---\n\n## Features\n- Integrates with Azure Open AI allowing the creation of components from a description alone.\n- Can convert a react component into an Azure API Management Widget.\n- Can be tested standalone using ladle which is built into the toolkit stack.\n- Unit and code coverage testing through Jest (including snapshot testing).\n- Uses css as code through Styled Components out of the box.\n- Includes a few example components including a markdown viewer, force-graph-3d component, etc.\n- Can create a new basic component with tests to scaffold a new component (or you can use AI and create a basis for a new one)\n- Automatically adds dependencies for created components.\n- Includes rollup for packaging components.\n\n## Install via azdev\n\n#### \u26a0\ufe0f Note\n```\nAutomatic Install uses azdev and includes installing Azure OpenAI and Azure APIM on your subscription.\n```\n\nPlease use the Official Azure Samples github instructions to install and configure the toolkit via azdev.\n\n[Azure Samples - react-component-toolkit](https://github.com/Azure-Samples/react-component-toolkit-openai-demo)\n\n## Manual Install\n\n#### \u26a0\ufe0f Note\n```\nManual installation requires .env file to be configured to enable Azure Open AI/Open AI and APIM Widget functionality.\nAn example .env.empty file is included, fill out the correct values and copy/rename to .env.\n```\n\nPre-requisites: Latest stable release of Node.js\n\nInstallation:\n\n1. Clone this repo and run ```node install.mjs``` in the root.\n\n## Quick Start\n\n* ```npm run ladle:dev``` to get started and see current component running with debugging available.\n* ```npm run ladle:prod``` to get started and see current components running with a full production build.\n* ```npm run cleanup``` - clear dist, build_artifacts and unittest coverage results.\n* ```npm run rollup``` - runs ```rollup -c```\n* ```npm run build``` - run ```npm cleanup``` followed by ```npm rollup```.\n* ```npm run test``` - run unit tests from all components (src/unittests).\n* ```npm run createnew [component_description]``` - create an AI generated component using Azure Open AI (configure via .env)\n* ```npm run createtemplate [component_name]``` - to create a new ready to run component template with stories and unit tests.\n* ```npm run removecomponent [component_name]``` - to delete a component and associated stories and unit tests.\n* ```npm run packagewidget [existing_component_name]``` - to package a component as a widget for Azure API Management Developer Portal.\n* ```npm run test (or npm test)``` - run all component unit tests\n* ```npm run test:update``` - update all snapshots for unit tests\n* ```npm run test:watch``` - run jest in watch mode\n\n## Notes\n\n- This project uses npm, however the initial package.json is dynamically created by running node install.mjs (see below)\n- package.json is dynamically created is to ensure that each component defines it's own dependencies, these dependencies are merged for testing in ladle, unit tests and using rollup to package the entire project\n- If you create the component manually:\n  - A package.json should be placed in the components directory which contains only name, version and its dependencies, devDependencies and peerDependencies (see any component under src/components for an example)\n- You must run npm updatepackages to cause an update to the root package.json.\n- If you use ```npm createnew [component_name]``` to create a new component, ```npm updatepackages``` will happen for you the first time.\n- In both cases, any time you add or remove npm dependencies you should add them to the component package.json under the component you are working on, or common/package.json if those dependencies are toolkit wid\ne, after doing that you should run ```npm updatepackages```\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "react-component-toolkit", "org_name": "microsoft", "org_repo": "microsoft/react-component-toolkit", "platform_org_repo": "github+microsoft/react-component-toolkit", "link_to_repo": "https://github.com/microsoft/react-component-toolkit", "platform": "github", "language": "JavaScript", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Robust Confidence Sequences\n\nThis project contains example notebooks exhibiting confidence sequences that are robust, i.e., converge for observations with infinite variance.\n\n  * [Robust Mean Demo](csrobustmean.ipynb): The basic technique for covering the running conditional mean in a nonstationary environment.  Includes the use of approximate sufficient statistics to bound space and time complexity.\n  * [Off-Policy Quantile Demo](csnsopquantile.ipynb): Off-policy quantile identification functions can exhibit infinite variance (unlike the on-policy case).  In this case the importance weights are Pareto distributed with infinite variance: this might arise in a continuous action problem.\n  * [Expectile Demo](csnsexpectile.ipynb): Expectile identification functions can exhibit heavy tails.  In this case the observation is Lognormal distributed.\n\n## Key Papers\n\n  * [A lower confidence sequence for the changing mean of non-negative right heavy-tailed observations with bounded mean](https://arxiv.org/abs/2210.11133): describes the techniques in this repository.\n  * [Anytime-valid off-policy inference for contextual bandits](https://arxiv.org/abs/2210.10768): contains important background material on anytime valid off policy inference.  This repository essentially extends that paper to support adapting to an unknown moment $q \\in (1, 2]$.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "csrobust", "org_name": "microsoft", "org_repo": "microsoft/csrobust", "platform_org_repo": "github+microsoft/csrobust", "link_to_repo": "https://github.com/microsoft/csrobust", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mixedreality.cloudlfs", "org_name": "microsoft", "org_repo": "microsoft/mixedreality.cloudlfs", "platform_org_repo": "github+microsoft/mixedreality.cloudlfs", "link_to_repo": "https://github.com/microsoft/mixedreality.cloudlfs", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Attack Simulation and Training\n\nWelcome to the Attack Simulation and Training repro:\n\nHere you will find some samples, scripts, tools and other pieces of information that we feel as a product group\nwill help you get the most out of our feature Attack Simulation and Training.\n\n\n\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "attacksimulationandtraining", "org_name": "microsoft", "org_repo": "microsoft/attacksimulationandtraining", "platform_org_repo": "github+microsoft/attacksimulationandtraining", "link_to_repo": "https://github.com/microsoft/attacksimulationandtraining", "platform": "github", "language": "PowerShell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "Project Mu UEFI Variables\r\n=========================\r\n\r\nThis repository contains a prototype for a new UEFI variable design for modern PCs.\r\n\r\nIt is extensible, secure, with built-in testing up and down the entire stack. The design and implementation are all\r\ntaking place here in this repo.\r\n\r\nProject Mu Top-Level information\r\n________________________________\r\n\r\nThis repository is part of Project Mu. Please see Project Mu for details: https://microsoft.github.io/mu.\r\n\r\nContributing\r\n============\r\n\r\nWe welcome everyone to file feature requests, bugs, participate in code reviews, submit code, update documentation,\r\nand help us build the best variable driver possible.\r\n\r\nAt this time, we are very early in the work so we're particularly interested in ideas around the future of UEFI\r\nvariables (including breaking UEFI specification compatibility) and suggestions to help shape the overall design.\r\n\r\nPlease open GitHub issues directly in this repo.\r\n\r\nBackground\r\n==========\r\n\r\nThe UEFI Specification describes an interface between the operating system (OS) and platform firmware. A UEFI\r\nSpecification compliant system must implement two high-level sets of services - Boot Services which consist of\r\nfunctions available prior to a successful call to ``EFI_BOOT_SERVICES.ExitBootServices()`` and Runtime Services which\r\nconsist of functions that are available before and after any call to ``EFI_BOOT_SERVICES.ExitBootServices()``.\r\n\r\nA fundamental Runtime Service is called the UEFI variable services. These services are comprised of an API that the\r\nplatform firmware must implement to satisfy the relevant API requirements defined in the UEFI Specification. While the\r\nunderlying implementation is platform-specific, the callers will include both the operating system and firmware\r\ncomponents.\r\n\r\nMotivation\r\n==========\r\n\r\nThe de facto open-source implementation of UEFI, `TianoCore`_, provides a commonly used set of `UEFI variable drivers`_\r\nin the `edk2`_ project that has served as the industry standard implementation for UEFI variable services for over a\r\ndecade. Over time, the UEFI variable driver has substantially grown in complexity to support an increasing number of\r\nfeatures.\r\n\r\nThe TianoCore driver is now over 15 years old. It's design is rigid and not accommodating to change. Over the span of\r\nits lifetime, many advancements have occurred in the PC industry that require better scale to support:\r\n\r\n1. New storage technologies have come to market\r\n2. Device trends have shifted to low-power ultra mobile devices and cloud server systems\r\n3. New offload engines like BMC and special security processors have become more common to process non-volatile data\r\n4. New expectations around device security have come into focus\r\n\r\n   - For example, resistance against physical attack has led to variable data confidentiality via encryption, data\r\n     integrity checks for tamper-proof storage guarantees, data replay protection, etc.\r\n5. Additional computer architectures have gained popularity such as AArch64 and RISC-V\r\n6. Operating systems have evolved and so have their security expectations\r\n\r\nThe TianoCore driver was written for a PI-centric boot flow assuming it was writing to SPI flash with no structured\r\ndesign to support extending the driver to support these advancements. In addition, while some industry standard tests\r\nsuch as the UEFI Self-Certification Tests exist, much of the stack is error prone to modify and difficult to assess\r\nbecause of its accumulated technical debt.\r\n\r\nDue to the importance of the driver, we concluded that a new design that takes into account these requirements with\r\ntesting built in could better support today's needs.\r\n\r\nCode of Conduct\r\n===============\r\n\r\nThis project has adopted the Microsoft Open Source Code of Conduct https://opensource.microsoft.com/codeofconduct/\r\n\r\nFor more information see the Code of Conduct FAQ https://opensource.microsoft.com/codeofconduct/faq/\r\nor contact `opencode@microsoft.com <mailto:opencode@microsoft.com>`_. with any additional questions or comments.\r\n\r\nBuilds\r\n======\r\n\r\nPlease follow the steps in the Project Mu docs to build for CI and local testing.\r\n`More Details <https://microsoft.github.io/mu/CodeDevelopment/compile/>`_\r\n\r\nCopyright & License\r\n===================\r\n\r\nSome files in this repository have their own copyright. Otherwise, the following copyright applies.\r\n\r\n| Copyright (C) Microsoft Corporation\r\n| SPDX-License-Identifier: BSD-2-Clause-Patent\r\n\r\n.. _edk2: https://github.com/tianocore/edk2\r\n.. _TianoCore: https://www.tianocore.org/\r\n.. _UEFI variable drivers: https://github.com/tianocore/edk2/tree/master/MdeModulePkg/Universal/Variable\r\n", "repo_name": "mu_feature_uefi_variable", "org_name": "microsoft", "org_repo": "microsoft/mu_feature_uefi_variable", "platform_org_repo": "github+microsoft/mu_feature_uefi_variable", "link_to_repo": "https://github.com/microsoft/mu_feature_uefi_variable", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "codal-jacdac", "org_name": "microsoft", "org_repo": "microsoft/codal-jacdac", "platform_org_repo": "github+microsoft/codal-jacdac", "link_to_repo": "https://github.com/microsoft/codal-jacdac", "platform": "github", "language": "C++", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Entra Permission Management Price Estimator\n\nThese estimators help estimate indictive price for onboarding your cloud service(s) to Entra Permissions Management (CloudKnox). Please click on the link for your cloud service to get started.\n\n- [EPM Estimator for Azure](https://github.com/microsoft/EPMEstimator/blob/main/Azure/Estimation.md)\n- [EPM Estimator for GCP](https://github.com/microsoft/EPMEstimator/blob/main/GCP/Estimation.md)\n- [EPM Estimator for AWS](https://github.com/microsoft/EPMEstimator/blob/main/AWS/Estimation.md)\n\n<blockquote>\n    <p>Note that the these calculators only provide a ballpark estimate based on list pricing. Please talk to your Microsoft account team for price / discounts applicable to you.</p>\n</blockquote>\n\n", "repo_name": "EPMEstimator", "org_name": "microsoft", "org_repo": "microsoft/EPMEstimator", "platform_org_repo": "github+microsoft/EPMEstimator", "link_to_repo": "https://github.com/microsoft/EPMEstimator", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# BBS Reference Implementation\n\n*WORK IN PROGRESS*\n\nTypeScript reference implementation for the [BBS signature scheme](https://github.com/decentralized-identity/bbs-signature). The goal is to help understand and verify the specification. This is NOT a production-ready implementation; testing is minimal and no effort is made to optimize and protect against specialized attacks (e.g., side-channel resistance). \n\nThis project aims to keep up to date with the [latest specification](https://identity.foundation/bbs-signature/draft-looker-cfrg-bbs-signatures.html), but may be behind since the specification changes often; the current implementation matches the *11 March 2023* version of the specification, matching the [draft-irtf-cfrg-bbs-signatures-02](https://datatracker.ietf.org/doc/draft-irtf-cfrg-bbs-signatures/02/) version submitted to the CFRG.\n\nGiven the rapid evolution of the BBS scheme, there might be inconsistencies between the specification and the code; please open issues or file PRs if you find any!\n\n## Known issues\n\nThis library is a work in progress. Here are some known issues:\n* Only the BLS12-381-SHA-256 ciphersuite is currently available\n\n## Setup\n\nMake sure [node.js](https://nodejs.org/) and [npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) are installed on your system; the latest Long-Term Support (LTS) version is recommended for both.\n\n1. Get the source, for example using `git`\n```\ngit clone -b main https://github.com/microsoft/bbs-node-reference.git\ncd bbs-node-reference\n```\n\n2. Build the `npm` package\n```\nnpm install\nnpm run build\n```\n\n3. Optionally, run the unit tests\n\n```\nnpm test\n```\n\n4. Optionally, run the sample program\n\n```\nnpm run bbs\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bbs-node-reference", "org_name": "microsoft", "org_repo": "microsoft/bbs-node-reference", "platform_org_repo": "github+microsoft/bbs-node-reference", "link_to_repo": "https://github.com/microsoft/bbs-node-reference", "platform": "github", "language": "TypeScript", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Microsoft Azure Data Manager for Energy Power BI Connector\n\n## About\n\nThis project is the Microsoft Azure Data Manager for Energy Power BI connector. The connector is used to query data from a Azure Data Manager for Energy instance and display it in Power BI reports.\n\n## Using the Connector\n\nFollow [Setup](./Documentation/Setup.md) for steps on using the connector. For developing & contributing to the connector follow the steps starting at [Development Machine Setup](#development-machine-setup).\n\n## Development Machine Setup\n\nYou will need Visual Studio 2019 and the Power Query SDK to compile the project.\n\n1. Install Visual Studio 2019\n1. Install the [Power Query SDK](https://marketplace.visualstudio.com/items?itemName=Dakahn.PowerQuerySDK) from the Visual Studio Marketplace\n\n## Testing the Connector\n\nTo test the connector you must configure your Azure Data Manager for Energy instance, and connector first.\n\n### Configuring Azure Data Manager for Energy\n\nAdd the below URI as a Single Page Application (SPA) redirect URI to your AD Application\n\n    https://oauth.powerbi.com/views/oauthredirect.html\n\n### Configuring the Connector\n\nProvide a client ID, tenant ID, Azure Data Manager for Energy instance name, and data partition ID in [MicrosoftAzureDataManagerForEnergy.query.pq](./MicrosoftAzureDataManagerForEnergy/MicrosoftAzureDataManagerForEnergy/MicrosoftAzureDataManagerForEnergy.query.pq). Once this is done, you can run the connector by pressing the Run button or F5.\n\n### Using the Sample Report\n\nThere is a sample template report that can be used to test your connection. The connection details need to be updated with the following steps:\n\n1. Open [Microsoft Azure Data Manager for Energy Wells Template.pbit](./Reports/Microsoft%20Azure%20Data%20Manager%20for%20Energy%20Wells%20Template.pbit)\n1. Close the authentication prompt\n1. Close the error dialog\n1. Edit the 'Wells' query\n1. Edit the 'Source' step\n1. Input your client ID, tenant ID, Azure Data Manager for Energy instance name, and data partition ID\n1. Click 'OK'\n1. Click 'Close & Apply'\n\n## Architecture\n\nThere are several pieces of the connector that warrant explanation: authentication, paging, and unit tests\n\n### Authentication\n\nThe connector makes use of [Proof Key for Code Exchange (PKCE)](AdjustPageSizeDependingOnUsersLimit) to handle OAuth authentication. PKCE gets rid of the need for client secrets, and instead uses code challenges and verifiers. The Azure AD application needs to be configured with a Single Page Application (SPA) authentication platform to support PKCE - Web authentication platforms do not support PKCE. More information can be found in the [RFC](https://www.rfc-editor.org/rfc/rfc7636).\n\n#### Redirect URI\n\nThe `redirectUri` variable can be any valid URI as long as the Azure AD Applications redirect URI matches it. However, for the connector to support on-premises data gateways, the `redirectUri` must be 'https://oauth.powerbi.com/views/oauthredirect.html'. More information can be found at [Handling Gateway Support](https://learn.microsoft.com/power-query/handlinggatewaysupport), [OAuth and Power BI](https://learn.microsoft.com/power-query/samples/github/readme#oauth-and-power-bi), and [Handling Gateway Support page does not specify that you need a specific redirect URI for the gateway to work](https://github.com/MicrosoftDocs/powerquery-docs/issues/284).\n\n### Paging\n\nAzure Data Manager for Energy only returns a maximum of 1000 records from [query_with_cursor](https://community.opengroup.org/osdu/platform/system/search-service/-/blob/master/docs/tutorial/SearchService.md#query-with-cursor). To get around this limitation, the connector uses the [Table.GenerateByPage](https://learn.microsoft.com/power-query/helperfunctions#tablegeneratebypage) helper method to retrieve all records. More information can be found on [Handling Paging](https://learn.microsoft.com/power-query/handlingpaging).\n\n#### Paging Behavior\n\nThe logic that determines when to stop fetching pages is contained in `RetrievedAllPages`. This function will continue to retrieve pages until all items have been retrieved, or the user's optional limit is met.\n\n#### Page Size\n\nThe default page size is 100 items, but it can be configured by adjusting the `pageSize` variable. There are scenarios where the `pageSize` needs to be adjusted, which is handled in `AdjustPageSizeDependingOnUsersLimit`. The `pageSize` will be reduced if a user provides a limit and it's less than the `pageSize`.\n\nThe query_with_cursor API does not respect changing `limits` (i.e. it will use whatever limit was specified when the cursor was created). If the final page size needs to be reduced to satisfy a user's limit, then logic in `GetSubsetOfRecordsIfNecessary` will keep only a subset of records from the final page.\n\n### Unit Tests\n\nThere are a handful of unit tests in the project that leverage the [unit test helper code](https://github.com/microsoft/DataConnectors/blob/master/samples/UnitTesting/UnitTesting.query.pq). You'll need to authenticate & set credentials twice before running the tests, this is because there are tests with differing query parameters. The unit tests will fail to run if you change the kind parameter. The unit tests will need refactoring to handle different kind parameters.\n\n## Possible Features for the Future\n\nThe connector supports basic search functionality, but there are some areas of improvement:\n\n### Support all search API arguments\n\nThe Search API has a number of parameters to it: kind, query, offset, limit, sort, queryAsOwner, spatialFilter, trackTotalCount, aggregateBy, and returnedFields. The connector could be extended to support sort, queryAsOwner, spatialFilter, trackTotalCount, and aggregateBy. It currently makes use of kind, query, offset, limit, and returnedFields.\n\n### Improve returnedRecords format\n\nPassing values to the returnedFields parameter is not the most user-friendly since each field needs to be enclosed in quotes (e.g. \"data.FacilityName\", \"data.Source\"). The connector could be modified to automatically add quotations around each field if the user doesn't include them.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-data-manager-for-energy-power-bi-connector", "org_name": "microsoft", "org_repo": "microsoft/azure-data-manager-for-energy-power-bi-connector", "platform_org_repo": "github+microsoft/azure-data-manager-for-energy-power-bi-connector", "link_to_repo": "https://github.com/microsoft/azure-data-manager-for-energy-power-bi-connector", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Welcome to the JDK!\n\nFor build instructions please see the\n[online documentation](https://openjdk.java.net/groups/build/doc/building.html),\nor either of these files:\n\n- [doc/building.html](doc/building.html) (html version)\n- [doc/building.md](doc/building.md) (markdown version)\n\nSee <https://openjdk.java.net/> for more information about\nthe OpenJDK Community and the JDK.\n", "repo_name": "openjdk-jdk17u", "org_name": "microsoft", "org_repo": "microsoft/openjdk-jdk17u", "platform_org_repo": "github+microsoft/openjdk-jdk17u", "link_to_repo": "https://github.com/microsoft/openjdk-jdk17u", "platform": "github", "language": "Java", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# TCP Trace - Hackathon Project\n\nTCP Trace is a PoC implementation for the [Hackathon Idea](https://hackbox.microsoft.com/project/772). The idea is to detect network errors by tracing internal TCP state on Linux:\n\n>This project aims to isolate network-related errors by tracing the server-side TCP Retransmits at the node level. The project will keep retransmit counts per service >identifier like cgroup and port. It will allow early detection of network-related issues, which will help during an incident, and, in some cases, help prevent customer-impacting problems.\n\nTCP Trace uses three [eBPF](https://ebpf.io/what-is-ebpf/) programs to hook into various **attach points** in the Linux kernel:\n\n- tc ingress: Number of incoming TCP packets.\n- tc egress: Number of outgoing TCP packets.\n- tcp_retransmit_skb: Number of TCP retransmits.\n\n## Build\n\nTCP Trace consists of two parts; eBPF objects (programs, maps) and a user-space program. User-space program uses [libebpf-bootsrap](https://github.com/libbpf/libbpf-bootstrap) to load and attach eBPF objects into the kernel. After being loaded, eBPF programs and maps completely run in the kernel space (crazy efficient!). Since eBPF maps are reachable from the userspace, It is very easy to share data between eBPF programs and the user-space application.\n\n![image](https://user-images.githubusercontent.com/10650777/188182966-2f855f51-b506-4555-a220-9359c99d282d.png)\n\n\n* Install build dependencies:\n\n  ```\n  sudo apt install clang llvm pkg-config libelf1 libelf-dev zlib1g-dev\n  ```\n* Install submodule dependencies (libbpf-bootstrap) and build\n\n  ```\n  make init && make\n  ```\n* Run the PoC binary\n  ```\n  sudo ./src/tcp_tracing\n  ```\n\n\n\n", "repo_name": "tcptrace", "org_name": "microsoft", "org_repo": "microsoft/tcptrace", "platform_org_repo": "github+microsoft/tcptrace", "link_to_repo": "https://github.com/microsoft/tcptrace", "platform": "github", "language": "C", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Azure Pipelines Task Codecoverage SDK\n\nCodecoverage libraries for [Azure Pipelines](https://azure.microsoft.com/en-us/services/devops/pipelines/) tasks.\n\n## Status\n\n|   | Build & Test |\n|---|:-----:|\n|![Win-x64](res/win_med.png) **Windows**|[![Build & Test][win-build-badge]][build]| \n|![macOS](res/apple_med.png) **macOS**|[![Build & Test][macOS-build-badge]][build]| \n|![Linux-x64](res/ubuntu_med.png) **Linux**|[![Build & Test][linux-build-badge]][build]|\n\n[win-build-badge]: https://dev.azure.com/mseng/PipelineTools/_apis/build/status/azure-pipelines-tasks-coverage-tools?branchName=main&jobname=windows\n[macOS-build-badge]: https://dev.azure.com/mseng/PipelineTools/_apis/build/status/azure-pipelines-tasks-coverage-tools?branchName=main&jobname=macOS\n[linux-build-badge]: https://dev.azure.com/mseng/PipelineTools/_apis/build/status/azure-pipelines-tasks-coverage-tools?branchName=main&jobname=linux\n[build]: https://dev.azure.com/mseng/PipelineTools/_build/latest?definitionId=13927\n\n[![NPM version][npm-lib-image]][npm-lib-url]\n\n[npm-lib-image]: https://img.shields.io/npm/v/azure-pipelines-tasks-codecoverage-tools.svg?style=flat\n[npm-lib-url]: https://www.npmjs.com/package/azure-pipelines-tasks-codecoverage-tools\n\n# Build\n\nOnce:  \n```bash\n$ npm install\n```\n\nBuild:  \n```bash\n$ npm run build\n```\n\nThe tool cache will be in the `_build` folder.  To clear the cache, build again.\n\n# Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Security issues\n\nDo you think there might be a security issue? Have you been phished or identified a security vulnerability? Please don't report it here - let us know by sending an email to secure@microsoft.com.", "repo_name": "azure-pipelines-tasks-coverage-tools", "org_name": "microsoft", "org_repo": "microsoft/azure-pipelines-tasks-coverage-tools", "platform_org_repo": "github+microsoft/azure-pipelines-tasks-coverage-tools", "link_to_repo": "https://github.com/microsoft/azure-pipelines-tasks-coverage-tools", "platform": "github", "language": "TypeScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Azure Data Manager for Energy - Samples\n\nThis repo contains samples for Azure Data Manager for Energy APIs.\n\n# REST API Reference\n\n* [CRS Catalog Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/crs_catalog_v2_openapi.yaml)\n* [CRS Conversion Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/crs_converter_openapi.yaml)\n* [Dataset Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/dataset_openapi.yaml)\n* [Entitlements Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/entitlements_openapi.yaml)\n* [File Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/file_service_openapi.yaml)\n* [Indexer Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/indexer_openapi.yaml)\n* [Legal Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/compliance_openapi.yaml)\n* [Notification Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/notification_openapi.yaml)\n* [Partition Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/partition_openapi.yaml)\n* [Register Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/register_openapi.yaml)\n* [Schema Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/schema_openapi.yaml)\n* [Search Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/search_openapi.yaml)\n* [Seismic DDMS Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M12/seismic_ddms_openapi.yaml)\n* [Storage Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/storage_openapi.yaml)\n* [Unit Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/unit_service_v3_openapi.yaml)\n* [Wellbore DDMS Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M12/wellbore_ddms_openapi.yaml)\n* [Well Delivery DDMS Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M12/welldelivery_ddms_openapi.yaml)\n* [Workflow Service](/adme-samples/rest-apis/index.html?page=/adme-samples/rest-apis/M14/workflow_openapi.yaml)\n* [Petrel-DDMS Service](https://github.com/microsoft/adme-samples/blob/main/rest-apis/M12/petrel_ddms_openapi.yaml)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "adme-samples", "org_name": "microsoft", "org_repo": "microsoft/adme-samples", "platform_org_repo": "github+microsoft/adme-samples", "link_to_repo": "https://github.com/microsoft/adme-samples", "platform": "github", "language": "HTML", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Azure Synapse End-End Demo\n\nThis repository provides one-click infrastructure and artifact deployment for Azure Synapse Analytics to get you started with Big Data Analytics on a \nlarge sized Health Care sample data. You will learn how to ingest, process, and serve large volumes of data using various components of Synapse.\n\n## Reference Architecture\n![image](https://user-images.githubusercontent.com/59613090/192642933-23285334-d36c-40e7-8fc1-2e3ed9006ba0.png)\n\n## CONTENTS\n* [Exercise 00 - Setup](Exercise00-Setup/README.md)\n* [Exercise 01 - Claims](Exercise01-Claims/README.md)\n* [Exercise 02 - Observations](Exercise02-Observations/README.md)\n* [Exercise 03 - Patients](Exercise03-Patients/README.md)\n* [Troubleshooting](Troubleshooting/Readme.md)\n", "repo_name": "AzureSynapseEndToEndDemo", "org_name": "microsoft", "org_repo": "microsoft/AzureSynapseEndToEndDemo", "platform_org_repo": "github+microsoft/AzureSynapseEndToEndDemo", "link_to_repo": "https://github.com/microsoft/AzureSynapseEndToEndDemo", "platform": "github", "language": null, "stargazers_count": 55, "watchers_count": 55}, {"README_text": "# Python Project Template\n\nThis project follows the Python Standards declared in [PEP 621](https://peps.python.org/pep-0621/).\nThis uses a pyproject.yaml to configuration the project. In this example, [flit](https://pypi.org/project/flit/) is used to simplify the build process, and publish to pypi.\n\n## Project Organization\n\n- .devcontainer - This directory contains required files for creating a [Codespace](https://github.com/features/codespaces).\n- .github\n  - workflows - Contains GitHub Actions used for building, testing and publishing.\n    - publish-test.yml - Publish wheels to [https://test.pypi.org/](https://test.pypi.org/)\n    - publish.yml - Publish wheels to [https://pypi.org/](https://pypi.org/)\n    - pull-request.yml - Build and Test pull requests before commiting to main.\n    - template-sync.yml - Update GitHub Repo with enhancments to base template\n- docs - collect documents (default format .md)\n- src - place new source code here\n  - python_package - sample package (this can be deleted when creating a new repository)\n- tests - contains Python based test cases to validation src code\n- .pre-commit-config.yaml - Contains various pre-check fixes for Python\n- .templateversionrc - used to track template version usage.\n- MANIFEST.in - Declares additional files to include in Python whl\n- pyproject.toml - Python Project Declaration\n- ws.code-workspace - Recommended configurations for [Visual Studio Code](https://code.visualstudio.com/)\n\n## pyproject.toml\n\nThe following sections are defined in the configuration toml.\n\n- build-system\n- project\n  - optional-dependencies\n  - urls\n- tool\n  - bandit\n  - coverage\n    - run\n    - report\n  - pyright\n  - pytest\n  - tox\n  - pylint\n    - MESSAGES CONTROL\n    - REPORTS\n    - REFACTORING\n    - BASIC\n    - FORMAT\n    - LOGGING\n    - MISCELLANEOUS\n    - SIMILARITIES\n    - SPELLING\n    - STRING\n    - TYPECHECK\n    - VARIABLES\n    - CLASSES\n    - DESIGN\n    - IMPORTS\n    - EXCEPTIONS\n\n### build-system\nTODO: add info on flit configuration\n\n### project\nThis section defines the project metadata, which may have been previously contained in a setup.py file.\n\n#### optional-dependencies\nThis are otpimal dependancey groups that can be installed via 'pip install .[tests]'.\nOne group is included for dependancies required for testing. A second group is included for PySpark based dependancies.\n\n### tool\nThis section defines the configurations for additional tools used to format, lint, type-check, and analysis Python code.\n\n#### bandit\nPerforms Security Static Analysis checks on code base.\n\n#### black\nAuto-formats code\n\n#### coverage\nConfigures code coverage reports generatated during testing.\n\n#### pyright\nPerforms static type checking on Python.\n\n#### pytest\nConfigures various test markers used during testing.\n\n#### pylint\nPerforms Linting and Static Analysis. Any modifictions made by the auto-formater (black) are always considered correct.\n\n## Publish to PyPi from GitHub\nIn order to publish to PyPi, a repostirory secret must be created, \"PYPI_PASSWORD\". In order to publish to the Test PyPi, a second secret must be added, \"TEST_PYPI_PASSWORD\". \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "pytest-azure", "org_name": "microsoft", "org_repo": "microsoft/pytest-azure", "platform_org_repo": "github+microsoft/pytest-azure", "link_to_repo": "https://github.com/microsoft/pytest-azure", "platform": "github", "language": "Python", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# action-python\n[![Test](https://github.com/microsoft/action-python/workflows/Test/badge.svg)](https://github.com/microsoft/action-python/actions?query=workflow%3ATest)\n[![reviewdog](https://github.com/microsoft/action-python/workflows/reviewdog/badge.svg)](https://github.com/microsoft/action-python/actions?query=workflow%3Areviewdog)\n[![depup](https://github.com/microsoft/action-python/workflows/depup/badge.svg)](https://github.com/microsoft/action-python/actions?query=workflow%3Adepup)\n[![release](https://github.com/microsoft/action-python/workflows/release/badge.svg)](https://github.com/microsoft/action-python/actions?query=workflow%3Arelease)\n[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/microsoft/action-python?logo=github&sort=semver)](https://github.com/microsoft/action-python/releases)\n[![action-bumpr supported](https://img.shields.io/badge/bumpr-supported-ff69b4?logo=github&link=https://github.com/haya14busa/action-bumpr)](https://github.com/haya14busa/action-bumpr)\n\nThis repo contains a action to run various Python tools including:\n- [bandit](https://pypi.org/project/bandit)\n- [black](https://pypi.org/project/black)\n- [flake8](https://pypi.org/project/flake8)\n- [pylint](https://pypi.org/project/pylint)\n- [pyright](https://pypi.org/project/pyright)\n- [pytest](https://pypi.org/project/pytest)\n\n## Input\n\n```yaml\ninputs:\n  black:\n    description: |\n      Run Black\n      Default is false.\n    default: false\n  bandit:\n    description: |\n      Run Bandit\n      Default is false.\n    default: false\n  pylint:\n    description: |\n      Run Pylint\n      Default is false.\n    default: false\n  pyright:\n    description: |\n      Run Pyright\n      Default is false.\n    default: false\n  flake8:\n    description: |\n      Run Flake8\n      Default is false.\n    default: false\n  testing:\n    description: |\n      Run tests with PyTest\n      Default is false.\n    default: false\n  publish:\n    description: |\n      Publish to PyPi\n      Default is false\n    default: false\n  publish_url:\n    description: |\n      PyPi Target. Use this to point to private or test locations.      \n      Default https://pypi.org\n    defualt: 'https://pypi.org'\n  github_token:\n    description: 'GITHUB_TOKEN'\n    default: '${{ github.token }}'\n  workdir:\n    description: 'Working directory relative to the root directory.'\n    default: 'src'\n  ### Flags for reviewdog ###\n  level:\n    description: 'Report level for reviewdog [info,warning,error]'\n    default: 'error'\n  reporter:\n    description: 'Reporter of reviewdog command [github-pr-check,github-pr-review].'\n    default: 'github-pr-check'\n  filter_mode:\n    description: |\n      Filtering mode for the reviewdog command [added,diff_context,file,nofilter].\n      Default is added.\n    default: 'added'\n  fail_on_error:\n    description: |\n      Exit code for reviewdog when errors are found [true,false]\n      Default is `false`.\n    default: 'false'\n  reviewdog_flags:\n    description: 'Additional reviewdog flags'\n    default: ''\n  toml:\n    description: |\n      pyproject.toml location.\n      Default pyproject.toml\n    default: 'pyproject.toml'\n  pylint_rc:\n    description: '.pylintrc configuration file'\n    default: '.pylintrc'\n```\n\n## Usage\n\n```yaml\nname: Pull Request\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  linting:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Black\n        uses: microsoft/action-python@0.7.0\n        with:\n          black: true\n\n      - name: Bandit\n        uses: microsoft/action-python@0.7.0\n        with:          \n          bandit: true\n\n      - name: Pylint\n        uses: microsoft/action-python@0.7.0\n        with:\n          pylint: true\n          \n      - name: Pyright\n        uses: microsoft/action-python@0.7.0\n        with:          \n          pyright: true\n          \n      - name: Flake8\n        uses: microsoft/action-python@0.7.0\n        with:          \n          flake8: true\n\n  testing:\n    runs-on: ubuntu-latest\n    steps:    \n      - name: Pytest\n        uses: microsoft/action-python@0.7.0\n        with:          \n          testing: true\n```\n\n## Development\n\n### Release\n\n#### [haya14busa/action-bumpr](https://github.com/haya14busa/action-bumpr)\nYou can bump version on merging Pull Requests with specific labels (bump:major,bump:minor,bump:patch).\nPushing tag manually by yourself also work.\n\n#### [haya14busa/action-update-semver](https://github.com/haya14busa/action-update-semver)\n\nThis action updates major/minor release tags on a tag push. e.g. Update v1 and v1.2 tag when released v1.2.3.\nref: https://help.github.com/en/articles/about-actions#versioning-your-action\n", "repo_name": "action-python", "org_name": "microsoft", "org_repo": "microsoft/action-python", "platform_org_repo": "github+microsoft/action-python", "link_to_repo": "https://github.com/microsoft/action-python", "platform": "github", "language": null, "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-gaming-devops", "org_name": "microsoft", "org_repo": "microsoft/azure-gaming-devops", "platform_org_repo": "github+microsoft/azure-gaming-devops", "link_to_repo": "https://github.com/microsoft/azure-gaming-devops", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Form-Recognizer-Toolkit", "org_name": "microsoft", "org_repo": "microsoft/Form-Recognizer-Toolkit", "platform_org_repo": "github+microsoft/Form-Recognizer-Toolkit", "link_to_repo": "https://github.com/microsoft/Form-Recognizer-Toolkit", "platform": "github", "language": "TypeScript", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# react-text-annotator\n\nreact-text-annotator is a labeler component that:\n\n-   Lays out text and overlays textual decorations like labels, predictions, and relations based on given data.\n-   Handles user interactions on tokens.\n-   Is extensible to allow for custom rendering of tokens and decoration overlays.\n-   Is accessible to use with full keyboard interactions.\n\nThis labeler is character tokenized, meaning that it will break all text sent in its props to character level tokens. Tokens are split to lines based on line breaks in the original text and the maximum number of characters allowed in each line. The labeler also ensures that contiguous words in the original text are not split across multiple lines.\n\n\n# Examples\nHere are some example of how to use the labeler component:\n\n```js\n    const annotations: AnnotationData[] = [\n        {\n            id: 'id1',\n            color: 'red',\n            endToken: 15,\n            startToken: 5,\n            name: 'label',\n            kind: 'label'\n        },\n        {\n            id: 'id2',\n            color: 'blue',\n            endToken: 25,\n            startToken: 10,\n            name: 'relation',\n            kind: 'relation'\n        }\n    ];\n\n    const labelerText = 'This is sample text to test the labeler functionality.';\n\n    return <Labeler text={labelerText} annotations={annotations} />;\n```\nthe result is:\n\n![result-1](src/labeler/docs/labeler-result-1.png)\n\n--- \n\n```js\n const annotations: AnnotationData[] = [\n        {\n            id: 'id1',\n            color: 'red',\n            endToken: 15,\n            startToken: 2,\n            name: 'label',\n            kind: 'label'\n        }\n    ];\n\n    const labelerText = `\u0627\u0633\u0645\u064a \u0645\u062d\u0645\u062f \u0648 \u0627\u0639\u0645\u0644 \u0644\u062f\u0649 \u0634\u0631\u0643\u0629 \u0645\u064a\u0643\u0631\u0648\u0633\u0648\u0641\u062a`;\n\n    return <Labeler text={labelerText} annotations={annotations} labelerConfigs={{ isRtl: true, tokenizationType: 'word' }} />;\n```\n\nthe result is:\n\n![result-2](src/labeler/docs/labeler-result-2.png)", "repo_name": "react-text-annotator", "org_name": "microsoft", "org_repo": "microsoft/react-text-annotator", "platform_org_repo": "github+microsoft/react-text-annotator", "link_to_repo": "https://github.com/microsoft/react-text-annotator", "platform": "github", "language": "TypeScript", "stargazers_count": 33, "watchers_count": 33}, {"README_text": "# Formatter extension for Visual Studio Code using `autopep8`\n\nA Visual Studio Code extension with support for the `autopep8` formatter. The extension ships with `autopep8=2.0.2`.\n\nNote:\n\n-   This extension is supported for all [actively supported versions](https://devguide.python.org/#status-of-python-branches) of the `python` language (i.e., python >= 3.7).\n-   The bundled `autopep8` is only used if there is no installed version of `autopep8` found in the selected `python` environment.\n-   Minimum supported version of `autopep8` is `1.7.0`.\n-   This extension is experimental. The plan is that it will eventually replace the `autopep8` formatting functionality of [the Python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python).\n\n## Usage\n\nOnce installed in Visual Studio Code, \"autopep8\" will be available as a formatter for python files. Please select \"autopep8\" (extension id:`ms-python.autopep8`) as the default formatter. You can do this either by using the context menu (right click on a open python file in the editor) and select \"Format Document With...\", or you can add the following to your settings:\n\n```json\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.autopep8\"\n  }\n```\n\nand change the following, if set:\n\n```json\n  \"python.formatting.provider\": \"none\"\n```\n\n### Format on save\n\nYou can enable format on save for python by having the following values in your settings:\n\n```json\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.autopep8\",\n    \"editor.formatOnSave\": true\n  }\n```\n\n### Disabling formatting with `autopep8`\n\nIf you want to disable autopep8 formatter, you can [disable this extension](https://code.visualstudio.com/docs/editor/extension-marketplace#_disable-an-extension) per workspace in Visual Studio Code.\n\n## Settings\n\n| Settings                  | Default      | Description                                                                                                                                                                                                                                                                          |\n| ------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| autopep8.args             | `[]`         | Custom arguments passed to `autopep8`. E.g `\"autopep8.args\" = [\"--config\", \"<file>\"]`                                                                                                                                                                                                |\n| autopep8.path             | `[]`         | Setting to provide custom `autopep8` executable. This will slow down formatting, since we will have to run `autopep8` executable every time or file save or open. Example 1: `[\"~/global_env/autopep8\"]` Example 2: `[\"conda\", \"run\", \"-n\", \"lint_env\", \"python\", \"-m\", \"autopep8\"]` |\n| autopep8.interpreter      | `[]`         | Path to a python interpreter to use to run the linter server.                                                                                                                                                                                                                        |\n| autopep8.importStrategy   | `useBundled` | Setting to choose where to load `autopep8` from. `useBundled` picks autopep8 bundled with the extension. `fromEnvironment` uses `autopep8` available in the environment.                                                                                                             |\n| autopep8.showNotification | `off`        | Setting to control when a notification is shown.                                                                                                                                                                                                                                     |\n\n## Commands\n\n| Command           | Description                       |\n| ----------------- | --------------------------------- |\n| autopep8: Restart | Force re-start the format server. |\n\n## Logging\n\nUse `Developer : Set Log Level...` command from the **Command Palette**, and select `autopep8` from the extensions list to set the Log Level for the extension. For detailed LSP traces, add `\"autopep8.trace.server\" : \"verbose\"` to your **User** `settings.json` file.\n", "repo_name": "vscode-autopep8", "org_name": "microsoft", "org_repo": "microsoft/vscode-autopep8", "platform_org_repo": "github+microsoft/vscode-autopep8", "link_to_repo": "https://github.com/microsoft/vscode-autopep8", "platform": "github", "language": "Python", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "<img src=\"https://microsoft.github.io/hlsl-specs/resources/HLSL.png\" width=\"128\">\n\n# HLSL Specifications\n\nThis repository contains documentation for HLSL feature proposals and specifications. Please see the documentation for\nthe HLSL Feature [Proposal Process](docs/Process.md) for more information on proposals.\n\nA listing of the active proposals is available [here](proposals/).\n\nThis repository also contains specifications for the HLSL language. The draft\nHLSL specification is available in\n[HTML](https://microsoft.github.io/hlsl-specs/specs/hlsl.html) and\n[PDF](https://microsoft.github.io/hlsl-specs/specs/hlsl.pdf).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "hlsl-specs", "org_name": "microsoft", "org_repo": "microsoft/hlsl-specs", "platform_org_repo": "github+microsoft/hlsl-specs", "link_to_repo": "https://github.com/microsoft/hlsl-specs", "platform": "github", "language": "TeX", "stargazers_count": 60, "watchers_count": 60}, {"README_text": "# FarmVibes.AI: Multi-Modal GeoSpatial ML Models for Agriculture and Sustainability\n\nWith FarmVibes.AI, you can develop rich geospatial insights for agriculture and sustainability.\n\nBuild models that fuse multiple geospatial and spatiotemporal datasets to obtain insights (e.g.\nestimate carbon footprint, understand growth rate, detect practices followed) that would be\nhard to obtain when these datasets are used in isolation. You can fuse together satellite imagery\n(RGB, SAR, multispectral), drone imagery, weather data, and more.\n\nFusing datasets this way helps generate more robust insights and unlocks new insights that are\notherwise not possible without fusion. This repo contains several fusion workflows (published and\nshown to be key for agriculture related problems) that help you build robust remote sensing, earth\nobservation, and geospatial models with focus on agriculture/farming with ease. Our main focus right\nnow is agriculture and sustainability, which the models are optimized for. However, the framework itself is generic\nenough to help you build models for other domains.\n\n## FarmVibes.AI Primer\n\nThere are three main pieces to FarmVibes.AI. The first one consists of data ingestion and\npre-processing workflows to help prepare data for fusion models tailored towards agriculture.\nAdditionally, we provide model training notebook examples that not only allow the configuration\nof pre-processing of data but also allow tuning existing models with ease. Finally, a compute\nengine that supports data ingestion as well as adjusting existing and creating novel workflows\nwith the tuned model.\n\n### FarmVibes.AI Fusion-Ready Dataset Preparation\n\nIn this step, you can select the datasets that you would like to fuse for building the insights.\nFarmVibes.AI comes with many dataset downloaders. These include satellite imagery from Sentinel 1\nand 2, US Cropland Data, USGS Elevation maps, NAIP imagery, NOAA weather data, private weather data\nfrom Ambient Weather. Additionally, you can also bring in any rasterized datasets that you\nwant to make them fusion-ready for FarmVibes.AI (e.g. drone imagery or other satellite imagery) and, in\nthe future, custom sensor data (such as weather sensors).\n\nThe key technique in FarmVibes.AI is to use as input for ML models data that goes much beyond\ntypes, space and time from where the labels are located. For example, when detecting grain silos\nfrom satellite imagery (labeled only in optical imagery), it is better to rely on optical as well as\nelevation and radar bands. In this scenario, it is also important to combine multiple data modalities with other known agriculture infrastructure entities. Likewise, it is also\nimportant to use as input the images of a given silo across various times of the year to help\ngenerate a more robust model. Including information from many data streams, while also incorporating\nhistorical data from nearby or similar locations  has been shown to improve\nrobustness of geospatial models (especially for yield, growth, and crop classification problems).\nFarmVibes.AI generates such input data for models with ease based on parameters that can be\nspecified.\n\nFarmVibes.AI enables a data scientist to massage and/or tune the datasets to their preferences. The\ntuning is enabled via a configurable workflow which is specified as a directed acyclic graph of data\ndownloading workflows and data preparation workflows. The preparation operators help create the\ninputs (e.g. fused pandas arrays or tensors containing all raw data) to training and inference\nmodules.\n\n### FarmVibes.AI Model Sample Notebook Library\n\nThe next step in FarmVibes.AI involves using the inbuilt notebooks to tune the models to achieve a\nlevel of accuracy for the parts of the world or seasons that you are focusing on. The library\nincludes notebooks for  detecting practices (e.g. harvest date detection), estimating climate impact\n(both seasonal carbon footprint and long term sustainability), micro climate prediction, and crop\nidentification.\n\nFarmVibes.AI comes with these notebooks to help you get started to train fusion models to combine\nthe geospatial datasets into robust insights tailored for your needs. The users can tune the model to\n a desired performance and publish the model to FarmVibes.AI. The model then shows up to be used later in an inference engine that can be employed for other parts of the world, other dates, or more.\n\n### FarmVibes.AI Inference Engine\n\nThe final stage in FarmVibes.AI is to combine the data connectors, pre-processing, and the model\npieces together into a robust inference workflow. The generated workflow can then be used for\nperforming inference in an area of interest and time range that can be passed as inputs to the\nworkflow. FarmVibes.AI can be configured such that it then runs the inference for the time range and\nupdates the results whenever upstream data is updated (e.g. new satellite imagery or sensor data is\nadded). You do this by creating a workflow that is composed of fused data preparation and fusion\nmodel workflows.\n\n## Operation Mode\n\nCurrently, we are open-sourcing the local FarmVibes.AI cluster, that uses pre-build operators and\nworkflows and runs them locally on your data science machine. This means that any data generated is\npersisted locally in your machine. The actual workflows and their implementations are provided via Docker images, with their description\navailable in the [workflow list documentation](https://microsoft.github.io/farmvibes-ai/docfiles/markdown/WORKFLOW_LIST.html).\n\nThe user can interact with the local FarmVibes.AI cluster via a REST API (in localhost) or a local\nPython client (inside a Jupyter Notebook, for example).\n\n## Installation\n\nPlease refer to the the [Quickstart guide](https://microsoft.github.io/farmvibes-ai/docfiles/markdown/QUICKSTART.html) for information on where to get started. If\nyou prefer to setup a dedicated Azure Virtual Machine to run FarmVibes.AI, you can find detailed\ninstructions [in the VM setup documentation](https://microsoft.github.io/farmvibes-ai/docfiles/markdown/VM-SETUP.html).\n\n## Notebook Examples\n\nIn the folder `notebooks` there are several examples to serve as starting points and demonstrating\nhow FarmVibes.AI can be used to create Agriculture insights. Some of the available notebooks are:\n\n* `helloworld`: a simple example on how to use the client to run a workflow and visualize the\nresponse.\n* `harvest_period`: showing how a NDVI time-series computed on top of Sentinel 2 data can\nbe obtained for a single field and planting season and used to estimate emergence and harvest dates.\n* `carbon`: illustrating how to simulate different soil carbon estimates based on different\nagriculture practices, leveraging the [COMET-Farm API](https://gitlab.com/comet-api/api-docs/-/tree/master/).\n* `deepmc`: showing how one can build micro-climate forecasts from weather station data using the\n[DeepMC model](https://spectrum.ieee.org/deepmc-weather-predicition).\n* `crop_segementation`: this\nexample shows how to train a crop identification model based on NDVI data computed on top of our\n[SpaceEye](https://arxiv.org/abs/2106.08408) cloud-free image generation model. In this example, you\ncan also then use the trained model in an inference workflow to obtain predictions in any area where\nwe are able to generate SpaceEye imagery.\n\nWe provide a [complete list of the notebooks available](https://microsoft.github.io/farmvibes-ai/docfiles/markdown/NOTEBOOK_LIST.html)\nand their description in our documentation.\n\n## Documentation\n\nMore detailed information about the different components can be found in the [FarmVibes.AI documentation](https://microsoft.github.io/farmvibes-ai/).\nIn this repository, this information is also accessible in:\n\n* [FARMVIBES_AI.md](./docs/source/docfiles/markdown/FARMVIBES_AI.md) describing how to setup and\nmanage the local cluster.\n* [WORKFLOWS.md](./docs/source/docfiles/markdown/WORKFLOWS.md) describing how workflows\ncan be written and how they function.\n* [CLIENT.md](./docs/source/docfiles/markdown/CLIENT.md) documenting the\nFarmVibes.AI client, which is the preferred way to run workflows and interact with the results.\n* [SECRETS.md](./docs/source/docfiles/markdown/SECRETS.md) describing how to manage and pass secrets to the cluster\n(such as API keys), so that they will be available when running workflows.\n* [TROUBLESHOOTING.md](./docs/source/docfiles/markdown/TROUBLESHOOTING.md) in case you run into any issues.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "farmvibes-ai", "org_name": "microsoft", "org_repo": "microsoft/farmvibes-ai", "platform_org_repo": "github+microsoft/farmvibes-ai", "link_to_repo": "https://github.com/microsoft/farmvibes-ai", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 496, "watchers_count": 496}, {"README_text": "# Visual Studio Code Icons\n\n![Plugin preview](preview.png)\n\nEnsure you have the following fonts install in order to use the plugin in Figma:\n\n- [Codicons](https://github.com/microsoft/vscode-codicons/blob/master/dist/codicon.ttf)\n- [Seti](https://github.com/jesseweed/seti-ui/blob/master/styles/_fonts/seti/seti.ttf)\n\n\n# Usage\n1. Install [plugin](https://www.figma.com/community/plugin/786075219184960694/Visual-Studio-Code-Icons)\n2. Find the symbol you want to use\n3. Click the symbol", "repo_name": "vscode-figma-icons", "org_name": "microsoft", "org_repo": "microsoft/vscode-figma-icons", "platform_org_repo": "github+microsoft/vscode-figma-icons", "link_to_repo": "https://github.com/microsoft/vscode-figma-icons", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# VS Code Merge/Diff Editor Development Companion Extension\n\nThis extension helps testing and developing the merge and diff editor in VS Code by providing various utility commands.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-merge-editor-dev-companion", "org_name": "microsoft", "org_repo": "microsoft/vscode-merge-editor-dev-companion", "platform_org_repo": "github+microsoft/vscode-merge-editor-dev-companion", "link_to_repo": "https://github.com/microsoft/vscode-merge-editor-dev-companion", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Virtual Visit Sample Connector\n\nSample custom connector for virtual appointment API.\n\n## Scenario overview\n\nA very common healthcare scenario for providers as well as payors is creating an on-demand virtual visit. The Teams platform Microsoft has a Virtual Appointment capability available through the Microsoft Graph that extends the *onlineMeeting* to include enhancements like a mobile-browser join (without requiring a custom communication service or download like the Teams app), an enhanced waiting room, and additional reporting/analytics for virtual appointments. Additionally, meetings can be associated to meeting ID's from 3rd party scheduling systems directly within the virtual appointment definition.\n\nUnfortunately, at this time there is not a published Connector for these API calls, so building a solution requires some additional steps and skills to either manually call the Graph API or build a custom connector.\n\n## Solution overview\n\nThis repository includes a sample OpenAPI (Swagger) definition which can be imported through the Power Platform maker portal and used to test out the capabilities. While creating a custom connector is not overly complex, it can be helpful to learn from an example instead of starting from a completely \"blank page\".\n\nAs of this release, the virtualAppointment capabilities are in [public preview](https://docs.microsoft.com/en-us/graph/versioning-and-support#beta-version), which is reflected in the OpenAPI definition.\n\n## Deploying the connector\n\nDeployment includes the following tasks:\n\n1. Set up an Azure Application Registration\n2. Add the custom connector in an environment\n\n### Task 1 - Set up an Azure Application Registration\n\n The steps for creating the Azure Application Registration are covered in this [Microsoft Tech Community video](https://techcommunity.microsoft.com/t5/healthcare-and-life-sciences/create-a-custom-powerapp-connector-to-graph-api/ba-p/3494104). When setting up the Azure Application Registration this connector, reference the following:\n\n- The Azure portal is accessed here: [Azure portal](https://portal.azure.com)\n- Web Redirect URI: `https://global.consent.azure-apim.net/redirect`\n- API Permissions (Microsoft Graph)\n  - `User.Read`\n  - `OnlineMeetings.ReadWrite`\n  - `UserAuthenticationMethod.ReadWrite.All` (Admin consent required)\n\nAt completion of this task, make sure that you have the following Application Registration details which are needed for the second task\n\n- `Client ID`\n- `Client Secret`\n\n### Task 2 - Create the custom connector in an environment\n\nUnlike the connector in the video, this connector is created from an OpenAPI definition (not a Postman collection). When setting up the Azure Application Registration this connector, reference the following:\n\n- The Power Apps maker portal is accessed here: [Power Apps maker portal](https://make.powerapps.com)\n- The custom connector OpenAPI sample definition is available from this repository: [Virtual Visit OpenAPI definition](./Virtual-Visit.swagger.json)\n- In your selected Environment, navigate to **Dataverse->Custom Connectors**\n- Select **New Custom Connector-->Import an OpenAPI file**\n- For the Security tab, reference the following values which are specific to this connector:\n\n|||\n|---|---|\n|**Client id**| `Client ID` (from the previous task)|\n|**Client secret**| `Client Secret` (from the previous task)|\n|**Resource URL**| `https://graph.microsoft.com/`|\n|**Scope**| `OnlineMeetings.ReadWrite`|\n\nThe official documentation for deploying a custom connector from an OpenAPI definition is available at [Microsoft Docs](https://docs.microsoft.com/en-us/connectors/custom-connectors/define-openapi-definition#import-the-openapi-definition-for-power-automate-and-power-apps).\n\n## Using the connector\n\n1. First, identify an online meeting ID for which you will set up as a virtualAppointment. If an online meeting does not exist, the custom connector includes an Action for doing this, which will supply an online meeting ID field, as well as the practitioner join link.\n2. Second, use the Create Virtual Appointment (beta) action to add the virtual visit capabilities and consumer join link.\n\nThe screenshot below shows an example of how this could be called from a Power App leveraging a Power Automate cloud flow automation, assuming no Online Meeting existed. It will return the join links to the app.\n\n![Cloud flow to create a virtual appointment](images/VirtualVisitAutomationSteps.png)\n\nThe join links can be distributed using SMS, email, secure portal messages, etc. through additional steps in this automation.\n\n## Background\n\nThe Microsoft Graph is the gateway to data and intelligence in Microsoft 365 includes many capabilities that are useful when building apps, including low-code applications using Logic Apps and the Power Platform. There are hundreds of Microsoft Connectors that make use of these services, while many more capabilities are available just out of sight.\n\nCustom connectors can provide a purpose-driven experience, standardizing how low-code developers interact with both well-known or custom APIs, improving time to value and reducing technology risk.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Virtual-Visit-Sample-Connector", "org_name": "microsoft", "org_repo": "microsoft/Virtual-Visit-Sample-Connector", "platform_org_repo": "github+microsoft/Virtual-Visit-Sample-Connector", "link_to_repo": "https://github.com/microsoft/Virtual-Visit-Sample-Connector", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# SQL Server database migration - PoC Environment Setup\n\nThis deployment package allows the user to deploy a Proof-of-Concept environment of Azure SQL VM migration to Azure SQL Managed Instance with encapsulate best practices and step by step execution steps that will enable you to test, adjust and fully deploy automated solution at scale. This is an approach that could help with large migrations at scale for specific workload use cases.\n\n## Prerequisites\n\nYou need to have at least owner role or contributor role for the Azure subscription. A separate resource group should be created and delegated roles necessary for this proof of concept.\n\nCheck this documentation for [RBAC role-assignments](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-steps).\n\n## [One-click SQL Migration PoC](deploy/README.md)\n\nTake advantage of this one-click SQL Migration PoC to accelerate your migration to Azure SQL.\n\n## Modernization Benefits\n\nSQL Modernization to Azure SQL MI offers a long list of benefits. We have just listed a few that can be easily understood by a customer with the lowest skillset on SQL.\n\n- Azure SQL MI is always up to date and never out of support, with backwards compatibility to SQL Server 2008\n- AI-powered features in Azure SQL enhances the performance and security of the workloads\n- Enhances productivity using Automated features.\n- Reduces and optimizes cost through eliminated Infra and reduced operational overhead.\n- Offers a built-in HA, backups that guarantee Availability, RTO and RPO\n- Accelerate time to market through CI/CD and DevOps processes.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n## Page Navigator\n\n[Index: Table of Contents](index.md)\n\n[Next: One-click Deploy](deploy/README.md)\n", "repo_name": "SQL-Migration-AzureSQL-PoC", "org_name": "microsoft", "org_repo": "microsoft/SQL-Migration-AzureSQL-PoC", "platform_org_repo": "github+microsoft/SQL-Migration-AzureSQL-PoC", "link_to_repo": "https://github.com/microsoft/SQL-Migration-AzureSQL-PoC", "platform": "github", "language": "PowerShell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# MICO\n\n\ud83e\udd47\ud83e\udd48[**Winners Announced!**](https://microsoft.github.io/MICO/) \n\nWelcome to the Microsoft Membership Inference Competition (MICO)!\n\nIn this competition, you will evaluate the effectiveness of differentially private model training as a mitigation against white-box membership inference attacks.\n\n* [What is Membership Inference?](#what-is-membership-inference)\n* [What is MICO?](#what-is-mico)\n* [Task Details](#task-details)\n* [Submissions and Scoring](#submissions-and-scoring)\n* [Winner Selection](#winner-selection)\n* [Important Dates](#important-dates)\n* [Terms and Conditions](#terms-and-conditions)\n* [CodaLab Competitions](#codalab-competitions)\n* [Getting Started](#getting-started)\n* [Contact](#contact)\n* [Contributing](#contributing)\n* [Trademarks](#trademarks)\n\n## What is Membership Inference?\n\nMembership inference is a widely-studied class of threats against Machine Learning (ML) models.\nThe goal of a membership inference attack is to infer whether a given record was used to train a specific ML model.\nAn attacker might have full access to the model and its weights (known as \"white-box\" access), or might only be able to query the model on inputs of their choice (\"black-box\" access).\nIn either case, a successful membership inference attack could have negative consequences, especially if the model was trained on sensitive data.\n\nMembership inference attacks vary in complexity.\nIn a simple case, the model might have overfitted to its training data, so that it outputs higher confidence predictions when queried on training records than when queried on records that the model has not seen during training.\nRecognizing this, an attacker could simply query the model on records of their interest, establish a threshold on the model's confidence, and infer that records with higher confidence are likely members of the training data.\nIn a white-box setting, as is the case for this competition, the attacker can use more sophisticated strategies that exploit access to the internals of the model.\n\n\n## What is MICO?\n\nIn MICO, your goal is to perform white-box membership inference against a series of trained ML models that we provide.\nSpecifically, given a model and a set of *challenge points*, the aim is to decide which of these challenge points were used to train the model.\n\nYou can compete on any of four separate membership inference tasks against classification models for image, text, and tabular data, as well as on a special _Differential Privacy Distinguisher_ task spanning all 3 modalities.\nEach task will be scored separately.\nYou do not need to participate in all of them, and can choose to participate in as many as you like.\nThroughout the competition, submissions will be scored on a subset of the evaluation data and ranked on a live scoreboard.\nWhen submission closes, the final scores will be computed on a separate subset of the evaluation data.\n\nThe winner of each task will be eligible for an award of **$2,000 USD** from Microsoft and the runner-up of each task for an award of **$1,000 USD** from Microsoft (in the event of tied entries, these awards may be adjusted).\nThis competition is co-located with the [IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2023](https://satml.org/), and the winners will be invited to present their strategies at the conference.\n\n\n## Task Details\n\nFor each of the four tasks, we provide a set of models trained on different splits of a public dataset.\nFor each of these models, we provide `m` challenge points; exactly half of which are _members_ (i.e., used to train the model) and half are _non-members_ (i.e., they come from the same dataset, but were not used to train the model).\nYour goal is to determine which challenge points are members and which are non-members.\n\nEach of the first three tasks consists of three different _scenarios_ with increasing difficulty, determined by the differential privacy guarantee of the algorithm used to train target models: $\\varepsilon = \\infty$, high $\\varepsilon$, and low $\\varepsilon$.\nAll scenarios share the same model architecture and are trained for the same number of epochs.\nThe $\\varepsilon = \\infty$ scenario uses Stochastic Gradient Descent (SGD) without any differential privacy guarantee, while the high $\\varepsilon$ and low $\\varepsilon$ scenarios use Differentially-Private SGD with a high and low privacy budget $\\varepsilon$, respectively.\nThe lower the privacy budget $\\varepsilon$, the more _private_ the model.\n\nIn the fourth task, the target models span all three modalities (image, text, and tabular data) and are trained with a low privacy budget.\nThe model architectures and hyperparameters are the same as for first three tasks.\nHowever, we reveal the training data of models except for the `m/2` member challenge points.\n\n\n| Task        | Scenario | Dataset      | Model Architecture         | $\\varepsilon$ | Other training points given |\n| :---           |    :----:   |  :----: |        :----:      |    :----:  |            :----:          |\n| Image            | I1       | CIFAR-10     | 4-layer CNN                | $\\infty$      | No                           |\n|                  | I2       | CIFAR-10     | 4-layer CNN                | High          | No                           |\n|                  | I3       | CIFAR-10     | 4-layer CNN                | Low           | No                           |\n| Text             | X1       | SST-2        | Roberta-Base               | $\\infty$      | No                           |\n|                  | X2       | SST-2        | Roberta-Base               | High          | No                           |\n|                  | X3       | SST-2        | Roberta-Base               | Low           | No                           |\n| Tabular Data     | T1       | Purchase-100 | 3-layer fully connected NN | $\\infty$      | No                           |\n|                  | T2       | Purchase-100 | 3-layer fully connected NN | High          | No                           |\n|                  | T3       | Purchase-100 | 3-layer fully connected NN | Low           | No                           |\n| DP Distinguisher | D1       | CIFAR-10     | 4-layer CNN                | Low           | Yes                           |\n|                  | D2       | SST-2        | Roberta-Base               | Low           | Yes                           |\n|                  | D3       | Purchase-100 | 3-layer fully connected NN | Low           | Yes                           |\n\n\n## Submissions and Scoring\n\nSubmissions will be ranked based on their performance in white-box membership inference against the provided models.\n\nThere are three sets of challenges: `train`, `dev`, and `final`.\nFor models in `train`, we reveal the full training dataset, and consequently the ground truth membership data for challenge points.\nThese models can be used by participants to develop their attacks.\nFor models in the `dev` and `final` sets, no ground truth is revealed and participants must submit their membership predictions for challenge points.\n\nDuring the competition, there will be a live scoreboard based on the `dev` challenges.\nThe final ranking will be decided on the `final` set; scoring for this dataset will be withheld until the competition ends.\n\nFor each challenge point, the submission must provide a value, indicating the confidence level with which the challenge point is a member.\nEach value must be a floating point number in the range `[0.0, 1.0]`, where `1.0` indicates certainty that the challenge point is a member, and `0.0` indicates certainty that it is a non-member.\n\nSubmissions will be evaluated according to their **True Positive Rate at 10% False Positive Rate** (i.e. `TPR @ 0.1 FPR`).\nIn this context, *positive* challenge points are members and *negative* challenge points are non-members.\nFor each submission, the scoring program concatenates the confidence values for all models (`dev` and `final` treated separately) and compares these to the reference ground truth.\nThe scoring program determines the minimum confidence threshold for membership such that at most 10% of the non-member challenge points are incorrectly classified as members.\nThe score is the True Positive Rate achieved by this threshold (i.e., the proportion of correctly classified member challenge points).\nThe live scoreboard shows additional scores (i.e., TPR at other FPRs, membership inference advantage, accuracy, AUC-ROC score), but these are only informational.\n\nYou are allowed to make multiple submissions, but only your latest submission will be considered.\nIn order for a submission to be valid, you must submit confidence values for all challenge points in all three scenarios of the task.\n\nHints and tips:\n- We do realize that the score of a submission leaks some information about the ground truth.\nHowever, using this information to optimize a submission based only on the live scoreboard (i.e., on `dev`) is a bad strategy, as this score has no relevance on the final ranking.\n- Pay a special attention to the evaluation metric (`TPR @ 0.1 FPR`).\nYour average accuracy at predicting membership in general may be misleading. Your attack should aim to maximize the number of predicted members whilst remaining below the specified FPR.\n\n\n## Winner Selection\n\nWinners will be selected independently for each task (i.e. if you choose not to participate in certain tasks, this will not affect your rank for the tasks in which you do participate).\nFor each task, the winner will be the one achieving the highest average score (`TPR @ 0.1 FPR`) across the three scenarios.\n\n\n## Important Dates\n\n- Submission opens: November 8, 2022\n- Submission closes: ~**January 12, 2023, 23:59 (Anywhere on Earth)**~ **January 26, 2023, 23:59 (Anywhere on Earth)**\n- Conference: February 8-10, 2023\n\n\n## Terms and Conditions\n\n- This challenge is subject to the [Microsoft Bounty Terms and Conditions](https://www.microsoft.com/en-us/msrc/bounty-terms).\n\n- Microsoft employees and students/employees of Imperial College London may submit solutions, but are not eligible to receive awards.\n\n- Submissions will be evaluated by a panel of judges according to the aims of the competition.\n\n- Winners may be asked to provide their code and/or a description of their strategy to the judges for verification purposes.\n\n## CodaLab Competitions\n\n- [Image (CIFAR-10)](https://codalab.lisn.upsaclay.fr/competitions/8551)\n- [Text (SST-2)](https://codalab.lisn.upsaclay.fr/competitions/8554)\n- [Tabular Data (Purchase-100)](https://codalab.lisn.upsaclay.fr/competitions/8553)\n- [DP Distinguisher](https://codalab.lisn.upsaclay.fr/competitions/8552)\n\n## Getting Started\n\nFirst, register on CodaLab for the tasks in which you would like to participate.\nOnce registered, you will be given URLs from which to download the challenge data.\n\nThis repository contains starting kit Jupyter notebooks which will guide you through making your first submission.\nTo use it, clone this repository and follow the steps below:\n- `pip install -r requirements.txt`. You may want to do this in a [virtualenv](https://docs.python.org/3/library/venv.html).\n- `pip install -e .`\n- `cd starting-kit/`\n- `pip install -r requirements-starting-kit.txt`\n- The corresponding starting kit notebook illustrates how to load the challenge data, run a basic membership inference attack, and prepare an archive to submit to CodaLab.\n\n\n## Contact\n\nFor any additional queries or suggestions, please contact [mico-competition@microsoft.com](mico-competition@microsoft.com).\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment).\nSimply follow the instructions provided by the bot.\nYou will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n![Mico Argentatus (Silvery Marmoset) - William Warby/Flickr](codalab-package/logo.png)\n\nMico Argentatus (Silvery Marmoset) - William Warby/Flickr\n\n", "repo_name": "MICO", "org_name": "microsoft", "org_repo": "microsoft/MICO", "platform_org_repo": "github+microsoft/MICO", "link_to_repo": "https://github.com/microsoft/MICO", "platform": "github", "language": "Python", "stargazers_count": 26, "watchers_count": 26}, {"README_text": "# LSKV\n\n[![Open in VSCode](https://img.shields.io/static/v1?label=Open+in&message=VSCode&logo=visualstudiocode&color=007ACC&logoColor=007ACC&labelColor=2C2C32)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/LSKV) [![LSKV CI](https://github.com/microsoft/LSKV/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/LSKV/actions/workflows/ci.yml)\n\nThe Ledger-backed Secure Key-Value store, also known as LSKV, is a research project\u00a0to investigate whether it is possible to build a\u00a0trustworthy distributed data store on top of the [Confidential Consortium Framework (CCF)](https://github.com/microsoft/CCF).\u00a0LSKV aims to provide gRPC & HTTP/JSON APIs, similar to that of existing key-value stores such as [etcd](https://etcd.io/), with support for common operations such as watches and leases, whilst taking advantage of the confidentiality guarantees, auditability, and multi-party governance provided by CCF.\n\n**This early stage research prototype should not be used in production.**\n\nLSKV went to FOSDEM! [Check out the presentation recording](https://fosdem.org/2023/schedule/event/cc_lskv/).\n\n## Targets\n\nCurrently LSKV can run in the following targets:\n\n- Virtual (non-attested, insecure, handy for development)\n- SGX (attested, secure)\n- SEV-SNP (attested, secure but not actively tested)\n\n## Install Dependencies\n\nThese instructions expect an Ubuntu 20.04 machine, or follow the docker instructions.\n\nThis repository and its dependencies can be checked out by clicking: [![Open in VSCode](https://img.shields.io/static/v1?label=Open+in&message=VSCode&logo=visualstudiocode&color=007ACC&logoColor=007ACC&labelColor=2C2C32)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/LSKV)\n\nAlternatively, CCF and its dependencies can be installed manually (for virtual mode):\n\n```bash\nmake install-ccf-virtual\n```\n\nOr\n\n```bash\nwget https://github.com/microsoft/CCF/releases/download/ccf-4.0.0-dev3/ccf_virtual_4.0.0_dev3_amd64.deb\nsudo dpkg -i ccf_virtual_4.0.0_dev3_amd64.deb # Installs CCF under /opt/ccf_virtual\ncat /opt/ccf_virtual/share/VERSION_LONG\n# ccf-4.0.0-dev3\n/opt/ccf_virtual/getting_started/setup_vm/run.sh /opt/ccf_virtual/getting_started/setup_vm/app-dev.yml  # Install dependencies\n```\n\nIf your organisation supports it, you can also checkout this repository in a Github codespace: [![Open in Github codespace](https://img.shields.io/static/v1?label=Open+in&message=GitHub+codespace&logo=github&color=2F363D&logoColor=white&labelColor=2C2C32)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=534240617&machine=basicLinux32gb&devcontainer_path=.devcontainer.json&location=WestEurope)\n\n## Build\n\nIn the local checkout of this repository:\n\n### Virtual (non-attested)\n\n```bash\nmake build-virtual\n```\n\nBuilds `build/liblskv.virtual.so`.\n\n#### Docker\n\nAlternatively, it is possible to build a runtime image of this application via docker:\n\n```bash\nmake build-docker-virtual\n```\n\n### SGX (attested)\n\n**Note**: This requires the SGX variant of CCF to be installed, try `make install-ccf-sgx`.\nCurrently this may require some APT fiddling if you have already installed `ccf-virtual`.\n\n```bash\nmake build-sgx\n```\n\nBuilds `build/liblskv.enclave.so.signed`.\n\n#### Docker\n\nAlternatively, it is possible to build a runtime image of this application via docker:\n\n```bash\nmake build-docker-sgx\n```\n\n## Build options\n\nThe cmake build can be configured with the following lskv-specific options:\n\n- `COMPILE_TARGET`: build LSKV for a specific deployment target, one of [virtual;sgx;snp], defaults to virtual\n  - **Note**: this requires the corresponding `ccf_${COMPILE_TARGET}` package to be installed\n- `PUBLIC_LEASES`: store lease data in public maps (publicly visible in the ledger)\n- `VERBOSE_LOGGING`: enable verbose logging which may output private data to logs\n\n## Testing\n\n### Etcd integration tests\n\nTo run the main LSKV tests:\n\n```sh\nmake tests\n```\n\nTo run some etcd integration tests, note that this isn't up-to-date so might lead to failures:\n\n```sh\nmake test-etcd-integration\n```\n\n## Run\n\n### Locally\n\n```bash\nmake run-virtual\n```\n\nOr\n\n```bash\n/opt/ccf_virtual/bin/sandbox.sh -p build/liblskv.virtual.so --http2\n```\n\nProducing:\n\n```sh\nSetting up Python environment...\nPython environment successfully setup\n[12:00:00.000] Virtual mode enabled\n[12:00:00.000] Starting 1 CCF node...\n[12:00:00.000] Started CCF network with the following nodes:\n[12:00:00.000]   Node [0] = https://127.0.0.1:8000\n[12:00:00.000] You can now issue business transactions to the ./liblskv.virtual.so application\n[12:00:00.000] Keys and certificates have been copied to the common folder: .../LSKV/build/workspace/sandbox_common\n[12:00:00.000] See https://microsoft.github.io/CCF/main/use_apps/issue_commands.html for more information\n[12:00:00.000] Press Ctrl+C to shutdown the network\n```\n\nOr, for an SGX-enabled application: `make run-sgx`.\n\n### With docker in Virtual mode\n\n**Note**: A Docker image following the latest changes on the `main` branch is available as `ccfmsrc.azurecr.io/public/lskv:latest-virtual`.\n\n```bash\nmake run-docker-virtual\n```\n\n### With docker in SGX mode\n\n**Note**: A Docker image following the latest changes on the `main` branch is available as `ccfmsrc.azurecr.io/public/lskv:latest-sgx`.\n\n```bash\nmake run-docker-sgx\n```\n\n## Interacting with the store\n\n**Note**: When running with Docker extra setup steps are currently required before interacting with the store as below, see [Running a CCF Service](https://microsoft.github.io/CCF/main/operations/start_network.html#opening-a-network-to-users).\nThe [`lskv_cluster.py`](./benchmark/lskv_cluster.py) may also be useful for setting up a local docker cluster (used inside the docker steps above).\n\n### etcdctl (gRPC API)\n\nYou can use the official etcd CLI client for interacting with the datastore over gRPC, for supported methods see the [gRPC API status](https://github.com/microsoft/LSKV/issues/35).\n\n```bash\n./etcdctl.sh put key value\n# OK\n\n./etcdctl.sh get key\n# key\n# value\n```\n\n### JSON API\n\nWe also allow calling with the [JSON API](https://etcd.io/docs/v3.5/dev-guide/api_grpc_gateway/).\nThe status of the JSON API follows that of the [gRPC API](https://github.com/microsoft/LSKV/issues/35).\n\nTo call an endpoint with curl:\n\n```sh\n# read an empty value from 'hello'\ncurl -X POST https://127.0.0.1:8000/v3/kv/range --cacert workspace/sandbox_common/service_cert.pem --key workspace/sandbox_common/user0_privk.pem --cert workspace/sandbox_common/user0_cert.pem  -H \"content-type: application/json\" -i --data-binary '{\"key\":\"aGVsbG8=\"}'\n\n# put a value 'world' at 'hello'\ncurl -X POST https://127.0.0.1:8000/v3/kv/put --cacert workspace/sandbox_common/service_cert.pem --key workspace/sandbox_common/user0_privk.pem --cert workspace/sandbox_common/user0_cert.pem  -H \"content-type: application/json\" -i --data-binary '{\"key\":\"aGVsbG8=\",\"value\":\"d29ybGQ=\"}'\n\n# read the put value at 'hello'\ncurl -X POST https://127.0.0.1:8000/v3/kv/range --cacert workspace/sandbox_common/service_cert.pem --key workspace/sandbox_common/user0_privk.pem --cert workspace/sandbox_common/user0_cert.pem  -H \"content-type: application/json\" -i --data-binary '{\"key\":\"aGVsbG8=\"}'\n\n# delete the put value at 'hello'\ncurl -X POST https://127.0.0.1:8000/v3/kv/delete_range --cacert workspace/sandbox_common/service_cert.pem --key workspace/sandbox_common/user0_privk.pem --cert workspace/sandbox_common/user0_cert.pem  -H \"content-type: application/json\" -i --data-binary '{\"key\":\"aGVsbG8=\"}'\n```\n\n## Benchmarking\n\nSee [BENCHMARKING.md](./BENCHMARKING.md) for instructions to run the benchmarks and analysis.\n\n## Receipts\n\nReceipts are cryptographic proofs that transactions which mutate the state of the service (i.e. `put` and `delete`) have been successfully committed to the ledger.\nThe receipt includes claims for this purpose, for LSKV these are outlined below.\n\nThe receipts are available through the `lskvserverpb.Receipt/GetReceipt` endpoint (`/v3/receipt/get_receipt` for json).\nThe receipt is a protobuf form of the [output available from CCF](https://microsoft.github.io/CCF/main/use_apps/verify_tx.html#write-receipts), see [`lskvserver.proto`](./proto/lskvserver.proto) for the definition of the message types.\nThe custom claims that are registered for the receipt take the form of the `ReceiptClaims` message in that `lskvserver.proto` file.\n\nFor verifying receipts see the tests at [`test_common.py`](./tests/test_common.py), specifically the `check_receipt` method.\n", "repo_name": "LSKV", "org_name": "microsoft", "org_repo": "microsoft/LSKV", "platform_org_repo": "github+microsoft/LSKV", "link_to_repo": "https://github.com/microsoft/LSKV", "platform": "github", "language": "Python", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# beta Xplat Performance Tool\n- Last version is v4.2.0\n- sha256sum 94034cf88557c50adc8bccc7b6d82c8e67087cc81139770a737473bc1224b68a\n\n# Context:\nThe 'beta Xplat Performance Tool' is intended for Linux performance data collection, CPU and Memory load investigation and analysis, when high CPU or Memory load is reported. It aims at quickly being able to determine a device\u2019s CPU and Memory load and ellaborate on mitigation, as well as propose fixes. It's very intuitive and easy to use, returns results that are easy to interpret and relies on very basic set of tools existent in virtually all Linux minimal installations.\n\n# What it does:\n'beta Xplat Performance Tool' captures CPU and Memory data for a period of time and is at the moment, independent of \u201cClient Analyzer\u201d for Linux: it does not depend on python or \"Client Analyzer\" code to be executed. It\u2019s a command-line tool, shellscript, that receives an interval of time as parameter, and captures CPU and Memory activity for that specified period. The processes being monitored are wdavdaemon (edr, rtp and av components) and audisp plugin. Can also be used in a 'long run' mode in the background, to spot memory leaks or track resource behavior for a long time.\n\n# Main advantages\n- Easy to use and to interpret data\n- Light-wheight\n- Fast to execute\n- Does not depend on python\n- Basic Shell Script using 'awk', 'sed', 'grep', 'tee' base install available Linux tools\n- Easily adaptable and scalable code\n- Simply execute the script and collect resulting package for investigation\n\n# Examples of performance graphs for RAM and CPU\n\n![194161484-c04fece5-ac7a-440f-b1f4-b221bdd6a344](https://user-images.githubusercontent.com/113130572/198121620-8c1ed95d-b36e-4686-9dd8-5a5c8f127fd5.png)\n![194161566-7e2be150-c480-485f-9eef-eee6941277b9](https://user-images.githubusercontent.com/113130572/198121631-efa6f791-ebe0-4cf1-8bc1-10e69d6639ea.png)\n![194161596-32769f74-9035-4a47-9f71-4d5c160de1a5](https://user-images.githubusercontent.com/113130572/198121645-ca0e0ccf-96ef-4055-874f-64351839cb2c.png)\n![194161620-09b648ce-4eb1-4e3b-bb7c-6586fdc95263](https://user-images.githubusercontent.com/113130572/198121656-92c6ae3c-4667-429c-81e5-6834f63d4e89.png)\n\n# Usage:\n### From the Linux server, get script:\n- wget https://aka.ms/betaxplatperformancetool -O betaXplatPerformanceTool.sh && chmod +x betaXplatPerformanceTool.sh\n### Read 'help' dialog for instructions:\n- ./betaXplatPerformanceTool.sh -h\n### Run script as needed. Below example runs for 5 minutes, 300 seconds:\n- ./betaXplatPerformanceTool.sh -ps 300\n### Confirm investigation package is created:\n- You should find a package named betaXplatPerformanceTool-<--date-->.zip\n\n# Known to work in the following systems:\n- RHEL 8.2 (Ootpa)\n- Ubuntu 20.04.4 LTS (Focal Fossa)\n- CentOS 7.9\n- Debian 11.5\n- Oracle Enterprise Linux 7.9\n- Oracle Enterprise Linux 8.6\n- RHEL 7.8\n- RHEL 8\n", "repo_name": "linux-tracer", "org_name": "microsoft", "org_repo": "microsoft/linux-tracer", "platform_org_repo": "github+microsoft/linux-tracer", "link_to_repo": "https://github.com/microsoft/linux-tracer", "platform": "github", "language": "Shell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "CMYK to RGB color space mapping\n\n### 1. Set up environment variables\n\nSet up the following environment variable on your development PC:\n\n| Environment Variable | Description |\n| ----- | ----- |\n| SIM_WORKSPACE | The workspace ID from [your workspace details](https://docs.microsoft.com/en-us/bonsai/cookbook/get-workspace-info). |\n| SIM_ACCESS_KEY | The access key from [your workspace details](https://docs.microsoft.com/en-us/bonsai/cookbook/get-workspace-info). |\n\nMake sure those environment variables have been applied in the command window that you use for the next steps.\n\n### 2. Set up Python environment\n\nSet up your python environment as described in the [Bonsai CLI Get started docs](https://docs.microsoft.com/en-us/bonsai/cli).\nThen install the Python requirements of this sample by:\n\n```\npip install -r requirements.txt\n```\n\n### 3. Connect local instance of the simulator\n\nRun the simulator locally by:\n\n```\npython main.py\n```\n\nThe output should say `Registered simulator` followed by--every several seconds--a line saying `Last Event: Idle`.\n\n> NOTE: The next step uses Bonsai CLI commands.\n> If you prefer, these opererations can also be performed using your [Bonsai worspace](https://preview.bons.ai/) GUI as described in [Link an unmanaged simulator to Bonsai](https://docs.microsoft.com/en-us/bonsai/guides/run-a-local-sim?tabs=bash%2Ctest-with-ui&pivots=sim-lang-python).\n\nWhile main.py continues to run, open a new commmand window and use the Bonsai CLI to create a Bonsai brain and start training by:\n\n```\nbonsai brain create -n simple-adder-brain\nbonsai brain version update-inkling -f machine_teacher.ink -n simple-adder-brain\nbonsai brain version start-training -n simple-adder-brain\nbonsai simulator unmanaged connect --simulator-name simple-adder-sim -a Train -b simple-adder-brain -c Concept\n```\n\nThe output should say `Simulators Connected: 1`. After a minute or so, you should see lots of activity in the console window that\nis running main.py and if you open your [Bonsai worspace](https://preview.bons.ai/) you should see that the brain named simple-adder-brain\nis running training episodes. We'll complete training in a faster way in the next step, so for now you can manually stop training by:\n\n```\nbonsai brain version stop-training -n simple-adder-brain\n```\n\nPress Ctrl+C to stop the simulator running main.py in your first console window.\n\n### 4. Build the simulator package and scale training using the cloud\n\n> For this step, you must have Docker installed on your local machine. The community edition of Docker is available for\n> [Windows](https://docs.docker.com/docker-for-windows/install), [Linux](https://docs.docker.com/engine/install), and\n> [MacOS](https://docs.docker.com/docker-for-mac/install).\n\nBuild a Docker container image and push it to your registry.\nIn the following commands, `<SUBSCRIPTION>` and `<WORKSPACE_ACR_PATH>` should be replaced with\n[your workspace details](https://docs.microsoft.com/en-us/bonsai/cookbook/get-workspace-info):\n\n```\ndocker build -t simple-adder-container:latest -f Dockerfile .\ndocker tag simple-adder-container:latest <WORKSPACE_ACR_PATH>/simple-adder-container:latest\naz acr login --subscription <SUBSCRIPTION> --name <WORKSPACE_ACR_PATH>\ndocker push <WORKSPACE_ACR_PATH>/simple-adder-container:latest\n```\n\n> NOTE: The next step uses Bonsai CLI commands.\n> If you prefer, these opererations can also be performed using your [Bonsai worspace](https://preview.bons.ai/) GUI as described\n> in [Add a training simulator to your Bonsai workspace](https://docs.microsoft.com/en-us/bonsai/guides/add-simulator?tabs=add-cli%2Ctrain-inkling&pivots=sim-platform-other).\n\nCreating a Bonsai simulator package and running training with it by:\n\n```\nbonsai simulator package container create -n simple-adder-pkg -u <WORKSPACE_ACR_PATH>/simple-adder-container:latest --max-instance-count 25 -r 1 -m 1 -p Linux\nbonsai brain version start-training -n simple-adder-brain --simulator-package-name simple-adder-pkg\n```\n\nNext, open your [Bonsai worspace](https://preview.bons.ai/) and you should see your simple-adder-brain brain is running training.\nIf you look in the Train tab, after a few minutes, you will see that simulators have started up and episodes are being executed.\nAfter approximately 200,000 iterations you should see in the training graph shows 100% goal satisfaction and 100% success rate.\nYou can stop the training at this point or let training continue to run. It will eventually stop when it can no longer find improvements\nto reach the goal in a more optimal fashion.\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bonsai-rgb", "org_name": "microsoft", "org_repo": "microsoft/bonsai-rgb", "platform_org_repo": "github+microsoft/bonsai-rgb", "link_to_repo": "https://github.com/microsoft/bonsai-rgb", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# SandboxSecurityTools\n\nThis repository contains tools that can be used to help test security vulnerabilities against the sandboxes supported by the [Microsoft Windows Insider Preview Bounty Program](https://www.microsoft.com/en-us/msrc/bounty-windows-insider-preview?rtc=1) in the local attack scenario category. The following list describes each tool and what sandboxes they can be used to test. For additional information such as how to compile and run each tool please see the README.md file contained in the tool's directory.\n\n- Edge Sandbox Test Tool   \nThe Edge Sandbox Testing Tool can be used to test code running inside the Chromium renderer process sandbox.   \n**Supported sandboxes:** Edge renderer process\n\n- Launch App Container   \nThe LaunchAppContainer tool can be used to run applications in AppContainer or Less Privileged AppContainer (LPAC) sandboxes.   \n**Supported sandboxes:** AppContainer and Less-Privileged AppContainer (LPAC)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SandboxSecurityTools", "org_name": "microsoft", "org_repo": "microsoft/SandboxSecurityTools", "platform_org_repo": "github+microsoft/SandboxSecurityTools", "link_to_repo": "https://github.com/microsoft/SandboxSecurityTools", "platform": "github", "language": "C++", "stargazers_count": 109, "watchers_count": 109}, {"README_text": "---\nArtifactType: website\nLanguage: C#\nPlatform: Linux Container\nTags: Container, Azure, WebSite, Inventory, ServerInfo\n---\n\n![GitHub](https://img.shields.io/github/license/microsoft/server-info) ![GitHub repo size](https://img.shields.io/github/repo-size/microsoft/server-info) [![Azure](https://badgen.net/badge/icon/azure?icon=azure&label)](https://azure.microsoft.com)\n\n# Server Info\n\nThis application exposes Server and Client details and informations available in code (dotnet).\n\nUsefull to IT Admins to know details of environment, mainly in diferent contexts, such as contaniner environment.\n\nUsefull to developers in development time to know the environment virables and other informations about destination environment.\n\nNot recommended to leave on production environments with public access.\n\nThis is the portable version to run in a Docker container.\n\n## 1) Site Overview\n\n<br/>\n\n- The home site exposes information about your infrastructure in a clean and very easy table. \n\n<br/>\n\n![Overview](images/server-info-1.png)\n\n<br/>\n\n- Client site exposes information about client (browser) that is accessing the site, as well some client informations that is available in code.\n\n<br/>\n\n![Overview](images/server-info-2.png)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Server-Info", "org_name": "microsoft", "org_repo": "microsoft/Server-Info", "platform_org_repo": "github+microsoft/Server-Info", "link_to_repo": "https://github.com/microsoft/Server-Info", "platform": "github", "language": "HTML", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Introduction\r\n\r\nSupporting code and analyses for [Explaining compound activity predictions with a substructure-aware loss for graph neural networks](https://chemrxiv.org/engage/chemrxiv/article-details/63cfabd3ae221a94e93b4b2d)\r\n\r\n# Getting Started\r\n\r\n## Installation process\r\n\r\nWe recommend the [conda](https://docs.conda.io/en/latest/miniconda.html) Python package manager, and while an GPU is technically not required to run the models and feature attribution methods reported here, it is heavily encouraged. Furthermore, the code has only been tested under Linux. Make a new environment with the provided `environment.yml` file:\r\n\r\n```bash\r\nconda env create --file=environment.yaml\r\nconda activate molucn\r\n```\r\n\r\n## Prerequisites and data structure\r\n\r\nTo reproduce the results presented in the paper, download the following compressed data tarball from [here](https://figshare.com/articles/dataset/molucn-data/21215477) (~26GB, when uncompressed):\r\n\r\n```bash\r\nwget -O data.tar.gz https://figshare.com/ndownloader/files/37624043\r\ntar -xf data.tar.gz\r\n```\r\n\r\nAs the original [benchmark study](https://github.com/josejimenezluna/xaibench_tf), the data provided here is composed of subdirectories organized by PDB identifier, contaning activity data for each target considered in the benchmark. Subfolders have the following structure:\r\n\r\n```bash\r\n(molucn):~/molucn/data/1D3G-BRE$ tree\r\n.\r\n\u251c\u2500\u2500 1D3G-BRE_heterodata_list.pt\r\n\u251c\u2500\u2500 1D3G-BRE_seed_1337_info.txt\r\n\u251c\u2500\u2500 1D3G-BRE_seed_1337_stats.csv\r\n\u251c\u2500\u2500 1D3G-BRE_seed_1337_test.pt\r\n\u2514\u2500\u2500 1D3G-BRE_seed_1337_train.pt\r\n```\r\n\r\nAn explanation for each file in the subdirectories is provided below:\r\n\r\n- `1D3G-BRE_heterodata_list.pt`: dataset with all pairs of ligands saved as `torch_geometric.data.HeteroData` objects with information containing the SMILES, grount-truth colorings, ligand activities, and molecule structures in `torch_geometric.data.Data` objects and the MCS boolean lists at different thresholds.\r\n- `1D3G-BRE_seed_1337_info.txt`: text file containing information on the congeneric series: number of different ligands/compounds, number of pairs, number of training and testing pairs after 1. splitting the compounds in training and testing sets, 2. keeping pairs with no overlap, 3. rebalancing the training and testing pairs to have a 80%/20% ratio.\r\n- `1D3G-BRE_seed_1337_stats.csv`: summarizes the previous .txt file into a .csv file to facilitate information extraction.\r\n- `1D3G-BRE_seed_1337_test.pt` and `1D3G-BRE_seed_1337_train.pt`: contains the test and train pairs, respectively, saved as `torch_geometric.data.HeteroData` objects obtained after the preprocessing and rebalancing pipelines.\r\n\r\nAll the `.pt` files can be read with the Python [dill](https://pypi.org/project/dill/) module.\r\n\r\n## Build and Test\r\n\r\nGiven a specific target protein and a feature attribution, the `main.py` file trains a GNN model and generates node colorings using the explainability method selected for all 3 losses proposed in the study: MSE, MSE+AC and MSE+UCN.\r\n\r\nThe trained GNN models and their logs (metrics) will be saved under `models/` and `logs/` subdirectories in in the root directory of the repo. The atom coloring produced by the feature attribution methods are saved in `colors/`. Metrics measuring the performance of the different feature attribution techniques will be saved under `results/`.\r\n\r\n## Example: Test on the data from 1 protein target\r\n\r\nTo train the GNN model and run a feature attribution for one target protein (e.g., 1D3G-BRE) run:\r\n\r\n```bash\r\npython molucn/main.py --target \"1D3G-BRE\" --explainer {\"diff\" | \"gradinput\" | \"ig\" | \"cam\" | \"gradcam\"}\r\n```\r\n\r\nFor the random forest and masking baseline:\r\n\r\n```bash\r\npython molucn/main_rf.py --target \"1D3G-BRE\"\r\n```\r\n\r\n## Test on all 350 protein targets\r\n\r\nTo reproduce the results for the 350 protein targets:\r\n\r\n- GNN-based methods :\r\n\r\n```bash\r\nbash main.sh {diff|gradinput|ig|cam|gradcam}\r\n```\r\n\r\n- RF masking:\r\n\r\n```\r\nbash main_rf.sh\r\n```\r\n\r\n## Citation\r\n\r\nIf you find this work or parts thereof useful, please consider citing:\r\n\r\n```\r\n@article{amara2022substructure,\r\n  title={A substructure-aware loss for feature attribution in drug discovery},\r\n  author={Amara, Kenza and Rodriguez-Perez, Raquel and Luna, Jos{\\'e} Jim{\\'e}nez},\r\n  year={2022}\r\n}\r\n```\r\n", "repo_name": "molucn", "org_name": "microsoft", "org_repo": "microsoft/molucn", "platform_org_repo": "github+microsoft/molucn", "link_to_repo": "https://github.com/microsoft/molucn", "platform": "github", "language": "Python", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# vivainsights <img src=\"https://raw.githubusercontent.com/microsoft/vivainsights/main/man/figures/logo.png\" align=\"right\" width=15% />\n\n[![R build status](https://github.com/microsoft/vivainsights/workflows/R-CMD-check/badge.svg)](https://github.com/microsoft/wpa/actions/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT/)\n[![CRAN status](https://www.r-pkg.org/badges/version/vivainsights)](https://CRAN.R-project.org/package=vivainsights/)\n[![CRAN last month downloads](https://cranlogs.r-pkg.org/badges/last-month/vivainsights?color=green/)](https://cran.r-project.org/package=vivainsights/)\n\n## Analyze and Visualize Viva Leader Insights data\n\nThis is an R package for analyzing and visualizing data from [Microsoft Viva Advanced Insights](https://www.microsoft.com/en-gb/microsoft-viva/insights/) (previously Microsoft Workplace Analytics). \n\n## With the **vivainsights** package, you can...\n\n1. **Run prebuilt analysis and visualizations** off advanced insights data with settings for HR variables, privacy threshold, etc.\n\n2. Leverage **advanced analytics functions** which are built for Viva Insights metrics\n\n3. Integrate analysis of Viva Insights data with your R workflow seamlessly\n\nThis library is compatible with the latest implementation of the Viva Insights Analyst Workbench. For the R library compatible with the legacy implementation, please visit the **wpa** package [website](https://microsoft.github.io/wpa/) or [GitHub repository](https://github.com/microsoft/wpa/) . \n\n\n## Related repositories\n\n- [Viva RMarkdown Report Marketplace](https://github.com/microsoft/VivaRMDReportMarketplace/)\n- [Viva Insights Sample Code](https://github.com/microsoft/viva-insights-sample-code/)\n- [Viva Insights Zoom Integration](https://github.com/microsoft/vivainsights_zoom_int/)\n- [Viva Insights OData Query Download](https://github.com/microsoft/vivainsights-odatadl/)\n- [Viva Insights R library (legacy)](https://microsoft.github.io/wpa/)\n\n---\n\n## Code of Conduct\n\nWe would ask you to please read the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/) prior to engaging with this package.\n\n\n**Trademarks** \n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n---\n\n## Finding this project useful? \n\n:star: Please star this repository to keep us going! \n", "repo_name": "vivainsights", "org_name": "microsoft", "org_repo": "microsoft/vivainsights", "platform_org_repo": "github+microsoft/vivainsights", "link_to_repo": "https://github.com/microsoft/vivainsights", "platform": "github", "language": "R", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# vscode-languagedetection\n\nDeep learning model to recommend the best extension for a given VS Code workspace.\n\n## Usage\n\nSee `test` and API docs in `lib/index.ts`.\n\n## Local development\n\nTo build from source, follow these steps:\n\n1. Clone the repository\n2. Run `npm install`\n3. Run `npm run watch`\n\nTo run the tests, simply run `npm test`.\n\nTo build a production package:\n\n1. Run `npm run build`\n2. Run `npm pack`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-extension-recommender", "org_name": "microsoft", "org_repo": "microsoft/vscode-extension-recommender", "platform_org_repo": "github+microsoft/vscode-extension-recommender", "link_to_repo": "https://github.com/microsoft/vscode-extension-recommender", "platform": "github", "language": "TypeScript", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Windows Forms Designer Extensibility Documents & Samples\n\nBefore you start developing Custom Controls for the Out-Of-Process WinForms Designer, please read the [Blog Post covering this topic](https://devblogs.microsoft.com/dotnet/custom-controls-for-winforms-out-of-process-designer/). To ramp up with the Out-Of-Process WinForms Designer in general and the motivation and necessity of its redesign, please [read the basic overview in this blog post](https://devblogs.microsoft.com/dotnet/state-of-the-windows-forms-designer-for-net-applications/).\n\n## Documents\n* [Using Visual Studio Solution Templates for creating Control Designers/Type Editors for the Out-Of-Process .NET Core WinForms Designer](https://github.com/microsoft/winforms-designer-extensibility/tree/main/Templates/TypeEditor).\n\n## Templates and Code Samples\n\n* [Creating a Framework-based Control Designer, which doesn't use a custom Type Editor.](https://github.com/microsoft/winforms-designer-extensibility/tree/main/Samples/TypeEditor/Framework/TileRepeater_Medium)\n* [Creating a complex Control Designer with a dedicated Type Editor.](https://github.com/microsoft/winforms-designer-extensibility/tree/main/Samples/TypeEditor/Dotnet/TileRepeater_Medium)\n* [C# Visual Studio Template Solution for creating a custom Type Editor.](https://github.com/microsoft/winforms-designer-extensibility/tree/main/Templates/TypeEditor/src/TemplateSolutions/CS.CustomTypeEditor)\n* [Visual Basic Visual Studio Template Solution for creating a custom Type Editor.](https://github.com/microsoft/winforms-designer-extensibility/tree/main/Templates/TypeEditor/src/TemplateSolutions/VB.CustomTypeEditor)\n\n## Code of Conduct\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n\n## License\n\nThese samples and templates are all licensed under the MIT license. See the [LICENSE](LICENSE) file in the root.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "winforms-designer-extensibility", "org_name": "microsoft", "org_repo": "microsoft/winforms-designer-extensibility", "platform_org_repo": "github+microsoft/winforms-designer-extensibility", "link_to_repo": "https://github.com/microsoft/winforms-designer-extensibility", "platform": "github", "language": "C#", "stargazers_count": 31, "watchers_count": 31}, {"README_text": "Monza is a research unikernel aimed to explore the design space for a non-POSIX environment for running micro-service type workloads in the cloud.\nIt offers a minimal C/C++ development environment focused on in-memory computation with no filesystem, registry or other locally persistent state.\n\nWith Monza we are exploring a number of different research directions:\n* New system-level components and abstractions to enable memory-safe and high-performance applications in cloud environments.\n* Minimal environment for application deployment without a traditional operating system onto isolated VMs (for example using [AMD SEV-SNP](https://www.amd.com/system/files/TechDocs/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf)).\n\nThis research project is at an early stage and is open sourced to facilitate academic collaborations. We are keen to engage in research collaborations on this project, please do reach out to discuss this.\n\nThe project is not ready to be used outside of research. It has no connection to any production services running in the cloud today and there are currently no plans to make it into a product.\n\n# Supported hypervisors and VMMs\nSince Monza is targeting virtualized workloads in the cloud, it assumes an underlying virtualization environment to avoid some legacy x86 code requirements.\nThe currently supported platforms:\n* QEMU with emulation.\n* QEMU with KVM.\n\n# Development Documents\n## [Organization](docs/organization.md)\n## [Building](docs/build.md)\n## [Contributing](CONTRIBUTING.md)\n## [Running tests and apps using QEMU](docs/qemu.md)\n## [Developing new apps](docs/apps.md)", "repo_name": "monza", "org_name": "microsoft", "org_repo": "microsoft/monza", "platform_org_repo": "github+microsoft/monza", "link_to_repo": "https://github.com/microsoft/monza", "platform": "github", "language": "C++", "stargazers_count": 43, "watchers_count": 43}, {"README_text": "# Project\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n# Dev Tunnels SSH Library\nA Secure Shell (SSH2) client and server protocol library, implemented in both\nC# and TypeScript.\n\n## Feature Highlights\n - SSH over any .NET Stream or JavaScript stream (including but not limited to\n   TCP socket streams)\n - Configurable, extensible, negotiated algorithms for key-exchange, encryption,\n   integrity (HMAC), and public-key authentication\n - Channel multiplexing, with ability to stream data to/from channels\n - Port-forwarding, with ability to stream data to/from remote ports\n - Piping between two sessions can relay all channels and port-forwarding\n - Extensible channel request handling (for \"exec\", \"shell\", or custom requests)\n - Supports [reconnecting](./ProtocolExtensions.md) a disconnected session\n   without disrupting channel streams.\n - Compatible with common SSH software. (Tested against OpenSSH.)\n - Supports importing and exporting several key formats, including\n   password-protected keys.\n\n### Limitations\nThe following features are not implemented in this library, though they could be built\non top of it:\n - Allowing a client to login to a user account on the server\n - Connecting to a shell on the server\n - Invoking shell commands on the server\n - Transferring files (SCP or SFTP)\n - Rendering a terminal on the client side\n\nFuture development may add support for some of these capabilities, likely in the\nform of additional optional packages.\n\n## C# (.NET Framework, .NET Core, .NET 6)\nThe C# library targets .NET Framework 4.8, .NET Standard 2.1 (.NET Core 3.1, .NET 5),\nand .NET 6. It's tested on Windows, Mac, & Ubuntu. For details about the .NET library,\nsee [src/cs/Ssh/README.md](./src/cs/Ssh/README.md).\n\n## TypeScript (Node.js or Browser)\nThe TypeScript implementation supports either Node.js (>= 14.x) or a browser\nenvironment. The Node.js version is tested on Windows, Mac & Ubuntu; the browser\nversion is tested on Chrome & Edge Chromium, though it should work in any modern\nbrowser that supports the web crypto API. Note that since script on a web page\ncannot access native TCP sockets, the standard use of SSH over TCP is not possible;\nsome other stream transport like a websocket may be used. For details about the\nTypeScript library, see [src/ts/ssh/README.md](./src/ts/ssh/README.md).\n\n## Packages\n\n|                                          | C# NuGet package | TS npm package |\n| ---------------------------------------- | ---------------- | -------------- |\n| SSH core protocol and crypto             | [**`Microsoft.DevTunnels.Ssh`**](https://www.nuget.org/packages/Microsoft.DevTunnels.Ssh) | [**`@microsoft/dev-tunnels-ssh`**](https://www.npmjs.com/package/@microsoft/dev-tunnels-ssh)\n| SSH public/private key import/export     | [**`Microsoft.DevTunnels.Ssh.Keys`**](https://www.nuget.org/packages/Microsoft.DevTunnels.Ssh.Keys/) | [**`@microsoft/dev-tunnels-ssh-keys`**](https://www.npmjs.com/package/@microsoft/dev-tunnels-ssh-keys)\n| SSH TCP connections and port-forwarding  | [**`Microsoft.DevTunnels.Ssh.Tcp`**](https://www.nuget.org/packages/Microsoft.DevTunnels.Ssh.Tcp/) | [**`@microsoft/dev-tunnels-ssh-tcp`**](https://www.npmjs.com/package/@microsoft/dev-tunnels-ssh-tcp)\n\nThe optional \"keys\" and \"TCP\" packages depend on the core package. All SSH packages\nin an app must be the same major and minor version; the patch version (3rd component)\nmay differ if necessary. In other words, any changes that impact cross-package\ndependencies will increment at least the minor version.\n\n## Development\nSee [README-dev.md](README-dev.md).\n\n## SSH Algorithms Support\nCrypto algorithms below rely on platform APIs in .NET ([System.Security.Cryptography](\nhttps://docs.microsoft.com/en-us/dotnet/api/system.security.cryptography?view=netstandard-2.1\n)), Node.js ([crypto module](https://nodejs.org/api/crypto.html)) or browsers ([web crypto](\nhttps://developer.mozilla.org/en-US/docs/Web/API/Web_Crypto_API)). There is _one_ use of\na 3rd-party library: the [diffie-hellman](https://github.com/crypto-browserify/diffie-hellman)\npackage is required in browsers because there is no corresponding web crypto API.\n\nLegend:  \n    \u2714\u2714\u2714 - Enabled and preferred in default session configuration.  \n    \u2714\u2714 - Enabled (but not preferred) in default session configuration.  \n    \u2714 - Supported and can be enabled in custom session configuration.  \n    \u2611 - Coming soon (working in a branch or PR).  \n    ?? - Under consideration for the future.\n\n| Type         | Algorithm Name                  | Status   |\n| -------------| ------------------------------- | -------- |\n| | |\n| key-exchange | `diffie-hellman-group16-sha512` | \u2714\u2714\n| key-exchange | `diffie-hellman-group14-sha256` | \u2714\u2714\n| key-exchange | `ecdh-sha2-nistp521`            | \u2714\n| key-exchange | `ecdh-sha2-nistp384`            | \u2714\u2714\u2714\n| key-exchange | `ecdh-sha2-nistp256`            | \u2714\u2714\n| key-exchange | `curve25519-sha256`             | ??   [1]\n| | |\n| public-key   | `rsa-sha2-512`                  | \u2714\u2714\u2714\n| public-key   | `rsa-sha2-256`                  | \u2714\u2714\n| public-key   | `ecdsa-sha2-nistp256`           | \u2714\u2714\n| public-key   | `ecdsa-sha2-nistp384`           | \u2714\u2714\n| public-key   | `ecdsa-sha2-nistp521`           | \u2714\n| public-key   | `ssh-ed25519`                   | ??   [1]\n| public-key   | `*-cert-v01@openssh.com`        | ??   [2]\n| | |\n| cipher       | `aes256-cbc`                    | \u2714\u2714  [3]\n| cipher       | `aes256-ctr`                    | \u2714\u2714\n| cipher       | `aes192-cbc`                    | \u2714\n| cipher       | `aes192-ctr`                    | \u2714\n| cipher       | `aes128-cbc`                    | \u2714\n| cipher       | `aes128-ctr`                    | \u2714\n| cipher       | `aes256-gcm@openssh.com`        | \u2714\u2714\u2714\n| cipher       | `aes128-gcm@openssh.com`        | \u2714\n| cipher       | `chacha20-poly1305@openssh.com` | ??   [1]\n| | |\n| mac          | `hmac-sha2-512`                 | \u2714\u2714\n| mac          | `hmac-sha2-256`                 | \u2714\u2714\n| mac          | `hmac-sha2-512-etm@openssh.com` | \u2714\u2714\u2714\n| mac          | `hmac-sha2-256-etm@openssh.com` | \u2714\u2714\n\n\n[1] May require use of 3rd-party libs, though Curve25519 APIs are under\nconsideration for [.NET](https://github.com/dotnet/runtime/issues/14741) and\n[web crypto](https://github.com/w3c/webcrypto/issues/233).  \n[2] OpenSSH certificate support should be possible with some work.  \n[3] AES-CBC is not supported in browsers due to a [limitation](\nhttps://github.com/w3c/webcrypto/issues/73) of the web crypto API. AES-CTR or\nAES-GCM works fine.\n\nThere is no plan to have built-in support for older algorithms known to be\ninsecure (for example SHA-1), though in some cases these can be easily added by\nthe application.\n\n## Key Format Support\nSupport for importing and exporting keys in various formats is provided in\nNuGet/npm packages separate from the core SSH functionality. Some key formats\nare only implemented in _either_ the C# or TS libraries, not both.\nSee also [src/cs/SSH.Keys/README.md](src/cs/SSH.Keys/README.md)\nor [src/ts/ssh-keys/README.md](src/ts/ssh-keys/README.md).\n\n| Key Format           | Key Algorithm | Password Protection | Format Description |\n| -------------------- | ------------- | ------------------- | ------------------ |\n| SSH public key       | RSA<br>ECDSA  | N/A                 | Single line key algorithm name, base64-encoded key bytes, and optional comment. Files conventionally end with `.pub`.\n| PKCS#1               | RSA           | _import&nbsp;only_  | Starts with one of:<br>`-----BEGIN RSA PUBLIC KEY-----`<br>`-----BEGIN RSA PRIVATE KEY-----`\n| SEC1                 | ECDSA         | _import&nbsp;only_  | Starts with:<br>`-----BEGIN EC PRIVATE KEY-----`\n| PKCS#8               | RSA<br>ECDSA  | \u2714                  | Starts with one of:<br>`-----BEGIN PUBLIC KEY-----`<br>`-----BEGIN PRIVATE KEY-----`<br>`-----BEGIN ENCRYPTED PRIVATE KEY-----`\n| SSH2<br>_C# only_    | RSA           | \u2714                  | Starts with one of:<br>`---- BEGIN SSH2 PUBLIC KEY ----`<br>`---- BEGIN SSH2 ENCRYPTED PRIVATE KEY ----`\n| OpenSSH<br>_C# only_ | RSA<br>ECDSA  | \u2714                  | Starts with one of:<br>`-----BEGIN OPENSSH PUBLIC KEY-----`<br>`-----BEGIN OPENSSH PRIVATE KEY-----`\n| JWK<br>_TS only_     | RSA<br>ECDSA  | N/A                 | JSON with key algorithm name and parameters\n\n## References\nThe following RFCs define the SSH protocol:\n - [RFC 4250 - SSH Protocol Assigned Numbers](https://tools.ietf.org/html/rfc4250)\n - [RFC 4251 - SSH Protocol Architecture](https://tools.ietf.org/html/rfc4251)\n - [RFC 4252 - SSH Authentication Protocol](https://tools.ietf.org/html/rfc4252)\n - [RFC 4253 - SSH Transport Layer Protocol](https://tools.ietf.org/html/rfc4253)\n - [RFC 4254 - SSH Connection Protocol](https://tools.ietf.org/html/rfc4254)\n - [RFC 4716 - SSH Public Key File Format](https://tools.ietf.org/html/rfc4716)\n - [RFC 5647 - AES GCM for the SSH Protocol](https://tools.ietf.org/html/rfc5647)\n - [RFC 5656 - EC Algorithm Integration in SSH](https://tools.ietf.org/html/rfc5656)\n - [RFC 8308 - SSH Extension Negotiation](https://tools.ietf.org/html/rfc8308)\n", "repo_name": "dev-tunnels-ssh", "org_name": "microsoft", "org_repo": "microsoft/dev-tunnels-ssh", "platform_org_repo": "github+microsoft/dev-tunnels-ssh", "link_to_repo": "https://github.com/microsoft/dev-tunnels-ssh", "platform": "github", "language": "C#", "stargazers_count": 84, "watchers_count": 84}, {"README_text": "# rpmoci\n*rpmoci is a nascent container build tool from Azure for Operators. It's currently used in a small number of internal container images builds.*\n\nrpmoci builds OCI container images from RPM packages, using [DNF](https://github.com/rpm-software-management/dnf). It's essentially a containerization wrapper around `dnf install --installroot=/some/rootfs PACKAGE [PACAKGE ...]`.\n\nrpmoci features:\n\n - **deterministic** rpmoci locks RPM dependencies using the package file/lockfile paradigm of bundler/cargo etc and supports vendoring of RPMs for later rebuilds.\n - **no container runtime required** rpmoci can build images in environments without docker access.\n - **small** rpmoci images are built solely from the RPMs you request, so don't contain unnecessary dependencies.\n\nThe design of rpmoci is influenced by [apko](https://github.com/chainguard-dev/apko) and [distroless](https://github.com/GoogleContainerTools/distroless) tooling.\n\n*rpmoci can loosely be considered a wrapper around `dnf install --installroot=/path/to/rootfs`.\"*\n*If you don't want to use rpmoci but like the idea of creating small RPM-based container images then you may be able to just use `dnf install --installroot` and [umoci](https://umo.ci/), which is how rpmoci started out.*\n## Installing\nrpmoci isn't published to a widely accessible location yet so you'll need build it from source with `cargo build`\n\nrpmoci has a runtime dependency on dnf.\n\n## Getting started\nYou need to create an rpmoci.toml file. An example is:\n\n```toml\n[contents] # specifies the RPMs that comprise the image\nrepositories = [ \"mariner-official-base\" ]\npackages = [\n  \"tini\"\n]\n\n[image] # specifies image configuration such as entrypoint, ports, cmd, etc.\nentrypoint = [ \"tini\", \"--\" ]\n```\n\nThis configures rpmoci to install `tini` and its dependencies from the mariner-official-base repository, and configures the image entrypoint to use `tini`.\n\n\nThis can then be built into an image:\n```bash\nsudo rpmoci build --image tini --tag my-first-rpmoci-image\n```\n\nThe image will be created in a OCI layout directory called `tini`.\nrpmoci doesn't handle image distribution - users are expected to use tools like [oras](https://oras.land/) or [skopeo](https://github.com/containers/skopeo) to push the image to a registry.\n\nA lockfile, `rpmoci.lock`, will be created so you can re-run the build later and get the same packages.\n*assuming they still exist in the specified repository... rpmoci supports vendoring RPMs so you can repeat locked builds without relying on that*\n\n## Reference\n### Package Specification\n#### Repository configuration\nThe repository section defines where RPMs are sourced from.\n\nIn the getting started example, the repository was specified by its repo id on the running system.\nIt is also possible to fully specify the repository in `rpmoci.toml`, if you want to create a portable `rpmoci.toml` that can say, build the same image when running on Fedora/Ubuntu/Mariner.\n\nRepositories can be specified via their base URL\n```toml\n[contents]\nrepositories = [\"https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64\"]\n```\n\nor defined with additional configuration options in the package manifest file (`rpmoci.toml` by default, can be specified via `-f FILE` on CLI)\n```toml\n[[contents.repositories]]\nurl = https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64/\noptions = { includepkgs = \"foo,bar\" }\n```\n\nBy default the `gpgcheck` and `sslverify` are enabled - these can be disabled via the `options` field.\n\nAll system repos are ignored, other than those explicitly specified via repo id.\n\n#### Package configuration\n\nPackage specifications are added under the `contents.packages` key. Both local and remote packages are supported\n\n```toml\n[contents]\nrepositories = [\"https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64\"]\npackages = [\n  \"postgreqsl\", # a package from the above repository\n  \"path/to/local.rpm\", # a local RPM\n]\n```\n\n#### GPG key configuration\nGPG keys can be configued via the repository options or the `gpgkeys` field\n\n```toml\n[contents]\nrepositories = [\"https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64\"]\ngpgkeys = [\n  \"https://raw.githubusercontent.com/microsoft/CBL-Mariner/2.0/SPECS/mariner-repos/MICROSOFT-RPM-GPG-KEY\"\n]\npackages = [\n  \"postgresql\"\n]\n```\n\nWhen building images the package signatures will be verified using the configured GPG keys, except for local packages or packages from repositories where `gpgcheck` has explicitly been disabled.\n\n#### Authenticated RPM repositories\nTo use a repository that requires HTTP basic authentication, specify an `id` for the repository in the toml file,\nand define the environment variables `RPMOCI_<id>_HTTP_USERNAME` and `RPMOCI_<id>_HTTP_PASSWORD` to be the HTTP authentication credentials, where `<id>` is the uppercased repo id.\n\nE.g with the following configuration you would need to define the environment variables `RPMOCI_FOO_HTTP_USERNAME` and `RPMOCI_FOO_HTTP_PASSWORD`:\n```toml\n[[contents.repositories]]\nurl = https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64/\nid = \"foo\"\n```\n\n#### Documentation\n\nWhether or not documentation files are included in the produced containers can be specified via the `content.docs` boolean field.\nBy default documentation files are not included, optimizing for image size.\n\n### Image building\n\nRunning `rpmoci build --image foo --tag bar` will build a container image in OCI format.\n\n```bash\n$ rpmoci build --image foo --tag bar\n...\n$ cat foo/index.json | jq\n{\n  \"schemaVersion\": 2,\n  \"manifests\": [\n    {\n      \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\",\n      \"digest\": \"sha256:1ad8cc1866d359e4e2ecb37fcc96759815540f06cb468811dcb9b8aac51da90d\",\n      \"size\": 350,\n      \"annotations\": {\n        \"org.opencontainers.image.ref.name\": \"bar\"\n      }\n    }\n  ]\n}\n```\n\nThis image can then be copied using OCI tools such as skopeo or oras. E.g to copy to a local docker daemon:\n```bash\n$ skopeo copy oci:foo:bar docker-daemon:foo:bar\nGetting image source signatures\nCopying blob 77b582c1f09c done  \nCopying config 577bea913f done  \nWriting manifest to image destination\nStoring signatures\n```\n\n#### Image configuration\n\nAdditional [image configuration](https://github.com/opencontainers/image-spec/blob/main/config.md#properties) can be specified under the `image` key:\n\n```toml\n[contents]\nrepositories = [\"https://packages.microsoft.com/cbl-mariner/2.0/prod/base/x86_64\"]\ngpgkeys = [\n  \"https://raw.githubusercontent.com/microsoft/CBL-Mariner/2.0/SPECS/mariner-repos/MICROSOFT-RPM-GPG-KEY\"\n]\npackages = [\n  \"postgresql\"\n]\n[image]\nentrypoint = [\"tini\", \"--\"]\ncmd = [ \"foo\" ]\nexposed_ports = [\"8080/tcp\"]\n```\n\nThe PATH environment variable is set to `/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin` by default, but can be overridden via the `image.envs` field.\n\n### Lockfiles\n\nrpmoci uses DNF to produce a lockfile of the build. This can be used to subsequently repeat the build with `rpmoci build --locked`.\n\nA lockfile can be created or updated by running `rpmoci update`:\n\n```bash\n$ rpmoci update\nAdding filesystem 1.1-10.cm2\nAdding grep 3.7-2.cm2\nAdding openssl 1.1.1k-17.cm2\nAdding libgcc 11.2.0-2.cm2\nAdding postgresql 14.2-2.cm2\nAdding libxml2 2.9.14-1.cm2\nAdding ncurses-libs 6.3-1.cm2\nAdding pcre 8.45-2.cm2\nAdding pcre-libs 8.45-2.cm2\nAdding glibc 2.35-2.cm2\nAdding bash 5.1.8-1.cm2\nAdding libsepol 3.2-2.cm2\nAdding libcap 2.60-1.cm2\nAdding krb5 1.19.3-1.cm2\nAdding openldap 2.4.57-7.cm2\nAdding coreutils 8.32-3.cm2\nAdding postgresql-libs 14.2-2.cm2\nAdding libselinux 3.2-1.cm2\nAdding openssl-libs 1.1.1k-17.cm2\nAdding readline 8.1-1.cm2\nAdding tzdata 2022a-1.cm2\nAdding xz-libs 5.2.5-1.cm2\nAdding libstdc++ 11.2.0-2.cm2\nAdding zlib 1.2.12-1.cm2\nAdding e2fsprogs-libs 1.46.5-1.cm2\nAdding gmp 6.2.1-2.cm2\nAdding bzip2-libs 1.0.8-1.cm2\n```\n\n### Vendoring\n\nRPMs can be vendored to a folder using `rpmoci vendor`. A vendor folder can be used during a build to avoid contacting package repositories.\n\n```bash\n$ rpmoci vendor --out-dir vendor\n$ ls vendor\nls vendor\n031e779a7ce198662c5b266d7b0dfc9eece9c0c888a657b6a9bb7731df0096d0.rpm  8ea3d75dbb48fa12eacf732af89a600bd97709b55f88d98fe129c13ab254de95.rpm\n...\n$ rpmoci build --image foo --tag bar --vendor-dir vendor\n```\n*Vendor directories from different invocations of `rpmoci vendor` should be kept isolated, as rpmoci currently attempts to install all RPMs from the vendor directory.*\n\n\n### SBOM support\nrpmoci doesn't have native SBOM support, but because it just uses standard OS package functionality SBOM generators like trivy and syft can be used to generate SBOMs for the produced images.\n\n## Developing\n\nrpmoci is written in Rust and currently resolves RPMs using DNF via an embedded Python module.\n\nIt has buildtime dependencies on `python3-devel` and `openssl-devel`.\n\nAfter checking out the project you can do\n```bash\ncargo run\n```\nto run it, or build an RPM using [cargo-generate-rpm](https://github.com/cat-in-136/cargo-generate-rpm):\n```bash\ncargo generate-rpm\n```\n\n", "repo_name": "rpmoci", "org_name": "microsoft", "org_repo": "microsoft/rpmoci", "platform_org_repo": "github+microsoft/rpmoci", "link_to_repo": "https://github.com/microsoft/rpmoci", "platform": "github", "language": "Rust", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# DatagenDV\n\nDatagenDV is a Python framework for generating C compatible test stimulus using YAML and random\nconstraints. The framework leverages multiple open-source projects by tying them together into base classes which can then be\nextended by its users.\n\nDatagenDV was made open source to provide an example of how python frameworks can be made with verifiation in mind. \nAlthough designed to be generic and project agnostic, it is not intended for easy plug and play into any project infrastructure at this time. \nIt is one way of doing things and is aimed a particular flow.  \nDatagenDV is better utilized as an example and starting point.\n\nPlease see the examples directory which contain Jupyter notebooks which explain and show how it is used.\nFor a fuller explanation of DatagenDV's features see the 2023 DVCon paper. \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "datagenDV", "org_name": "microsoft", "org_repo": "microsoft/datagenDV", "platform_org_repo": "github+microsoft/datagenDV", "link_to_repo": "https://github.com/microsoft/datagenDV", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Overview\n\nThis repository produces metadata for the Windows Driver Kit (WDK) using the tooling provided by the [win32metadata](https://github.com/microsoft/win32metadata) project. The metadata is published to NuGet.org as the [Microsoft.Windows.WDK.Win32Metadata](https://www.nuget.org/packages/Microsoft.Windows.WDK.Win32Metadata/) NuGet package. This mirrors the [Microsoft.Windows.SDK.Win32Metadata](https://www.nuget.org/packages/Microsoft.Windows.SDK.Win32Metadata/) package that it depends on.\n\nDISCLAIMER: wdkmetadata is EXPERIMENTAL and intended to provide a place for the community to build out rich metadata for the WDK to enable driver development in language projections like the [windows crate](https://github.com/microsoft/windows-rs) for Rust.\n\n# Principles\n\n* This repository is sponsored by Microsoft but will primarily be driven by outside collaborators and community contributions\n* This repository will use public tooling provided by [Microsoft.Windows.WinmdGenerator](https://www.nuget.org/packages/Microsoft.Windows.WinmdGenerator/) wherever possible\n* This repository will aim to mirror [win32metadata](https://github.com/microsoft/win32metadata) in layout and development experience to provide a consistent contributor experience across the WDK and the SDK\n* This repository will provide foundational metadata for native APIs that ship in the WDK and will not provide abstractions\n* This repository does not change the license of the original WDK and SDK headers used to generate the metadata. Any such artifacts checked into this repository support metadata production only and do not imply a change of their original licenses.\n\n# Repository Bootstrap\n\n* [DONE] Clone the win32metadata repo\n* [DONE] Remove win32metadata baggage (samples, scripts, sources, tests, etc.) and onboard to Microsoft.Windows.WinmdGenerator\n* [DONE] Rename generation/SDK to generation/WDK and rename Windows.Win32.winmd to Windows.Wdk.winmd\n* [DONE] Import WDK headers\n  * NOTE: This repository needs both SDK headers and WDK headers since the WDK headers depend on the SDK headers to compile. Original SDK and WDK headers for Windows 11 Build 22000 were checked into [IdlHeaders](generation/WDK/IdlHeaders) and then recompiled to [RecompiledIdlHeaders](generation/WDK/RecompiledIdlHeaders) using an adapted version of [RecompileIdlFilesForScraping](scripts/RecompileIdlFilesForScraping.ps1). Only WDK headers should be added to partitions in this repository.\n* [DONE] Create example API partition (https://github.com/microsoft/win32metadata/issues/509)\n* [DONE] Produce Microsoft.Windows.WDK.Win32Metadata NuGet package for Windows.Wdk.winmd\n* [DONE] Port over full/incremental build support from [win32metadata](https://github.com/microsoft/win32metadata)\n* [DONE] Port over baseline diffing support from [win32metadata](https://github.com/microsoft/win32metadata)\n* [DONE] Port over CI and release pipelines from [win32metadata](https://github.com/microsoft/win32metadata)\n* [DONE] Port over [header ingestion script](https://github.com/microsoft/win32metadata/blob/main/scripts/RecompileIdlFilesForScraping.ps1) from [win32metadata](https://github.com/microsoft/win32metadata) and include WDK headers\n* [DONE] Port over automatic import library detection to minimize changes to libMappings.rsp\n  * NOTE: When adding new headers or troubleshooting why APIs may not be emitted in the winmd, make sure that [libMappings.rsp](generation/WDK/libMappings.rsp) has entries for the APIs with a valid import library that includes the proper extension (e.g. dll, drv, exe, etc.).\n* Onboard outside collaborators and community contributors", "repo_name": "wdkmetadata", "org_name": "microsoft", "org_repo": "microsoft/wdkmetadata", "platform_org_repo": "github+microsoft/wdkmetadata", "link_to_repo": "https://github.com/microsoft/wdkmetadata", "platform": "github", "language": "C++", "stargazers_count": 54, "watchers_count": 54}, {"README_text": "# Sample extension for pxt-minecraft\n\nThis repo provides a sample extension for the Microsoft MakeCode Minecraft Education Edition editor.\n\nFor helpful tips on defining blocks, see [creating blocks](./creating-blocks.md).\n\n## Getting started\n\nMicrosoft MakeCode can only be used with Minecraft Education Edition. You can download Minecraft Education Edition here:\n\nhttps://education.minecraft.net/en-us/get-started/download\n\nOnce installed, you can access the MakeCode editor by following these steps:\n\n1. Log in using your Employee or Educational institution credentials\n2. Create a new world. Leave all of the default options for the world as-is (or see the \"world options\" section below for some helpful tips)\n3. Once the world has loaded, press \"c\" on your keyboard and select \"MakeCode\" from the list of options to open the editor\n\n\n## Extension dev loop\n\nThe easiest way to develop an extension is to first write your code from within the MakeCode editor inside of Minecraft Education Edition. The extension development process looks something like this:\n\n1. Write your code inside MakeCode Education Edition and test it.\n2. Once your code is in a good state, navigate to https://minecraft.makecode.com/?github=1 in a browser\n3. Create a new project and turn it into a GitHub repo by pressing the GitHub button in the bottom bar\n4. Switch to the JavaScript editor and copy over your code from Minecraft Education Edition and define your blocks\n\nFor helpful tips on defining blocks, see [creating blocks](./creating-blocks.md).\n\n## Helpful tips\n\n### World Options\n\nThere are a few useful options you can check when creating a world:\n\n1. Set the default game mode to \"Creative\". This will allow you to fly and prevent you from dying. You can change the game mode at any time by opening the chat and typing `/gamemode s` for survival or `/gamemode c` for creative\n2. Make sure the difficulty is higher than peaceful. Peaceful will prevent any monster mobs from spawning.\n3. If you are going to be collaborating with someone else inside this world, change the \"Permission level for players who join your world\" setting to \"Operator\" so that they can also run code\n4. Under \"World Options\", toggle \"Show Coordinates\" to on\n\n### Performance options\n\nIf you are running another performance intensive app (e.g. Microsoft Teams) at the same time as Minecraft, it can be useful to change your graphics settings to improve perf. To access the video settings:\n\n1. While inside a world, press esc to open the pause menu\n2. Click \"Settings\"\n3. Click \"Video\" in the left-hand bar\n\nThe following settings can drastically improve performance:\n\n1. Turn off \"Fancy Leaves\", \"Beautiful Skies\", \"Smooth Lighting\", and \"Fancy Graphics\"\n2. Set the \"Render Distance\" to the minimum value\n3. Set \"Anti-Aliasing\" to 1\n\nAdditionally, disconnecting any external monitors will also improve performance by quite a bit.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "minecraft-zoobuilder", "org_name": "microsoft", "org_repo": "microsoft/minecraft-zoobuilder", "platform_org_repo": "github+microsoft/minecraft-zoobuilder", "link_to_repo": "https://github.com/microsoft/minecraft-zoobuilder", "platform": "github", "language": "TypeScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Commercial Marketplace Offer Templates\r\nThis repository provides an easy way to get started on publishing an offer to the Microsoft [Commercial Marketplace](https://learn.microsoft.com/en-us/azure/marketplace/overview).\r\n\r\nIncluded in this repository are:\r\n- Azure Application and Azure Virtual Machine offer type templates\r\n- GitHub actions and starter workflows to help you get started with automation\r\n- Scripts to help you run common tasks, such as creating a VM image\r\n\r\nThese templates are designed to be a starting point for your offer and are not intended to be a complete solution. You are free to modify the templates to meet your needs.\r\n\r\n## Getting Started\r\n\r\n### Step 1: Create a repository\r\nThe easiest way to get started is to use the [GitHub Template Repository](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-from-a-template) feature to create a new repository from this template. You can also fork or clone this repository and modify the files to meet your needs.\r\n\r\n### Step 2: Install Tools\r\n\r\nThe templates and scripts in this repository use the following tools:\r\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)\r\n- [Bicep](https://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/install#azure-cli)\r\n- [Packer](https://www.packer.io/downloads)\r\n- [Azure Partner Center CLI](https://github.com/microsoft/az-partner-center-cli)\r\n- [Pester](https://pester.dev/docs/introduction/installation)\r\n- [PowerShell](https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell)\r\n- [Python 3.7+](https://www.python.org/downloads/)\r\n\r\nPlease refer to the setup scripts for your operating system for easy installation:\r\n- [Linux (Ubuntu/Debian)](setup/linux_ubuntu_debian.sh)\r\n- [Linux (CentOS/RHEL)](setup/linux_centos_rhel.sh)\r\n- [macOS](setup/macos.sh) - installs and uses [Homebrew](https://brew.sh/) to install tools.\r\n- [Windows](setup/windows.ps1) - installs and uses [Chocolatey](https://chocolatey.org/) (default) to install tools. To install without Chocolatey, run `./windows.ps1 -useChocolatey $false`.\r\n\r\n### Step 3: Associate an Azure AD application with your Partner Center account\r\n\r\nThe scripts in this repository use the Partner Center API to create and manage offers. To use the scripts, you must first associate an Azure AD application with your Partner Center account. Follow the instructions in the Commercial Marketplace [documentation](https://learn.microsoft.com/en-us/azure/marketplace/submission-api-onboard) to create an Azure AD application and associate it with your Partner Center account.\r\n\r\n### Step 4: Modify Offer Template Files\r\n\r\nUse the following offer templates to build your own Commercial Marketplace offerings. Each offer template includes a README file that contains instructions on how to build the offer and modify it for your use case.\r\n\r\n### [Solution Template with Custom Script Extension](marketplace/application/base-image-vm/README.md)\r\n\r\nThis offer template demonstrates how to build a solution template Azure Application offer. This offer deploys a Windows Server 2019 VM and runs a custom script extension that writes content to a file. The custom script extension runs a PowerShell script after the VM has been provisioned.\r\n\r\n### [Windows Server 2019 Virtual Machine with Added Tools](marketplace/virtual-machine/basic-windows-vm/README.md)\r\n\r\nThis offer template demonstrates how to create a Windows Server 2019 virtual machine image with Chocolatey and Microsoft Edge installed. The image can then be used to create an Azure Virtual Machine offer.\r\n\r\n### [Solution Template using a Virtual Machine Offer Image](marketplace/application/vm-offer-image/README.md)\r\n\r\nThis offer template demonstrates how to build a solution template Azure Application offer that uses a VM image from a Virtual Machine offer.\r\n\r\n### Step 5: Create GitHub Workflows\r\n\r\nGitHub [actions](.github/actions/) and [starter workflows](workflow-templates/) are provided to automate the build, test and publish process for Commercial Marketplace offers.\r\n\r\nBefore using a workflow or action, create a new **actions** repository secret (AZURE_CREDENTIALS) with the following:\r\n```\r\n{\r\n  \"clientId\": \"<Azure AD application client ID>\",\r\n  \"clientSecret\": \"<Azure AD application client secret>\",\r\n  \"subscriptionId\": \"<Azure subscription ID>\",\r\n  \"tenantId\": \"<Azure AD application tenant ID>\",\r\n  \"resourceManagerEndpointUrl\": \"https://management.azure.com/\"\r\n}\r\n```\r\n\r\nFor more information on creating a GitHub actions repository secret, see [Creating encrypted secrets for a repository](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-a-repository).\r\n\r\nTo use an action, refer to it as follows in your workflow:\r\n```\r\nmicrosoft/commercial-marketplace-offer-solution/.github/actions/[ACTION DIRECTORY]\r\n```\r\nFor example:\r\n```\r\nsteps:\r\n  - name: Build and create/update offer\r\n    uses: microsoft/commercial-marketplace-offer-solution/.github/actions/commercial-marketplace\r\n```\r\nRefer to the GitHub documentation for more information on using [starter workflows](https://docs.github.com/en/actions/using-workflows/using-starter-workflows).\r\n\r\n## Future Work and Updates\r\nThis repository is still under development. We are working to add more offer templates and scripts for testing and automation.\r\n\r\nWe have included an example [template sync workflow](workflow-templates/template-sync.trigger.yml). Create a new GitHub action by copying this file to `.github/workflows/`. When triggered, this action will create a new Pull Request with the latest changes from the template repository. If the action initially fails, manual merging may be required for the first time.\r\nWhen using the repository, we recommend that the original files included remain unchanged, and that new files are created or copied. This will help reduce the likelihood of future merge issues.\r\n\r\n\r\n## Contribute\r\nContributions to this repository are welcome. Here's how you can contribute:\r\n- Submit bugs and help us verify fixes.\r\n- Submit feature requests and help us implement them.\r\n- Submit pull requests for bug fixes and features.\r\n\r\nPlease refer to [Contribution Guidelines](CONTRIBUTING.md) for more details.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "commercial-marketplace-offer-solution", "org_name": "microsoft", "org_repo": "microsoft/commercial-marketplace-offer-solution", "platform_org_repo": "github+microsoft/commercial-marketplace-offer-solution", "link_to_repo": "https://github.com/microsoft/commercial-marketplace-offer-solution", "platform": "github", "language": "PowerShell", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "![Dataset Preview](docs/img/sx-data.jpg)\n\n# DigiFace-1M Dataset\n\nThe DigiFace-1M dataset is a collection of over one million diverse synthetic face images for face recognition.\n\nIt was introduced in our paper [DigiFace-1M: 1 Million Digital Face Images for Face Recognition](https://microsoft.github.io/DigiFace1M) and can be used to train deep learning models for facial recognition.\n\nThe dataset contains:\n\n- 720K images with 10K identities (72 images per identity). For each identity, 4 different sets of accessories are sampled and 18 images are rendered for each set.\n- 500K images with 100K identities (5 images per identity). For each identity, only one set of accessories is sampled.\n\nThe DigiFace-1M dataset can be used for **non-commercial** research, and is licensed under the license found in [LICENSE](LICENSE).\n\n## Downloading the Dataset\n\nFor convenience the dataset is split into 8 parts which can be downloaded here: \n\n72 images per identity \n- [P1](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_0-1999_72_imgs.zip)\n- [P2](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_2000-3999_72_imgs.zip)\n- [P3](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_4000-5999_72_imgs.zip)\n- [P4](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_6000-7999_72_imgs.zip)\n- [P5](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_8000-9999_72_imgs.zip)\n\n5 images per identity\n- [P1](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_100000-133332_5_imgs.zip)\n- [P2](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_133333-166665_5_imgs.zip)\n- [P3](https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_166666-199998_5_imgs.zip)\n\n## Dataset Layout\n\nThe DigiFace-1M dataset contains cropped color images in the following layout.\n\n```\nsubj_id_n\n\u251c\u2500\u2500 0.png                 # First rendered image of subject subj_id_n\n\u251c\u2500\u2500 1.png                 # Second rendered image of subject subj_id_n\n...\n\u251c\u2500\u2500 k.png                 # k+1 rendered image of subject subj_id_n\n```\n\n## Disclaimer\n\nSome of our rendered faces may be close in appearance to the faces of real people.\nAny such similarity is naturally unintentional, as it would be in a dataset of real images, where people may appear similar to others unknown to them.\n\n## Citation\n\nIf you use the DigiFace-1M dataset in your work, please cite the following [paper](https://github.com/microsoft/DigiFace1M/raw/main/paper.pdf):\n\n```\n@inproceedings{bae2023digiface1m,\n  title={DigiFace-1M: 1 Million Digital Face Images for Face Recognition},\n  author={Bae, Gwangbin and de La Gorce, Martin and Baltru{\\v{s}}aitis, Tadas and Hewitt, Charlie and Chen, Dong and Valentin, Julien and Cipolla, Roberto and Shen, Jingjing},\n  booktitle={2023 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n  year={2023},\n  organization={IEEE}\n}\n```\n", "repo_name": "DigiFace1M", "org_name": "microsoft", "org_repo": "microsoft/DigiFace1M", "platform_org_repo": "github+microsoft/DigiFace1M", "link_to_repo": "https://github.com/microsoft/DigiFace1M", "platform": "github", "language": null, "stargazers_count": 181, "watchers_count": 181}, {"README_text": "# Project\n\nThis project provides a VS Code extension to run Python code in the Web (e.g. vscode.dev) using WebAssembly as an execution engine.\n\n# Index\n\n[Limitations](#limitations)<br>\n[Pre-requisites](#pre-requisites)<br>\n[Python Environments](#python-environments)<br>\n[Versioning](#versioning)<br>\n[Contributing](#contributing)<br>\n[Trademarks](#trademarks)<br>\n\n## Limitations\n\nThe extension's intension is to serve as an experimentation environment to help the VS Code team to gain experiences about running Python code in the Web using WebAssembly technologies. It should not be used to do productive Python development since there are limitation in WebAssembly Python interpreted as well as limitations in VS Code itself.\n\n### Python interpreter limitations\n\n- no pip support.\n- no socket support.\n- no support for native Python modules.\n- no thread support. As a consequence there is no async support either.\n\n## Pre-requisites\n\nThe extension depends on the [Github Repositories](https://marketplace.visualstudio.com/items?itemName=GitHub.remotehub) extensions. It also requires you to authenticate with GitHub.\n\n## Python Environments\n\nThe extension uses a pre-configured Python environment based on the [CPython WebAssembly builds](https://github.com/tiran/cpython-wasm-test/releases). The used build is `Python-3.11.0rc1-wasm32-wasi-16.zip`\n\nYou can setup your own Python environment, included source wheel Python packages, following these steps:\n\n- create a new GitHub repository.\n- download a `wasm-wasi-16` build from https://github.com/tiran/cpython-wasm-test/releases and expand it into the root of the repository\n- to add source wheel packages do the following:\n  - create a `site-packages` folder in the root\n  - install the package using the following command `pip install my_package --target ./site-packages`. Note that you need to have a Python installation in your OS including pip.\n- commit the changes\n- change the `python.wasm.runtime` setting to point to your GitHub repository. For example:\n```\n{\n    \"python.wasm.runtime\": \"https://github.com/dbaeumer/python-3.11.0rc\"\n}\n```\n\n## History\n\n- 0.7.0: added basic debugging support\n- 0.5.0: initial version\n\n## Versioning\n\nOdd major, minor or patch version numbers indicate an insider or pre-release. So `1.0.0` is a pre-release, `0.1.0` will be a pre-release as well. `2.0.2` will be a regular release.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-python-web-wasm", "org_name": "microsoft", "org_repo": "microsoft/vscode-python-web-wasm", "platform_org_repo": "github+microsoft/vscode-python-web-wasm", "link_to_repo": "https://github.com/microsoft/vscode-python-web-wasm", "platform": "github", "language": "Python", "stargazers_count": 57, "watchers_count": 57}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "lakeFriends", "org_name": "microsoft", "org_repo": "microsoft/lakeFriends", "platform_org_repo": "github+microsoft/lakeFriends", "link_to_repo": "https://github.com/microsoft/lakeFriends", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Durable Task Framework for Go\n\n[![Build](https://github.com/microsoft/durabletask-go/actions/workflows/pr-validation.yml/badge.svg)](https://github.com/microsoft/durabletask-go/actions/workflows/pr-validation.yml)\n\nThe Durable Task Framework is a lightweight, embeddable engine for writing durable, fault-tolerant business logic (*orchestrations*) as ordinary code. The engine itself is written in Go and intended to be embedded into other Go-based processes. It exposes a gRPC endpoint to support writing durable flows in any language. There are currently SDKs that consume this gRPC endpoint for [.NET](https://github.com/microsoft/durabletask-dotnet) and [Java](https://github.com/microsoft/durabletask-java), with more to come. It's also possible to write orchestrations directly in Go and run them in the local process.\n\nThis project is largely a Go clone of the [.NET-based Durable Task Framework](https://github.com/Azure/durabletask), which is used by various cloud service teams at Microsoft for building reliable control planes and managing infrastructure. It also takes inspiration from the [Go Workflows](https://github.com/cschleiden/go-workflows) project, which itself is a Go project that borrows heavily from both the Durable Task Framework and [Temporal](https://github.com/temporalio/temporal). The main difference is that the Durable Task engine is designed to be used in sidecar architectures.\n\nThe Durable Task engine is also intended to be used as the basis for the [Dapr embedded workflow engine](https://github.com/dapr/dapr/issues/4576).\n\n> This project is a work-in-progress and should not be used for production workloads. The public API surface is also not yet stable. The project itself is also in the very early stages and is missing some of the basics, such as contribution guidelines, etc.\n\n## Storage providers\n\nThis project includes a [sqlite](https://sqlite.org/) storage provider for persisting app state to disk.\n\n```go\n// Persists state to a file named test.sqlite3. Use \"\" for in-memory storage.\noptions := sqlite.NewSqliteOptions(\"test.sqlite3\")\nbe := sqlite.NewSqliteBackend(options, backend.DefaultLogger())\n```\n\nAdditional storage providers can be created by extending the `Backend` interface.\n\n## Creating the standalone gRPC sidecar\n\nSee the `main.go` file for an example of how to create a standalone gRPC sidecar that embeds the Durable Task engine. In short, you must create an `Backend` (for storage), an `Executor` (for executing user code), and host them as a `TaskHubWorker`.\n\nThe following code creates a `TaskHub` worker with sqlite `Backend` and a gRPC `Executor` implementations.\n\n```go\n// Use the default logger or provide your own\nlogger := backend.DefaultLogger()\n\n// Configure the sqlite backend that will store the runtime state\nsqliteOptions := sqlite.NewSqliteOptions(sqliteFilePath)\nbe := sqlite.NewSqliteBackend(sqliteOptions, logger)\n\n// Create a gRPC server that the language SDKs will connect to\ngrpcServer := grpc.NewServer()\nexecutor := backend.NewGrpcExecutor(grpcServer, be, logger)\n\n// Construct and start the task hub worker object, which polls the backend for new work\norchestrationWorker := backend.NewOrchestrationWorker(be, executor, logger)\nactivityWorker := backend.NewActivityTaskWorker(be, executor, logger)\ntaskHubWorker := backend.NewTaskHubWorker(be, orchestrationWorker, activityWorker, logger)\ntaskHubWorker.Start(context.Background())\n\n// Start listening.\nlis, _ := net.Listen(\"tcp\", \"localhost:4001\")\nfmt.Printf(\"server listening at %v\\n\", lis.Addr())\ngrpcServer.Serve(lis)\n```\n\nNote that the Durable Task gRPC service implementation is designed to serve one client at a time, just like with any sidecar architecture. Scale out is achieved by adding new pod replicas that contain both the app process and the sidecar (connected to a common database).\n\n### Language SDKs for gRPC\n\nThe Durable Task Framework for Go currently supports writing orchestrations in the following languages:\n\n| Language/Stack | Package | Project Home | Samples |\n| - | - | - | - |\n| .NET | [![NuGet](https://img.shields.io/nuget/v/Microsoft.DurableTask.Client.svg?style=flat)](https://www.nuget.org/packages/Microsoft.DurableTask.Client/) | [GitHub](https://github.com/microsoft/durabletask-dotnet) | [Samples](https://github.com/microsoft/durabletask-dotnet/tree/main/samples) |\n| Java | [![Maven Central](https://img.shields.io/maven-central/v/com.microsoft/durabletask-client?label=durabletask-client)](https://search.maven.org/artifact/com.microsoft/durabletask-client) | [GitHub](https://github.com/microsoft/durabletask-java) | [Samples](https://github.com/microsoft/durabletask-java/tree/main/samples/src/main/java/io/durabletask/samples) |\n| Python | [![PyPI version](https://badge.fury.io/py/durabletask.svg)](https://badge.fury.io/py/durabletask) | [GitHub](https://github.com/microsoft/durabletask-python) | [Samples](https://github.com/microsoft/durabletask-python/tree/main/examples) |\n\nMore language SDKs are planned to be added in the future. In particular, SDKs for Python and JavaScript/TypeScript. Anyone can theoretically create an SDK using a language that supports gRPC. However, there is not yet a guide for how to do this, so developers would need to reference existing SDK code as a reference. Starting with the Java implementation is recommended. The gRPC API is defined [here](https://github.com/microsoft/durabletask-protobuf).\n\n## Embedded orchestrations\n\nIt's also possible to create orchestrations in Go and run them in the local process. You can find code samples in the [samples](./samples/) directory. The full set of Durable Task features is not yet available as part of the Go SDK, but will be added over time.\n\n### Activity sequence example\n\nActivity sequences like the following are the simplest and most common pattern used in the Durable Task Framework.\n\n```go\n// ActivitySequenceOrchestrator makes three activity calls in sequence and results the results\n// as an array.\nfunc ActivitySequenceOrchestrator(ctx *task.OrchestrationContext) (any, error) {\n\tvar helloTokyo string\n\tif err := ctx.CallActivity(SayHelloActivity, \"Tokyo\").Await(&helloTokyo); err != nil {\n\t\treturn nil, err\n\t}\n\tvar helloLondon string\n\tif err := ctx.CallActivity(SayHelloActivity, \"London\").Await(&helloLondon); err != nil {\n\t\treturn nil, err\n\t}\n\tvar helloSeattle string\n\tif err := ctx.CallActivity(SayHelloActivity, \"Seattle\").Await(&helloSeattle); err != nil {\n\t\treturn nil, err\n\t}\n\treturn []string{helloTokyo, helloLondon, helloSeattle}, nil\n}\n\n// SayHelloActivity can be called by an orchestrator function and will return a friendly greeting.\nfunc SayHelloActivity(ctx task.ActivityContext) (any, error) {\n\tvar input string\n\tif err := ctx.GetInput(&input); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn fmt.Sprintf(\"Hello, %s!\", input), nil\n}\n```\n\nYou can find the full sample [here](./samples/sequence.go).\n\n### Fan-out / fan-in execution example\n\nThe next most common pattern is \"fan-out / fan-in\" where multiple activities are run in parallel, as shown in the snippet below (note that the `GetDevicesToUpdate` and `UpdateDevice` activity definitions are left out of the snippet below for brevity):\n\n```go\n// UpdateDevicesOrchestrator is an orchestrator that runs activities in parallel\nfunc UpdateDevicesOrchestrator(ctx *task.OrchestrationContext) (any, error) {\n\t// Get a dynamic list of devices to perform updates on\n\tvar devices []string\n\tif err := ctx.CallActivity(GetDevicesToUpdate, nil).Await(&devices); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Start a dynamic number of tasks in parallel, not waiting for any to complete (yet)\n\ttasks := make([]task.Task, 0, len(devices))\n\tfor _, id := range devices {\n\t\ttasks = append(tasks, ctx.CallActivity(UpdateDevice, id))\n\t}\n\n\t// Now that all are started, wait for them to complete and then return the success rate\n\tsuccessCount := 0\n\tfor _, task := range tasks {\n\t\tvar succeeded bool\n\t\tif err := task.Await(&succeeded); err == nil && succeeded {\n\t\t\tsuccessCount++\n\t\t}\n\t}\n\treturn float32(successCount) / float32(len(devices)), nil\n}\n```\n\nThe full sample can be found [here](./samples/parallel.go).\n\n### External orchestration inputs (events) example\n\nSometimes orchestrations need asynchronous input from external systems. For example, an approval workflow may require a manual approval signal from an authorized user. Or perhaps an orchestration pauses and waits for a command from an operator. The `WaitForSingleEvent` method can be used in an orchestrator function to pause execution and wait for such inputs. You an even specify a timeout value indicating how long to wait for the input before resuming execution (use `-1` to indicate infinite timeout).\n\n```go\n// ExternalEventOrchestrator is an orchestrator function that blocks for 30 seconds or\n// until a \"Name\" event is sent to it.\nfunc ExternalEventOrchestrator(ctx *task.OrchestrationContext) (any, error) {\n\tvar nameInput string\n\tif err := ctx.WaitForSingleEvent(\"Name\", 30*time.Second).Await(&nameInput); err != nil {\n\t\t// Timeout expired\n\t\treturn nil, err\n\t}\n\n\treturn fmt.Sprintf(\"Hello, %s!\", nameInput), nil\n}\n```\n\nSending an event to a waiting orchestration can be done using the `RaiseEvent` method of the task hub client. These events are durably buffered in the orchestration state and are consumed as soon as the target orchestration calls `WaitForSingleEvent` with a matching event name. The following code shows how to use the `RaiseEvent` method to send an event with a payload to a running orchestration. See [Managing local orchestrations](#managing-local-orchestrations) for more information on how to interact with local orchestrations in Go.\n\n```go\nid, _ := client.ScheduleNewOrchestration(ctx, ExternalEventOrchestrator)\n\n// Prompt the user for their name and send that to the orchestrator\ngo func() {\n\tfmt.Println(\"Enter your first name: \")\n\tvar nameInput string\n\tfmt.Scanln(&nameInput)\n\t\n\topts := api.WithJsonSerializableEventData(nameInput)\n\tclient.RaiseEvent(ctx, id, \"Name\", opts)\n}()\n```\n\nThe full sample can be found [here](./samples/externalevents.go).\n\n### Managing local orchestrations\n\nThe following code snippet provides an example of how you can configure and run orchestrations. The `TaskRegistry` type allows you to register orchestrator and activity functions, and the `TaskHubClient` allows you to start, query, terminate, suspend, resume, and wait for orchestrations to complete.\n\nThe code snippet below demonstrates how to register and start a new instance of the `ActivitySequenceOrchestrator` orchestrator and wait for it to complete. The initialization of the client and worker are left out for brevity.\n\n```go\nr := task.NewTaskRegistry()\nr.AddOrchestrator(ActivitySequenceOrchestrator)\nr.AddActivity(SayHelloActivity)\n\nctx := context.Background()\nclient, worker := Init(ctx, r)\ndefer worker.Shutdown(ctx)\n\nid, err := client.ScheduleNewOrchestration(ctx, ActivitySequenceOrchestrator)\nif err != nil {\n  panic(err)\n}\nmetadata, err := client.WaitForOrchestrationCompletion(ctx, id)\nif err != nil {\n  panic(err)\n}\nfmt.Printf(\"orchestration completed: %v\\n\", metadata)\n```\n\nEach sample linked above has a full implementation you can use as a reference.\n\n## Distributed tracing support\n\nThe Durable Task Framework for Go supports publishing distributed traces to any configured [Open Telemetry](https://opentelemetry.io/)-compatible exporter. Simply use [`otel.SetTracerProvider(tp)`](https://pkg.go.dev/go.opentelemetry.io/otel#SetTracerProvider) to register a global `TracerProvider` as part of your application startup and the task hub worker will automatically use it to emit OLTP trace spans.\n\nThe following example code shows how you can configure distributed trace collection with [Zipkin](https://zipkin.io/), a popular open source distributed tracing system. The example assumes Zipkin is running locally, as shown in the code.\n\n```go\nfunc ConfigureZipkinTracing() *trace.TracerProvider {\n\t// Inspired by this sample: https://github.com/open-telemetry/opentelemetry-go/blob/main/example/zipkin/main.go\n\texp, err := zipkin.New(\"http://localhost:9411/api/v2/spans\")\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\t// NOTE: The simple span processor is not recommended for production.\n\t//       Instead, the batch span processor should be used for production.\n\tprocessor := trace.NewSimpleSpanProcessor(exp)\n\t// processor := trace.NewBatchSpanProcessor(exp)\n\n\ttp := trace.NewTracerProvider(\n\t\ttrace.WithSpanProcessor(processor),\n\t\ttrace.WithSampler(trace.AlwaysSample()),\n\t\ttrace.WithResource(resource.NewWithAttributes(\n\t\t\t\"durabletask.io\",\n\t\t\tattribute.KeyValue{Key: \"service.name\", Value: attribute.StringValue(\"sample-app\")},\n\t\t)),\n\t)\n\totel.SetTracerProvider(tp)\n\treturn tp\n}\n```\n\nYou can find this code in the [distributedtracing.go](./samples/distributedtracing.go) sample. The following is a screenshot showing the trace for the sample's orchestration, which calls an activity, creates a 2-second durable timer, and uses another activity to make an HTTP request to bing.com:\n\n![image](https://user-images.githubusercontent.com/2704139/205171291-8d12d6fe-5d4f-40c7-9a48-2586a4c4af49.png)\n\nNote that each orchestration is represented as a single span with activities, timers, and sub-orchestrations as child spans. The generated spans contain a variety of attributes that include information such as orchestration instance IDs, task names, task IDs, etc.\n\n## Cloning this repository\n\nThis repository contains submodules. Be sure to clone it with the option to include submodules. Otherwise you will not be able to generate the protobuf code.\n\n```bash\ngit clone --recurse-submodules https://github.com/microsoft/durabletask-go \n```\n\n## Building the project\n\nThis project requires go v1.18.x or greater. You can build a standalone executable by simply running `go build` at the project root.\n\n### Generating protobuf\n\nUse the following command to regenerate the protobuf from the submodule. Use this whenever updating the submodule reference.\n\n```bash\n# NOTE: assumes the .proto file defines: option go_package = \"/internal/protos\"\nprotoc --go_out=. --go-grpc_out=. -I submodules/durabletask-protobuf/protos orchestrator_service.proto\n```\n\n### Generating mocks for testing\n\nTest mocks were generated using [mockery](https://github.com/vektra/mockery). Use the following command at the project root to regenerate the mocks.\n\n```bash\nmockery --dir ./backend --name=\"^Backend|^Executor|^TaskWorker\" --output ./tests/mocks --with-expecter\n```\n\n## Running tests\n\nAll automated tests are under `./tests`. A separate test package hierarchy was chosen intentionally to prioritize [black box testing](https://en.wikipedia.org/wiki/Black-box_testing). This strategy also makes it easier to catch accidental breaking API changes.\n\nRun tests with the following command.\n\n```bash\ngo test ./tests/... -coverpkg ./api,./task,./client,./backend/...,./internal/helpers\n```\n\n## Running integration tests\n\nYou can run pre-built container images to run full integration tests against the durable task host over gRPC.\n\n### .NET Durable Task client SDK tests\n\nUse the following docker command to run tests against a running worker.\n\n```bash\ndocker run -e GRPC_HOST=\"host.docker.internal\" cgillum/durabletask-dotnet-tester:0.5.0-beta\n```\n\nNote that the test assumes the gRPC server can be reached over `localhost` on port `4001` on the host machine. These values can be overridden with the following environment variables:\n\n* `GRPC_HOST`: Use this to change from the default `127.0.0.1` to some other value, for example `host.docker.internal`.\n* `GRPC_PORT`: Set this environment variable to change the default port from `4001` to something else.\n\nIf successful, you should see output that looks like the following:\n\n```\nTest run for /root/out/bin/Debug/Microsoft.DurableTask.Tests/net6.0/Microsoft.DurableTask.Tests.dll (.NETCoreApp,Version=v6.0)\nMicrosoft (R) Test Execution Command Line Tool Version 17.3.1 (x64)\nCopyright (c) Microsoft Corporation.  All rights reserved.\n\nStarting test execution, please wait...\nA total of 1 test files matched the specified pattern.\n[xUnit.net 00:00:00.00] xUnit.net VSTest Adapter v2.4.3+1b45f5407b (64-bit .NET 6.0.10)\n[xUnit.net 00:00:00.82]   Discovering: Microsoft.DurableTask.Tests\n[xUnit.net 00:00:00.90]   Discovered:  Microsoft.DurableTask.Tests\n[xUnit.net 00:00:00.90]   Starting:    Microsoft.DurableTask.Tests\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.ExternalEvents(eventCount: 100) [6 s]\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.ExternalEvents(eventCount: 1) [309 ms]\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.LongTimer [8 s]\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.SubOrchestration [1 s]\n  ...\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.ActivityFanOut [914 ms]\n[xUnit.net 00:01:01.04]   Finished:    Microsoft.DurableTask.Tests\n  Passed Microsoft.DurableTask.Tests.OrchestrationPatterns.SingleActivity_Async [365 ms]\n\nTest Run Successful.\nTotal tests: 33\n     Passed: 33\n Total time: 1.0290 Minutes\n```\n\n## Running locally\n\nYou can run the engine locally by pressing `F5` in [Visual Studio Code](https://code.visualstudio.com/) (the recommended editor). You can also simply run `go run main.go` to start a local Durable Task gRPC server that listens on port 4001.\n\n```bash\ngo run main.go --port 4001 --db ./test.sqlite3\n```\n\nThe following is the expected output:\n\n```\n2022/09/14 17:26:50 backend started: sqlite::./test.sqlite3\n2022/09/14 17:26:50 server listening at 127.0.0.1:4001\n2022/09/14 17:26:50 orchestration-processor: waiting for new work items...\n2022/09/14 17:26:50 activity-processor: waiting for new work items...\n```\n\nAt this point you can use one of the [language SDKs](#language-sdks) mentioned earlier in a separate process to implement and execute durable orchestrations. Those SDKs will connect to port `4001` by default to interact with the Durable Task engine.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "durabletask-go", "org_name": "microsoft", "org_repo": "microsoft/durabletask-go", "platform_org_repo": "github+microsoft/durabletask-go", "link_to_repo": "https://github.com/microsoft/durabletask-go", "platform": "github", "language": "Go", "stargazers_count": 77, "watchers_count": 77}, {"README_text": "# Sound and Music MakeCode extension for Minecraft: Education Edition\n\n> Open this page at [https://microsoft.github.io/sound-and-music-minecraft-makecode-ext/](https://microsoft.github.io/sound-and-music-minecraft-makecode-ext/)\n\n## Use the extension in Minecraft\n\n* open [Minecraft: Education Edition](https://education.minecraft.net/) and log in\n* create or join a world (\"Code Builder\" must be enabled under the \"Classroom\" world settings)\n* open the code builder (by default, the \"c\" key)\n* click on **New Project**\n* click on **Extensions** in the left menu\n* search for **https://github.com/microsoft/sound-and-music-minecraft-makecode-ext#main** and import\n\nThe left menu should now contain a new **Music** category!\n\n## Block documentation\n\n### `playMusic(builtInMusicDisc, musicOption)`\n\nPlay a Minecraft music disc.\n\nExamples:\n\n```ts\nplayMusic(MusicDisc.Eleven, MusicOption.Once);\nplayMusic(MusicDisc.Otherside, MusicOption.Forever);\n```\n\nAvailable `MusicDisc`s include:\n\n* Eleven\n* Thirteen\n* Blocks\n* Cat\n* Chirp\n* Far\n* Mall\n* Mellohi\n* Stal\n* Strad\n* Wait\n* Ward\n* Otherside\n* Pigstep\n\nAvailable `MusicOption`s include:\n\n* Once\n* Forever\n\n### `stopMusic()`\n\nStop the music.\n\nExample:\n```ts\nstopMusic();\n```\n\n### `playSound(sound)`\n\nPlays the specified Minecraft game sound.\n\nExamples:\n\n```ts\nplaySound(Sound.CatMeow);\nplaySound(Sound.Firework);\n```\n\nAvailable `Sound`s include:\n\n* Blaze\n* BucketSplash\n* CatHiss\n* CatMeow\n* CatPurr\n* CatPurreow\n* Chicken\n* Click\n* Cow\n* Creeper\n* Dolphin\n* Drink\n* Drowned\n* Eat\n* ElderGuardian\n* Endermen\n* Explode\n* FallDamage\n* Fire\n* Firework\n* FireworkLarge\n* FireworkTwinkle\n* Fizz\n* Fuse\n* Ghast\n* Horse\n* Hurt\n* LevelUp\n* LightningImpact\n* Llama\n* Pig\n* PlayerAttack\n* PlayerDie\n* PlayerHurt\n* Rain\n* Sheep\n* Skeleton\n* Slime\n* Spider\n* Splash\n* Thunder\n* Totem\n* Trident\n* VillagerHaggle\n* VillagerNo\n* VillagerYes\n* WolfBark\n* WolfGrowl\n* WolfWhine\n* Zombie\n\n### `playNote(note, instrument)`\n\nPlays the specified note on the specified Minecraft instrument, as if it was played on a note block.\n\nMinecraft can play notes from Low F# (`Note.FSharp3`) to High F# (`Note.FSharp5`).\n\nExamples:\n\n```ts\nplayNote(Note.E4, Instrument.Harp);\nplayNote(Note.FSharp4, Instrument.Banjo);\n```\n\nAvailable `Instrument`s include:\n\n* Harp\n* Bass\n* SnareDrum\n* HiHat\n* BassDrum\n* Bell\n* Flute\n* Chime\n* Guitar\n* Xylophone\n* Vibraphone\n* CowBell\n* Didgeridoo\n* Bit\n* Banjo\n* Pling\n\n### `setVolume(newVolume)`\n\nChanges the volume that `playSound` and `playNote` will play at.\n\nVolumes can range from 0 to 100 and starts at 100.\n\nThis is *separate* from all Minecraft audio settings. It only affects the other blocks provided by this extension.\n\nExamples:\n\n```ts\nsetVolume(0); // mute\nsetVolume(100); // max volume\n```\n\n### `volume()`\n\nReturns the volume previously set by `setVolume`.\n\nExamples:\n\n```ts\nvolume(); // returns 100 by default\nsetVolume(50);\nvolume(); // now returns 50\n```\n\n## Test the extension in a browser\n\nThis repository can be added as an **extension** in the MakeCode online editor (outside of Minecraft). *None of the functionality will work outside of Minecraft*, but this is a quick way to test out what the MakeCode blocks will look like without loading up the game.\n\n* open [https://minecraft.makecode.com/](https://minecraft.makecode.com/)\n* click on **New Project**\n* click on **Extensions** under the gearwheel menu\n* search for **https://github.com/microsoft/sound-and-music-minecraft-makecode-ext#main** and import\n\n## Edit this project\n\nTo edit this repository in MakeCode:\n\n* open [https://minecraft.makecode.com/](https://minecraft.makecode.com/)\n* click on **Import** then click on **Import URL**\n* paste **https://github.com/microsoft/sound-and-music-minecraft-makecode-ext#main** and click import\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n#### Metadata (used for search, rendering)\n\n* for PXT/minecraft\n<script src=\"https://makecode.com/gh-pages-embed.js\"></script><script>makeCodeRender(\"{{ site.makecode.home_url }}\", \"{{ site.github.owner_name }}/{{ site.github.repository_name }}\");</script>\n", "repo_name": "sound-and-music-minecraft-makecode-ext", "org_name": "microsoft", "org_repo": "microsoft/sound-and-music-minecraft-makecode-ext", "platform_org_repo": "github+microsoft/sound-and-music-minecraft-makecode-ext", "link_to_repo": "https://github.com/microsoft/sound-and-music-minecraft-makecode-ext", "platform": "github", "language": "TypeScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Confidential Sidecar Containers\nThis repository contains the code needed to build the sidecar containers used for [confidential containers.](https://techcommunity.microsoft.com/t5/azure-confidential-computing/microsoft-introduces-preview-of-confidential-containers-on-azure/ba-p/3410394)\n\nThe code in this repository should be located at ``$GOPATH/src/microsoft/confidential-sidecar-containers``.\n\n## Secure key release (SKR) sidecar\nThe ``docker/skr/build.sh`` script builds all necessary Go tools for secure key release as standalone binaries and creates a Docker image that contains them so that it \ncan be used as a sidecar container. The skr sidecar container is executed by calling the script ``skr.sh``. More information about the skr API can be found [here.](cmd/skr/README.md)\n\nThe skr sidecar can be queried by application containers hosted in the same pod (or container group) for retrieving attestation reports and for releasing secrets from managed HSM key vaults.\n\nThe ``examples/skr`` shows an example of how the skr sidecar can be deployed and tested within a confidential container group on ACI.\n\n## Fetching an attestion report.\n``tools/get-snp-report provides a tool which will return an SNP attestation report from the AMD PSP via linux IOCTLs. it can take a hex encoded report data value on the command line. The output is a hex encoded binary object. If piped through hex2report it can be read by people. There are two implementations inside the one tool to support the different IOCTLs requirements between linux 5.15 and 6.1 and later.\n\n### Third-party code \nWe modified the [AES unwrap key without padding method](https://github.com/NickBall/go-aes-key-wrap/blob/master/keywrap.go) to implement the aes key unwrap with padding method.\n\n## Encrypted filesystem sidecar\nThe ``docker/encfs/build.sh`` script builds all necessary Go tools (for encrypted filesystems) and creates a Docker image that contains them so that it can be used as a sidecar container. The encrypted filesystem sidecar container is executed by calling the script ``encfs.sh`` with a base64-encoded string or as an environment variable. The entry point to the sidecar is the [remotefs tool](cmd/remotefs/README.md) which leverages the [azmount tool.](cmd/azmount/README.md) \n\nThe encrypted filesystem sidecar uses the SKR library to release key material from Azure Key Vault instances required for mounting the encrypted filesystems required by the application.\n\nThe ``examples/encfs`` shows an example of how the encrypted filesystem sidecar can be deployed within a confidential container group on ACI.\n## Dependencies:\n- Golang 1.19 or later\n- Docker\n- GCC 9.4.0 or later\n\n# Contributing\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "confidential-sidecar-containers", "org_name": "microsoft", "org_repo": "microsoft/confidential-sidecar-containers", "platform_org_repo": "github+microsoft/confidential-sidecar-containers", "link_to_repo": "https://github.com/microsoft/confidential-sidecar-containers", "platform": "github", "language": "Go", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# Interactive KQL Query Store\n\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://aka.ms/kql-query-store)\n\nCurrently many KQL queries are published on GitHub by Microsoft and Security Community on GitHub. All the queries are scattered as unstructured data and disorganized in various places making it difficult to discover for defenders and detection authors. \n\nGitHub search interface is not flexible to satisfy various custom search needs for defenders to effectively search various KQL queries by datasource , KQL operators , parsing of complex fields in data sources, custom tags if available etc. Having it easy to discover will help defenders in referencing existing work while writing new queries, reuse complex parsing examples in specific data sources and much more. \n\n## Project Goals\n\n- Organized data store of KQL queries as a structured data store\n- Easy discoverability of KQL Queries based on tags, KQL operators, Datasource etc. \n- Point to relevant sources and GitHub links. \n- Interactive dashboard to explore the structured data.\n- Insights on various KQL queries from Azure Sentinel\n\n## Architecture\n![raw_image](https://raw.github.com/microsoft/kql-query-store/master/images/DataFlowDiagram.png)\n\n\n## Docker instruction\nif you wish to host this locally/in-house, you can use below instructions to build docker images and host it. For more detailed instructions, check out Streamlit docs. [Deploy Streamlit using Docker](https://docs.streamlit.io/knowledge-base/tutorials/deploy/docker)\n\nBuild image\n\n`docker build -t kql-query-store .`\n\nRun the docker container\n\n`docker run -p 8501:8501 kql-query-store`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "kql-query-store", "org_name": "microsoft", "org_repo": "microsoft/kql-query-store", "platform_org_repo": "github+microsoft/kql-query-store", "link_to_repo": "https://github.com/microsoft/kql-query-store", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Microsoft Technology Center Israel Official Repository\nWelcome to MTC Israel Official Repository \n\n## Workshops \n1. [Multi Environment Application - MRE On Azure](https://github.com/microsoft/MTC_IL_WORKSHOP_Multi_Runtime_Environment_Application)\n2. [Dev Box And Deployment Enviroments](https://github.com/microsoft/MTC_IL_WORKSHOP_DevBox_DeploymentEnvironments)\n3. [Container Apps with KeyVault](https://github.com/microsoft/MTC_IL_WORKSHOP_Container_Apps_With_KeyVault)\n4. [Azure Administrator](https://github.com/microsoft/MTC_IL_WORKSHOP_Azure_Administrator)\n5. [Big-Data analytics and visualization](https://github.com/microsoft/MTC_IL_WORKSHOP_Big_data_analytics_and_visualization)\n", "repo_name": "MTC_IL", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL", "platform_org_repo": "github+microsoft/MTC_IL", "link_to_repo": "https://github.com/microsoft/MTC_IL", "platform": "github", "language": "HCL", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Inclusiveness Analyzer\n\n> Make your code inclusive!\n\nThe Inclusiveness Analyzer is a GitHub action that checks your repository for offensive / exclusive terms.\n\nIt also provides context on why a word is exclusive and suggests alternate terms that can be used instead.\n\n## Install Guide\n\n### Install the Inclusiveness Analyzer Extension\n\n* Browse to the [Inclusiveness Analyzer Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=InclusivenessAnalyzer.InclusivenessAnalyzerAzureDevOps) page.\n* Select **Get it free**\n![Screenshot showing install page with Get it free button clicked.](src/images/ado-install0.png)\n* Select your organisation from the list and select **Install**\n![Screenshot showing install page with Get it free button clicked.](src/images/ado-install1.png)\n\n### Add Inclusive Analyzer task to your build pipeline\n\nOnce you have installed the extension in your organization you can use it in any of the organization's Build pipelines.\n\n* In your Azure DevOps project browse to **Pipelines** and either add or edit a Pipeline.\n* Search for **Inclusiveness Analyzer** from the **Tasks** pane on the right.\n* Customize the options if needed and select **Add**\n![Screenshot showing Inclusiveness Analyzer being added to a build.](src/images/screenshot0.png)\n\n* Run the pipeline and view the output generated by the Inclusiveness Analyzer step.\n![Screenshot showing Inclusiveness Analyzer warning of the work blacklist being used.](src/images/screenshot1.png)\n\n### Task control options\n\nUse the options below to configure exclusions and build state when non-inclusive terms are found in the repository.\n\n**`buildStatusOnNonInclusiveTerm`**\n\nStatus of the build when non-inclusive terms are found.\n\nOptions:\n\n* `warning` (Default) - Build completes with a warning state.\n* `failed` -  Breaks the build if non-inclusive terms are found.\n* `success` - Build completes successfully\n\n<br/>**`excludeUnchangedFiles`**\n\nIf `true` (Default) limits the scan to files changed in the latest commit. If `false` a full scan is run on each commit.\n\nThe git checkout step needs to have at least 'with: fetch-depth: 2' configured.|\n\n<br/>**`excludeFromScan`**\n\nComma separated list of file patterns to exclude from analysis. [Glob patterns](https://github.com/isaacs/node-glob#glob-primer) are supported with a prefix of `**/`\n\nEg. `**/skipme.txt,**/donotscan/*`\n\n<br/>**`excludeTerms`**\n\nComma separated list of non-inclusive terms to exclude from analysis.\n\nEg. `he,she`\n\n\n## Inclusiveness Analyzer for other Platforms\n\n* [Inclusiveness Analyzer GitHub Action](https://github.com/microsoft/InclusivenessAnalyzer)\n* [Inclusiveness Analyzer Visual Studio Extension](https://github.com/microsoft/InclusivenessAnalyzerVisualStudio)\n\n## About the project\n\nAs humans, we hold many unconscious and implicit biases that we rely on to react quickly to our environment and any novel stimuli. However, since the unconscious brain processes and reacts with speed, we sometimes speak quickly without thinking, which may cause us to slip offensive terms and stereotypes although we mean no malice.\n\nIn order to confront these biases that we see in ourselves and others, we must rewire ourselves to regularly use inclusive practices (such as the words we speak). If you don't intentionally and proactively include, you will unintentionally exclude.\n\n> Join our effort to push out exclusive terms and make inclusive terms a part of our everyday vocabulary!\n\nHelp us confront these biases by pushing out exclusive terms and making inclusive terms a part of our everyday vocabulary!\n\n## Developer Guide\n\n### Local Development\n\n* Clone the repository\n* Run `yarn install` to install all dependencies.\n* Run `yarn run dev` to run locally with sample data. See package.json for dev config and params.\n\n> Tip: In VSCode, open package.json, hover over 'Scripts' (line 6) and click on Debug and select Dev to interactively debug the code.\n\n### Manual Publishing to Visual Studio Marketplace\n\nRun the following commands when you are in the /src folder. The extension packaging does not work when running from the root of the repository.\n\n* Increment build number in `package.json`, `task.json` and 'vss-extension.json'\n* Run `yarn run prepare`\n* Run `yarn run build`\n* Run `tfx extension create --manifest-globs vss-extension.json`\n* Upload the file to the VS Marketplace.\n\nNote: If you make changes to the task.json, you might need to delete and re-install the extension on the Azure DevOps Organization to see updates.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "InclusivenessAnalyzerAzureDevOps", "org_name": "microsoft", "org_repo": "microsoft/InclusivenessAnalyzerAzureDevOps", "platform_org_repo": "github+microsoft/InclusivenessAnalyzerAzureDevOps", "link_to_repo": "https://github.com/microsoft/InclusivenessAnalyzerAzureDevOps", "platform": "github", "language": "JavaScript", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Inclusiveness Analyzer\n\n> Make your code inclusive!\n\nThe Inclusiveness Analyzer is a Visual Studio extension that checks code for offensive / exclusive terms.\n\nIt also provides context on why a word is exclusive and suggests alternate terms that can be used instead.\n\n![Intro clip](docs/assets/intro.gif)\n\n## Installing via the NuGet Package\n\nThe Inclusiveness Analyzer can be added to any C# project. Just install using NuGet and start writing code. The extension will be automatically loaded in Visual Studio for anyone that opens your project. Using the NuGet package is best when you are working on a project with a team.\n\n[Get from NuGet](https://www.nuget.org/packages/InclusivenessAnalyzer/)\n\n### Install using the NuGet Package Manager user interface\n\n* Open the C# project using Visual Studio\n* Select **Tools** from the menu\n* Select **NuGet Package Manager**\n* Select **Manage NuGet Packages for Solution...**\n* Select **Browse**\n* Search for **inclusiveness**\n* Select the checkbox next to the project(s)\n* Select **Install**\n\n![Screenshot showing nuget install from user interface.](docs/assets/nuget-screenshot1.png)\n\n### Install using the NuGet Package Manager console\n\n* Open the C# project using Visual Studio\n* Select **Tools** from the menu\n* Select **NuGet Package Manager**\n* Select **Package Manager Console**\n* Run `Install-Package InclusivenessAnalyzer`\n\n![Screenshot showing nuget install from command line.](docs/assets/nuget-screenshot2.png)\n> Important Note: The Inclusiveness Analyzer is only used during development time and does not affect your projects outputs or binaries.\n\n## Installing as a Visual Studio Extension\n\nIf you would like the Inclusiveness Analyzer to run on any Visual Studio project you can install the extension directly into Visual Studio.\n\n* View and install [Inclusiveness Analyzer for Visual Studio 2022](https://marketplace.visualstudio.com/items?itemName=InclusivenessAnalyzer.inclusivenessanalyzer2022)\n* View and install [Inclusiveness Analyzer for Visual Studio 2019](https://marketplace.visualstudio.com/items?itemName=InclusivenessAnalyzer.inclusivenessanalyzer)\n\n* Open Visual Studio 2019 or Visual Studio 2022\n* Select **Extensions** from the menu\n* Select **Manage Extensions**\n* Search for **Inclusiveness**\n* Select **Download**\n\nThe extension is scheduled for install. Your extension will be installed after all instances of Visual Studio have been closed.\n\n![Screenshot showing Inclusiveness Analyzer being added to Visual Studio.](docs/assets/vs-screenshot1.png)\n\nHappy inclusive :heart: coding!\n\n## Inclusiveness Analyzer for other Platforms\n\n* [Inclusiveness Analyzer GitHub Action](https://github.com/microsoft/InclusivenessAnalyzer)\n* [Inclusiveness Analyzer Azure DevOps Extension](https://github.com/microsoft/InclusivenessAnalyzerAzureDevOps)\n\n## About the project\n\nAs humans, we hold many unconscious and implicit biases that we rely on to react quickly to our environment and any novel stimuli. However, since the unconscious brain processes and reacts with speed, we sometimes speak quickly without thinking, which may cause us to slip offensive terms and stereotypes although we mean no malice.\n\nIn order to confront these biases that we see in ourselves and others, we must rewire ourselves to regularly use inclusive practices (such as the words we speak). If you don't intentionally and proactively include, you will unintentionally exclude.\n\n> Join our effort to push out exclusive terms and make inclusive terms a part of our everyday vocabulary!\n\nHelp us confront these biases by pushing out exclusive terms and making inclusive terms a part of our everyday vocabulary!\n\n> Icons made by [Freepik](https://www.flaticon.com/authors/freepik) from [www.flaticon.com](https://www.flaticon.com/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "InclusivenessAnalyzerVisualStudio", "org_name": "microsoft", "org_repo": "microsoft/InclusivenessAnalyzerVisualStudio", "platform_org_repo": "github+microsoft/InclusivenessAnalyzerVisualStudio", "link_to_repo": "https://github.com/microsoft/InclusivenessAnalyzerVisualStudio", "platform": "github", "language": "C#", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# TdsLib\n\n[![Nuget](https://img.shields.io/nuget/v/Microsoft.Data.Tools.TdsLib)](https://www.nuget.org/packages/Microsoft.Data.Tools.TdsLib)\n\n\nThis repository contains an open implementation of the [TDS protocol](https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-tds/) (version 7.4) in managed C# code. It is not a full implementation, current support is focused on the login steps and some generic TDS features. It does not support _normal_ data streams or T-SQL commands.\n\nThis library is mainly used for probing and for diagnosing TDS connections but can be used to manually construct and handle any step of a TDS connection. It can also be used to mock TDS clients.\n\n## Requirements\n\n- [dotnet](https://dotnet.microsoft.com/en-us/)\n- [net standard 2.0](https://learn.microsoft.com/en-us/dotnet/standard/net-standard?tabs=net-standard-2-0)\n\n## Code structure\n\n```text\nsrc\n  Microsoft.Data.Tools.TdsLib - TdsLib code\n    Buffer - Buffer class to handle type conversion\n    IO - Connection related classes\n    Messages - Logical message format used to generate Packets to be sent to SQL Server\n    Packets - Packet related classes\n    Payloads - Message Payloads\n    Tokens - Tokens that can be received from SQL server\n\ntest\n  Microsoft.Data.Tools.TdsLib.UnitTest - Unit tests\n  Microsoft.Data.Tools.TdsLib.IntegrationTest - Integration tests\n```\n\n## Example\n\n### Establish a TDS connection and login to a SQL server\n\n```csharp\nusing Microsoft.Data.Tools.TdsLib;\nusing Microsoft.Data.Tools.TdsLib.IO.Connection.Tcp;\nusing Microsoft.Data.Tools.TdsLib.Messages;\nusing Microsoft.Data.Tools.TdsLib.Packets;\nusing Microsoft.Data.Tools.TdsLib.Payloads.Login7;\nusing Microsoft.Data.Tools.TdsLib.Payloads.PreLogin;\nusing Microsoft.Data.Tools.TdsLib.Tokens.Error;\n\nstring hostname = \"sqlserver.contoso.net\";\nint port = 1433;\n\nusing TdsClient client = new TdsClient(new TcpServerEndpoint(hostname, port));\n\n// Send PreLogin message\nawait client.MessageHandler.SendMessage(new Message(PacketType.PreLogin)\n{\n    Payload = new PreLoginPayload(encrypt: true)\n});\n\n// Receive PreLogin message\nvar preLoginResponseMessage = await client.MessageHandler.ReceiveMessage(b => new PreLoginPayload(b));\n\n// Perform TLS handshake\nawait client.PerformTlsHandshake();\n\n// Prepare Login7 message\nLogin7Payload login7Payload = new Login7Payload()\n{\n    Hostname = hostname,\n    ServerName = \"MyServerName\",\n    AppName = \"MyAppName\",\n    Language = \"us_english\",\n    Database = \"MyDatabaseName\",\n    //Username,\n    //Password\n};\n\nlogin7Payload.TypeFlags.AccessIntent = OptionAccessIntent.ReadWrite;\n\nMessage login7Message = new Message(PacketType.Login7) { Payload = login7Payload };\n\n// Send Login7 message\nawait client.MessageHandler.SendMessage(login7Message);\n\n// Receive Login response tokens\nawait client.TokenStreamHandler.ReceiveTokensAsync(tokenEvent =>\n{\n    if (tokenEvent.Token is ErrorToken errorToken)\n    {\n        // Error was received from the server\n    }\n});\n\n// If no error token was received, and SQL server did not close the connection, then the connection to the server is now established and the user is logged in.\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "TdsLib", "org_name": "microsoft", "org_repo": "microsoft/TdsLib", "platform_org_repo": "github+microsoft/TdsLib", "link_to_repo": "https://github.com/microsoft/TdsLib", "platform": "github", "language": "C#", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Batch Effects Normalization (BEN)\r\n\r\nThis respository contains the code accompanying the paper \"[Incorporating knowledge of plates in batch normalization improves generalization of deep learning for microscopy images](https://www.biorxiv.org/content/10.1101/2022.10.14.512286v1)\" by [Alex Lin](https://sites.google.com/view/alexanderlin) and [Alex Lu](https://www.alexluresearch.com/).  Please find licensing details in the file `license.txt`.\r\n\r\n### Overview\r\n- Our code is built on the [PyTorch](https://pytorch.org/) library.\r\n- The majority of the code is packaged into modules in the `biomass` directory.  Here, you will find specific implementations of models, data loaders, data augmentation transforms, etc.  To simply run the code, you do not need to directly edit anything in this directory.\r\n- The Python scripts in the root directory (e.g. `train_erm.py`, `train_simclr.py`, etc.) are the outer functions that run the experiments for the paper.  To run a particular experiment, execute `python [script_name].py` (e.g. `python train_erm.py`) in the command line.  \r\n- Each script has an associated YAML file in the `configs` directory, so you do not need to edit the scripts themselves to vary experimental settings.  Within each YAML config, there is a dictionary of hyperparameter configurations for each experiment.  You can vary these to re-run our experiments with different settings.  Our codebase uses [Hydra](https://hydra.cc/) for experiment management; for more information on how Python scripts and YAML files interact, please consult the Hydra documentation.\r\n- When you run a script, our codebase will create three things:\r\n    - In the `outputs` directory, it will create a directory named by the timestamp in which you ran the script.  The YAML config file associated with the run will be copied and dumped in this directory so you can refer back to what hyperparameters you used at that time.\r\n    - In the `runs` directory, it will dump a [TensorBoard](https://www.tensorflow.org/tensorboard) charting the progress of model training.  Within the tensorboard, we record metrics such as accuracy, loss, gradient dynamics, etc.\r\n    - In the `checkpoints` directory, it will dump a checkpoint of the PyTorch model associated with the experiment.\r\n- The `requirements.txt` file contains all of the Python packages and associated versions that we used.  You can run `pip install -r requirements.txt` in the command line to automatically install the correct versions for each package.\r\n- The `misc` directory contains miscellaneous files needed for running certain experiments.\r\n\r\n### Supervised Learning Experiments\r\n- To reproduce our supervised learning experiments, use the script `train_erm.py`.  (Note that ERM stands for empirical risk minimization, i.e. another term for supervised learning.)\r\n- The current associated config file `configs/train_erm.yaml` is set up to run supervised learning with BEN, our batch effects correction method.\r\n- To run vanilla supervised learning (i.e. without BEN), make the following edits to `configs/train_erm.yaml`: delete `train_groupby`, delete `max_groups`, set `eval_plate_sampler: False`, set `eval_batch_size: 75` (or whatever batch size you prefer), and set `use_train_at_eval: False`.\r\n- The first time you run `train_erm.py`, note that our script should automatically download the RxRx1-Wilds dataset for you from the [Wilds package](https://github.com/p-lambda/wilds).       \r\n\r\n### Self-Supervised Learning Experiments\r\n- To reproduce our self-supervised learning experiments, use the script `train_simclr.py` for training the base model and `train_classifier.py` for fitting the linear classifier on the learned representations.\r\n- Thus, in the `train_classifier.yaml` file, there is an argument `model_path: xxxxx` that needs to point to a saved checkpoint obtained from running `train_simclr.py`.  Make sure to manually set the correct path for `train_classifier.yaml` after running `train_simclr.py`.\r\n- In both `train_simclr.yaml` and `train_classifier.yaml`, there is an argument `img_dir` that needs to point to a directory of cropped cells for RxRx1-Wilds.  A zipped file of this directory can be downloaded at this link (note it is about ~4 GB in size): https://zenodo.org/record/7272553#.Y2KkNuzMJTZ  \r\n- The current config file `train_simclr.yaml` is setup to train the vanilla SimCLR algorithm (without BEN).  To run SimCLR + BEN, simply change the argument `sampler: random` to `sampler: plate`.  Then, to apply BEN while training the classifier, go to `train_classifier.yaml` and change the arguments `sampler: random` -> `sampler: plate` and `model_train_mode: False` -> `model_train_mode: True`. \r\n- To use MinCLR (i.e. multiple instance constrastive learning, a new method that we developed) instead of SimCLR, simply go to `train_simclr.yaml` and change `mode: random_single` to `mode: random` (this will define positive anchors as random cells from the same image instead of random augmentations of the same single cell).  To run MinCLR + BEN, follow the aforementioned instructions for SimCLR + BEN. \r\n- To increase the number of positive anchors during training, go to `train_simclr.yaml` and change `num_img: 2` to any other value (e.g. `num_img: 5`).  Make sure that this value also matches the argument `num_views: 2` (e.g. `num_views: 5`).  \r\n- To train representations from a cell-level supervised classifier (instead of a self-supervised learner), use the script `train_supervised_cell.py`.  The current YAML file `train_supervised_cell.yaml` is designed for standard supervised learning (without BEN).  To use BEN, simply change `sampler: random` to `sampler: plate`.\r\n\r\n### Transfer Learning Experiments\r\n- To reproduce our transfer learning experiments, use the notebook `pybbbc.ipynb`.  \r\n- To obtain the dataset for BBBC021, use the following package: https://github.com/giacomodeodato/pybbbc and follow the instructions for \"Data download\" and \"Dataset creation\".  Note that this can take several hours.  Afterwards, the data should be dumped by default into a directory called `~/.cache/`.\r\n- You also need to obtain two files that list the nuclei centers for cells in this dataset.  You can download these files (called `supplement_Object.txt` and `supplement_Image.txt`) from the supplementary files of this paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3884769/ (under the title \"Data S2\").\r\n- Since this is transfer learning, you should also have a link to the checkpoint of a pre-trained model on the RxRx1-Wilds (cell level) dataset, obtained by running a self-supervised learning experiment (for example).  This checkpoint will need to be loaded into the state dict of the PyTorch model (see notebook).\r\n- Towards the end of the notebook, we calculate both NSC and NSCB accuracy (see the paper for more details on these metrics).    ", "repo_name": "batch-effects-normalization", "org_name": "microsoft", "org_repo": "microsoft/batch-effects-normalization", "platform_org_repo": "github+microsoft/batch-effects-normalization", "link_to_repo": "https://github.com/microsoft/batch-effects-normalization", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Experience Lab - Microsoft Azure Data Manager for Energy\n\n## Build Status\n\n[![CI](https://github.com/microsoft/azure-data-manager-for-energy-experience-lab/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/azure-data-manager-for-energy-experience-lab/actions/workflows/ci.yml)\n\n## About\n\nExperience Lab is an automated, end-to-end deployment accelerator for Microsoft Azure Data Manager for Energy that provides easy, fast deployments with sample dataset(s) for learning, testing, demo and training purposes for customers and partners.\n\nExperience Lab makes it easy to create an instance of Azure Data Manager for Energy. It includes a simple web UI to perform basic management tasks like the creation of users and legal tags, and performing data load of standard, sample data sets. It also includes integration with Power BI Desktop to perform data validation and visualization of loaded data.\n\nIt therefore allows even the audience that is not deeply technical to be able to create fully configured, data-loaded instances of Azure Data Manager for Energy quickly, with ease.\n\nExperience Lab is only recommended for non-production use cases. It is open source, so that our customers and partners can freely use it and extend it to their bespoke use cases, including automation of deploying their own applications with Azure Data Manager for Energy.\n\n### Components\n\n- [Control plane](./control-plane)\n- [Portal](./developer-portal)\n  - [REST Scripts](./rest-scripts)\n  - [Swagger](./developer-portal/src/assets/swagger.yaml)\n- [Data load](./data-load)\n\n## Installing and running Experience Lab\n\n### Requirements\n\n- Experience Lab must be deployed in a region currently supported by Azure Data Manager for Energy.\n- The default installation script for the Experience Lab control plane requires the following privileges:\n  - Owner, Service Administrator, Co-Administrator, or Contributor + User Access Administrator at the subscription level.\n  - Permission to register an application with your Azure AD tenant.\n\n### Create Control Plane Via ARM Template\n\nUse the button below to deploy the Experience Lab Control Plane to your Azure Subscription. Further instructions are in [`/control-plane`](/control-plane)\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fazure-data-manager-for-energy-experience-lab%2Fmain%2Fazuredeploy.json)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-data-manager-for-energy-experience-lab", "org_name": "microsoft", "org_repo": "microsoft/azure-data-manager-for-energy-experience-lab", "platform_org_repo": "github+microsoft/azure-data-manager-for-energy-experience-lab", "link_to_repo": "https://github.com/microsoft/azure-data-manager-for-energy-experience-lab", "platform": "github", "language": "TypeScript", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# A Banner of Pride\nLast year, we open-sourced our Pride flag design representing 40 LGBTQIA+ communities. Our latest update reflects almost 50 identities and comes with a new fan-requested side layout. Plus, we are [open-sourcing the entire Pride design](https://github.com/microsoft/Pride) and invite you to remix, share and make more Pride.\n\nCreated by the LGBTQIA+ people at Microsoft, this year\u2019s flag design represents almost 50 individual LGBTQIA+ communities\u2014with one powerful graphic that reflects a message of unity, solidarity, and intersectionality. It celebrates a community that\u2019s wide, global, and ever-growing. We\u2019re making it available here on GitHub for everyone to use, share and build on. Because Pride should be open source. \n\nLearn more about [Pride at Microsoft.](https://unlocked.microsoft.com/pride/) \n\n\n<img width=\"1004\" alt=\"Screenshot 2023-05-31 175847\" src=\"https://github.com/microsoft/Pride-flag/assets/113071293/5106a2ba-1444-48aa-8b44-1ec3f6aad4ed\">\n\n\n\n# An Emblem of Unity\nThis year\u2019s design builds on our earlier release and adds new LGBTQIA+ flags and layouts.\n\nThe following flags are represented in this graphic: A-spec, Abrosexual, Aceflux, Achillean, Agender, Ally, Ambiamorous, Androgynous, Aroace, Aroflux, Abrosexual, Aceflux, Achillean, Agender, Ally, Ambiamorous, Androgynous, Aroace, Aroflux, Aromantic, Asexual, A-spec, Bigender, Bisexual, Demiboy, Demifluid, Demigender, Demigirl, Demiromantic, Demisexual, Diamoric, Gay / MLM, Gender questioning, Genderfluid, Genderflux, Genderqueer, Graysexual, Intersex , Lesbian , Maverique , Multigender , Multisexual , Neutrois , Non binary , Omnisexual , Pangender , Pansexual , Polyamorous , Polysexual , Pride , Queer , Sapphic , Transfeminine , Transgender , Transmasculine , Transneutral , Trigender, Two Spirits and Unlabeled.\n\nLearn more about our approach to this [design](https://medium.com/microsoft-design/pride-should-be-open-source-e4eb50fae2f9).\n\nGet the assets here and on [Figma](https://www.figma.com/community/file/1158808367098375909).\n\n![FY23Pride-Open_Source_PrideFlag_Cropped_without_Ally_1280x720](https://github.com/microsoft/Pride-flag/assets/113071293/fced6e62-72cb-4d3c-b92f-dbcb76f95d31)\n\n# Make it yours\n\nWe welcome your contributions to this project! With hundreds of community flags already available\u2014and more being added every day\u2014we\u2019re excited for people to keep creating more. Please note that most contributions require you to agree to a Contribution License Agreement (CLA.) Visit https://cla.opensource.microsoft.com.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Updates at a glance\n\nThis update features new flags and new layouts. Plus, current flag assets are available as well.\n\n![flags compare](https://github.com/microsoft/Pride-flag/assets/113071293/1c76fc5e-6a32-42f0-bedc-d2902866fafd)\n\n# License\n\nMicrosoft and any contributors grant you a license to content in this repository under the [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode), see the [LICENSE](LICENSE) file. Privacy information can be found at https://privacy.microsoft.com/en-us/. Progress Pride Flag \u00a9quasar.digital LLC 2022. Polyamory Pride Flag by Molly Colleen Bennett Wilvich.\n", "repo_name": "Pride-flag", "org_name": "microsoft", "org_repo": "microsoft/Pride-flag", "platform_org_repo": "github+microsoft/Pride-flag", "link_to_repo": "https://github.com/microsoft/Pride-flag", "platform": "github", "language": null, "stargazers_count": 123, "watchers_count": 123}, {"README_text": "## Overview\n\nWelcome to the bc2dataverse extension page!\n\nThis tool helps you achieve quicker integration of your Dynamics 365 Business Central with [Dataverse](https://powerplatform.microsoft.com/en-us/dataverse/), that powers your Power Apps or Dynamics 365 Customer Engagement apps. Using inputs from you, it generates the [AL code](https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/developer/devenv-programming-in-al) that can then be added to, or published as, an [AL extension](https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/developer/devenv-dev-overview). The generated code conforms to the guidelines as specified at [Customizing an Integration with Microsoft Dataverse](https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/administration/administration-custom-cds-integration).\n\n> **This tool is a prototype and it is the sole responsibility of the user to ensure that the code generated using this tool is correct. The makers of the tool are not responsible for the consequences of executing such code in a production environment. You are welcome to visit [the Support page](./SUPPORT.md). In case you wish to engage directly with us, please write to bc2dataverse@microsoft.com. As we are a small team, please expect delays in getting back to you.**\n\n## Features\n\nThis extension includes the following VS Code commands that become available on the Visual Studio Code [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette),\n- `bc2dataverse: Generate proxy table` generates the proxy table inside Business Central that mirrors the data on the Dataverse.\n- `bc2dataverse: Generate list page` generates a list page for the proxy table created above so users can view/ synchronize data for individual entities.\n- `bc2dataverse: Map to existing table` generates the AL codeunit that holds the code to map fields and tables between the Business Central data and the Dataverse data.\n\n## Start using this tool\n\n### Prerequisites\n\nPlease be mindful of these before you run this tool,\n- In order to use this extension, make sure that you are in a valid AL project by following the instructions at [Get Started with AL](https://learn.microsoft.com/da-dk/dynamics365/business-central/dev-itpro/developer/devenv-get-started).\n- Ensure that you have the [AL Language extension](https://marketplace.visualstudio.com/items?itemName=ms-dynamics-smb.al) installed in Visual Studio Code.\n- Always make sure that you have compiled your AL project before invoking any of the VS Code commands.\n\n### Installation\nThe most updated version of the installable can be found at the [https://marketplace.visualstudio.com/items?itemName=TheBc2dataverseteam-MSDenmark.bc2dataverse](https://marketplace.visualstudio.com/items?itemName=TheBc2dataverseteam-MSDenmark.bc2dataverse). Install this extension into your Visual Studio Code environment by following the steps at [Browse for extensions](https://code.visualstudio.com/docs/editor/extension-marketplace#_browse-for-extensions).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n**Enjoy!**\n", "repo_name": "bc2dataverse", "org_name": "microsoft", "org_repo": "microsoft/bc2dataverse", "platform_org_repo": "github+microsoft/bc2dataverse", "link_to_repo": "https://github.com/microsoft/bc2dataverse", "platform": "github", "language": "TypeScript", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# action-armttk - Preview/In-Progress \n\n[![Test](https://github.com/microsoft/action-armttk/workflows/Test/badge.svg)](https://github.com/microsoft/action-armttk/actions?query=workflow%3ATest)\n[![reviewdog](https://github.com/microsoft/action-armttk/workflows/reviewdog/badge.svg)](https://github.com/microsoft/action-armttk/actions?query=workflow%3Areviewdog)\n[![depup](https://github.com/microsoft/action-armttk/workflows/depup/badge.svg)](https://github.com/microsoft/action-pylint/actions?query=workflow%3Adepup)\n[![release](https://github.com/microsoft/action-armttk/workflows/release/badge.svg)](https://github.com/microsoft/action-armttk/actions?query=workflow%3Arelease)\n[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/microsoft/action-armttk?logo=github&sort=semver)](https://github.com/microsoft/action-armttk/releases)\n[![action-bumpr supported](https://img.shields.io/badge/bumpr-supported-ff69b4?logo=github&link=https://github.com/haya14busa/action-bumpr)](https://github.com/haya14busa/action-bumpr)\n\nThis repo contains a action to run [armttk](https://github.com/azure/armttk).\n\n## Quick Start\nUse this predefined workflow to quickly get started using this GitHub Action.\n\n```yaml\nname: On pull request\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  validate-module-files-with-armttk:\n    uses: microsoft/action-armttk/.github/workflows/arm-ttk@0.0.5\n    with:\n      bicepFile: main.bicep\n      workingPath: .\n```\n\n## Input\n\n```yaml\ninputs:\n  github_token:\n    description: \"GITHUB_TOKEN\"\n    required: true\n    default: \"${{ github.token }}\"\n  workdir:\n    description: \"Working directory relative to the root directory.\"\n    required: false\n    default: \".\"\n  glob_pattern:\n    description: \"Glob pattern of the files to lint\"\n    required: false\n    default: \".\"\n  ### Flags for reviewdog ###\n  tool_name:\n    description: \"Tool name to use for reviewdog reporter.\"\n    required: false\n    default: \"armttk\"\n  level:\n    description: \"Report level for reviewdog [info,warning,error]\"\n    required: false\n    default: \"error\"\n  reporter:\n    description: \"Reporter of reviewdog command [github-pr-check,github-pr-review].\"\n    required: false\n    default: \"github-pr-check\"\n  filter_mode:\n    description: |\n      Filtering mode for the reviewdog command [added,diff_context,file,nofilter].\n      Default is added.\n    required: false\n    default: \"added\"\n  fail_on_error:\n    description: |\n      Exit code for reviewdog when errors are found [true,false]\n      Default is `false`.\n    required: false\n    default: \"false\"\n  reviewdog_flags:\n    description: \"Additional reviewdog flags\"\n    required: false\n    default: \"\"\n  armttkVersion:\n    description: latest or marketplace version of ARM-TTK \n```\n\n## Usage\n\n```yaml\nname: reviewdog\non: [pull_request]\njobs:\n  armttk:\n    name: runner / armttk\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: microsoft/action-armttk@v1\n        with:\n          github_token: ${{ secrets.github_token }}\n          # Change reviewdog reporter if you need [github-pr-check,github-check,github-pr-review].\n          reporter: github-pr-review\n          # Change reporter level if you need.\n          # GitHub Status Check won't become failure with warning.\n          level: warning\n          glob_pattern: \"**/*.json\"\n```\n\n## Development\n\n### Release\n\n#### [haya14busa/action-bumpr](https://github.com/haya14busa/action-bumpr)\nYou can bump version on merging Pull Requests with specific labels (bump:major,bump:minor,bump:patch).\nPushing tag manually by yourself also work.\n\n#### [haya14busa/action-update-semver](https://github.com/haya14busa/action-update-semver)\n\nThis action updates major/minor release tags on a tag push. e.g. Update v1 and v1.2 tag when released v1.2.3.\nref: https://help.github.com/en/articles/about-actions#versioning-your-action\n\n### Lint - reviewdog integration\n\nThis reviewdog action template itself is integrated with reviewdog to run lints\nwhich is useful for Docker container based actions.\n\n![reviewdog integration](https://user-images.githubusercontent.com/3797062/72735107-7fbb9600-3bde-11ea-8087-12af76e7ee6f.png)\n\nSupported linters:\n\n- [reviewdog/action-shellcheck](https://github.com/reviewdog/action-shellcheck)\n- [reviewdog/action-hadolint](https://github.com/reviewdog/action-hadolint)\n- [reviewdog/action-misspell](https://github.com/reviewdog/action-misspell)\n\n### Dependencies Update Automation\n\nThis repository uses [reviewdog/action-depup](https://github.com/reviewdog/action-depup) to update\nreviewdog version.\n\n[![reviewdog depup demo](https://user-images.githubusercontent.com/3797062/73154254-170e7500-411a-11ea-8211-912e9de7c936.png)](https://github.com/reviewdog/action-template/pull/6)\n", "repo_name": "action-armttk", "org_name": "microsoft", "org_repo": "microsoft/action-armttk", "platform_org_repo": "github+microsoft/action-armttk", "link_to_repo": "https://github.com/microsoft/action-armttk", "platform": "github", "language": "Shell", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# AutoRL Research\n\nThis repository contains code for a series of research projects on Automated Reinforcement Learning (AutoRL).\n\n## News\n\n* 2022.9.21 [Towards Applicable Reinforcement Learning: Improving the Generalization and Sample Efficiency with Policy Ensemble](https://seqml.github.io/eppo/) is now available in [eppo](eppo).\n* 2022.10.12 [Reinforcement Learning with Automated Auxiliary Loss Search](https://seqml.github.io/a2ls/) is now available in [a2ls](a2ls).\n* 2023.3.10 [Bootstrapped Transformer for Offline Reinforcement Learning](https://seqml.github.io/bootorl/) is now available in [bootorl](bootorl).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "autorl-research", "org_name": "microsoft", "org_repo": "microsoft/autorl-research", "platform_org_repo": "github+microsoft/autorl-research", "link_to_repo": "https://github.com/microsoft/autorl-research", "platform": "github", "language": "Python", "stargazers_count": 31, "watchers_count": 31}, {"README_text": "# Microsoft Build 2023 - Community-Led Parties resources\n\n![Microsoft Build 2023 banner](./Assets/Microsoft%20Build%202023/EmailHeaders_2023MSBuild_02_B-Header.png)\n\n## Quick start\n\n* [How to host a Communtiy-Led Party](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Community-led_Party_Guide_Build_2023.pdf)\n\n## Host resources\n\n* Digital Assets\n  * [Briefing Deck](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Microsoft%20Build%202023%Community-Led%Parties%-Briefing%Deck.pdf)\n  * [Logos](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Logos)\n  * [Email headers](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Email%20Headers)\n  * [Microsoft Teams backgrounds](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Teams%20Background)\n  * [Social post templates](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Social%20Posts%20Templates)\n  * [PowerPoint templates](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Powerpoint%20Templates)\n  * [Digital Swag](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Digital%20Swag)\n  * [Promotional single slide (Powerpoint) - Please feel free to customise to your unique Learn Collections AKA Link/QR Code](https://github.com/microsoft/Microsoft-Community-Led-Parties/tree/main/Assets/Microsoft%20Build%202023/Build-Watch-Together-PromoSlide.pptx)\n\n<br/>\n\n## Book of News\n[Microsoft Build 2023 Book of News](https://news.microsoft.com/build-2023-book-of-news/)  \n\n<br/>\n \n## Surveys\n* [Attendee Survey](https://aka.ms/msbuild-comm-led-parties-survey)\n* [Host Survey](https://aka.ms/msbuild-comm-led-parties-hosts-survey)\n\n<br/>\n\n## Learn Collections \nA unique AKA link is in the format of https://aka.ms/msbuild-lc-h<?>, where LC = learn collection and h= host, is assigned to you.\n\nPromotion of Learn Collections \nYou can do this via any channel, and any time leading up to July 7. This can be through \t\tyour social channels, blog, during your party, and even YouTube video description (after \tyou share your party on-demand)! Feel free to think of any creative means!  \n \nAs a token of appreciation, the Community-Led Parties Workstream will be rewarding the top 3 sharers with a small appreciation token end July. The top 3 will be rewarded with a 50USD Gift Voucher.  \n\n<br/>\n\n---\n\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Microsoft-Community-Led-Parties", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-Community-Led-Parties", "platform_org_repo": "github+microsoft/Microsoft-Community-Led-Parties", "link_to_repo": "https://github.com/microsoft/Microsoft-Community-Led-Parties", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Jupyter Cell Tags support in VS Code\n\nThis extension provides support for notebook cell tags in Visual Studio Code for workng with tools like [`papermill`](https://github.com/nteract/papermill), [`nbconvert`](https://github.com/jupyter/nbconvert), [`nbgrader`](https://github.com/jupyter/nbgrader) and many more. Support for adding slide types to notebook cells are provided by the [Jupyter Slide Show](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-jupyter-slideshow) extension.\n\n### Features:\n- Add a tag to the cell you're on by opening the Command Palette (`Cmd+Shift+P`) and selecting **Add Cell Tag** or by clicking **+ Tag** on the cell ![Add cell tag](images/add-cell-tag.png)\n- Add multiple tags to the cell you're on by opening the Command Palette (`Cmd+Shift+P`) and selecting **Jupyter: Focus on Cell Tags View** and clicking on **+** ![Cell tags view](images/cell-tags-view.png)\n- Modify tags in the notebook's metadata (JSON format) by opening the Command Palette (`Cmd+Shift+P`) and selecting **Edit Cell Tags (JSON)** or by clicking out to it from the Cell Tags View ![Cell tags json](images/cell-tags-json.png)\n\nThis extension comes with the [Jupyter extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) and can be disabled or uninstalled.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-jupyter-cell-tags", "org_name": "microsoft", "org_repo": "microsoft/vscode-jupyter-cell-tags", "platform_org_repo": "github+microsoft/vscode-jupyter-cell-tags", "link_to_repo": "https://github.com/microsoft/vscode-jupyter-cell-tags", "platform": "github", "language": "TypeScript", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Jupyter Slide Show support in VS Code\n\nThis extension provides support for adding slide types to notebook cells for working with tools like [`nbconvert`](https://github.com/jupyter/nbconvert) to help easily convert your `.ipynb` file into a slide show. Supported slide types are:\n![Slide types](images/slide-types.png)\n\nSupport for additional and more flexible cell tagging is provided by the [Jupyter Cell Tags](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-jupyter-cell-tags) extension.\n\n### Features:\n- Add a slide type to the cell you're on by opening the Command Palette (`Cmd+Shift+P`) and selecting **Switch Slide Type**\n- Modify slide types for notebook cells by selecting the slide type on the cell ![Modify slide type](images/modify-slide-type.png)\n- Add or modify slide type for multiple cells by editing the notebook's metadata (JSON format) by opening the Command Palette (`Cmd+Shift+P`) and selecting **Edit Slide Type (JSON)**\n\n### Usage:\nAfter assigning slide types to your cells, create an HTML slideshow presentation by opening the integrated terminal and running the command, `jupyter nbconvert '<notebook-file-name>.ipynb' --to slides --post serve`.\n\nThis extension comes with the [Jupyter extension for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) and can be disabled or uninstalled.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-jupyter-slideshow", "org_name": "microsoft", "org_repo": "microsoft/vscode-jupyter-slideshow", "platform_org_repo": "github+microsoft/vscode-jupyter-slideshow", "link_to_repo": "https://github.com/microsoft/vscode-jupyter-slideshow", "platform": "github", "language": "TypeScript", "stargazers_count": 17, "watchers_count": 17}, {"README_text": "# Project\n\nThis project contains an ARM template for easy deployment of \"Clinical Trials to FHIR Server Patients Matching\" using \"Health Decision Support\" service with \n[\"New Hope\" Trial Matching technology](https://www.microsoft.com/en-us/research/project/project-new-hope-clinical-trials-matching/) model\n\nThis service is still in preview, for more info please contact healthil-solutions@microsoft.com\n\n\n## deployment\nTo deploy in your own subscription, you should already have:\n- private ACR username/password (contact team for more information)\n- azure fhir server with patient data\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FPatientsTrialMatchingBlueprint%2Fmain%2Fsrc%2Ftemplate.json)\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PatientsTrialMatchingBlueprint", "org_name": "microsoft", "org_repo": "microsoft/PatientsTrialMatchingBlueprint", "platform_org_repo": "github+microsoft/PatientsTrialMatchingBlueprint", "link_to_repo": "https://github.com/microsoft/PatientsTrialMatchingBlueprint", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "msticpy-data", "org_name": "microsoft", "org_repo": "microsoft/msticpy-data", "platform_org_repo": "github+microsoft/msticpy-data", "link_to_repo": "https://github.com/microsoft/msticpy-data", "platform": "github", "language": null, "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Try Out Development Containers: SQL Server & Azure SQL\n\n[![Open in Remote - Containers](https://img.shields.io/static/v1?label=Remote%20-%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/vscode-remote-try-sqlserver)\n\nA **development container** is a running [Docker](https://www.docker.com) container with a well-defined tool/runtime stack and its prerequisites. You can try out development containers with **[GitHub Codespaces](https://github.com/features/codespaces)** or **[Visual Studio Code Remote - Containers](https://aka.ms/vscode-remote/containers)**.\n\nThis is a sample project that lets you try out either option in a few easy steps. We have a variety of other [vscode-remote-try-*](https://github.com/search?q=org%3Amicrosoft+vscode-remote-try-&type=Repositories) sample projects, too.\n\n> **Note:** If you already have a Codespace or dev container, you can jump to the [Things to try](#things-to-try) section.\n\n\n## Setting up the development container\n\n### GitHub Codespaces\n\nFollow these steps to open this sample in a Codespaces:\n\n1. Click the Code drop-down menu and select the **Codespaces** tab.\n1. Click on **Create codespaces on main** at the bottom of the pane.\n\nFor more info, check out the [GitHub documentation](https://docs.github.com/en/free-pro-team@latest/github/developing-online-with-codespaces/creating-a-codespace#creating-a-codespace).\n\n### VS Code Dev Containers\n\nIf you already have VS Code and Docker installed, you can click the badge above or [here](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/vscode-remote-try-sqlserver) to get started. Clicking these links will cause VS Code to automatically install the Dev Containers extension if needed, clone the source code into a container volume, and spin up a dev container for use.\n\nFollow these steps to open this sample in a container using the VS Code Dev Containers extension:\n\n1. If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in the [getting started steps](https://aka.ms/vscode-remote/containers/getting-started).\n\n2. To use this repository, you can either open the repository in an isolated Docker volume:\n\n    - Press <kbd>F1</kbd> and select the **Dev Containers: Try a Sample...** command.\n    - Choose the \".NET Core\" sample, wait for the container to start, and try things out!\n        > **Note:** Under the hood, this will use the **Dev Containers: Clone Repository in Container Volume...** command to clone the source code in a Docker volume instead of the local filesystem. [Volumes](https://docs.docker.com/storage/volumes/) are the preferred mechanism for persisting container data.\n\n   Or open a locally cloned copy of the code:\n\n   - Clone this repository to your local filesystem.\n   - Press <kbd>F1</kbd> and select the **Dev Containers: Open Folder in Container...** command.\n   - Select the cloned copy of this folder, wait for the container to start, and try things out!\n\n## Things to try\n\nOnce you have this sample opened, you'll be able to work with it like you would locally.\n\n> **Note:** This container runs as a non-root user with sudo access by default. Comment out `\"remoteUser\": \"vscode\"` in `.devcontainer/devcontainer.json` if you'd prefer to run as root.\n\n1. **Connect via SQLCMD and create a new database**\n\n    SQLCMD is already installed within the container. You can use it from the **Terminal** tab, using the *bash* shell. For example, you can execute a SQL Script. This example creates a new database.\n\n    ```sql\n    sqlcmd -S localhost -U sa -P P@ssw0rd -d master -i 01-CreateDatabase.sql\n    ```\n\n    > Note: The SQL Server instance is created with user `sa` and password `P@ssw0rd`. You will need it for the next steps. This password is defined in the `devcontainer.json` file. This password is not secure. It could be used in local development scenarios but must not be used elsewhere (hosted team development server, or production server).\n\n2. **Deploy schema with SQL Database projects**\n\n    SQL Server Database projects allow you to organize the code artifacts, generate a dacpac, or easily deploy schema changes on an instance. In this repository, you'll find a sample Database project that creates a single `User` table and populate it with some records. Let's deploy it on the SQL Instance integrated into the dev container.\n\n    - On the primary sidebar (on the left), click on **Database projects** tab.\n    - The _Database Projects_ pane appears. The `TryDbProjects` project shows up. You can just right-click the database project name and click **Publish**. You'll have a series of prompts. Answer with these items (Prompt -> Answer):\n        - Select where to publish the project to -> Publish to an existing SQL server\n        - Select publish profile to load -> Don't use profile\n        - Choose a connection profile from the list below -> mssql-container\n        - Select database -> TryDbProject\n        - Choose action -> Publish\n    - After a minute or so, the database schema should be deployed. You can follow the deployment via the notification or through _Database Projects_ output. \n\n> Note: you might have to update the extension setting _dotnet SDK location_ to `/usr/bin/` to execute the publish step. This setting is called _Dotnet SQL Location_ under _Extensions_ > _Database Projects_.\n\n3. **Explore your database with SQL Server extension**\n\n    SQL Server extension allows you to explore your SQL Server and Azure SQL instances right within Visual Studio Code. Let's explore the database we have just created.\n\n    - On the primary sidebar (on the left), click on **SQL Server** tab.\n    - The _SQL Server_ pane appears. You should already see `mssql-container` listed. Click on it\n    - Type the password listed previously\n    - The treeview will be populated with Database items. For example, you can explore the tables or even select the rows in the `dbo.Users` table created with the SQL Database project at the previous step.\n\n## Things to try with an Azure Subscription\n\nThis dev container uses the Azure CLI [dev container feature](https://github.com/devcontainers/features), that allows to interact with your Azure subscription. Here we will create a new Azure SQL Server and deploy our schema onto it. \n\n> Note: In this example, we use the Azure CLI to create our Azure resources. In a production environment, we would probably rely on Infrastructure tools like [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep). \n\n1. **Login to your Azure account and create resources**\n\n    - Open the terminal pane and type `az login --use-device-code`. Follow the instructions\n    - Once logged-in, execute `./infrastructure/createAzureSQLServer.sh`\n    - Copy the connection string, this will be necessary in the next step.\n\n    This script will create an Azure SQL logical server, an Azure SQL Database, and will create a firewall rule opening the traffic to the whole Internet. **You should not use this script for production scenarios**. \n    \n    >Remember to execute the Cleanup step below to avoid excessive billing.\n\n2. **Prepare database project and deploy it to Azure**\n\n    - On the primary sidebar (on the left), click on **Database projects** tab.\n    - The _Database Projects_ pane appears. The `TryDbProjects` project shows up.\n    - Right click on the project name, and click on **Change target platform**, and select **Azure SQL Database**\n    - You can now deploy the database schema like you've done in the previous example, by using the connection string generated in step 1.\n\n    Your database schema will be published. You can go back on the **SQL Server** Pane to explore it.\n\n3. Cleanup Azure resources\n\n    The scripts executed in step 1 creates Azure resources on your subscription. This will incur some billing. Once you've finished trying this feature, you can simply delete the created resource group. The Azure CLI command to execute is displayed on step 1. It looks like `az group delete --name resourceGroup`\n\n> **About Azure Billing**\n> In this example, we are using the [Serverless compute tier](https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql) of Azure SQL. This should incur a maximum billing of few euros if used for an hour. Depending on your subscription, you can leverage the [Azure Free Tier](https://azure.microsoft.com/free/).\n> \n> As a good practice, in cloud environments, you should delete resources you don't use.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nCopyright \u00a9 Microsoft Corporation All rights reserved.<br />\nLicensed under the MIT License. See LICENSE in the project root for license information.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-remote-try-sqlserver", "org_name": "microsoft", "org_repo": "microsoft/vscode-remote-try-sqlserver", "platform_org_repo": "github+microsoft/vscode-remote-try-sqlserver", "link_to_repo": "https://github.com/microsoft/vscode-remote-try-sqlserver", "platform": "github", "language": "TSQL", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "![Fast Track for Azure](./readme_assets/fta.png)\nMove to Azure efficiently with customized guidance from Azure engineering. [FastTrack for Azure \u2013 Benefits and FAQ | Microsoft Azure](https://azure.microsoft.com/en-us/programs/azure-fasttrack/)\n\n# Analytics CI/CD CLI Utility\n\nCommand line tool for quickly setting up CI/CD for Azure Data Factory and Synapse Analytics.\n\n## Table of Contents\n- [Overview](#overview)\n- [Prerequisites](#prerequisites)\n- [Solution Architecture](#solution-architecture)\n- [Usage](#usage)\n- [Result](#result)\n- [Contributing](#contributing)\n- [Trademarks](#trademarks)\n\n## Overview\n\nAzure Data Factory and Synapse Analytics both have built-in Git integration. With a few clicks, a user may link their workspace with a Git repository (in either GitHub or Azure DevOps) and start applying version control.\n\nHowever, there is some additional complexity when we consider multiple environments - development, test and production, for example. There are multiple solutions to this problem, and the project team must carefully choose and set up a CI/CD mechanism that fits their needs.\n\nThis repository provides an interactive command line tool that helps you set up CI/CD for Data Factory and Synapse Analytics. This template is simple, yet effective and adaptable to most use-case scenarios, while following best practices around DevOps for Analytics.\n\n## Prerequisites\n\n- Active Azure Subscription\n- Write access to an Azure DevOps project\n- Azure Data Factory or Synapse Workspaces (or permission to create them)\n\n## Solution Architecture\n\nBelow is the architecture diagram of this solution and how it interacts with your existing subscriptions.\n\n![Solution Architecture](./readme_assets/architecture.drawio.png)\n\nThe Analytics CI/CD Setup Tool will interact with the user, requesting them to choose the target subscriptions and workspaces. Then, it will collect the git configuration from the development workspace, and use the provided information to create the Deploy pipeline.\n\nTriggering the Deploy pipeline will publish the development environment to production. The tooling will allow you to deploy the desired version, even if new features have been published and are under active development.\n\n## Usage\n\n| Step | Description | Detail |\n| --- | ------------ | ----- |\n| 1 | Create Data Factory or Synapse workspaces | ![Identify workspaces](./readme_assets/1_verify_workspaces.png) |\n| 2 | Set up Azure DevOps project with a git repository | ![Setup Git Repository](./readme_assets/3_setup_git_repository.png) |\n| 3 | Connect your development workspace with the git repository | ![Setup Git Integration](./readme_assets/4_setup_git_integration.png) |\n| 4 | Set up Azure CLI <br> - Install az cli <br> - Log in to the correct tenant <br> - Make sure the `datafactory` or `synapse`, as well as `devops` plugins are installed  | ----- |\n| 5 | Clone this repository | ```git clone https://github.com/microsoft/ftatoolkit-analytics-cicd-cli.git``` |\n| 6 | Run `sh setup.sh` | ![Start setup](./readme_assets/2_start_setup.png) |\n| 7 | Follow the interactive prompts | ----- |\n| 8 | Review newly created pipeline | ![Start setup](./readme_assets/5_review_pipelines.png) |\n\n## Result\n\nAfter executing the setup script, you may run the Workspace Deploy Pipeline to replicate your Development environment to Production. You may also customize the pipeline to better suit your use case, if necessary.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ftatoolkit-analytics-cicd-cli", "org_name": "microsoft", "org_repo": "microsoft/ftatoolkit-analytics-cicd-cli", "platform_org_repo": "github+microsoft/ftatoolkit-analytics-cicd-cli", "link_to_repo": "https://github.com/microsoft/ftatoolkit-analytics-cicd-cli", "platform": "github", "language": "Shell", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# UniSumm\n<p align=\"center\">\n    <img src=\"./figures/11.png\" width=\"444\" alt=\"method\">\n</p>\n\n\n## 0. Overview\nIn this work, we introduce, **UniSumm** a unified model that can be easily prefix-tuned to excel at any few-shot summarization tasks and a new benchmark, **SummZoo** for diverse and robust few-shot summarization evaluation.\n\n*[UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning](https://arxiv.org/abs/2211.09783)*\n\nIf you use UniSumm and SummZoo, please kindly cite our paper:\n```\n@article{chen2022unisumm,\n  title={UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and Prefix-Tuning},\n  author={Chen, Yulong and Liu, Yang and Xu, Ruochen and Yang, Ziyi and Zhu, Chenguang and Zeng, Michael and Zhang, Yue},\n  journal={arXiv preprint arXiv:2211.09783},\n  year={2022}\n}\n```\n\n### Content\n1. [**UniSumm**](#1-unisumm)\n2. [**SummZoo**](#summzoo)\n3. [**Setup**](#setup)\n4. [**Usage**](#usage)\n5. [**Evaluation**](#evaluation)\n6. [**Citation**](#citation)\n\n\n\n## 1. UniSumm\n<p align=\"center\">\n    <img src=\"./figures/unisumm.png\" width=\"888\" alt=\"method\">\n</p>\n\nAs shown in the above figure, the overall framework of UniSumm contains 2 phases: 1) learning summarization general knowledge by multi-task pre-training and; 2)learning target task knowledge by prefix-tuning.\n\n### (a) Multi-task Pre-Training with Prefix\nWe take BART as the foundation model, and further pre-train it on a set of summarization datasets.\nIn particular, for each task $t$, we inject task-specific encoder and decoder prefx vectors $P^t = [P^t_{en};P^t_{de}]$ into the model, which are prepended to each Transformer layer.\nIn this stage, we optimize the parameters of BART and prefixes togeter.\n\nIn addition, we also pre-train a universal encoder and decoder prefix vectors, $P^*$.\nFor each training instance from task $t$ it has a 15\\% probability to be coupled with this universal prefix vectors, instead of its original prefix $P^t$.\nThis universal prefix vector can be used as a robust initialization for few-shot prefix-tuning in the 2nd phase.\n\nWe release the multi-task pre-trained UniSumm checkpoint, which can be used for few-shot summarization.\n\n### (b) Prefix-Tuning for Few-Shot Summarization\nThrough multi-task pre-training, the model is equipped with diverse summarization knowledge.\nFor an unseen task $u$, we create a new prefix vector $P^u$, which can be initialized using the above universal prefix $P^*$.\nThen we freeze the parameter of summarization model and only update this new prefix vector.\n\n## SummZoo\nPrevious work on few-shot summariztaion either focuses on one type of data, e.g., monologue, or experiments on miscellaneous few-shot samples.\nThese make the evaluation from different research not comparable.\n\nThus, we assemble a new benchmark, SummZoo, based on two principles, namely diversity and robustness.\n\n<p align=\"center\">\n    <img src=\"./figures/summzoo.png\" width=\"666\" alt=\"summzoo\">\n</p>\n\n### Data Collection\nWe assemble SummZoo from 8 popular summarization datasets.\nThe statistic is shown in the above table.\n\nWe tokenized data in SummZoo, which can be directly used for training UniSumm.\nYou can download the tokenized data [here](https://drive.google.com/file/d/1gjOqQ_GKJp2LZxUCiblGGqA60V7epQHB/view?usp=share_link), and untokenized data [here](https://drive.google.com/file/d/1_sTerMkKJVPSXVDDK-F-mzZ1fTvR4NB9/view?usp=share_link).\n\n\n## Setup\nFor training and inference, please setup the following environment using Python>=3.7 (mind the cuda version suitable to your GPU):\n\n```\npip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\ngit clone https://github.com/microsoft/UniSumm.git\ncd UniSumm/unisumm\npip install -r requirements.txt\n```\n\nTo use ```fp16```, you need to install [apex](https://github.com/NVIDIA/apex) by running:\n```\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n```\n\nAnd you can unzip the downloaded SummZoo data in the data folder (/UniSumm/unisumm/data): ```unzip Summzoo.zip -d data```.\n\n\n## Usage\n### Get the Multi-task Pre-trained UniSumm (Phase-1 Model)\nWe release the checkpoint of multi-task pre-trained UniSumm [here](https://drive.google.com/file/d/1dDF284w5FfzwcZ_08b070pdy4bdbc02a/view?usp=share_link).\n\nAfter downloaded, unzip it in the current folder (UniSumm/unisumm): ```unzip unisumm_model.zip```.\n\n### Prefix-tune the Phase-1 Model\nThen, ```cd nlg-finetune```, and you can easily tune UniSumm using prefix-tuning. \nThe following command shows an example of how to tune UniSumm on 10-shot QMSum (sample group 42) on 4 A100 GPUs.\n```\nexport TRAIN_FILE=../data/Summzoo/qmsum/10-shot/train.jsonl.h10.s42\nexport OUTPUT_DIR=../few-shot-unisumm/qmsum/10-42\nexport CACHE_DIR=../cache/unisumm\nexport LOAD_FROM=../unisumm_model/ckpt-300000/\nexport TASK_MAP_FILE=../unisumm_model/task_map.json\n\nrm $OUTPUT_DIR -rf\n\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython -m torch.distributed.launch --master_port 8888 --nproc_per_node 4 \\\nrun_seq2seq.py \\\n--train_file $TRAIN_FILE \\\n--output_dir $OUTPUT_DIR  \\\n--model_type bart \\\n--model_name_or_path facebook/bart-large \\\n--fp16 \\\n--fp16_opt_level O2 \\\n--max_source_seq_length 2048 \\\n--max_target_seq_length 400 \\\n--per_gpu_train_batch_size 2 \\\n--gradient_accumulation_steps 4 \\\n--learning_rate 1.5e-4 \\\n--num_warmup_steps 10 \\\n--num_training_steps 100 \\\n--cache_dir $CACHE_DIR \\\n--save_steps 100 \\\n--num_shards 1 \\\n--num_prefixs_per_task 256 \\\n--tasks 'universal' \\\n--logging_steps 50 \\\n--use_prefix 1 \\\n--only_train_prefix 1 \\\n--load_from $LOAD_FROM \\\n--task_map_file $TASK_MAP_FILE\n```\n\nNote that for fair comparison, the hyperparameter choice is based on previous work and empirical consideration.\nIn real practice, you could search hyperparameters for better performance.\n\n### Inference\nThe following command shows an example of using prefix-tuned UniSumm to conduct inference on the QMSum testset:\n\n```\nexport MODEL_PATH=../few-shot-unisumm/qmsum/10-42/ckpt-100\nexport SAVE_PATH=../unisumm_outs/qmsum.10.42\nexport TASK_MAP_FILE=../few-shot-unisumm/qmsum/10-42/cached_features_for_training/task_map.json\nexport INPUT_FILE=../data/Summzoo/qmsum/test/qmsum.test.bart.uncased.jsonl\n\nmkdir ../unisumm_outs\n\nCUDA_VISIBLE_DEVICES=0 python decode_seq2seq.py \\\n--fp16 \\\n--do_lower_case \\\n--model_path $MODEL_PATH \\\n--max_seq_length 2048 \\\n--max_tgt_length 256 \\\n--batch_size 4 \\\n--beam_size 5 \\\n--length_penalty 0.6 \\\n--mode s2s \\\n--min_len 60 \\\n--input_file $INPUT_FILE \\\n--decode_task 'universal' \\\n--task_map_file $TASK_MAP_FILE \\\n--output_file $SAVE_PATH\n```\n\nThen you should see the output file ```qmsum.10.42``` in the ```/UniSumm/unisumm/unisumm_outs``` folder.\n\n\n### Get All Results in One Command\nIn the ```nlg-finetune``` folder, by running the below command, you will get models trained on all sub-tasks (5 sample sets in both 10/100 settings) in SummZoo, using 4 GPUs, and output files generated by those models.\n```\nbash run.sh\n```\nNote that the above command may not utilize your GPUs effiently and the inference time can be parcularly long, you need to modifiy it according to your resources.\n\n\n## Evaluation\nWe use files2rouge for evaluation, you could refer to [files2rouge](https://github.com/pltrdy/files2rouge#getting-started) to setup and evaluate.\nThe reference is stored as ```test.tgt``` in the ```test``` folder of each task folder, for example: ```Summzoo/qmsum/test/test.tgt```.\nYou should get similar scores by evaluating the above generated summary on QMSum testset:\n```\nRouge-1 F1: 0.36482\nRouge-2 F1: 0.11488\nRouge-L F1: 0.23300\n```\n\n## Citation\nWhen using SummZoo for evaluation, please cite all individual papers where orginial datasets are introduced. \nSee [Summzoo.bib](https://github.com/microsoft/UniSumm/blob/main/Summzoo.bib)\n\n\n\n", "repo_name": "UniSumm", "org_name": "microsoft", "org_repo": "microsoft/UniSumm", "platform_org_repo": "github+microsoft/UniSumm", "link_to_repo": "https://github.com/microsoft/UniSumm", "platform": "github", "language": "Python", "stargazers_count": 57, "watchers_count": 57}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cxplat", "org_name": "microsoft", "org_repo": "microsoft/cxplat", "platform_org_repo": "github+microsoft/cxplat", "link_to_repo": "https://github.com/microsoft/cxplat", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Node.js Hello World App \n\nThis sample demonstrates a tiny Hello World node.js app for [App Service Web App](https://docs.microsoft.com/azure/app-service-web).\n\n## Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "MTC_IL_Nodejs_Hello_World", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_Nodejs_Hello_World", "platform_org_repo": "github+microsoft/MTC_IL_Nodejs_Hello_World", "link_to_repo": "https://github.com/microsoft/MTC_IL_Nodejs_Hello_World", "platform": "github", "language": "JavaScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Mask-based Latent Reconstruction for Reinforcement Learning\n\nThis is the official implementation of *[Masked-based Latent Reconstruction for Reinforcement Learning](https://arxiv.org/abs/2201.12096)* (accepted by NeurIPS 2022), which outperforms the state-of-the-art sample-efficient reinforcement learning methods such as [CURL](https://arxiv.org/abs/2004.04136), [DrQ](https://arxiv.org/abs/2004.13649), [SPR](https://openreview.net/forum?id=uCQfPZwRaUu), [PlayVirtual](https://arxiv.org/abs/2106.04152), etc.\n\n- [arXiv](https://openreview.net/forum?id=GSHFVNejxs7&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2021%2FConference%2FAuthors%23your-submissions))\n- [OpenReview](https://openreview.net/forum?id=-zlJOVc580)\n- [SlidesLive](https://recorder-v3.slideslive.com/#/share?share=74702&s=2295c61d-8048-439f-a718-54adb5b8b629)\n\n## Abstract\nFor deep reinforcement learning (RL) from pixels, learning effective state representations is crucial for achieving high performance. However, in practice, limited experience and high-dimensional inputs prevent effective representation learning. To address this, motivated by the success of mask-based modeling in other research fields, we introduce mask-based reconstruction to promote state representation learning in RL. Specifically, we propose a simple yet effective self-supervised method, Mask-based Latent Reconstruction (MLR), to predict complete state representations in the latent space from the observations with spatially and temporally masked pixels. MLR enables better use of context information when learning state representations to make them more informative, which facilitates the training of RL agents. Extensive experiments show that our MLR significantly improves the sample efficiency in RL and outperforms the state-of-the-art sample-efficient RL methods on multiple continuous and discrete control benchmarks. \n\n## Framework\n\n![image](./figs/framework.png)\n\nFigure 1. The framework of the proposed MLR. We perform a random spatial-temporal masking (i.e., *cube* masking) on the sequence of consecutive observations in the pixel space. The masked observations are encoded to be the latent states through an online encoder. We further introduce a predictive latent decoder to decode/predict the latent states conditioned on the corresponding action sequence and temporal positional embeddings. Our method trains the networks to reconstruct the information available in the missing contents in an appropriate *latent* space using a cosine similarity based distance metric applied between the predicted features of the reconstructed states and the target features inferred from original observations by momentum networks.\n\n  \n## Run MLR\nWe provide codes for two benchmarks: Atari and DMControl.\n~~~\n.\n\u251c\u2500\u2500 Atari\n|   \u251c\u2500\u2500 README.md\n|   \u2514\u2500\u2500 ...\n|\u2500\u2500 DMControl\n|   \u251c\u2500\u2500 README.md\n|   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 CODE_OF_CONDUCT.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 SUPPORT.md\n\u2514\u2500\u2500 SECURITY.md\n~~~\n\nRun Atari code: enter ./Atari for more information.\n~~~\ncd ./Atari\n~~~\nRun DMControl code: enter ./DMControl for more information.\n~~~\ncd ./DMControl\n~~~\n\n## Citation\nPlease use the following BibTeX to cite our work.  \n```\n@article{yu2022mask,\n  title={Mask-based latent reconstruction for reinforcement learning},\n  author={Yu, Tao and Zhang, Zhizheng and Lan, Cuiling and Lu, Yan and Chen, Zhibo},\n  journal={Advances in Neural Information Processing Systems},\n  volume={35},\n  pages={25117--25131},\n  year={2022}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Mask-based-Latent-Reconstruction", "org_name": "microsoft", "org_repo": "microsoft/Mask-based-Latent-Reconstruction", "platform_org_repo": "github+microsoft/Mask-based-Latent-Reconstruction", "link_to_repo": "https://github.com/microsoft/Mask-based-Latent-Reconstruction", "platform": "github", "language": "Python", "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# ML4C: Seeing Causality Through Latent Vicinity\n\nML4C (Machine Learning for Causality) is a **supervised** causal discovery approach on **observational** data (and currently only supports **discrete** data) with theoretical guarantee. Starting from an input dataset with the corresponding skeleton provided, ML4C classifies (orients) whether each unshielded triple is a v-structure or not, and then outputs the corresponding CPDAG. Theoretically, ML4C is asymptotically correct by considering the graphical predicates in vicinity of each unshielded triple. Empirically, ML4C remarkably outperforms other state-of-the-art algorithms in terms of accuracy, reliability, robustness and tolerance. See our [paper](https://arxiv.org/abs/2110.00637) for more details.\n\n---\n\n## Basic Usage Example\n\n```bash\ncd Examples/\npython main.py\n```\n\nThis example orients a given skeleton. A simple call to `orient_skeleton` would work. Specifically, the arguments are:\n\n+ `datapath`: `str`. Path to the observational data records. Should end with `.npy`, with the array in shape `(n_samples, n_variables)`. Now we only support discrete data, so the entries of this data array must be integers.\n+ `skeletonpath`: `str`. Path to the provided skeleton's adjacency matrix. Should end with `.txt`, with the array in shape `(n_variables, n_variables)`. If `i--j` is in the skeleton, then `a[i, j] = a[j, i] = 1`, and otherwise `a[i, j] = a[j, i] = 0`. Note that, \n  + You may obtain the skeleton from data using standard algorithms (e.g., PC, GES, etc.), and then undirect all edges.\n  + Or alternatively, you may try out our [ML4S](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/ML4S-camera-ready.pdf) with [code](https://github.com/microsoft/reliableAI/tree/main/causal-kit/ML4S).\n+ `savedir`: `str`. The directory to save the result CPDAG's adjacency matrix. If `a[i, j] = 1` and `a[j, i] = 0` then there is a directed edge `i->j`. If `a[i, j] = a[j, i] = 1` then there is an undirected edge `i--j`. Otherwise there is no edge between `i` and `j`.\n\n\n## Classifier Training\n\nIn this repository, we provide a pre-trained classifier `./Learner/ML4C_learner.model`. To reproduce this classifier, you may run the following steps:\n\n```bash\ncd Learner/\npython SynthesizeData.py      # Generate synthetic graph strutures and data records\npython GenerateFeatures.py    # Generate features for each unshielded triple, based on the vicinity information\n```\n\nThen train a supervised classifier using whatever framework you like (e.g., here we use [XGBoost](https://xgboost.readthedocs.io/en/stable/)). Note that if you would like to customize your own classifier based on your own synthetic data (e.g., for continuous case), you may also follow the steps above.\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ML4C", "org_name": "microsoft", "org_repo": "microsoft/ML4C", "platform_org_repo": "github+microsoft/ML4C", "link_to_repo": "https://github.com/microsoft/ML4C", "platform": "github", "language": "Python", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AMRL-ICLR2020", "org_name": "microsoft", "org_repo": "microsoft/AMRL-ICLR2020", "platform_org_repo": "github+microsoft/AMRL-ICLR2020", "link_to_repo": "https://github.com/microsoft/AMRL-ICLR2020", "platform": "github", "language": "Python", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Unreal Cloud DDC\n\n## Setup\n\nFollowing the directions provided here to setup a [service principal with a secret](https://github.com/Azure/login#configure-a-service-principal-with-a-secret).\nThis will be used to deploy the resources for the repository.\n\n## Template Sync\nTo pull the latest changes from the template, add a new secret GitHub PAT (and include workflow permission to sync all changes).\n\n## Deploy\nUse the following command to deploy Unreal Cloud DDC using a parameters file.\n\n```sh\naz login\naz account set -s <Insert-Subscription-Name-or-ID>\n./scripts/deploy.sh configs/studio/template.parameters.json $APP_SECRET\n```\n", "repo_name": "unreal-cloud-ddc-on-azure", "org_name": "microsoft", "org_repo": "microsoft/unreal-cloud-ddc-on-azure", "platform_org_repo": "github+microsoft/unreal-cloud-ddc-on-azure", "link_to_repo": "https://github.com/microsoft/unreal-cloud-ddc-on-azure", "platform": "github", "language": "Bicep", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "DataOpsWorkshop", "org_name": "microsoft", "org_repo": "microsoft/DataOpsWorkshop", "platform_org_repo": "github+microsoft/DataOpsWorkshop", "link_to_repo": "https://github.com/microsoft/DataOpsWorkshop", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# CSuite: A Suite of Benchmark Datasets for Causality\n\nCSuite is a collection of synthetic datasets for benchmarking causal machine learning algorithms. Each dataset consists of\n - the true causal graph, for benchmarking causal discovery;\n - 4000 rows of observational training data;\n - 2000 rows of observational test data;\n - interventional test data, for benchmarking estimation of average treatment effect (ATE) and conditional average treatment effect (CATE), 2000 rows per interventional environment.\n \nThe data was generated from known hand-crafted structural equation models (SEMs). Different datasets are intended to test different features of causal discovery and inference algorithms. CSuite was originally introduced in [this paper](https://arxiv.org/pdf/2202.02195.pdf). The [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py) is publicly available.\n\n## Versioning\nCSuite datasets are versioned so that we can amend and add datasets, whilst ensuring backwards compatibility with older versions of the data. Full reproducibility with CSuite requires specifying the correct version.\n\n## Summary of datasets\n\nThe download URLs here are for the latest version.\n\n|  Dataset | No. nodes  | No. edges  | Additive noise model?  |  Discrete/continuous | ATE benchmarking |  CATE benchmarking | Download link |\n| :------------ | :------------ | :------------ | :------------ | :------------ |:------------ |:------------ |:------------ |\n| lingauss  | 2  | 1  | Y  | Continuous  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_lingauss.zip |\n| linexp  | 2  | 1 | Y  | Continuous  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_linexp.zip |\n| nonlingauss  | 2  | 1  | Y  | Continuous  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_nonlingauss.zip |\n| nonlin_simpson  | 4  | 4  | Y  | Continuous  | Y | Y | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_nonlin_simpson.zip |\n| symprod_simpson  | 4  |  4 | Y   |  Continuous | Y | Y | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_symprod_simpson.zip |\n| large_backdoor  | 9  | 10  | Y  | Continuous  | Y | Y | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_large_backdoor.zip |\n| weak_arrows  | 9  | 15  | Y  |  Continuous  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_weak_arrows.zip |\n| cat_to_cts  | 2  | 1  | N  | Mixed  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_cat_to_cts.zip |\n| cts_to_cat  | 2  | 1  | N  | Mixed  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_cts_to_cat.zip |\n| mixed_simpson  | 4  |4  | N  | Mixed  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_mixed_simpson.zip |\n| large_backdoor_binary_t  | 9  | 10  | N  | Mixed  | Y | Y | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_large_backdoor_binary_t.zip |\n| weak_arrows_binary_t  | 9  | 15  | N  | Mixed  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_weak_arrows_binary_t.zip |\n| mixed_confounding  | 12  | 15  | N  | Mixed  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_mixed_confounding.zip |\n| cat_chain  | 3  | 2  | N  | Discrete  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_cat_chain.zip |\n| cat_collider  | 3  | 2  | N  | Discrete  | Y | N | https://github.com/microsoft/csuite/releases/download/v0.1/csuite_cat_collider.zip |\n\n\n## Data format\n\nEach dataset consists of the following files\n - `adj_matrix.csv`, which describes the causal graph used to generate the data; a value `1` in row `i`, column `j` indicates an edge from node `i` to node `j`;\n - `train.csv`, the observational training data;\n - `test.csv`, the observational test data;\n - `interventions.json`, a JSON containing interventional test data.\n\nThe interventional data JSON consists of *pairs* of interventional environments, which can be used to estimate (C)ATE. The two environments are the 'primary' and 'reference' environments. Conditional data was generating using HMC. The format of the interventional data is\n\n```\n{\n    \"environments\": [\n        {\n            \"conditioning_idxs\": <optional list containing indices of nodes to that were conditioned on>,\n            \"conditioning_values\": <list of values set on the conditioning nodes>,\n            \"effect_idxs\": <list containing indices of nodes to be considered effect variables>,\n            \"intervention_idxs\": <list of indices of nodes that were acted on with do-intervention>,\n            \"intervention_values\": <list of values set on the intervention nodes in the primary do-intervention: for example, receiving a medicine>,\n            \"intervention_reference\": <list of values set on the intervention nodes in the reference do-intervention: for example, not receiving the medicine>,\n            \"test_data\": <array of data from the primary do-intervention, same number of columns as train.csv>,\n            \"reference_data\": <array of data from the reference do-intervention>\n        },\n        ...\n    ],\n    \"metadata\": {\n        \"columns_to_nodes\": <matches to columns to their corresponding nodes, only important for vector-values nodes>\n    }\n}\n```\n\n## Download\n\n### From the terminal\n\nYou can download CSuite datasets from any previous version using the following URL pattern\n```\n$ curl -O https://github.com/microsoft/csuite/releases/download/v<version>/csuite_<name>.zip\n```\nwhere `<name>` and `<version>` should be set appropriately.\n\n### From Python\n\nThe uncompressed files listed under [Data format](#data-format) are also directly available from a public storage account. These may either be accessed through their HTTP links, e.g. https://azuastoragepublic.blob.core.windows.net/datasets/csuite_linexp/train.csv or their equivalent Azure blob storage paths. To load these directly in python:\n\n```python\nimport pandas as pd\n\n# Load over HTTP\ndf = pd.read_csv(\"https://azuastoragepublic.blob.core.windows.net/datasets/csuite_linexp/train.csv\")\n\n# Load using `adlfs` (`pip install adlfs`)\ndf = pd.read_csv(\"az://datasets@azuastoragepublic.blob.core.windows.net/csuite_linexp/train.csv\")\n```\n\n## Citation\nIf you use CSuite datasets in your work, please cite the following [paper](https://arxiv.org/pdf/2202.02195.pdf) which originally introduced these datasets\n```\n@article{geffner2022deep,\n    title={Deep End-to-end Causal Inference},\n    author={Geffner, Tomas and Antoran, Javier and Foster, Adam and Gong, Wenbo and Ma, Chao and Kiciman, Emre and Sharma, Amit and Lamb, Angus and Kukla, Martin and Pawlowski, Nick and  Allamanis, Miltiadis and Zhang, Cheng},\n    journal={arXiv preprint arXiv:2202.02195},\n    year={2022}\n}\n```\n\n## Detailed descriptions of datasets\n\n### lingauss\n\n<img src=\"figures/two_node.PNG\" alt=\"Two Node Graph X0 -> X1\" width=\"250px\" />\n\nA two node linear Gaussian system. The structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim N(0, 1) \\\\\nX_1 &= \\frac{1}{2}X_0 + \\frac{\\sqrt{3}}{2}Z_1\n\\end{align}\n$$\n\nwhere $Z_1 \\sim N(0,1)$ is independent of $X_0$. The dataset is constructed so that the observational distribution is the same if $X_0$ and $X_1$ are swapped and both nodes have the same marginal variance of 1. This model is not structural identifiable from observational data.\n\n\n\n### linexp\n\n<img src=\"figures/two_node.PNG\" alt=\"Two Node Graph X0 -> X1\" width=\"250px\" />\n\nA two node linear system with exponentially distributed noise. The structural equations are\n\n$$\n\\begin{align}\nX_0 &= Z_0 - 1 \\\\\nX_1 &= \\frac{1}{2}X_0 + \\frac{\\sqrt{3}}{2}(Z_1-1)\n\\end{align}\n$$\n\nwhere $Z_0, Z_1 \\sim \\textup{Exp}(1)$ are independent variables. The dataset is constructed so that both nodes have the same marginal variance of 1. This model is structural identifiable given a non-Gaussian additive noise assumption.\n\n\n### nonlingauss\n\n<img src=\"figures/two_node.PNG\" alt=\"Two Node Graph X0 -> X1\" width=\"250px\" />\n\nA two node non-linear system with Gaussian distributed noise. The structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim N(0,1) \\\\\nX_1 &=  \\sqrt{6} \\exp(-X_0^2) + \\alpha Z_1\n\\end{align}\n$$\n\nwhere $Z_1 \\sim N(0,1)$ is independent of $X_0$ and \n\n$$\n\\alpha^2 = 1 - 6 \\left(\\frac{1}{\\sqrt{5}} - \\frac{1}{3} \\right).\n$$\n\nThe dataset is constructed so that $\\textup{Var}(X_0) = \\textup{Var}(X_1) = 1$ and $\\textup{Cov}(X_0,X_1)=0$. This model is structural identifiable given a nonlinear additive noise assumption.\n\n### nonlin_simpson\n\n<img src=\"figures/nonlin_simpson.PNG\" alt=\"Four Node Graph X0 -> X1, X0 -> X2, X1 -> X2, X2 -> X3\" width=\"300px\" />\n\nAn example of [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson's_paradox) using a continuous SEM. The dataset is constructed so that $\\textup{Cov}(X_1,X_2)$ has the opposite sign to $\\textup{Cov}(X_1,X_2\\mid X_0)$. Estimating the treatment effects correctly in this SEM is highly sensitive to accurate causal discovery.\n\nThe structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim N(0,1) \\\\\nX_1 &= s(1 - X_0) + \\sqrt{\\frac{3}{20}} Z_1\\\\\nX_2 &= \\tanh(2X_1) + \\frac{3}{2}X_0 -1 + \\tanh(Z_2)\\\\\nX_3 &= 5 \\tanh\\left(\\frac{X_2 - 4}{5}\\right) + 3 + \\frac{1}{\\sqrt{10}} Z_3\n\\end{align}\n$$\n\nwhere $Z_1,Z_2 \\sim N(0,1)$ and $Z_3 \\sim \\textup{Laplace}(1)$ are mutually independent and independent of $X_0$, $s(x) = \\log(1+\\exp(x))$ is the softplus function. Constants were chosen so that each variable has a marginal variance of (approximately) 1.  \n\n### symprod_simpson\n\n<img src=\"figures/symprod_simpson.PNG\" alt=\"Four Node Graph X0 -> X1, X0 -> X2, X1 -> X2, X2 -> X3\" width=\"300px\" />\n\nA dataset exhibiting multi-modality that is suitable for benchmarking CATE estimation. Nonlinear function estimation is important since $\\textup{Cov}(X_0,X_2)=\\textup{Cov}(X_1,X_2)=0$.\n\nThe structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim N(0,1) \\\\\nX_1 &= 2\\tanh(2X_0) + \\frac{1}{\\sqrt{10}} Z_1\\\\\nX_2 &= \\frac{1}{2}X_0 X_1 + \\frac{1}{\\sqrt{2}} Z_2\\\\\nX_3 &= \\tanh\\left(\\frac{3}{2}  X_0\\right) + \\sqrt{\\frac{3}{10}} Z_3\n\\end{align}\n$$\n\nwhere $Z_1 \\sim t_3,Z_2 \\sim \\textup{Laplace}(1)$ and $Z_3 \\sim N(0,1)$ are mutually independent and independent of $X_0$. Constants were chosen so that each variable has a marginal variance of (approximately) 1.  \n\n### large_backdoor\n\n<img src=\"figures/large_backdoor.PNG\" alt=\"Large backdoor graph\" width=\"400px\" />\n\nA larger dataset with a pyramidal graph structure. This dataset is constructed so that there are many possible choices of backdoor adjustment set for estimating the treatment effect of $X_7$ on $X_8$. While both minimal and maximal adjustment sets can result in a correct solution, the a minimal adjustment set results in a much lower-dimensional adjustment problem and thus will result in lower variance solutions.\n\nA complete description of the structural equations can be found in the [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py).\n\n### weak_arrows\n\n<img src=\"figures/weak_arrows.PNG\" alt=\"Weak arrows graph\" width=\"400px\" />\n\nA larger dataset that is similar to `large_backdoor`, but with many additional edges. The causal discovery challenge revolves\naround finding all arrows, which are scaled to be relatively weak, but which have significant predictive power for $X_8$ in aggregate.\n\nA complete description of the structural equations can be found in the [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py).\n\n\n### cat_to_cts\n\n<img src=\"figures/two_node.PNG\" alt=\"Two Node Graph X0 -> X1\" width=\"250px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Discrete on $\\\\{0,1,2\\\\}$ |\n|  $X_1$ | Continuous  |\n\n\nA two node system with one categorical and one continuous variable. The structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim \\text{Cat}\\left(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}\\right)\\\\\nX_1 &= \\frac{1}{2}(X_0-1) + \\frac{9}{25}\\mathbf{1}_{\\\\{X_0=2\\\\}} + \\frac{8}{5}(s(Z_1) - 1)\n\\end{align}\n$$\n\nwhere $s(x) = \\log(1+\\exp(x))$ is the softplus function, and $Z_1 \\sim N(0,1)$ is independent of $X_0$. \n\n### cts_to_cat\n\n<img src=\"figures/two_node.PNG\" alt=\"Two Node Graph X0 -> X1\" width=\"250px\" />\n\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Continuous |\n|  $X_1$ | Discrete on $\\\\{0,1,2\\\\}$  |\n\nA two node system with one categorical and one continuous variable. The structural equations are\n\n$$\n\\begin{align}\n    X_0 &\\sim U(-\\sqrt{3},\\sqrt{3})\\\\\n    p(X_1|X_0=x) &= \\begin{cases}\n    \\left(\\tfrac{6}{13},\\tfrac{6}{13},\\tfrac{1}{13} \\right) & \\text{ if } x < -\\tfrac{\\sqrt{3}}{3} \\\\\n    \\left(\\tfrac{1}{8},\\tfrac{3}{4},\\tfrac{1}{8} \\right) & \\text{ if } -\\tfrac{\\sqrt{3}}{3} \\le x < \\tfrac{\\sqrt{3}}{3} \\\\\n    \\left(\\tfrac{1}{3},\\tfrac{1}{3},\\tfrac{1}{3} \\right) & \\text{ if } x > \\tfrac{\\sqrt{3}}{3} \\\\\n    \\end{cases}\n\\end{align}\n$$\n\n### mixed_simpson\n\n<img src=\"figures/nonlin_simpson2.PNG\" alt=\"Four Node Graph X2 -> X0, X2 -> X1, X0 -> X1, X1 -> X3\" width=\"300px\" />\n\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Discrete on $\\\\{0,1\\\\}$  |\n|  $X_1$ | Continuous  |\n|  $X_2$ | Discrete on $\\\\{0,1,2,3,4,5\\\\}$ |\n|  $X_3$ | Continuous  |\n\n\nAnother example of [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson's_paradox) using a mixed-type SEM. The dataset is constructed so that $\\textup{Cov}(X_0,X_1)$ has the opposite sign to $\\textup{Cov}(X_0,X_1\\mid X_2)$. Estimating the treatment effects correctly in this SEM is highly sensitive to accurate causal discovery.\n\nThe structural equations are\n\n$$\n\\begin{align}\nX_2 &\\sim \\text{Cat}\\left(\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6},\\frac{1}{6}\\right) \\\\\np(X_0|X_2=x)  &= \\begin{cases}\n    \\left(\\tfrac{1}{12},\\tfrac{11}{12} \\right) & \\text{ if } x < 3 \\\\\n    \\left(\\tfrac{11}{12},\\tfrac{1}{12} \\right) & \\text{ if } x \\ge 3 \\\\\n    \\end{cases} \\\\\nX_1 &= \\frac{7}{10}\\left(X_0 + X_2 - 4\\right) + s\\left(\\frac{1}{2}  Z_1 \\right) \\\\\nX_3 &= \\frac{10}{3} \\tanh\\left(\\frac{X_1}{3}\\right) + \\frac{1}{10}(Z_3 -1)\n\\end{align}\n$$\n\nwhere $Z_1 \\sim N(0,1),Z_3\\sim \\textup{Exp}(1)$ are independent noise random variables and $s(x)=\\log(1+\\exp(x))$.\n\n### large_backdoor_binary_t\n\n<img src=\"figures/large_backdoor.PNG\" alt=\"Large backdoor graph\" width=\"400px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Continuous  |\n|  $X_1$ | Continuous  |\n|  $X_2$ | Continuous  |\n|  $X_3$ | Continuous  |\n|  $X_4$ | Continuous  |\n|  $X_5$ | Continuous  |\n|  $X_6$ | Continuous  |\n|  $X_7$ | Discrete on $\\\\{0,1\\\\}$ |\n|  $X_8$ | Continuous  |\n\nAn adaptation of `large_backdoor` with a binary variable $X_7$ which is considered the treatment variable.\n\nA complete description of the structural equations can be found in the [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py).\n\n### weak_arrow_binary_t\n\n<img src=\"figures/weak_arrows.PNG\" alt=\"Weak arrows graph\" width=\"400px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Continuous  |\n|  $X_1$ | Continuous  |\n|  $X_2$ | Continuous  |\n|  $X_3$ | Continuous  |\n|  $X_4$ | Continuous  |\n|  $X_5$ | Continuous  |\n|  $X_6$ | Continuous  |\n|  $X_7$ | Discrete on $\\\\{0,1\\\\}$ |\n|  $X_8$ | Continuous  |\n\nAn adaptation of `weak_arrows` with a binary variable $X_7$ which is considered the treatment variable.\n\nA complete description of the structural equations can be found in the [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py).\n\n### mixed_confounding\n\n<img src=\"figures/mixed_confounding.PNG\" alt=\"Mixed confounding graph\" width=\"400px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Discrete on $\\\\{0,1\\\\}$  |\n|  $X_1$ | Continuous  |\n|  $X_2$ | Continuous  |\n|  $X_3$ | Continuous  |\n|  $X_4$ | Discrete on $\\\\{0,1\\\\}$  |\n|  $X_5$ | Discrete on $\\\\{0,1,2\\\\}$  |\n|  $X_6$ | Continuous  |\n|  $X_7$ | Discrete on $\\\\{0,1,2\\\\}$ |\n|  $X_8$ | Continuous  |\n|  $X_9$ | Continuous  |\n|  $X_{10}$ | Continuous  |\n|  $X_{11}$ | Continuous  |\n\n\n\nA larger dataset with treatment node $X_0$ and outcome node $X_1$. There are different variables that are: confounders, causes of $X_0$ only, causes of $X_1$ only, downstream of $X_0$, downstream of $X_1$, collider caused by $X_0$ and $X_1$.\n\nA complete description of the structural equations can be found in the [data generation code for CSuite](https://github.com/microsoft/causica/blob/b3c79a01f30f44ed36c582ffe2b4522058d82a73/causica/data_generation/csuite/simulate.py).\n\n\n### cat_chain\n\n<img src=\"figures/chain.PNG\" alt=\"Chain graph X0->X1->X2\" width=\"400px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Discrete on $\\\\{0,1,2\\\\}$  |\n|  $X_1$ | Discrete on $\\\\{0,1,2\\\\}$  |\n|  $X_2$ | Discrete on $\\\\{0,1\\\\}$ |\n\nA chain graph with discrete variables. The structural equations are \n\n$$\n\\begin{align}\nX_0 &\\sim \\text{Cat}\\left(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}\\right)\\\\\np(X_1|X_0=x) &= \\begin{cases}\n    \\left(\\tfrac{3}{4},\\tfrac{1}{8},\\tfrac{1}{8} \\right) & \\text{ if } x=0 \\\\\n    \\left(\\tfrac{1}{8},\\tfrac{3}{4},\\tfrac{1}{8} \\right) & \\text{ if } x=1 \\\\\n    \\left(\\tfrac{1}{8},\\tfrac{1}{8},\\tfrac{3}{4} \\right) & \\text{ if } x=2 \\\\\n    \\end{cases} \\\\\np(X_2|X_1=y) &= \\begin{cases}\n    \\left(\\tfrac{6}{7},\\tfrac{1}{7} \\right) & \\text{ if } y=0 \\\\\n    \\left(\\tfrac{6}{7},\\tfrac{1}{7} \\right) & \\text{ if } y=1 \\\\\n    \\left(\\tfrac{1}{7},\\tfrac{6}{7} \\right) & \\text{ if } y=2. \\\\\n    \\end{cases} \\\\\n\\end{align}\n$$\n\n\n### cat_collider\n\n<img src=\"figures/collider.PNG\" alt=\"Collider graph X0->X1<-X2\" width=\"400px\" />\n\n| Variable  | Discrete/continuous |\n| ------------ | ------------ |\n|  $X_0$ | Discrete on $\\\\{0,1,2\\\\}$  |\n|  $X_1$ | Discrete on $\\\\{0,1,2\\\\}$  |\n|  $X_2$ | Discrete on $\\\\{0,1\\\\}$ |\n\nA collider graph with discrete variables. The structural equations are\n\n$$\n\\begin{align}\nX_0 &\\sim \\text{Cat}\\left(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}\\right)\\\\\nX_2 &\\sim \\text{Cat}\\left(\\frac{1}{2}, \\frac{1}{2}\\right) \\\\\np(X_1|X_0=x,X_1=y) &= \\begin{cases}\n    \\left(\\tfrac{11}{13},\\tfrac{1}{13},\\tfrac{1}{13} \\right) & \\text{ if } x=0,y=0 \\\\\n    \\left(\\tfrac{1}{13},\\tfrac{11}{13},\\tfrac{1}{13} \\right) & \\text{ if } x=1,y=0 \\\\\n    \\left(\\tfrac{1}{13},\\tfrac{1}{13},\\tfrac{11}{13} \\right) & \\text{ if } x=2,y=0 \\\\\n    \\left(\\tfrac{31}{43},\\tfrac{11}{43},\\tfrac{1}{43} \\right) & \\text{ if } x=0,y=1 \\\\\n    \\left(\\tfrac{21}{43},\\tfrac{21}{43},\\tfrac{1}{43} \\right) & \\text{ if } x=1,y=1 \\\\\n    \\left(\\tfrac{21}{43},\\tfrac{11}{43},\\tfrac{11}{43} \\right) & \\text{ if } x=2,y=1. \\\\\n    \\end{cases}\n\\end{align}\n$$\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.\n\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n", "repo_name": "csuite", "org_name": "microsoft", "org_repo": "microsoft/csuite", "platform_org_repo": "github+microsoft/csuite", "link_to_repo": "https://github.com/microsoft/csuite", "platform": "github", "language": "TeX", "stargazers_count": 35, "watchers_count": 35}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "DataSecOps", "org_name": "microsoft", "org_repo": "microsoft/DataSecOps", "platform_org_repo": "github+microsoft/DataSecOps", "link_to_repo": "https://github.com/microsoft/DataSecOps", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# WavText5K\nThis repository contains WavText5K crawl from [Audio Retrieval with WavText5K and CLAP Training](https://arxiv.org/abs/2209.14275). The repository provides .csv containing metadata like descriptions, titles, tags and python script to download and resample the audio files. \n\n## Setup\n- The setup assumes [Anaconda](https://www.anaconda.com) is installed\n- Open the anaconda terminal and follow the below commands. The symbol `{..}` indicates user input. \n```shell\n> git clone https://github.com/microsoft/WavText5K.git\n> cd WavText5K\n> conda create -n wavtext python=3.8\n> conda activate wavtext\n> pip install -r requirements.txt\n> python process.py --csv_path WavText5K.csv --save_folder_path Webcrawl --resample_rate {sampling rate} --processes {no. of process}\n```\n\n## Citation\n```\n@article{deshmukh2022audio,\n  title={Audio Retrieval with WavText5K and CLAP Training},\n  author={Deshmukh, Soham and Elizalde, Benjamin and Wang, Huaming},\n  journal={arXiv preprint arXiv:2209.14275},\n  year={2022}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "WavText5K", "org_name": "microsoft", "org_repo": "microsoft/WavText5K", "platform_org_repo": "github+microsoft/WavText5K", "link_to_repo": "https://github.com/microsoft/WavText5K", "platform": "github", "language": "Python", "stargazers_count": 35, "watchers_count": 35}, {"README_text": "# UICaption dataset\n\nWe release the UICaption dataset consisting of UI images paired with descriptions of their functionality. This dataset was used to train Lexi, a pre-trained vision and language model for UI language understanding. The dataset and model are presented in \"Lexi: Self-Supervised Learning of the UI Language\" by Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta Baral, and Oriana Riva.\n\nUICaption is released as a workflow by which you can generate the actual data. \n\nIf you used this dataset please cite the following paper:\n\n``` bibtex\n@inproceedings{oriva:lexi22,\n  title = {Lexi: Self-Supervised Learning of the UI Language},\n  author = {Pratyay Banerjee and Shweti Mahajan and Kushal Arora and Chitta Baral and Oriana Riva},\n  publisher = {Association for Computational Linguistics},\n  booktitle = {Proc. of the 2022 Conference on Empirical Methods in Natural Language Processing},\n  year = {2022},\n  month = {December}\n}\n```\n\n# Generate UICaption dataset\n\n### Crawl images and texts from support websites\n\nUse the compiled list of support and how-to websites provided in `tech_urls.txt` to extract UI images and associated descriptions from the web. Save the output to a folder, e.g., `crawled_uidata`:\n\n```\npython crawl_uidata.py -i tech_urls.txt -o crawled_uidata\n```\nThis script will generate three files stored in the specified output folder: `ui_images.p` which contains URLs of the UI images, `ui_alt_texts.csv` which contains alt-text associated with each UI image, `ui_instructions_preceding.csv` and `ui_instructions_succeeding.csv` which contains texts appearing before and after the image occurence in the webpage respectively.\n\nThen download the UI images:\n```\npython download_images.py -i crawled_uidata/ui_images.txt\n```\n\n### Generate image-text pairs\n\nFinally, use the crawled UI data to assemble the UICaption dataset:\n```\npython gen_uicaption_dataset.py -i crawled_uidata -o ui_caption_dataset.json\n```\n\nThe same UI image may appear in multiple websites, hence the script associates one or multiple alt-text descriptions and instructions with each UI image. The script produces a json file with the following format:\n\n|Name|Description|\n|----|-----------|\n|image_path| path at which the UI image is stored|\n|alt_text_list| one or multiple alt-texts associated with the UI image|\n|instruction_list| one or more instructions associated with the UI image|\n\n\n## Disclaimer\n\nThe code and dataset in this repository are intended to be used for research purposes. Microsoft takes no responsibility for what users use this tool for or for any damages caused from using this code. By downloading and using this software, you agree that you take full responsibility for any damages and liability.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "UICaption", "org_name": "microsoft", "org_repo": "microsoft/UICaption", "platform_org_repo": "github+microsoft/UICaption", "link_to_repo": "https://github.com/microsoft/UICaption", "platform": "github", "language": "Python", "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# Threat Matrix for Kubernetes\n\nMicrosoft Defender for Cloud threat matrix for Kubernetes contains attack tactics, techniques and mitigations relevant for Kubernetes environment. \n\nThe threat matrix is best viewed online via: [http://aka.ms/KubernetesThreatMatrix](http://aka.ms/KubernetesThreatMatrix)\n\n[![build doc pages](https://github.com/microsoft/Threat-Matrix-for-Kubernetes/actions/workflows/main.yml/badge.svg)](https://github.com/microsoft/Threat-Matrix-for-Kubernetes/actions/workflows/main.yml)\n[![pages-build-deployment](https://github.com/microsoft/Threat-Matrix-for-Kubernetes/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/Threat-Matrix-for-Kubernetes/actions/workflows/pages/pages-build-deployment)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Threat-Matrix-for-Kubernetes", "org_name": "microsoft", "org_repo": "microsoft/Threat-Matrix-for-Kubernetes", "platform_org_repo": "github+microsoft/Threat-Matrix-for-Kubernetes", "link_to_repo": "https://github.com/microsoft/Threat-Matrix-for-Kubernetes", "platform": "github", "language": "Batchfile", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Microsoft System Center - Service Manager Support Scripts\n\n<a href=\"https://microsoft.github.io/CSS-SystemCenter-ServiceManager/\">\n  <img src=\"/docs/SCSM.png\" width=\"100\" />\n</a>\n\nThis repository is the home of several scripts for Microsoft System Center - Service Manager (SCSM).\nThe scripts are intended for identifying and resolving a wide range of issues regarding SCSM.\n\nFor more information, see the documentation of individual scripts: \n\nhttps://microsoft.github.io/CSS-SystemCenter-ServiceManager/\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nSee the documentation of individual scripts to learn how to contribute:\nhttps://microsoft.github.io/CSS-SystemCenter-ServiceManager/\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CSS-SystemCenter-ServiceManager", "org_name": "microsoft", "org_repo": "microsoft/CSS-SystemCenter-ServiceManager", "platform_org_repo": "github+microsoft/CSS-SystemCenter-ServiceManager", "link_to_repo": "https://github.com/microsoft/CSS-SystemCenter-ServiceManager", "platform": "github", "language": "PowerShell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Microsoft Build of OpenJDK\n\n## Eclipse Adoptium Marketplace Metadata\n\nThis repository contains metadata used for listing Microsoft Build of OpenJDK binaries on the Eclipse Adoptium marketplace.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "openjdk-adoptium-marketplace-data", "org_name": "microsoft", "org_repo": "microsoft/openjdk-adoptium-marketplace-data", "platform_org_repo": "github+microsoft/openjdk-adoptium-marketplace-data", "link_to_repo": "https://github.com/microsoft/openjdk-adoptium-marketplace-data", "platform": "github", "language": "Shell", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Conversational Speaker\n## Now with [Semantic Kernel](https://aka.ms/skrepo) and [ChatGPT](https://openai.com/blog/chatgpt)!\nThe Conversational Speaker, a.k.a. \"Friend Bot\", uses a Raspberry Pi (or desktop) to enable spoken conversation with OpenAI large language models. This implementation listens to speech, processes the conversation through the OpenAI service, and responds back.\n\nThis project is written in .NET 6 which supports Linux/Raspbian, macOS, and Windows.\n\n![Conversational Speaker](/package.png \"Conversational Speaker\")\n\n**Read time**: 15 minutes\n\n**Build time**: 30 minutes\n\n**Cost**: \n - Hardware\n   - $ for [Raspberry PI 4 Model B](https://aka.ms/maker/rpi/four)\n   - $ for [USB Omnidirectional Speakerphone](https://aka.ms/maker/usbspeakerphone)\n   - $ for an SD card (to setup the Raspberry Pi OS)\n- Software\n  - [Azure Cognitive Speech Services](https://aka.ms/friendbot/azurecog)\n    - **Free tier**: 5 audio hours per month and 1 concurrent request. \n    - **Free $200 credit**: With a new Azure account that can be used during the first 30 days.\n  - [OpenAI](https://aka.ms/maker/openai/pricing)\n    - **$0.002 / 1K tokens / ~750 words**: ChatGPT (gpt-3.5-turbo)\n    - **Free $18 credit**: With a new OpenAI account that can be used during your first 90 days.\n    \n# Setup\nYou will need an instance of Azure Cognitive Services and an OpenAI account. \nYou can run the software on nearly any platform, but let's start with a Raspberry Pi.\n\nYou may also use an instance of Azure OpenAI in place of OpenAI as well. \n\n\n## Raspberry Pi\n_If you are new to Raspberry Pis, check out this [getting started](https://aka.ms/maker/rpi/gettingstarted) guide._\n### 1. OS\n1. Insert an SD card into your PC.\n1. Go to https://aka.ms/maker/rpi/software then download and run the Raspberry Pi Imager. \n1. Click `Choose OS` and select the default Raspberry Pi OS (32-bit).\n1. Click `Choose Storage`, select the SD card.\n1. Click `Write` and wait for the imaging to complete.\n1. Put the SD card into your Raspberry Pi and connect a keyboard, mouse, and monitor.\n1. Complete the initial setup, making sure to configure Wi-Fi.\n\n### 2. USB Speaker/Microphone\n1. Plug in the USB speaker/microphone if you have not already.\n1. On the Raspberry PI OS desktop, right-click on the volume icon in the top-right of the screen and make sure the USB device is selected.\n1. Right-click on the microphone icon in the top-right of the screen and make sure the USB device is selected.\n\n## Azure\nThe conversational speaker uses Azure Cognitive Service for speech-to-text and text-to-speech. Below are the steps to create an Azure account and an instance of Azure Cognitive Services.\n### 1. Azure Account\n  1. In a web browser, navigate to https://aka.ms/friendbot/azure and click on `Try Azure for Free`.\n  1. Click on `Start Free` to start creating a free Azure account.\n  1. Sign in with your Microsoft or GitHub account.\n  1. After signing in, you will be prompted to enter some information.\n        > NOTE: Even though this is a free account, Azure still requires credit card information. You will not be charged unless you change settings later.\n  1. After your account setup is complete, navigate to https://aka.ms/friendbot/azureportal.\n\n### 2. Azure Cognitive Services\n  1. Sign into your account at https://aka.ms/friendbot/azureportal.\n  1. In the search bar at the top, enter `Cognitive Services`. Under `Marketplace` select `Cognitive Services`. (It may take a few seconds to populate.)\n  1. Verify the correct subscription is selected. Under `Resource Group` select `Create New`. Enter a resource group name (e.g. `conv-speak-rg`).\n  1. Select a region and a name for your instance of Azure Cognitive Services (e.g. `my-conv-speak-cog-001`). \n        > NOTE: EastUS, WestEurope, or SoutheastAsia are recommended, as those regions tend to support the greatest number of features.  \n  1. Click on `Review + Create`. After validation passes, click `Create`.\n  1. When deployment has completed you can click `Go to resource` to view your Azure Cognitive Services resource.\n  1. On the left side navigation bar, under `Resourse Management`, select `Keys and Endpoint`.\n  1. Copy either of the two Cognitive Services keys. Save this key in a secure location for later.\n\n  > Windows 11 users: If the application is stalling when calling the text-to-speech API, make sure you have applied all current security updates ([link](https://learn.microsoft.com/en-us/windows/release-health/resolved-issues-windows-11-22h2#2924msgdesc)).\n\n## OpenAI\nThe conversational speaker uses OpenAI's models to hold a friendly conversation. Below are the steps to create a new account and access the AI models.\n### 1. OpenAI Account\n  1. In a web browser, navigate to https://aka.ms/maker/openai. Click `Sign up`.\n        > NOTE: can use a Google account, Microsoft account, or email to create a new account.\n  1. Complete the sign-up process (e.g., create a password, verify your email, etc.).\n        > NOTE: If you are new to OpenAI, please review the usage guidelines (https://beta.openai.com/docs/usage-guidelines).\n  1. In the top-right corner click on your account. Click on `View API keys`.\n  1. Click `+ Create new secret key`. Copy the generated key and save it in a secure location for later.\n\n  _If you are curious to play with the large language models directly, check out the https://platform.openai.com/playground?mode=chat at the top of the page after logging in to https://aka.ms/maker/openai._\n\n# The Code\n## 1. Code Configuration\n1. On the Raspberry Pi or your PC, open a command-line terminal.\n1. Install .NET 6 SDK.\n   - For Raspberry Pi and Linux:\n     ```bash\n     curl -sSL https://dot.net/v1/dotnet-install.sh | bash /dev/stdin --channel 6.0\n     ``` \n     After installation is complete (it may take a few minutes), add dotnet to the command search paths.\n     ```bash\n     echo 'export DOTNET_ROOT=$HOME/.dotnet' >> ~/.bashrc\n     echo 'export PATH=$PATH:$HOME/.dotnet' >> ~/.bashrc\n     source ~/.bashrc\n     ```\n     Verify dotnet was installed successfully by checking the version.\n     ```bash\n     dotnet --version\n     ```\n   - For Windows, go to https://aka.ms/maker/dotnet/download, under .NET 6.0 click `Download .NET SDK x64`, and run the installer.\n1. Clone the repo.\n   ```bash\n   git clone https://github.com/microsoft/conversational-speaker.git\n   ```\n1. Set your API keys: Replace `{MyCognitiveServicesKey}` with your Azure Cognitive Services key and `{MyOpenAIKey}` with your OpenAI API key from the sections above.\n   ```bash\n   cd ~/conversational-speaker/src/ConversationalSpeaker\n   dotnet user-secrets set \"AzureCognitiveServices:Key\" \"{MyCognitiveServicesKey}\"\n   dotnet user-secrets set \"AzureCognitiveServices:Region\" \"{MyCognitiveServicesRegion (e.g., EastUS)}\"\n   dotnet user-secrets set \"OpenAI:Key\" \"{MyOpenAIKey}\"\n   ```\n1. Build and run the code!\n   ```bash\n   cd ~/conversational-speaker/src/ConversationalSpeaker\n   dotnet build\n   dotnet run\n   ```\n\n## 2. (Optional) Application Setup on Boot\nThere are several ways to run a program when the Raspberry Pi boots. Below is a suggested method which runs the application in a visible terminal window automatically. This allows you to not only see the output but also cancel the application by clicking on the terminal window and pressing CTRL+C. \n1. Create a file `/etc/xdg/autostart/friendbot.desktop`\n   ```bash\n   sudo nano /etc/xdg/autostart/friendbot.desktop\n   ```\n1. Put the following content into the file.\n   ```bash\n   [Desktop Entry]\n   Exec=lxterminal --command \"/bin/bash -c '~/.dotnet/dotnet run --project ~/conversational-speaker/src/ConversationalSpeaker; /bin/bash'\"\n   ```\n   Press CTRL+O to save the file and CTRL+X to exit. This will run the application in a terminal window after the Raspberry Pi has finished booting.\n1. To test out the changes by rebooting. \n   ```bash\n   reboot\n   ```\n\n## 3. (Optional) Create a custom wake phrase\nThe code base has a default wake phrase (`\"Hey, Computer.\"`) already, which I suggest you use first. If you want to create your own (free!) custom wake word, then follow the steps below.\n  1. Create a custom keyword model using the directions here: https://aka.ms/hackster/microsoft/wakeword. \n  1. Download the model, extract the `.table` file and copy it to `src/ConversationalSpeaker/Handlers/WakePhrases`.\n  1. Update `ConversationalSpekaer.csproj` file to include your wake phrase file in the build.\n     ```xml\n     <ItemGroup>\n       <None Update=\"Handlers\\WakePhrases\\{YOUR FILE}.table\">\n         <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>\n       </None>\n     </ItemGroup>\n     ```\n  1. Rebuild and run the project to use your custom wake word.\n\n## 4. Usage\n- To start a new conversation, say \"Start a new conversation\". \n- To set context, the following phrase is recommended: \"Hello, my name is \\<your name\\> and I live in \\<your location\\>.\"\n- Continue conversing!\n- For more usage settings, view `~/conversational-speaker/src/ConversationalSpeaker/configuration.json`. \n  - Change the AI's name (`PromptEngine:OutputPrefix`), \n  - Change the AI's voice (`AzureCognitiveServices:SpeechSynthesisVoiceName`)\n  - Change the AI's personality (`PromptEngine:Description`)\n  - Switch to text input by changing the `System:TextListener` to `true` (good for testing changes).\n- Take a look at Azure Cognitive Service's [style support page](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=stt-tts#voice-styles-and-roles) to see which languages support which emotional styles, then play with the `AzureCognitiveServices:SpeechSynthesisVoiceName` and `PromptEngine` settings in `src/ConverstationalSpeaker/configuration.json`.\n\n# How it works\n## Primary logic\nThis application uses .NET's generic \"HostBuilder\" paradigm. The HostBuilder encapsulates handling dependencies (i.e., dependency injection), configuration, logging, and running a set of hosted services. In this example, there is only one hosted service, `ConversationLoopHostedService`, which contains the primary logic loop.\n```C#\n// ConversationLoopHostedService.cs\nwhile (!cancellationToken.IsCancellationRequested)\n{     \n      // Listen to the user.\n      string userMessage = await _listener.ListenAsync(cancellationToken);\n      // Run the message through the AI and get a response.\n      string response = await _conversationHandler.ProcessAsync(userMessage, cancellationToken);\n      // Speak the response.\n      await _speaker.SpeakAsync(response, cancellationToken);\n}\n```\n\n## Wake Word or Phrase\nAzure Cognitive Service's has an excellent (and free!) wake word support. After generating a keyword model (see \"Create a custom wake word\" above), we load it into the speech SDK and wait for the system to recognize the keyword.\n```C#\n// AzCognitiveServicesWakeWordListener.cs\n_keywordModel = KeywordRecognitionModel.FromFile(keywordModelPath);\n_audioConfig = AudioConfig.FromDefaultMicrophoneInput();\n_keywordRecognizer = new KeywordRecognizer(_audioConfig);\ndo\n{\n   result = await _keywordRecognizer.RecognizeOnceAsync(_keywordModel);\n} while (result.Reason != ResultReason.RecognizedKeyword);\n```\n\n## Listening\nTo listen to the user, the application leverages Azure Cognitive Service's [speech-to-text feature](https://aka.ms/friendbot/speech-to-text). The feature supports many languages and configurations. This project's default language is english (`en-US`) and uses the default system microphone.\n```C#\n// AzCognitiveServicesListener.cs\n// Configure the connection to Azure.\nSpeechConfig speechConfig = SpeechConfig.FromSubscription(_options.Key, _options.Region);\nspeechConfig.SpeechRecognitionLanguage = _options.SpeechRecognitionLanguage;\nspeechConfig.SetProperty(PropertyId.SpeechServiceResponse_PostProcessingOption, \"TrueText\");\n\n// Configure the local audio setup\n_audioConfig = AudioConfig.FromDefaultMicrophoneInput();\n_speechRecognizer = new SpeechRecognizer(speechConfig, _audioConfig);\n```\n\n## Speaking\nAnd last, but not least, we head back to Azure Cognitive Services for its text-to-speech feature to give a voice to our AI. Since we are parsing out a style cue from OpenAI, we'll need to use the text-to-speech's Speech Synthesis Markup Language (SSML) support.\n```C#\n// AzCognitiveServicesSpeaker.cs\nSpeechConfig speechConfig = SpeechConfig.FromSubscription(_options.Key, _options.Region);\nspeechConfig.SpeechSynthesisVoiceName = _options.SpeechSynthesisVoiceName;\n_speechSynthesizer = new SpeechSynthesizer(speechConfig);\nmessage = ExtractStyle(message, out string style);\nstring ssml = GenerateSsml(message, style, _options.SpeechSynthesisVoiceName);\nawait _speechSynthesizer.SpeakSsmlAsync(ssml);\n```\nIn the case of speaking `\"That's great to hear! ~~excited~~\"`, the SSML sent to Azure Cognitive Services would like like this: \n```XML\n<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\">\n  <voice name=\"en-US-JennyNeural\">\n    <mstts:express-as style=\"excited\">That's great to hear!</mstts:express-as>\n  </voice>\n</speak>\n```\n\n# Troubleshooting\n## Speech recognizer session canceled.\nThis can occur when the Azure Speech SDK is having trouble accessing your microphone. To get more details on\nthe issue, enable debug logging by setting the `Logging:LogLevel:Default` setting in `configution.json` to `Debug`\nand run the application again.\n\nAdditionally, make sure your microphone is not being used by another application and is not set to \"Do not allow apps to access your microphone\".\n\n\n# Contributing\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use\nof Microsoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion\nor imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "conversational-speaker", "org_name": "microsoft", "org_repo": "microsoft/conversational-speaker", "platform_org_repo": "github+microsoft/conversational-speaker", "link_to_repo": "https://github.com/microsoft/conversational-speaker", "platform": "github", "language": "C#", "stargazers_count": 112, "watchers_count": 112}, {"README_text": "---\nArtifactType: nupkg\nDocumentation: URL\nLanguage: csharp, powershell\nPlatform: dotnet\nStackoverflow: URL\nTags: ScriptDOM, SqlDOM\n---\n\n# SQL ScriptDOM\n\nSQL Script DOM is a .NET library that provides formatting and parsing capabilities to analyze SQL Scripts. It can be used by Powershell and C#.\n\nScript DOM is used by DacFX and as an standalone library for client applications. \n\n## Resources:\n- \ud83d\udce6 NuGet package: https://www.nuget.org/packages/Microsoft.SqlServer.TransactSql.ScriptDom\n- \ud83d\udcda API docs: https://learn.microsoft.com/dotnet/api/microsoft.sqlserver.transactsql.scriptdom\n- \ud83d\udee0\ufe0f Contributor's guide: [CONTRIBUTING.md](./CONTRIBUTING.md)\n- \ud83d\udcac Discussions: [DacFx discussions](https://github.com/microsoft/DacFx/discussions)\n\n\n## Built With\n* https://www.antlr.org/\n\n## Contributing\n\nPlease read our [CONTRIBUTING.md](CONTRIBUTING.md) which provides helpful information about contributing to this project through reporting issues, requesting changes, and submitting pull requests.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT License](LICENSE).\n", "repo_name": "SqlScriptDOM", "org_name": "microsoft", "org_repo": "microsoft/SqlScriptDOM", "platform_org_repo": "github+microsoft/SqlScriptDOM", "link_to_repo": "https://github.com/microsoft/SqlScriptDOM", "platform": "github", "language": "GAP", "stargazers_count": 73, "watchers_count": 73}, {"README_text": "# TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches\n\n<div align=\"center\">\n<img src=\"./fig/taccl_workflow_main.png\" width=\"800\">\n</img></div>\n\n\n> **TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches** <br/>\n> Aashaka Shah, Vijay Chidambaram, Meghan Cowan, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Jacob Nelson, Olli Saarikivi, Rachee Singh <br/>\n> **NSDI 2023** [https://arxiv.org/pdf/2111.04867.pdf]\n\nTACCL is a tool to generate algorithms for collective communication like AllGather, AllToAll, and AllReduce for any given hardware configuration. TACCL takes a human-in-loop approach to algorithm design in which a user provides _communication sketches_ to guide the synthesis process. TACCL outputs TACCL-EF, an execution format that contains the schedule of GPU data transfers to efficiently implement the target collective. TACCL's schedules can be run by registering TACCL-EF files using the [MSCCL](https://github.com/microsoft/msccl-tools) tool stack.\n\n\n## Installation\nWe use Gurobi to solve the optimization problem. Please obtain a [Gurobi license](https://www.gurobi.com/downloads/) online and then proceed to install the Gurobi licensing tools as follows.\nWithin an anaconda environment, run:\n```\nconda config --add channels http://conda.anaconda.org/gurobi\nconda install -c conda-forge gurobi -y\n<command to add Gurobi license>\n```\nFinally, run\n```\npip install .\n```\n\n\n## Usage\n\n### Generating the algorithm\nTo generate data transfer algorithm for a collective <coll> for a specific topology <topo>, please provide a topology file <topo-file.json> containing profiling information of links in the node and a sketch file <sketch.json> specifying the communication sketch. Using these, a JSON file of data transfer steps for the collective algorithm can be generated with the following command:\n\n```\n$ taccl solve <topo> <coll> --topology-file <topo-file.json> --sketch-file <sketch.json> -o <output.json>\n```\n\nFor example, in order to generate an algorithm for Allgather used in the Evaluation for _dgx2-sk-1_, we run the following command:\n```\n$ cd taccl/examples\n$ taccl solve DGX2 Allgather --topology-file ../taccl/examples/topo/topo-dgx2-1MB.json --sketch-file ../taccl/examples/sketch/sk1-dgx2-n2.json\n```\n\nTo generate schedules for the combining collective AllReduce, we first obtain an AllGather algorithm and use it to generate the AllReduce algorithm. <ts> is the timestamp that is used to save the send_dict of AllGather algorithm and can be obtained from the suffix of the json algorithm file.\n```\n$ cd taccl/examples\n$ taccl solve DGX2 Allgather --topology-file ../taccl/examples/topo/topo-dgx2-1MB.json --sketch-file ../taccl/examples/sketch/sk1-dgx2-n2.json\n$ taccl combine DGX2 Allgather --topology-file ../taccl/examples/topo/topo-dgx2-1MB.json --sketch-file ../taccl/examples/sketch/sk1-dgx2-n2.json --ts <ts>\n```\n\n`./commands.sh` shows the commands with sketches and profiled topologies that can be used to obtain the results in our paper.\n\n\n#### Providing topology\n- \"\\<topo\\>\" should be selected from the 'known_topologies' constructor and can be either of \"custom\", \"HubAndSpoke\", \"DGX2\", or \"NDv2\". `./taccl/topologies/generic.py` defines the link connection matrix within a node for each \\<topo>. `links[dst][src]` is 1 if there is one link going from `src` to `dst`.\n- \"\\<topo-file.json\\>\" contains profiling information about the node, like latency and bandwidth costs of intra-node and inter-node transfers as well as number of GPUs and NICs in the node. Depending on the profiling information given in the topology-file provided by the user, the profile attributes `alpha`, `betas`, `invbws`, `nics_per_node`, `remote_invbw`, `remote_alpha`, and `remote_beta` are used to obtain a NodeTopology object. `taccl/examples/topo` directory gives examples of topology files for DGX-2 and NDv2 nodes that can be provided to `--topology-file`.\n\nYou can add new node topologies as a separate function in `./taccl/cli/known_topologies.py` or use the \"custom\" option for topologies and provide all details of the topology in the user-provided topology-file.\n\n#### Providing communication sketch\nA communication sketch has the following three purposes:\n1. Create a logical topology that determines how different nodes are connected to each other\n2. Annotate links which form a switch\n3. Annotate symmetry planes in the topology\n\n`./taccl/examples/sketch` directory gives examples of some communication sketches that can be used for NVIDIA DGX-2 and Azure NDv2 nodes.\n\n### Lowering to TACCL-EF\nOnce the algorithm is generated, we lower it into a TACCL-EF file. The number of instances \\<instances> determines the multiple we will use to increase the number of channels that are used to perform sends and receives. When there are already many threadblocks being used per GPU, you should use a single instance for the best performance.\n```\n$ taccl ncclize <output.json> --instances <instances>\n```\n\n### Running with MSCCL\nThe TACCL-EF file can be used as input by the [MSCCL runtime](https://github.com/microsoft/msccl) to actually run the algorithm on the hardware. Please follow the setup instructions in the MSCCL repository to run TACCL algorithms. Once MSCCL is setup, TACCL-EF files can be benchmarked using nccl-tests as follows:\n```\n$ mpirun -np <ngpus> -x LD_LIBRARY_PATH=msccl/build/lib/:$LD_LIBRARY_PATH -x NCCL_DEBUG=INFO -x NCCL_DEBUG_SUBSYS=INIT,ENV -x MSCCL_XML_FILES=<taccl-ef> -x NCCL_ALGO=MSCCL,RING,TREE  nccl-tests/build/<nccl-test-binary> -b 128 -e 32MB -f 2 -g 1 -c 1 -n 100 -w 100 -G 100 -z 0\n```\nTACCL-EF files can also be registered in the [MSCCL-toolkit](https://github.com/microsoft/msccl-tools) to be used in frameworks like PyTorch and Tensorflow.\n\n## Guide to providing topology profiles\n[INPUT_GUIDE.md](./INPUT_GUIDE.md) provides more details of how to specify different profiles for topologies.\n\n\n## Citation\n> Shah, A., Chidambaram, V., Cowan, M., Maleki, S., Musuvathi, M., Mytkowicz, T., Nelson, J. and Saarikivi, O., 2023. {TACCL}: Guiding Collective Algorithm Synthesis using Communication Sketches. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) (pp. 593-612).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "taccl", "org_name": "microsoft", "org_repo": "microsoft/taccl", "platform_org_repo": "github+microsoft/taccl", "link_to_repo": "https://github.com/microsoft/taccl", "platform": "github", "language": "Python", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Launch vscode.dev\r\n\r\nThis is a Chrome/Edge extension for opening GitHub and Azure Repos repositories in https://vscode.dev.\r\n\r\n## Features\r\n\r\nType `code` into your browser's search bar to activate the omnibox and launch your recent GitHub repositories in https://vscode.dev.\r\n\r\n![Use the omnibox to open recently opened GitHub repositories in vscode.dev](./omnibox.gif)\r\n\r\nOr launch https://vscode.dev from any GitHub or Azure Repos repository or PR page by\r\n1. Typing `Ctrl+.` (default) or `Cmd+.` (Mac)\r\n2. Clicking on the extension icon\r\n\r\n![image](./launch.gif)\r\n\r\nTo use https://insiders.vscode.dev by default, you can configure your preference for Stable or Insiders in extension options.\r\n\r\n## Acknowledgments\r\n\r\nThis extension's omnibox functionality is heavily inspired by Pine Wu's [goto-github-repo extension](https://github.com/octref/goto-github-repo) with many thanks.\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \r\ntrademarks or logos is subject to and must follow \r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "vscode-dev-chrome-launcher", "org_name": "microsoft", "org_repo": "microsoft/vscode-dev-chrome-launcher", "platform_org_repo": "github+microsoft/vscode-dev-chrome-launcher", "link_to_repo": "https://github.com/microsoft/vscode-dev-chrome-launcher", "platform": "github", "language": "JavaScript", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Samples for TPM-based attestation using Microsoft Azure Attestation\n\nThese code samples show how attestation can be performed using the TPM. Additionally the sample takes advantage of the https://www.nuget.org/packages/Microsoft.Attestation.Client\n\n## List of Samples\n\n### **Boot attestation (sample_boot_att.exe)**\n\nThis sample provides the code implementation to perform boot attestation, and retrieve an attestation token from Microsoft Azure Attestation.\n\n### **TPM key attestation (sample_tpm_key_att.exe)**\n\nThis sample provides the code implementation to perform boot and TPM key attestation, and retrieve an attestation token from Microsoft Azure Attestation.\nThis sample creates a TPM key named \"att_sample_key\" which is attested by Microsoft Azure Attestation. The creation of a TPM key may take up to a few minutes depending on the TPM hardware.\n\n## Sample Requirements\n\n* The machine must have a Trusted Platform Module (TPM).\n\n* The following environment variables must be set by the user:\n\n    * **AZURE_TENANT_ID** - Tenant ID for the Azure account. Used for authenticated calls to the attestation service.\n    * **AZURE_CLIENT_ID** - The client ID to authenticate the request. Used for authenticated calls to the attestation service.\n    * **AZURE_CLIENT_SECRET** - The client secret. Used for authenticated calls to the attestation service.\n    * **AZURE_MAA_URI** - Microsoft Azure Attestation provider's Attest URI (as shown in portal). Format is similar to \"https://\\<ProviderName\\>.\\<Region\\>.attest.azure.net\".\n\n* An AIK named \"att_sample_aik\" must be available. Run the EnrollAik.ps1 script to create the key and retrieve an AIK certificate for it (notice that the command below allows the key to be accessed by all users on the machine):\n\n        EnrollAik.ps1 att_sample_aik BUILTIN\\Users\n\n## Running attestation samples\n\n1. Set up an Azure Attestation provider with a **TPM** policy using the instructions at https://docs.microsoft.com/en-us/azure/attestation/quickstart-portal.\n2. Build the project (it can be built by opening the folder in Visual Studio 2022).\n3. Make sure the sample requirements above are met.\n4. Run one of the samples listed above.\n\n## EnrollAik.ps1\n\nThis script automates the process of generating an Attestation Identity Key (AIK). It invokes the Cert Request utility which is provided as part of Windows and requests that a new key be generated in the TPM. It then registers this key with the certificate service and gets an AIK certificate that can then be used in the attestation flow. The script also provides an option to set the ACL on the key file (used by the Platform/TPM Key Storage Provider) such that it can be loaded from a user-mode process, as otherwise administrator privileges would be required for the client attestation application. \n\nNote: EnrollAik.ps1 won't be able to get an AIK certificate on a virtual machine because a virtual TPM does not have a trusted Endorsement Key certificate which is used by Azure Certificate Services to validate the TPM.\n\n## References\n\n* Microsoft Azure Attestation: https://learn.microsoft.com/en-us/azure/attestation/\n* TPM attestation: https://learn.microsoft.com/en-us/azure/attestation/tpm-attestation-concepts\n* Attestation policy: https://learn.microsoft.com/en-us/azure/attestation/policy-version-1-2\n* Trusted Computing Group TPM 2.0 Spec: https://trustedcomputinggroup.org/resource/tpm-library-specification/\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Attestation-Client-Samples", "org_name": "microsoft", "org_repo": "microsoft/Attestation-Client-Samples", "platform_org_repo": "github+microsoft/Attestation-Client-Samples", "link_to_repo": "https://github.com/microsoft/Attestation-Client-Samples", "platform": "github", "language": "C++", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Yet Another Demo App\n\nThis repo contains code for a 3-tier application that can be used to explore workload platforms. Other demo or helloworld apps just show a fancy page, potentially even with some functionality like To Do lists, voting options or even simulating enterprise apps. YADA (Yet Another Demo App) doesn't pretend to be a real application, but instead it will give you diagnostics information that will help you understand the underlying infrastructure, such as:\n\n- IP address (private and public)\n- IP address with which the database sees the app tier\n- HTTP request headers and cookies\n- Instance MetaData Service (IMDS)\n- DNS and reverse resolution\n- Access between tiers\n- Outbound connectivity with curl\n- Drive up CPU load to test autoscaling\n- Storage benchnmarking\n\nYADA is composed of a web tier and a REST-based application tier that will access a database. The database can be SQL Server, MySQL or Postgres, and no special databases need to be created:\n\n![Application architecture](web/app_arch.orig.png)\n\nBoth the web tier and the application tier will give information about the platform where they are running (hostname, public IP address, IMDS and more). The web tier is customizable with different brandings and background colors. With the default branding and cyan background it looks like this:\n\n![Web tier](./web/homepage_screenshot.png)\n\nBoth app and web tiers are containerized and can be deployed in different platforms: Virtual machines with Docker (such as [Flatcar](https://www.flatcar.org/)), Kubernetes, Azure Container Apps, Azure Container Instances, Azure Web Apps or any other container-based architecture.\n\nYou can find the images for the web and API tier in these public images in Dockerhub:\n\n- erjosito/yadaweb:1.0\n- erjosito/yadaapi:1.0\n\n## Deployment guides\n\nThe following files contain instructions to deploy YADA on different platforms:\n\n- [Deploy on Docker containers](./deploy/docker.md)\n- [Deploy on public Azure Container Apps](./deploy/ACA.md)\n- [Deploy on public Azure Container Instances](./deploy/ACI_public.md)\n- [Deploy on public ACI with TLS on nginx](./deploy/ACI_nginx_sidecar.md)\n- [Deploy on Kubernetes](./deploy/k8s.md)\n- [Deploy on WebApps](./deploy/webapp.md)\n- [Deploy on virtual machines](./deploy/vm.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "YADA", "org_name": "microsoft", "org_repo": "microsoft/YADA", "platform_org_repo": "github+microsoft/YADA", "link_to_repo": "https://github.com/microsoft/YADA", "platform": "github", "language": "Python", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# Welcome\n\nWelcome to the Azure Edge Solutions Lab GitHub Page.\n\n\nThis repo includes the following labs:\n\nLab | Description of Labs\n--- | ---\n[Lab Demo for Quick Service Resturants (QSR)](https://github.com/microsoft/Azure-Edge-Solutions-Lab/tree/main/Lab%20Demo%20QSR) | AI services running on AzSHCI and AKS with ARC.\n[Lab Demo distributing QSR app across AzSHCI and Azure Edge Zone](https://github.com/microsoft/Azure-Edge-Solutions-Lab/tree/main/Edgezone-QSR%20Demo) | Cloud native AI app is running distributed on AzS HCI and Azure Edge Zones. Both HCI and Azure Edge Zone have AKS orchestrating running our container based AI app. Microservices are split/ distributed, meaning some microsoveices are running on-prem on AzS HCI and one microservice is running on a Azure cloud geo-located near the on-prem deployment in Azure Edge Zone.\n", "repo_name": "Azure-Edge-Solutions-Lab", "org_name": "microsoft", "org_repo": "microsoft/Azure-Edge-Solutions-Lab", "platform_org_repo": "github+microsoft/Azure-Edge-Solutions-Lab", "link_to_repo": "https://github.com/microsoft/Azure-Edge-Solutions-Lab", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure data lake storage\n- azure-form-recognizer\n- azure-logic-apps\n- azure-functions-app\n- azure cosmos db\n- power-bi\n---\n\n![SafetyFormProcessing](./Deployment/Images/SA-EAE-Banner.png)\n\n# Azure PDF Form Processing Automation Solution Accelerator\n\nForm processing is a critical business function across industries. Many companies are still relying on manual processes, which are costly, time-consuming, and error prone. Replacing these manual processes not only reduces a company\u2019s cost and risk but is also an essential part of a company\u2019s digital transformation journey. \n\nThis solution accelerator empowers companies to automate the processing of PDF forms to modernize their operations, save time, and reduce cost.\n\nThe solution accelerator receives the PDF forms, extracts the fields from the form, and saves the data in Azure Cosmos DB. Power BI is then used to visualize the data.\n\nThe solution accelerator was designed with a modular, metadata-driven methodology. It can be utilized directly without code modification to process and visualize any single-page PDF forms such as safety forms, invoices, incident records, health screening forms, payment authorization forms, and many others.\n\nTo use the solution accelerator, you only need to collect sample PDF forms, train a new model to learn the form's layout and plug the model into the solution. The Power BI report will need to be re-designed for your specific data sets to drive insights.\n\n**Who can leverage this solution?** Businesses have many types of single-page forms to be processed and analyzed. For example, safety forms, invoices, incident records, housing application forms, credit card application forms, job applications forms, and many others.\n\n**Outcome of the solution**: Key data fields are extracted from many single-page PDF forms, stored in Azure Data Lake Storage and Cosmos DB. The data is visualized in dashboard pages to drive actionable insights. \n\n**Possible extension**: Businesses can utilize data fields stored in Azure Data Lake Storage or Azure Cosmos DB to perform further processing. For example, if safety forms are processed, the data can be used for work place safety analysis, incident analysis, and compliance reporting. If invoices are processed, the processed data can be used for invoice payment applications. \n\n**What are the input?** Input to this solutions are (1) single-page pdf forms or (2) multiple page pdf documents with each page as a self-contained form. In this case, the solution has a 'split pdf file' feature to split the multiple page pdf file into single-page pdf forms. \n\n**Limitations:** Each PDF form must fit into **a single page pdf file**. If the PDF file contains multiple pages, the system assumes that each page is a self-contained PDF form, and will split the multi-page PDF file into single pages before processing. After splitting, each split file will be only one page, containing a single form. Please review the sample files posted here to see the format: [Single Page PDF File Sample](./Deployment/Data/samples/test/contoso_set_1/ContosoSafety360-Sample-1.pdf) and [Multi Page PDF File Sample](./Deployment/Data/samples/test/contoso_set_1/ContosoSafety360-Combo-1.pdf).\n\n**How to deploy and test the solution?**  The solution is supplied with sample manufacture safety forms, form recognizer labeled files, and a Power BI model. Please follow the step by step [Deployment Guide](./Deployment/README.md) to deploy and set up the solution to your own Azure subscription, test the solution with the test forms supplied, and then visualized the data using the Power BI model supplied. \n\n**Key Azure technologies** utilized in this solution are:  Azure Data Lake Storage,  Azure Form Recognizer, Azure Logic Apps, Azure Functions App, Azure Cosmos DB, and Power BI.  **The Azure Form Recognizer** is a cloud-based Azure Applied AI service that uses machine learning models to extract and analyze fields, text, and tables from documents or images. **Azure Logic App** is a cloud-based platform for creating and running automated end-to-end workflows. **Azure Functions App** provides low-cost, custom application logic development and data processing capabilities to help businesses solve complex problems with ease of design, development, deployment, and maintenance. **Azure Cosmos DB** is a fully managed, serverless NoSQL database for high-performance applications of any size or scale. \n\n![Process flow](./Deployment/Images/Process-Flow.png \"Process flow\")\n\n## Prerequisites\n\nTo use this solution accelerator, you will need access to an [Azure subscription](https://azure.microsoft.com/en-us/free/). An understanding of Azure Form Recognizer, Azure Form Recognizer Studio, Azure Logic Apps, Azure Functions, Azure Cosmos DB, and Power BI will be helpful. \n\nFor additional training and support, please review:\n\n1. [Azure Form Recognizer](https://azure.microsoft.com/en-us/services/form-recognizer/)\n2. [Azure Logic Apps](https://azure.microsoft.com/en-us/services/logic-apps/#overview)\n3. [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview)\n4. [Azure Data Lake Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction)\n5. [Azure Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n6. [Power BI](https://docs.microsoft.com/en-us/power-bi/fundamentals/power-bi-overview)\n\n\n## Getting Started\nGet started by deploying the solution accelerator to a specified resource group in your own subscription. Go to the [Deployment Guide](./Deployment/README.md) to set up your Azure environment, create necessary Azure resources, and test the solution. \n\n## Architecture\nBelow architecture diagram illustrates the main components and information flow of this solution accelerator. For the work flow details, please refer to the page for [Architecture Description](./Deployment/Architecture_Description/README.md). \n\n![Architecture Diagram](./Deployment/Images/Arch-SA-PDF-Form-Processing-Automation-AAC.png \"PDF Form Processing Automation Architecture Diagram\")\n\n## Power BI Dashboard\n\nBelow Power BI dashboard illustrates overview of sample safety form processing results, showing the number of occurrences by selected categories. The category is the field key defined by the Azure Form Recognizer labeling tool.  \n\n![PBI w Text Search](./Deployment/Images/PBI-Overview.png)\n\nIn addition, you can have a quick overview of the safety forms categorized by important fields such as Department, Owner, Date created. From these charts, you can recognize patterns and trends, as illustrated below. \n\n![PBI w Summary](./Deployment/Images/PBI-Metrics.png)\n\n## License\nMIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n\n## Note about Libraries with MPL-2.0 License\nThe following libraries are not explicitly included in this repository, but users who use this Solution Accelerator may need to install them locally and in Azure Functions to fully utilize this Solution Accelerator. However, the actual binaries and files associated with the libraries are not included as part of this repository, but they are available for installation via the PyPI library using the pip installation tool.\n\nLibraries: certifi\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n", "repo_name": "Azure-PDF-Form-Processing-Automation-Solution-Accelerator", "org_name": "microsoft", "org_repo": "microsoft/Azure-PDF-Form-Processing-Automation-Solution-Accelerator", "platform_org_repo": "github+microsoft/Azure-PDF-Form-Processing-Automation-Solution-Accelerator", "link_to_repo": "https://github.com/microsoft/Azure-PDF-Form-Processing-Automation-Solution-Accelerator", "platform": "github", "language": "Bicep", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "sarifer", "org_name": "microsoft", "org_repo": "microsoft/sarifer", "platform_org_repo": "github+microsoft/sarifer", "link_to_repo": "https://github.com/microsoft/sarifer", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# [K-LITE: Learning Transferable Visual Models with External Knowledge ](https://arxiv.org/pdf/2204.09222.pdf)\n\nThis is the official Pytorch implementation of KLITE:\n\n[\"**K-LITE: Learning Transferable Visual Models with External Knowledge**\"](https://arxiv.org/pdf/2204.09222.pdf0), NeurIPS 2022 (Oral, 1%) by \n\n[Sheng Shen*](https://sincerass.github.io/), [Chunyuan Li*](https://chunyuan.li/), [Xiaowei Hu*](https://scholar.google.com/citations?user=Pj0TwxwAAAAJ&hl=en), [Yujia Xie](https://scholar.google.com/citations?user=r2FiAE4AAAAJ&hl=en), [Jianwei Yang](https://jwyang.github.io/), [Pengchuan Zhang](https://pzzhang.github.io/pzzhang/), [Zhe Gan](https://zhegan27.github.io/), [Lijuan Wang](https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=zh-CN), [Lu Yuan](https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=en), [Ce Liu](http://people.csail.mit.edu/celiu/), [Kurt Keutzer](http://people.eecs.berkeley.edu/~keutzer/), [Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/), [Anna Rohrbach](https://anna-rohrbach.net/) and [Jianfeng Gao](https://www.microsoft.com/en-us/research/people/jfgao/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fjfgao%2F).\n\n## Update\n\n- Jan 17 2023. [**REACT**](https://react-vl.github.io/) extends KLITE from *text knowledge from a dictionary* to ***retrieved multimodal knoweldge from the web***.\n\n## Introduction\n\n<p align=\"center\">\n  <img src=\"./figs/knowledge_pipepline-1.png\" width=60%/>\n  <img src=\"./figs/knowledge_applications-1.png\" width=28%/>\n</p>\n\nIn this paper,  we propose **K-LITE**, a simple strategy to leverage **external knowledge** for building transferable visual systems: In training, it enriches entities in text with WordNet and Wiktionary knowledge, leading to an **efficient and scalable approach** to learning image representations that uses knowledge about the visual concepts. In evaluation, the text is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. We study the performance of K-LITE on two important computer vision problems,\nimage classification (IC) and object detection (OD) in [ELEVATER](https://computer-vision-in-the-wild.github.io/ELEVATER/) benchmark, on 20 and 13 different existing datasets, respectively. The proposed knowledge-augmented models show **6.29%** average improvement on 20 IC tasks and **4.2%** average improvement on 13 OD tasks in performance over existing methods. \n\nWe provide two illustrative examples why **K-LITE** could be helpful from Oxford-Flowers and Food-101 IC tasks. \n\n<p align=\"center\">\n  <img src=\"./figs/flower_success_a-1.png\" width=40%/>\n  <img src=\"./figs/food101_success_a-1.png\" width=41%/>\n</p>\n\n\n## Benchmarking\n\n### UniCL training with image-label data and image-text pairs\n<!-- | Swin-T | IN-21K | 28.5 | 37.8 | - | [ckpt](https://projects4jw.blob.core.windows.net/unicl/release/in21k.pth)/[config](configs/klite_swin_tiny.yaml) -->\n| Model | Training Set | ZS on IN-1K | ZS on 20 datasets | Download\n| :----: | :---: | :---: | :---: | :---: |\n| Swin-T | IN-21K | 28.5 | 27.1 | [ckpt](https://projects4jw.blob.core.windows.net/unicl/release/in21k.pth)/[config](configs/klite_swin_tiny.yaml)\n| Swin-T | IN-21K + GCC-15M | 46.9 | 39.8 | [ckpt](https://cvinw.blob.core.windows.net/model/unicl/in21k_gcc15m/tiny/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml)\n| Swin-T | IN-21K + GCC-15M + YFCC-14M | 49.3 | 40.5 | [ckpt](https://cvinw.blob.core.windows.net/model/unicl/in21k_gcc15m_yfcc14m/tiny/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml)\n| Swin-B | IN-21K + GCC-15M | 50.0 | 39.4 | [ckpt](https://cvinw.blob.core.windows.net/model/unicl/in21k_gcc15m/base/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml)\n| Swin-B | IN-21K + GCC-15M + YFCC-14M | 52.3 | 42.5 | [ckpt](https://cvinw.blob.core.windows.net/model/unicl/in21k_gcc15m_yfcc14m/base/model_state_dict.pt)/[config](configs/unicl_swin_base.yaml)\n\n\n### K-LITE training with image-label data and image-text pairs augmented by knowledge data\n\n| Model | Training Set | ZS on IN-1K | ZD on IN-1k (+5 GPT-3 Knowledge) | ZS on 20 datasets| ZS on 20 datasets (+5 GPT-3 Knowledge) | Download\n| :----: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Swin-T | IN-21K | 30.5 |  32.0 | 33.5 | 33.8 | [ckpt](https://cvinw.blob.core.windows.net/model/klite/in21k/tiny/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml) \n| Swin-T | IN-21K + GCC-15M | 49.9 |  51.6  | 41.1 | 42.3 | [ckpt](https://cvinw.blob.core.windows.net/model/klite/in21k_gcc15m/tiny/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml)\n| Swin-T | IN-21K + GCC-15M + YFCC-14M | 49.6 |  51.9 | 40.3 | 41.6 | [ckpt](https://cvinw.blob.core.windows.net/model/klite/in21k_gcc15m_yfcc14m/tiny/model_state_dict.pt)/[config](configs/klite_swin_tiny.yaml)\n| Swin-B | IN-21K + GCC-15M | 52.7 | 55.0  | 42.8 | 43.6 | [ckpt](https://cvinw.blob.core.windows.net/model/klite/in21k_gcc15m/base/model_state_dict.pt)/[config](configs/klite_swin_base.yaml)\n| Swin-B | IN-21K + GCC-15M + YFCC-14M | 55.8 |  58.0 | 42.5 | 44.8 | [ckpt](https://cvinw.blob.core.windows.net/model/klite/in21k_gcc15m_yfcc14m/base/model_state_dict.pt)/[config](configs/klite_swin_base.yaml)\n\n**NOTE**: Setting \"ZS on 20 datasets\" is used in the ICinW benchmark. All the above models are trained **without** strong data augmentations like mixup and cutmix.\n\n## Getting Started\n### Setup\n\nTo setup the environment, please run \n```bash\npip install -r requirements.txt\npip install -e .\n```\nNote that for run `main.py` for potential training and evaluation, you need to install [apex](https://github.com/NVIDIA/apex). \n\nAlso, see [klite/load_wiki](https://github.com/microsoft/klite/load_wiki) for constructing image-text pairs or image-label data (train/validation) augmented by knowledge. \n\n### Data preparation\n\nPlease following [DATA.md](./DATA.md) for data preparation.\n\n### **Evaluation**\n\n#### **ImageNet Evaluation**\n\nTo evaluate a pre-trained K-LITE on ImageNet val, run:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node <num-of-gpus-to-use> --master_port 12345 main.py --eval \\\n--cfg <config-file> --resume <checkpoint> --data-path <imagenet-path>  --use_knowledge\n```\nor\n```bash\nMODE: pretrain method (klite or unicl)\nNGPUS: number of gpus\nCFG: model config (configs/klite_swin_tiny.yaml or configs/klite_swin_base.yaml)\nCKPT_DIR: directory to the ckeckpoint\nIMAGENETPATH: path to ImageNet\n\nbash scripts/run_in1k_eval.sh $MODE $NGPUS $CFG $CKPT_DIR $IMAGENETPATH\n```\n\nFor example, to evaluate the KLITE-Swin-Tiny trained on IN-21K + GCC-15M with a single GPU:\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 1 --master_port 12345 main.py --eval \\\n--cfg configs/klite_swin_tiny.yaml --resume ckpt/klite/in21k_gcc15m/tiny/model_state_dict.pt --data-path <imagenet-path>  --use_knowledge\n```\n\n#### **20 ELEVATER Image Classification  tasks Evaluation**\nFor evaluating KLITE for downstream image classification tasks, and comparing performance on the same task suite, we include the [evaluation toolkit](https://github.com/Computer-Vision-in-the-Wild/Elevater_Toolkit_IC) here at `klite/vision_bechmark/`. Please run the [setup](#setup) before evalutaing on the 20 [ELEVATER](https://computer-vision-in-the-wild.github.io/ELEVATER/) Image Classification tasks. \n\nThen, to evaluate a pre-trained K-LITE on 20 [ELEVATER](https://computer-vision-in-the-wild.github.io/ELEVATER/) Image Classification tasks in a zero-shot way, run:\n\n```bash\nMODE: pretrain method (klite or unicl)\nCFG: model config (clip_swin_tiny or clip_swin_base)\nCKPT_PATH: path to the checkpoint \nCKPT_ID: the dataset used to pretrain the model (in21k, in21k_gcc15m, in21k_gcc15m_yfcc14m)\n\nbash scripts/run_elevater_eval.sh $MODE $CFG $CKPT_PATH $CKPT_ID\n```\n\nFor example, to evaluate the KLITE-Swin-Tiny trained on IN-21K + GCC-15M with a single GPU:\n\n```bash\nCUDA_VISIBLE_DEVICES=0 bash  scripts/run_elevater_eval.sh klite clip_swin_tiny ckpt/klite/in21k_gcc15m_yfcc14m/tiny/model_state_dict.pt\n```\n\n\nMore details for [ELEVATER](https://computer-vision-in-the-wild.github.io/ELEVATER/) benchmark can be found: [[Benchmark]](https://computer-vision-in-the-wild.github.io/ELEVATER/) [[Toolkit]](https://github.com/Computer-Vision-in-the-Wild/Elevater_Toolkit_IC) [[Paper]](https://arxiv.org/abs/2204.08790)\n\n## Citation\n\nIf you find this repo useful to your project, please consider to cite it with following bib:\n\n```\n@inproceedings{shen2022k,\n    title={K-lite: Learning transferable visual models with external knowledge},\n    author={Shen, Sheng and Li, Chunyuan and Hu, Xiaowei and Xie, Yujia and Yang, Jianwei and Zhang, Pengchuan and Rohrbach, Anna and Gan, Zhe and Wang, Lijuan and Yuan, Lu and others},\n    booktitle={NeurIPS},\n    year={2022}\n}\n```\n## Acknowledgement\n\nOur codebase is built based on [UniCL](https://github.com/microsoft/UniCL) and [ELEVATER](https://github.com/Computer-Vision-in-the-Wild/Elevater_Toolkit_IC).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "klite", "org_name": "microsoft", "org_repo": "microsoft/klite", "platform_org_repo": "github+microsoft/klite", "link_to_repo": "https://github.com/microsoft/klite", "platform": "github", "language": "Python", "stargazers_count": 37, "watchers_count": 37}, {"README_text": "# FHIR Link\n\n> FHIR Link is an open source function to help enable tighter integration between Azure Health Data Services FHIR Service (or Azure API for FHIR) and Dynamics 365 Customer Insights.\n\n## Scenario\n\nDifferent EHR systems handle merge and unmerge patient records differently. When ingesting patient data into FHIR, one common way to represent merged patients is with patient links. \n\nCustomer Insights (D365) unifies disparate records into a single dataset that provides a unified view of that data. For instance, from FHIR and CRM systems in order to build patient segments for targeted outreach. Patients merged in FHIR through patient.link, need to be respected as a part of the unification process in Customer Insights. \n\n## Solution \n\nOur solution is an Azure Function App that sits between the FHIR Service and Customer Insights, providing a list of merged patients to Customer Insights via Azure Data Lake for processing:\n- queries FHIR (Azure Health Data Services or Azure API for FHIR) for patients merged together using patient.link (get patients with Link attribute). \n- takes FHIR ids of those patients linked with the replaced-by link type, and writes it to a template CSV file readable by Customer Insights.\n- the resulting CSV file is placed in an `alwaysmerge` directory\n- with each run, previous files are moved from the `alwaysmerge` directory to an `archive` directory\n\nCustomer Insights can be configured to connect to this Azure Data Lake container as a data source, to be used in the Unify process as list of patients that will Always Match.\n\nInstructions to deploy and configure this Function App can be found in [setup.md](./docs/setup.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Community Discord\nIf you are interested in projects like this or connecting with the health and life sciences developer community, please join our Discord server at https://aka.ms/HLS-Discord. We're a technology agnostic community seeking to share and collaborate on all things related to developing healthcare solutions. For in-depth questions specific to this project, please use the \"Discussions\" tab on GitHub. We welcome your thoughts and feedback.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fhir-link", "org_name": "microsoft", "org_repo": "microsoft/fhir-link", "platform_org_repo": "github+microsoft/fhir-link", "link_to_repo": "https://github.com/microsoft/fhir-link", "platform": "github", "language": "C#", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "micro-frontend-tools", "org_name": "microsoft", "org_repo": "microsoft/micro-frontend-tools", "platform_org_repo": "github+microsoft/micro-frontend-tools", "link_to_repo": "https://github.com/microsoft/micro-frontend-tools", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# CCF App Samples [![Open in VSCode](https://img.shields.io/static/v1?label=Open+in&message=VSCode&logo=visualstudiocode&color=007ACC&logoColor=007ACC&labelColor=2C2C32)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/ccf-app-samples)\n\n[![CCF App Samples CI](https://github.com/microsoft/ccf-app-samples/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/ccf-app-samples/actions/workflows/ci.yml)\n\nSample applications for the [Confidential Consortium Framework (CCF)](https://ccf.microsoft.com/).\n\n## Quickstart\n\nThe quickest way to build and run sample applications is to checkout this repository locally in its development container by clicking:\n\n[![Open in VSCode](https://img.shields.io/static/v1?label=Open+in&message=VSCode&logo=visualstudiocode&color=007ACC&logoColor=007ACC&labelColor=2C2C32)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/ccf-app-samples)\n\nAll dependencies will be automatically installed (takes ~2 mins on first checkout).\n\nAlternatively, if your organisation supports it, you can checkout this repository in a Github codespace:\n\n[![Open in GitHub Codespaces](https://img.shields.io/static/v1?label=Open+in&message=GitHub+codespace&logo=github&color=2F363D&logoColor=white&labelColor=2C2C32)](https://github.com/codespaces/new?hide_repo_select=true&repo=microsoft%2Fccf-app-samples)\n\nPlease choose a sample to learn more.\n- [Auditable Logging App](./auditable-logging-app/README.md)\n- [Banking App](./banking-app/README.md)\n- [Data Reconciliation App](./data-reconciliation-app/README.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ccf-app-samples", "org_name": "microsoft", "org_repo": "microsoft/ccf-app-samples", "platform_org_repo": "github+microsoft/ccf-app-samples", "link_to_repo": "https://github.com/microsoft/ccf-app-samples", "platform": "github", "language": "TypeScript", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# MSTICPy Training Materials\n\nThis repo is holds training materials and tools used for demos for the\n[MSTICPy Python Package](https://github.com/microsoft/msticpy).\nPlease refer to that site and the [MSTICPy documentation](https://msticpy.readthedocs.io/) on\nReadTheDocs.\n\n\n# Docker Instructions\n\nInstall docker for desktop on Windows : \n\nFollow the instuctions at docker docs - https://docs.docker.com/desktop/install/windows-install/\n\nBuild Docker image locally:\n\n`docker build -t msticpy-training -f .\\.devcontainer\\Dockerfile .`\n\nRun docker image:\n\n`docker run -p 8888:8888 msticpy-training`\n\nConnect VSCode to Connector: [Attach to a Docker Container](https://code.visualstudio.com/docs/remote/attach-container)\n\nTo attach to a Docker container, either select ***Dev Containers: Attach to Running Container...*** from the Command Palette (F1) or use the ***Remote Explorer*** in the Activity Bar and from the ***Containers*** view, select the ***Attach to Container*** inline action on the container you want to connect to\n\n- Once you open a notebook in VSCode. You will see Select Kernel option on right hand top corner.\n<img src=\"./workshops/Oct2022/media/Docker-01-Select-kernel.png\"\nalt=\"Docker Select Kernel\"\ntitle=\"Docker Select Kernel\" height=\"70\" />\n\n- Once you click on it, you will be prompted to install Jupyter VSCode extension in container.\n<img src=\"./workshops/Oct2022/media/Docker-02-Install-VSCode-Extension.png\"\nalt=\"Install VSCode extension\"\ntitle=\"Install VSCode extension\" height=\"70\" />\n\n- After you finished installation, you can then click Select Kernel and choose Connect to local Jupyter server.\n<img src=\"./workshops/Oct2022/media/Docker-03-Connect-to-Jupyter.png\"\nalt=\"Connect to Jupyter\"\ntitle=\"Connect to Jupyter\" height=\"70\" />\n\n- Run Jupyter server URI from VSCode Terminal.\n\n<img src=\"./workshops/Oct2022/media/Docker-04-Retrieve-Server-URL.png\"\nalt=\"Retrieve Server URL\"\ntitle=\"Retrieve Server URL\" height=\"70\" />\n\n- and then enter the URL on the next screen.\n\n<img src=\"./workshops/Oct2022/media/Docker-05-Enter-URI.png\"\nalt=\"Enter URI\"\ntitle=\"Enter URI\" height=\"70\" />\n\n- Finally, select a Remote Kernel.\n\n<img src=\"./workshops/Oct2022/media/Docker-06-Remote-Kernel.png\"\nalt=\"Remote Kernel\"\ntitle=\"Remote Kernel\" height=\"70\" />\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "msticpy-training", "org_name": "microsoft", "org_repo": "microsoft/msticpy-training", "platform_org_repo": "github+microsoft/msticpy-training", "link_to_repo": "https://github.com/microsoft/msticpy-training", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "\n\n# Devhack \u52a8\u624b\u5b9e\u9a8c\uff1aModernize Applications with Azure PaaS Services\n\n\n\u5e94\u7528\u73b0\u4ee3\u5316\u52a8\u624b\u5b9e\u9a8c\uff0c\u7528\u4e8e\u5c06 Java \u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230Azure\u7684\u591a\u4e2a\u573a\u666f\uff0c\u5e76\u9009\u62e9\u6700\u80fd\u6ee1\u8db3\u56e2\u961f\u9700\u6c42\u7684\u9009\u9879\u3002\n\n\n\n\n\n## \u5b66\u4e60\u76ee\u6807\n\u5b8c\u6210\u672c\u6559\u7a0b\u540e\uff0c\u53ef\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n\n1. \u63cf\u8ff0\u5e76\u533a\u5206\u9002\u7528\u4e8e Java \u5e94\u7528\u7a0b\u5e8f\u7684 Azure \u90e8\u7f72\u9009\u9879\u3002\n2. \u9009\u62e9\u7528\u4e8e\u90e8\u7f72 Java \u5e94\u7528\u7a0b\u5e8f\u7684\u6700\u4f73 Azure \u670d\u52a1\u3002\n3. \u638c\u63e1\u5982\u4f55\u4ece\u57fa\u4e8eIaaS\u7684\u5e94\u7528\u8fc1\u79fb\u5230Azure \u4e91\u539f\u751f\uff08PaaS\uff09\u670d\u52a1\n\n## \u52a8\u624b\u5b9e\u9a8c\n\n- [1 - \u5c06Java\u5e94\u7528\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS](1-\u5c06Java\u5e94\u7528\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS/\u5c06Java\u5e94\u7528\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS.md)\n- [2 - \u5c06JavaWeb\u5e94\u7528\u5229\u7528Azure Migrate\u8fc1\u79fb\u5230AKS](2-\u5c06JavaWeb\u5e94\u7528\u5229\u7528AzureMigrate\u8fc1\u79fb\u5230AKS/\u5c06JavaWeb\u5e94\u7528\u5229\u7528AzureMigrate\u8fc1\u79fb\u5230AKS.md)\n- [3 - \u5c06SpringBoot\u5e94\u7528\u5229\u7528Jib\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS](3-\u5c06SpringBoot\u5e94\u7528\u5229\u7528Jib\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS/\u5c06SpringBoot\u5e94\u7528\u5229\u7528Jib\u5bb9\u5668\u5316\u5e76\u90e8\u7f72\u5230AKS.md)\n- [4 - \u5c06SpringCloud\u5e94\u7528\u8fc1\u79fb\u5230AzureSpringApps](4-\u5c06SpringCloud\u5e94\u7528\u8fc1\u79fb\u5230AzureSpringApps/\u5c06SpringCloud\u5e94\u7528\u8fc1\u79fb\u5230AzureSpringApps.md)\n\n## \u6587\u6863\u8d21\u732e\u8005\n\n\u5fae\u8f6fGPS China CSA\u56e2\u961f\u57fa\u4e8e\u5fae\u8f6f\u5b9e\u9a8c\u6587\u6863\u6307\u5f15\uff0c\u5f00\u53d1\u53ca\u6574\u7406\u3002\n\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "gps-csa-devhack-iaastopaas", "org_name": "microsoft", "org_repo": "microsoft/gps-csa-devhack-iaastopaas", "platform_org_repo": "github+microsoft/gps-csa-devhack-iaastopaas", "link_to_repo": "https://github.com/microsoft/gps-csa-devhack-iaastopaas", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "![BannerImage](images/EventCard_10banner.png)\n# .NET Conf 2022 Student Zone Content and Resources \n\n## Table of Contents\n- [.NET Conf 2022 Student Zone](#net-conf-2022-student-zone)\n  - [\u2b50November 7, 2022\u2b50](#november-7-2022)\n  - [Table of Contents](#table-of-contents)\n  - [What is the .NET Conf Student Zone?](#what-is-the-net-conf-student-zone)\n  - [When is the Student Zone?](#when-is-the-student-zone)\n  - [Register for the event](#register-for-the-event)\n  - [Join the challenge to win SWAG \ud83c\udf81\ud83c\udf89](#join-the-challenge-to-win-swag-)\n  - [Digital Swag for the .NET Conf Student Zone \ud83c\udf81\ud83c\udf89](#digital-swag-for-the-net-conf-student-zone-)\n- [More Event and Setup Information](#more-event-and-setup-information)\n  - [Agenda](#agenda)\n  - [Using this repo and development container](#using-this-repo-and-development-container)\n    - [GitHub Codespaces](#github-codespaces)\n    - [VS Code Remote - Containers](#vs-code-remote---containers)\n- [Student Resources](#student-resources)\n- [Learning Resources](#learning-resources)\n  - [Speakers](#speakers)\n- [Trademarks](#trademarks)\n\n## What is the .NET Conf Student Zone?\nAs part of [.NET Conf this year](https://www.dotnetconf.net/), we are hosting a .NET Student Zone on Monday, November 7! This is a livestreamed event where experts will introduce you to .NET and and build awesome, follow-along projects. You will walk away with a project portfolio on your very own portfolio website. In total the event will be 4+ hours of content.\n\n## Join the challenge to win SWAG \ud83c\udf81\ud83c\udf89\n[Join the .NET Conf Student Zone Microsoft Learn, Cloud Skills Challenge and Win Swag](http://aka.ms/dotnetstudententry )\nAfter enjoying the .NET Conf Student Zone, you will be ready to complete the .NET Conf Student Zone Cloud Skills Challenge. The challenge is open to all students who registered for .NET Conf on November 7 until November 29th and three lucky winners will get a .NET Conf SWAG bag!\n\n## Digital Swag for the .NET Conf Student Zone \ud83c\udf81\ud83c\udf89\n[Download .NET Bot digital pet for VSCode](https://aka.ms/vscodepet)\n\n![DotNetPet](/images/dotnetbotpet.jpg)\n\nPuts a small, .NET Bot, bored cat, an enthusiastic dog, a feisty snake, a rubber duck, or Clippy \ud83d\udcce in your VS Code editor to boost productivity.\n\n# More Event and Setup Information\n\n## Agenda\nIn each session, you will build an app or project to add to your .NET portfolio. You will build web apps, a mobile app, an ML project, and more!\n\n| Session Title | Speaker(s) | Tools | Session Code | Video | Presentation |\n|-------|---------|-----------|---|---|---|\n| Welcome to the Student Zone!| Scott Hanselman, Katie Savage  |  |  | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=90) | |\n| Create a GitHub Profile |Bethany Jepchumba  | GitHub | [Create a GitHub Profile](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Create%20a%20GitHub%20Profile) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=972) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Getting-up-and-running-with-GitHub.pdf) |\n| Build your Project Portfolio website with .NET | Matt Soucoup | Blazor | [Project Portfolio with Blazor Session](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Build%20your%20Project%20Portfolio%20website%20with%20.NET/README.md) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=2596) |[Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Build-a-portfolio-using-blazor.pdf) |\n| Detect water bottle consumption from IoT sensors | Krzysztof Wicher | IoT | [IOT Session](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Using%20IOT%20and%20.NET/README.md) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=5091) |[Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Using-IOT-and-NET.pdf) |\n| Use machine learning to estimate future water consumption  | Carlotta Castelluccio | ML.NET | [ML.NET Session](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Using%20ML.NET%20for%20Machine%20Learning/README.md) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=8147) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/DotNet_StudentZone%20-%20Use%20ML%20to%20estimate%20water%20consumption.pdf) |\n| Add a backend to your website  | Chris Noring | Minimal API | [Adding Backend](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Add%20a%20backend%20to%20your%20website/README.md)| [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=8147) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Add%20a%20backend%20to%20your%20website.pdf)|\n| Build a mobile app to track water consumption | Someleze Diko  | .NET MAUI | [using .NET MAUI to Build a Mobile App](/Using%20.NET%20MAUI%20to%20Build%20a%20Mobile%20App/Readme.md) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=11774) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Using-NET-MAUI-to-create-a-Mobile-App.pdf) |\n| Build a water consumption tracker website    | Justin Yoo | Blazor | [Blazor Session](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Using%20.NET%20Blazor%20to%20Build%20a%20Web%20App/README.md) | [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=14022) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Using-NET-Blazor-to-Build-a-Web-App.pdf) |\n| Ace your next assignment with .NET  | Diego Colombo | Visual Studio | [.NET and TDD Session](https://github.com/microsoft/dotnetconf-studentzone/tree/main/Next%20Steps%20Getting%20Top%20Marks%20for%20your%20next%20assignment/README.md)| [Watch On-Demand](https://youtu.be/SCJu7YPNtdQ?t=16397) | [Presentation](https://github.com/microsoft/dotnetconf-studentzone/blob/main/decks/Next%20Steps%20Getting%20Top%20Marks%20for%20your%20next%20assignment.pdf) |\n\n\n## Using this repo and development container\n\n### GitHub Codespaces\n\nFollow these steps to open this sample in a Codespace:\n1. Click the Code drop-down menu and select the **Open with Codespaces** option.\n1. Select **+ New codespace** at the bottom on the pane.\n\nFor more info, check out the [GitHub documentation](https://docs.github.com/en/free-pro-team@latest/github/developing-online-with-codespaces/creating-a-codespace#creating-a-codespace).\n\n### VS Code Remote - Containers\nFollow these steps to open this sample in a container using the VS Code Remote - Containers extension:\n\n1. If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in the [getting started steps](https://aka.ms/vscode-remote/containers/getting-started).\n\n2. To use this repository, you can either open the repository in an isolated Docker volume:\n\n    - Press <kbd>F1</kbd> and select the **Remote-Containers: Try a Sample...** command.\n    - Choose the \"Python\" sample, wait for the container to start, and try things out!\n        > **Note:** Under the hood, this will use the **Remote-Containers: Clone Repository in Container Volume...** command to clone the source code in a Docker volume instead of the local filesystem. [Volumes](https://docs.docker.com/storage/volumes/) are the preferred mechanism for persisting container data.   \n\n   Or open a locally cloned copy of the code:\n\n   - Clone this repository to your local filesystem.\n   - Press <kbd>F1</kbd> and select the **Remote-Containers: Open Folder in Container...** command.\n   - Select the cloned copy of this folder, wait for the container to start, and try things out!\n\n3. **Rebuild or update your container**\n\n   You may want to make changes to your container, such as installing a different version of a software or forwarding a new port. You'll rebuild your container for your changes to take effect. \n\n   **Open browser automatically:** As an example change, let's update the `portsAttributes` in the `.devcontainer/devcontainer.json` file to open a browser when our port is automatically forwarded.\n   \n   - Open the `.devcontainer/devcontainer.json` file.\n   - Modify the `\"onAutoForward\"` attribute in your `portsAttributes` from `\"notify\"` to `\"openBrowser\"`.\n   - Press <kbd>F1</kbd> and select the **Remote-Containers: Rebuild Container** or **Codespaces: Rebuild Container** command so the modifications are picked up.  \n\n# Student Resources \n1. [Microsoft Student Resources](http://aka.ms/learnstudent)  \n2. [.NET Student Zone Cloud Skills Challenge](http://aka.ms/dotnetstudentcsc)   \n3. [Azure for Student](http://aka.ms/azure4student) \n4. [GitHub Student Developer Pack](http://aka.ms/GitHubStudentPack) \n\n# Learning Resources\nWant more .NET Learning resources?\n1. [Learn more C# and .NET](https://aka.ms/mslearn-dotnet)    \n2. [Beginner video series](https://aka.ms/dotnetvideos)\n3. [.NET Learning Paths on Microsoft Learn](https://aka.ms/mslearn-dotnet) \n4. [Beginner Videos on Blazor, .NET MAUI, ML.NET, and more](http://aka.ms/blazorvideos)\n5. [.NET Documentation](http://aka.ms/dotnetdoc) \n6. [.NET MAUI](http://aka.ms/MAUIcrossplat) \n7. [Blazor Web Applications](http://aka.ms/blazorwebapp) \n8. [Minimal APIs](http://aka.ms/dotnetminimalapi) \n9. [ML.NET](http://aka.ms/trainmldotnet) \n10. [.NET IoT](http://aka.ms/dotnetIOT) \n\n\nWatch [.NET Conf](https://www.dotnetconf.net/)! Sessions start November 8th.\n\n## Speakers\n\n<b>Scott Hanselman</b>\n>Scott has been a developer for 30 years and has been blogging at hanselman.com for 20 years! He works in Open Source on .NET and the Azure Cloud for Microsoft out of his home office in Portland, Oregon. Scott has been podcasting for over 800 episodes of hanselminutes.com over 15 years and 700 episodes of azurefriday.com. He's written a number of technical books and spoken in person to over one million developers worldwide! He's also on TikTok, which was very likely a huge mistake.\n\n<b>Katie Savage </b>\n>Katie is a Product Manager on the DevDiv Community Team at Microsoft. Her focus is on students, career switchers, and new developers using C# and .NET! Before joining the Community Team, Katie was involved in Computer Science education as an intern with Microsoft MakeCode. These experience, as well as her involvement in Girls Who Code, have grown Katie\u2019s passion for Computer Science education.\n\n<b>Bethany Jepchumba</b>\n>Bethany Jepchumba is an Academic Cloud Advocate at Microsoft, focused on Data Machine Learning and AI. Prior to joining the role, she was a Gold Microsoft Learn Student Ambassador. She comes from a community of marathon runners but decided to run code instead.\n\n<b>Matt Soucoup</b>\n>Matt is a Principal Cloud Developer Advocate for .NET at Microsoft spreading the love of developing for Azure with .NET. Matt's been a professional developer for over 20 years and wants to make your experience creating apps delightful. Matt blogs at codemillmatt.com, podcasts at dotnetmauipodcast.com and is just a Bing search away.\n\n<b>Krzysztof Wicher</b>\n>Krzysztof is a developer on the .NET team, currently working on System.Text.Json and one of the owners of the .NET IoT v-team. He's also one of the people who make .NET more secure. Before joining Microsoft, he studied control engineering and robotics.\n\n<b>Carlotta Castelluccio</b>\n>Carlotta Castelluccio is an Academic Cloud Advocate at Microsoft, focused on Machine Learning and AI. She works on skilling and engaging educational communities to create and grow with Azure Cloud, by contributing to technical learning content and supporting students and educators in their learning journey with Microsoft technologies. Before joining the Cloud Advocacy team, she has been working as an Azure and AI consultant in Microsoft Industry Solutions team, involved in customer-face engagements focused on Conversational AI solutions.\n\n<b>Chris Noring</b>\n>Chris is a Senior Academic cloud advocate at Microsoft, focused on App Dev. Chris also manages the feedback process as well as the academic learn portfolio. He\u2019s a prolific speaker and published author on Go and JavaScript.\n\n<b>Someleze Diko</b>\n>Someleze is a Microsoft Academic Cloud Advocate that is enthusiastic about upskilling people from different communities using the different technologies at his disposal through being involved with initiatives that upskill and empower people.\n\n<b>Justin Yoo</b>\n>Justin is a Senior Cloud Advocate at Microsoft, specializing in Azure, .NET and Fusion Development. His main interests are app modernisation using Azure PaaS, Serverless, .NET and Power Platform.\n\n<b>Diego Colombo</b>\n>I have spent most of my life building tools and frameworks for a wide set of industries: robotics, video games, finance, and developer tools. Creating next generation tools to enable developers and researcher to achieve their goals is my drive and passion, today I am lucky enough to work with the .NET Interactive team on modern developer experiences. I have contributed to the Microsoft XNA framework, Microsoft Robotics Studio, .NET interactive and other initiatives. My Academic background is rooted in Robotics and Realtime graphics, with a PhD on Realtime metaprogramming, I am still active giving some guest lectures and collaborating with external research partners on scientific publications. I have studied in Pisa and IMT Lucca and worked in very diverse companies, from start-up to corporation. Today I work on .NET Interactive and Polyglot Notebooks, bringing new workflows and tools to my friends out there. The .NET is vast and infinite.\n\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dotnetconf-studentzone", "org_name": "microsoft", "org_repo": "microsoft/dotnetconf-studentzone", "platform_org_repo": "github+microsoft/dotnetconf-studentzone", "link_to_repo": "https://github.com/microsoft/dotnetconf-studentzone", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 93, "watchers_count": 93}, {"README_text": "# aka.ms/commands\n\nThis page lists popular aka.ms links for Microsoft portal pages.\n\nHere is how it works. Open a new tab and type [aka.ms/ad/{command}](https://aka.ms/command) using any of the commands below. \n\n:rocket: Don't forget to super charge this with the [Address bar search](#search-bar-support) tip below.\n\nGet to this page by browsing to [aka.ms/commands](https://aka.ms/commands).\n\nPS: For a comprehensive list and a shorter command line including a browser extension see [cmd.ms](https://cmd.ms)\n\n## Azure Active Directory - Admins\n\n|aka.ms|Command|Portal Blade|\n|-----|----|---|\n|[aka.ms/ad/ca](https://aka.ms/ad/ca)|ca|Conditional Access|\n|[aka.ms/ad/cawhatif](https://aka.ms/ad/cawhatif)|cawhatif|Conditional Access What If|\n|[aka.ms/ad/pim](https://aka.ms/ad/pim)|pim|Privileged Identity Management|\n|[aka.ms/ad/users](https://aka.ms/ad/users)|users|Users|\n|[aka.ms/ad/groups](https://aka.ms/ad/groups)|groups|Groups|\n|[aka.ms/ad/devices](https://aka.ms/ad/devices)|devices|Devices|\n|[aka.ms/ad/apps](https://aka.ms/ad/apps)|apps|Enterprise Applications|\n|[aka.ms/ad/appreg](https://aka.ms/ad/appreg)|appreg|Application Registrations|\n|[aka.ms/ad/auth](https://aka.ms/ad/auth)|auth|Authentication Methods Policies|\n|[aka.ms/ad/legacymfa](https://aka.ms/ad/legacymfa)|legacymfa|Legacy MFA|\n|[aka.ms/ad/guests](https://aka.ms/ad/guests)|guests|Guest Access Settings|\n|[aka.ms/ad/logs](https://aka.ms/ad/logs)|logs|Sign in Logs|\n|[aka.ms/ad/xtap](https://aka.ms/ad/xtap)|xtap|Cross Tenant Access Settings|\n|[aka.ms/ad/roles](https://aka.ms/ad/roles)|roles|Azure AD Roles|\n|[aka.ms/ad/sspr](https://aka.ms/ad/sspr)|sspr|Password Reset|\n|[aka.ms/ad/security](https://aka.ms/ad/security)|security|Security|\n|[aka.ms/ad/mfaunblock](https://aka.ms/ad/mfaunblock)|mfaunblock|MFA Unblock|\n|[aka.ms/ad/reviews](https://aka.ms/ad/reviews)|reviews|Access Reviews|\n|[aka.ms/ad/score](https://aka.ms/ad/score)|score|Secure Score|\n|[aka.ms/ad/license](https://aka.ms/ad/license)|license|Licenses|\n|[aka.ms/ad/synclog](https://aka.ms/ad/synclog)|synclog|AAD Connect Sync Errors|\n|[aka.ms/ad/adfslog](https://aka.ms/ad/adfslog)|adfslog|ADFS Log|\n|[aka.ms/ad/consent](https://aka.ms/ad/consent)|consent|Consents and Permissions|\n|[aka.ms/ad/support](https://aka.ms/ad/support)|support|Support|\n|[aka.ms/ad/list](https://aka.ms/ad/list)|list|List all these shortcuts|\n\n## Microsoft Admin Portals\n\n|aka.ms|Command|Page|\n|-----|----|---|\n|[aka.ms/admin](https://aka.ms/admin)|admin|[M365 Admin Portal](https://admin.microsoft.com)|\n|[aka.ms/azad](https://aka.ms/azad)|azad|[Azure AD Portal](https://portal.azure.com)|\n|[aka.ms/ge](https://aka.ms/ge)|ge|[Graph Explorer](https://developer.microsoft.com/graph/graph-explorer)|\n|[aka.ms/intune](https://aka.ms/intune)|intune|[Intune](https://endpoint.microsoft.com)|\n|[aka.ms/ppac](https://aka.ms/ppac)|ppac|[Power Platform](https://admin.powerplatform.microsoft.com/)|\n\n## Microsoft Intune Portals\n\n|aka.ms|Command|Page|\n|-----|----|---|\n|[aka.ms/in](https://aka.ms/in)|in|Intune admin center|\n|[aka.ms/intuneshd](https://aka.ms/intuneshd)|intuneshd|Intune service health|\n|[aka.ms/intunesupport](https://aka.ms/intunesupport)|support|Get Intune Support|\n|[aka.ms/enrollmymac](https://aka.ms/enrollmymac)|enrollmymac|Download the Intune Company Portal for Macs|\n\n## Microsoft 365 Defender\n\n|aka.ms|Command|Portal Blade|\n|-----|----|---|\n|[aka.ms/de](https://aka.ms/de)|de|Microsoft 365 Defender|\n|[aka.ms/de/incidents](https://aka.ms/de/incidents)|incidents|Incidents|\n|[aka.ms/de/hunting](https://aka.ms/de/hunting)|hunting|Hunting|\n|[aka.ms/de/actions](https://aka.ms/de/actions)|actions|Action Center|\n|[aka.ms/de/explorer](https://aka.ms/de/explorer)|explorer|Explorer|\n\n## Microsoft User Portals\n\n|aka.ms|Page|\n|-----|---|\n|[aka.ms/sspr](https://aka.ms/sspr)|Self Service Password Reset|\n|[aka.ms/mysecurity](https://aka.ms/mysecurity)|My Security|\n|[aka.ms/myapps](https://aka.ms/myapps)|My Apps|\n|[aka.ms/my-account](https://aka.ms/my-account)|My Account|\n|[aka.ms/my-groups](https://aka.ms/my-groups)|My Groups|\n|[aka.ms/my-access](https://aka.ms/my-access)|My Access Packages|\n|[aka.ms/mystaff](https://aka.ms/mystaff)|My Access Packages|\n|[aka.ms/mfasetup](https://aka.ms/mfasetup)|Alternative for My Security|\n\n## Identity Protection\n|aka.ms|Page|\n|-----|---|\n|[aka.ms/identityprotection](https://aka.ms/identityprotection)|Identity Protection|\n\n## Miscellaneous\n|aka.ms|Page|\n|-----|---|\n|[aka.ms/entradeprecations](https://aka.ms/entradeprecations)|Entra/Azure AD related retirements/deprecations|\n|[aka.ms/entratemplates](https://aka.ms/entratemplates)|Email templates & posters to roll out Azure Active Directory features|\n\n\n## Search Bar Support\n\nTo reduce typing aka.ms/ all the time you can add a prefix to make it super quick to get to the aka.ms links.\n\nOnce set up all you have to do is \n* Open a new tab\n* Type **a** and press **SPACE** or **TAB** \n* Type in the aka.ms shortcut. eg. **ad/users**\n\n![Search bar demo](/images/SearchBarDemo.gif)\n\n### Microsoft Edge\n\n* Go to **Settings** > **Privacy, Search and Services** > Scroll all the way down to **Services**\n* Select **Address bar and search** > **Manage search engines**\n* Select **Add**\n* Fill in the following\n  * Search engine: **aka.ms**\n  * Shortcut: **a** (You can choose any prefix you like)\n  * Url: **https://aka.ms/%s**\n* Select **Add**\n\nA quick alternative to get to the **Manage search enginess** page is to search for **Manage search engines** under **Settings** and select **Address bar search**\n\n<img src=\"/images/configedge.png\" alt=\"Screenshot of Edge config of address bar\" width=\"500\" >\n\n### Google Chrome\n\n* Go to **Settings** > **Search engine** > **Manage search engines and site search**\n* Scroll down to **Site Search**\n* Select **Add**\n* Fill in the following\n  * Search engine: **aka.ms**\n  * Shortcut: **a** (You can choose any prefix you like)\n  * Url: **https://aka.ms/%s**\n* Select **Add**\n\n### Firefox\n\n* Press Ctrl+B/Cmd+B to open the **Bookmarks Sidebar**\n* Right click **Other Bookmarks** and select **Add Bookmark**\n* Fill in the following:\n  * Name: **aka.ms**\n  * URL: **https://aka.ms/%s**\n  * Keyword: **a** (You can choose any keyword you like)\n* Select **Save**\n\n## Suggestions\n\nNotice anything missing or have suggestions? Please create an issue, submit a PR or reach out to [@merill](https://twitter.com/merill) on [Twitter](https://twitter.com/merill).\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aka", "org_name": "microsoft", "org_repo": "microsoft/aka", "platform_org_repo": "github+microsoft/aka", "link_to_repo": "https://github.com/microsoft/aka", "platform": "github", "language": null, "stargazers_count": 234, "watchers_count": 234}, {"README_text": "# What is World Locking Tools\nWorld Locking Tools provides a stable and reliable world-locked coordinate system, binding the virtual/holographic world to the physical world.\n\nWorld Locking Tools locks the holographic space of your application to the physical world. A hologram put in position relative to physical world features will stay fixed relative to those features, and remain fixed relative to other holograms.\n\nWorld Locking Tools also includes Space Pins, which reposition the scene to match real-world features like QRCodes.\n\n\n# Using the Plugin\nAdd the plugin to your project from either GitHub or the Marketplace and enable it in the Plugins window.  Call the \"StartWorldLockingTools\" function to start using world locking tools with the given parameters.\n\nThis plugin was written for HoloLens, and is meant to be used with the [Microsoft OpenXR](https://github.com/microsoft/Microsoft-OpenXR-Unreal) plugin.  The underlying FrozenWorld binaries that world locking tools depends on are available for Android and iOS, and this plugin uses ARBlueprintLibrary when using core AR features like ARPins, so this can be modified to build for ARKit and ARCore as well.\n\n\n## From GitHub source\nCopy the WLT_Project/Plugins/WorldLockingTools directory into your game's Plugins directory.  Reopen your project and enable the World Locking Tools plugin in your Plugins window.\n\n\n## World Locking Tools Pawn\nWorld Locking Tools works by repositioning the users' head to match a pose the underlying frozen world system is generating from nearby anchors that have been created while walking around.  To do this, the camera on the pawn must have a parent and grandparent component that are children of the DefaultSceneRoot.  The sample's WLTPawn is setup this way, and one is also included in the plugin under Source/WorldLockingTools/Public/WorldLockingToolsPawn.h.\n\n\n## Space Pins\nSpace Pins have a virtual position in your scene and need to be given a physical position in your world to reposition the scene to match the real-world.  This is accomplished by calling SetSpongyPose on a SpacePin with a transform in the device's tracking space.  When multiple space pins are locked in a scene, the relative real-world offsets between them should closely match how they are setup in the virtual scene to minimize drift as you walk from one space pin to another.\n\n\n## Try the example project\nThe sample project has two levels which demonstrate starting world locking tools, and using space pins with either hand manipulation in one level or QRCodes in the other level.  Both levels have an 8 meter long hologram with a space pin on either side.  Build, deploy, and run the sample on a HoloLens and take note of the direction the hologram is facing.  Close the application, rotate your body to face a different direction, then relaunch the application.\n\nWithout using world locking tools, the scene will face the new direction you are facing.  With world locking tools, the scene will be oriented based on the first saved launch.\n\nTest the space pins by grabbing the cube on one end of the 8 meter hologram.  It will turn pink when it can be interacted with.  Face the direction of the white arrow off of this cube and move your hand around while gripped to move the space pin around.  The entire scene will move around as the space pin moves.  Walk to the other space pin, face the same direction as its white arrow, grip and move this space pin around too.  Once both space pins have been positioned, walk between the two and observe that the scene will shift into place to match the space pin's locked real-world position as you get closer to it.\n\nThe second scene does the same thing, but positions the space pins based on the QRCodes in the Images directory.\n\n\n# See Also\n[World Locking Tools](https://learn.microsoft.com/en-us/mixed-reality/world-locking-tools/)\n\n[Space Pins](https://learn.microsoft.com/en-us/mixed-reality/world-locking-tools/documentation/concepts/advanced/spacepins)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "WorldLockingTools-Unreal", "org_name": "microsoft", "org_repo": "microsoft/WorldLockingTools-Unreal", "platform_org_repo": "github+microsoft/WorldLockingTools-Unreal", "link_to_repo": "https://github.com/microsoft/WorldLockingTools-Unreal", "platform": "github", "language": "C++", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# MPNODE.jl \n\nThis repo contains the code associated with the paper [Learning Modular Simulations for Homogenous Networks](https://arxiv.org/abs/2210.16294). Also see the [blogpost](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/towards-modular-data-driven-simulations/).\n\n![image](https://user-images.githubusercontent.com/1785175/204102864-29c46654-9c51-408c-95d1-586bbd5d30f4.png)\n\n## Prerequisites\n\nFirst install Julia from https://julialang.org/downloads/ and ensure that it's available on your `$PATH`. Currently only tested on Ubuntu 18.04 and Julia v1.6.\n\n## Usage\n\nAfter cloning the repository, run:\n\n```bash\njulia --project=exps 'using Pkg; Pkg.instantiate()'\n```\n\n`train.jl` allows training a particular model defined in the config on a particular system dataset.\n\nFor example, to train on the system `default_lorenz3` system on the L2S model:\n```bash\njulia --project=exps exps/train.jl default_lorenz3 --conf configs/gnnsc_de0_sys_32.toml --evalconf configs/evaltemplate.toml --savedir /tmp/mpdifflogs/l2strial\n```\nor on the MP-NODE model:\n```bash\njulia --project=exps exps/train.jl default_lorenz3 --conf configs/empode_sys_32_3.toml --evalconf configs/evaltemplate.toml --savedir /tmp/mpdifflogs/mpnodetrial\n```\n\n## Code overview\n\n- `src/systems`: contains the systems we evaluate the MP-NODE and other baselines on.\n- `src/datagen.jl`: contains utilities to generate data from the systems.\n- `src/empde.jl`: contains the pieces required to setup MP-NODE models.\n- `src/gnnode.jl`: contains the pieces required to setup L2S models.\n- `src/nde.jl`: contains the pieces required to setup NODE models.\n- `src/learn.jl` and `src/learn_utils.jl`: contains the model training utilities.\n- `exps/models.jl`: contains the specific models used in the paper.\n- `exps/train_pipeline.jl`: contains orchestration for full training pipeline.\n- `exps/tasks.jl`: contains the specific systems used in the paper.  \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MPNODE.jl", "org_name": "microsoft", "org_repo": "microsoft/MPNODE.jl", "platform_org_repo": "github+microsoft/MPNODE.jl", "link_to_repo": "https://github.com/microsoft/MPNODE.jl", "platform": "github", "language": "Julia", "stargazers_count": 28, "watchers_count": 28}, {"README_text": "# did:x509\n\nThis repository contains the DRAFT specification of the did:x509 [DID](https://www.w3.org/TR/did-core/) method. It aims to achieve interoperability between existing X.509 solutions and Decentralized Identifiers (DIDs) to support operational models in which a full transition to DIDs is not achievable or desired yet.\n\nNOTE: This specification is in its early development and is published to invite feedback from the community. Please contribute by opening issues and pull requests!\n\n## Specification\n\nSee [specification.md](specification.md).\n\n## Reference implementation\n\nThis repository contains a non-production reference implementation written in Python.\n\nFirst, install the required Python packages:\n\n```\npip install -r requirements.txt\n```\n\nThen, run the resolver with an example DID and matching certificate chain:\n\n```sh\npython did_x509.py resolve did:x509:0:sha256:hH32p4SXlD8n_HLrk_mmNzIKArVh0KkbCeh6eAftfGE::subject:CN:Microsoft%20Corporation --chain test-data/ms-code-signing.pem\n# Output: { <DID document> }\n```\n\nTo convert a certificate chain to the JSON data model defined in the specification, run:\n\n```sh\npython did_x509.py convert test-data/ms-code-signing.pem\n# Output: [ Certificate chain in JSON ]\n```\n\nTo percent-encode a string for use in policies, run:\n\n```sh\npython did_x509.py encode \"My Org\"\n# Output: My%20Org\n```\n\nRun tests with:\n\n```\npytest -v test.py\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Please see the [Contribution guidelines](CONTRIBUTING.md).\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "did-x509", "org_name": "microsoft", "org_repo": "microsoft/did-x509", "platform_org_repo": "github+microsoft/did-x509", "link_to_repo": "https://github.com/microsoft/did-x509", "platform": "github", "language": "Python", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Fault-aware Neural Code Rankers\n\n> This repo has the code accompanying the CodeRanker NeurIPS'22 [paper](https://arxiv.org/pdf/2206.03865.pdf). \n\n## Installation\n```\npip install -r requirements.txt\n```\n\n## Dataset \nThe ranker datasets are available through GIT LFS in dataset/ directory\n\n\n## Usage\nFirst, cd into src directory\n\n1. Training rankers\n```\nbash finetune.sh DATADIR MODELDIR CACHEDIR TASK\n```\nwhere DATADIR is the location of the directory containing the desired ranker dataset  \nMODELDIR is the location of the output dirctory where the trained model will be stored  \nCACHEDIR is the location of the cache directory both for caching the model and for caching the dataset  \nTASK is one of binary, ternary, intent_error, execution_error, execution_error_with_line\n\n2. Inference with rankers\n```\nbash eval.sh DATADIR MODELDIR CACHEDIR TASK {val|test}.json PREDICT_FILENAME\n```\nwhere PREDICT_FILENAME is the name of the file inside MODELDIR where the inferenced logits will be stored. \n\n3. Computing the ranked metrics\n```\npython3 compute_metrics.py --data_file=DATADIR/{val|test}.json --logits_prediction_file=MODELDIR/PREDICT_FILENAME --labels_file=DATADIR/labels_TASK.txt --task=TASK\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CodeRanker", "org_name": "microsoft", "org_repo": "microsoft/CodeRanker", "platform_org_repo": "github+microsoft/CodeRanker", "link_to_repo": "https://github.com/microsoft/CodeRanker", "platform": "github", "language": "Python", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "![Banner](docs/images/MLOps_for_databricks_Solution_Acclerator_logo.JPG)\n\n---\n---\n <br>\n\n\n # Version History And Updates \n\n## Azure ML Integration Now Live For GitHub Deployment.\nDevelopment for Azure DevOps Deployment in Progress (expect instability until end of April)\n\n # MLOps for Databricks with CI/CD (GitHub Actions)\n---\n \n ## MLOps Architecture\n\n![image](https://user-images.githubusercontent.com/108273509/207945308-14e4794e-e86b-4bee-aa21-088698983703.png)\n\nFeatures to be included in future releases:\n- Model testing & promotion \n- Metrics & Monitoring \n---\n\n\n## Youtube Demo - Slightly Outdated\n\nThe deployment instructions for the video are slightly outdated (albeit still usefull). \nPlease follow instructions below instead. The video still provides useful content for concepts outwith the deployment. \n\n[![Youtube Demo](docs/images/YoutubeThumbNail.png)](https://youtu.be/g57N3GYXuDI)\n\n---\n\n## About This Repository\n\nThis Repository contains an Azure Databricks Continuous Deployment _and_ Continuous Development Framework for delivering Data Engineering/Machine Learning projects based on the below Azure Technologies:\n\n\n\n| Azure Databricks | Azure Log Analytics | Azure Monitor Service  | Azure Key Vault        |\n| ---------------- |:-------------------:| ----------------------:| ----------------------:|\n\n\n\nAzure Databricks is a powerful technology, used by Data Engineers and Scientists ubiquitously. However, operationalizing it within a Continuous Integration and Deployment setup that is fully automated, may prove challenging. \n\nThe net effect is a disproportionate amount of the Data Scientist/Engineers time contemplating DevOps matters. This Repository's guiding vision is to automate as much of the infrastructure as possible.\n\n---\n---\n\n## Prerequisites\n<details open>\n<summary>Click Dropdown... </summary>\n<br>\n  \n- Github Account\n- Microsoft Azure Subscription\n- VS Code\n- Azure CLI Installed (This Accelerator is tested on version 2.39)\n\n</details>\n\n---\n---\n\n## Details of The Solution Accelerator\n\n- Creation of four environments:\n  - Sandbox\n  - Development \n  - User Acceptance Testing (UAT)\n  - Production\n- Full CI/CD between environments\n- Infrastructure-as-Code for interacting with Databricks API and also CLI\n- Azure Service Principal Authentication\n- Azure resource deployment using BICEP\n- Databricks Feature Store + MLFlow Tracking + Model Registry + Model Experiments\n- DBX by Data Labs for Continuous Deployment of Jobs/Workflows (source code/ parameters files packaged within DBFS)\n\n\n---\n# Deployment Instructions \n\n## Create Repository\n<details open>\n<summary>Click Dropdown... </summary>\n<br>\n  \n- Fork this repository [here](https://github.com/microsoft/dstoolkit-mlops-databricks/fork) \n- In your Forked Repo, click on 'Actions' and then 'Enable'\n- Within your VS Code click, \"View\", then \"Command Pallette\", \"Git: Clone\", and finally select your Repo\n</details>\n\n---\n \n## Login To Azure\n- All Code Throughout To Go Into VS Code **PowerShell Terminal** \n\n ```ps\naz login\n\n# If There Are Multiple Tenants In Your Subscription, Ensure You Specify The Correct Tenant \"az login --tenant\"\n\n# ** Microsoft Employees Use: az login --tenant fdpo.onmicrosoft.com (New Non Prod Tenant )\n\n```\n\n## GitHub Account\n```ps\necho \"Enter Your Git Username... \"\n# Example: \"Ciaran28\"\n$Git_Configuration = \"GitHub_Username\"\n```\n\n## GitHub Repos Within Databricks\n  ```ps\necho \"Enter Your Git Repo Url (this could be any Repository In Your Account )... \"\n# Example: \"https://github.com/ciaran28/dstoolkit-mlops-databricks\" \n$Repo_ConfigurationURL = \"\"\n```\n\n## Updates Parameter Files & Git Push To Remote\n  ```ps\necho \"From root execute... \"\n\n./setup.ps1\n\n\n```\n---\n\n## Create Environments \nFollow the naming convention (case sensitive)\n<img width=\"971\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/205917146-a7deb2ae-674a-4ec1-a9b8-4859bcdce25f.png\">\n\n\n## Secrets\n\n**For each environment** create GitHub Secrets entitled **ARM_CLIENT_ID**, **ARM_CLIENT_SECRET** and **ARM_TENANT_ID** using the output in VS Code PowerShell Terminal from previous step.\n(Note: The Service Principal below was destroyed, and therefore the credentials are useless )\n\n<img width=\"656\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/194619649-2ef7e325-a6bb-4760-9a82-1e3b4775adbd.png\">\n\nIn addition generate a GitHub Personal Access Token and use it to create a secret named **PAT_GITHUB**:\n\n<img width=\"883\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/205918329-9592e20f-439b-4e1b-b7c4-983579e295de.png\">\n \n---\n---\n\n\n \n## Final Snapshot of GitHub Secrets\n\nSecrets in GitHub should look exactly like below. The secrets are case sensitive, therefore be very cautious when creating. \n\n<img width=\"587\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/205921220-9ad2116a-7c85-4725-a70c-e178a0af2914.png\">\n\n\n---\n---\n \n## Deploy The Azure Environments \n\n- In GitHub you can manually run the pipeline to deploy the environments to Azure using \"onDeploy.yaml\" found [here](.github/workflows/onDeploy.yaml). Use the instructions below to run the workflow.\n\n<img width=\"893\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/205954210-c123c407-4c83-4952-ab4b-cd6c485efc2f.png\">\n\n- Azure Resources created (Production Environment snapshot - For speed I have hashed out all environment deployments except Sandbox. Update onDeploy.yaml to deploy all environments)\n  \n<img width=\"1175\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/194638664-fa6e1809-809e-45b2-9655-9312f32f24bb.png\">\n\n\n---\n---\n\n## Running Pipelines\n\n- The end to end machine learning pipleine will be pre-configured in the \"workflows\" section in databricks. This utilises a Job Cluster which will automatically upload the necessary dependencies contained within a python wheel file \n\n- If you wish to run the machine learning scripts from the Notebook instead, first upload the dependencies (automatic upload is in development). Simply navigate to python wheel file contained within the dist/ folder. Manually upload the python wheel file to the cluster that you wish to run for the Notebook. \n\n---\n---\n\n ## Continuous Deployment And Branching Strategy\n\nThe Branching Strategy I have chosen is configured automatically as part of the accelerator. It follows a GitHub Flow paradigm in order to facilitate rapid Continuous Integration, with some nuances. (see Footnote 1 which contains the SST Git Flow Article written by Willie Ahlers for the Data Science Toolkit - This provides a narrative explaining the numbers below)[^1]\n\n\nThe branching strategy is easy to change via updating the \"if conditions\" within .github/workflows/onRelease.yaml.\n\n<img width=\"805\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/186166011-527144d5-ebc1-4869-a0a6-83c5538b4521.png\">\n\n-   Pull Request from Feature Branch to Main Branch: C.I Tests\n-   Pull Request approved from Feature Branch to Main Branch: C.D. to Development Environment \n-   Pull Request from Main Branch to Release Branch: C.I. Test\n-   Pull Request approved from Main Branch to Release Branch: C.D. to User Acceptance Testing (UAT) Environment\n-   Tag Version and Push to Release Branch: C.D. to Production Environment \n- Naming conventions for branches (to ensure the CD pipelines will deploy - onRelease.yaml for more details ):\n  - Feature Branches: \"feature/<insertname>\"\n  - Main Branch: \"main\"\n  - Release branch \"release/<insertname>\"\n\n---\n---\n## MLOps Paradigm: Deploy Code, not Models\n\nIn most situations, Databricks recommends that during the ML development process, you promote code, rather than models, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see Model deployment patterns.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/machine-learning/mlops/deployment-patterns\n\n<img width=\"427\" alt=\"image\" src=\"https://user-images.githubusercontent.com/108273509/211937862-2aaf118f-85c1-4d98-af75-56628837ffa4.png\">\n\n\n---\n---\n## Feature Store Integration \n\nIn an organization, thousands of features are buried in different scripts and in different formats; they are not captured, organized, or preserved, and thus cannot be reused and leveraged by teams other than those who generated them.\n\nBecause feature engineering is so important for machine learning models and features cannot be shared, data scientists must duplicate their feature engineering efforts across teams.\n\nTo solve those problems, a concept called feature store was developed, so that:\n\n- Features are centralized in an organization and can be reused\n- Features can be served in real-time with low latency\n\n![image](https://user-images.githubusercontent.com/108273509/216114586-0c4dea68-a98c-4cf6-938a-ceecf11b12a8.png)\n\n", "repo_name": "dstoolkit-mlops-databricks", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-mlops-databricks", "platform_org_repo": "github+microsoft/dstoolkit-mlops-databricks", "link_to_repo": "https://github.com/microsoft/dstoolkit-mlops-databricks", "platform": "github", "language": "Python", "stargazers_count": 44, "watchers_count": 44}, {"README_text": "# bonsai-sample-playbook\nThe Bonsai Sample Project Playbook intends to provide an overview of our approach for the successful delivery of a sample project. \nThis site aggregates best practices and resources for project scoping and delivery.\n\nView the playbook at https://microsoft.github.io/bonsai-sample-playbook/\n", "repo_name": "bonsai-sample-playbook", "org_name": "microsoft", "org_repo": "microsoft/bonsai-sample-playbook", "platform_org_repo": "github+microsoft/bonsai-sample-playbook", "link_to_repo": "https://github.com/microsoft/bonsai-sample-playbook", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Damage Assessment Visualizer\n\nThe Damage Assessment Visualizer leverages satellite imagery from a disaster region to visualize conditions of building and structures before and after a disaster. It includes a visual layer on top of the satellite images which predicts the extent of damage of individual structures. This information provides critical information for frontline workers on the ground to assist with logistics operations after a disaster hits.\n\n# Building the Visualizer\n\n**Jump to: [Setup](#setup) | [Tutorial](#tutorial)**\n\nThis is a simple web-based visualizer for comparing satellite imagery from two points in time with a modeled building damage overlay.\n\n![](images/example_screenshot.png)\n\n## Setup\n\nThe tool uses config files passed via the \"config\" URL parameter to run different \"instances\", e.g.: the URL `https://server.com/?config=new_orleans_example.json` will load the \"new_orleans_example.json\" file.\n\n### Config file example\n\nThe below shows the format of the config file used to instantiate an instance of the tool.\n\n```js\n{\n    \"preImageryLayer\": {\n        \"basemapURL\": \"http://server.com/basemap-pre/{z}/{x}/{y}.png\", // URL for the \"pre\" imagery XYZ tiles\n        \"date\": \"January 1st, 2021\", // date assosciated with the \"pre\" imagery\n        \"attribution\": \"\", // string representation of the image attribution\n        \"bounds\": [[lat1, lon1], [lat2, lon2]] // the bounding box for which the basemap is valid\n    },\n    \"postImageryLayer\": {\n        \"basemapURL\": \"http://server.com/basemap-pre/{z}/{x}/{y}.png\", // URL for the \"post\" imagery XYZ tiles\n        \"date\": \"January 2nd, 2021\", // date assosciated with the \"post\" imagery\n        \"attribution\": \"\", // string representation of the image attribution\n        \"bounds\": [[lat1, lon1], [lat2, lon2]] // the bounding box for which the basemap is valid\n    },\n    \"changeImageryLayer\": {\n        \"basemapURL\": \"http://server.com/basemap-change/{z}/{x}/{y}.png\", // URL for the \"change\" imagery XYZ tiles\n        \"bounds\": [[lat1, lon1], [lat2, lon2]] // the bounding box for which the basemap is valid\n    },\n    \"center\": [lat, lon], // the latitude and longitude of the initial map view\n    \"initialZoom\": 12, // initial zoom level of the initial map view\n    \"location\": \"...\", // the location that this file represents\n    \"imageryAttribution\": \"...\", // HTML string for attributing the source imagery\n    \"license\": \"...\"  // HTML string for the source imagery license\n}\n```\n\n## Tutorial\n\nThis section provides detailed instruction on how to set up a demo instance of this tool using high-resolution imagery from the USDA's National Agriculture Imagery Program (NAIP).\nWe use this NAIP imagery simply for example purposes, it is not practically useful for distaster response applications as it is only collected once every two years on a state-by-state basis in the US. That said, it is freely available on the [Planetary Computer](https://planetarycomputer.microsoft.com/) and will allow us to easily demo the steps needed to set up the web interface with new imagery from scratch.\n\nThe following steps assume that you are running a linux version of the [Data Science Virtual Machine (DSVM)](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) instance in Microsoft Azure and know the hostname/IP address of the machine. The steps to setup an example instance of the visualizer are as follows:\n\n1. Clone the repo\n\n```bash\ngit clone https://github.com/microsoft/Nonprofits.git nonprofits\ncd nonprofits/visualizer/\n```\n\n2. Create a conda environment that contains the necessary packages (particularly GDAL).\n\n```bash\nconda config --set channel_priority strict\nconda env create --file environment.yml\nconda activate visualizer\n```\n\n3. Download the example NAIP data. We use NAIP scenes from 2013 and 2019 that overlap the Microsoft Redmond campus. These are formatted as [GeoTIFFs](https://en.wikipedia.org/wiki/GeoTIFF), a common image format for satellite or aerial imagery.\n\n```bash\nwget https://naipeuwest.blob.core.windows.net/naip/v002/wa/2013/wa_100cm_2013/47122/m_4712223_se_10_1_20130910.tif\nwget https://naipeuwest.blob.core.windows.net/naip/v002/wa/2019/wa_60cm_2019/47122/m_4712223_se_10_060_20191011.tif\n```\n\n4. In the coming steps we will want to render our scenes using the `gdal2tiles.py` command which requires 3-channel (RGB) data formatted as \"Bytes\", so we need to preprocess the data we have into this format. The example NAIP data comes as 4-channel GeoTIFFs with \"Byte\" data types already -- to see this for one of the scenes you can run `gdalinfo m_4712223_se_10_1_20130910.tif` -- so we will just need to extract the RGB bands into their own file. Other sources of imagery may have different data types, different channels orderings, etc. and require more preprocessing. We use [gdal_translate](https://gdal.org/programs/gdal_translate.html) to do this preprocessing:\n\n```bash\ngdal_translate -b 1 -b 2 -b 3 -co BIGTIFF=YES -co NUM_THREADS=ALL_CPUS -co COMPRESS=LZW -co PREDICTOR=2 m_4712223_se_10_1_20130910.tif 2013_rgb.tif\ngdal_translate -b 1 -b 2 -b 3 -co BIGTIFF=YES -co NUM_THREADS=ALL_CPUS -co COMPRESS=LZW -co PREDICTOR=2 m_4712223_se_10_060_20191011.tif 2019_rgb.tif\nrm m_4712223_se_10_1_20130910.tif m_4712223_se_10_060_20191011.tif\n```\n\n5. Next, we need to make sure that the data is _pixel-aligned_ over the same area on Earth. To do this we crop and resample the post-imagery to the same spatial extent and pixel resolution as the pre-imagery.\n\n```\npython utils/align_inputs.py --template-fn 2013_rgb.tif --input-fn 2019_rgb.tif --output-fn 2019_aligned_rgb.tif\nmv 2019_aligned_rgb.tif 2019_rgb.tif\n```\n\n6. Now that we have pixel-aligned pre- and post-imagery layers, we can run the building damage assessment model.\n\n```\npython utils/inference.py --pre-imagery 2013_rgb.tif --post-imagery 2019_rgb.tif --output-fn damage_predictions.tif\n```\n\n7. We can now use `gdal2tiles.py` to render the tiles that will be shown on the web map interface. This step will produce three directories, `data/2013_tiles/`, `data/2019_tiles/`, and `data/damage_tiles/`, that contain rendered PNG versions of the imagery that will be displayed on the web map.\n\n```bash\ngdal_translate -of vrt -expand rgba damage_predictions.tif damage_predictions.vrt\ngdal2tiles.py -z 8-18 2013_rgb.tif data/2013_tiles/\ngdal2tiles.py -z 8-18 2019_rgb.tif data/2019_tiles/\ngdal2tiles.py -z 8-18 damage_predictions.vrt data/damage_tiles/\nrm damage_predictions.vrt\n```\n\n8. Now, to setup the configuration file we need some metadata from the input GeoTIFFs. We include a script that provides this:\n\n```\npython utils/get_bounds.py --input-fn 2013_rgb.tif\n```\n\n9. Finally, create a new configuration file (similar to the \"new_orleans_example.json\" file) called \"local_example.json\" using the bounds and centroid information shown by `utils/get_bounds.py` and the path to the directories we created.\n\n```json\n{\n  \"preImageryLayer\": {\n    \"basemapURL\": \"http://<REPLACE WITH YOUR VM'S HOSTNAME/IP>:8080/data/2013_tiles/{z}/{x}/{y}.png\",\n    \"date\": \"2013 imagery\",\n    \"attribution\": \"\",\n    \"bounds\": [\n      [47.62170620047876, -122.12082716224859],\n      [47.69079013651763, -122.19162910382583]\n    ]\n  },\n  \"postImageryLayer\": {\n    \"basemapURL\": \"http://<REPLACE WITH YOUR VM'S HOSTNAME/IP>:8080/data/2019_tiles/{z}/{x}/{y}.png\",\n    \"date\": \"2019 imagery\",\n    \"attribution\": \"\",\n    \"bounds\": [\n      [47.62170620047876, -122.12082716224859],\n      [47.69079013651763, -122.19162910382583]\n    ]\n  },\n  \"changeImageryLayer\": {\n    \"basemapURL\": \"http://<REPLACE WITH YOUR VM'S HOSTNAME/IP>:8080/data/damage_tiles/{z}/{x}/{y}.png\",\n    \"bounds\": [\n      [47.62170620047876, -122.12082716224859],\n      [47.69079013651763, -122.19162910382583]\n    ]\n  },\n  \"center\": [47.656253585524624, -122.15620486525859],\n  \"initialZoom\": 14,\n  \"location\": \"Redmond, Washington\",\n  \"imageryAttribution\": \"<a href='https://planetarycomputer.microsoft.com/dataset/naip'>NAIP Imagery</a>\",\n  \"license\": \"Proprietary\"\n}\n```\n\n10. Run a local http server with `python -m http.server 8080`\n    - Make sure port 8080 is not blocked by the Azure level firewall through the \"Network\" tab of your VM in the Azure Portal. See [this page](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/nsg-quickstart-portal) for more information.\n11. Finally, navigate to http://<REPLACE WITH YOUR VM'S HOSTNAME/IP>:8080/?config=local_example.json in your browser to see the example in action!\n\nOnce you have confirmed that the local example is working, we suggest you move the contents of the visualizer to a stable web server. As the visualizer is completely static, it could be easily hosted on an [Azure blob container](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website) or through a [local web server such as Apache](https://www.digitalocean.com/community/tutorials/how-to-install-the-apache-web-server-on-ubuntu-18-04).\n\n## Setup With Docker\n\n### Prerequisite\n\n- [Docker CE](https://docs.docker.com/engine/install/ubuntu/#installation-methods)\n\n### Configuration\n\nThe docker file use the [.env](.env) file for configuring the location of the code files and the mounting of a local data directory into the container. In addition, the input images for the data container should be specify in this file is using docker compose.\n\n```\napp_dir=/app\napp_port=8080\nlocal_data_dir=.\napp_data_dir=/app/data\nconda_env_name=visualizer\npre_imagery_file_name=2013_rgb.tif\npost_imagery_file_name=2019_aligned_rgb.tif\nimagery_output_file_name=test\n\n```\n\n### Run inference model and generate tiles for visualizer automatically\n\nThis requires two input RGB tiff images which need to be pixel aligned. Do steps 1 thru 5 of the **[Tutorial](#Tutorial)** section of this readme for an example of how to generate the correct image format from the sample data.\nDo not forget to set your \".env\" file accordingly.\n\n```bash\n\ndocker compose -f docker-compose-data.yml up\n\n```\n\n### Run Web Site\n\nRun below docker command to build and run the website. By default the website will run in port 8080. Navigate to http://localhost:8080 after running the command\n\n```bash\n\ndocker compose up -d\n\n```\n\nAfter your docker container is up, you can also set up a interactive session with it and run the steps from this tutorial, from 3 to 9, without the need to setup a Anaconda environment on your machine.\nThe \"data\" sub directory in the container is mapped to the root folder of the visualizer on the host.\n\n```bash\n\ndocker exec -it damage_web_1 /bin/bash\n\n```\n\n## List of third party javascript libraries/versions\n\nList of the libraries used by the tool:\n\n- [leaflet 1.3.1](https://leafletjs.com/download.html)\n- [NOTY 3.1.4](https://github.com/needim/noty)\n- [jquery 3.3.1](https://jquery.com/download/)\n- [leaflet side-by-side 2.0.0](https://github.com/digidem/leaflet-side-by-side)\n- [leaflet EasyButton 2.4.0](https://github.com/CliffCloud/Leaflet.EasyButton)\n- [font-awesome 4.1.0](https://github.com/FortAwesome/Font-Awesome)\n\nThis libraries are included in the `css/`, `js/`, and `fonts/` directories.\n\n## Additional information\n\nThe model used in the Damage Visualizer is courtesy of Microsoft AI for Humanitarian Action project in collaboration with the NLRC 510 global initiative.\nFor additional information and resources, please refer to: https://github.com/microsoft/building-damage-assessment-cnn-siamese\n", "repo_name": "Damage_Assessment_Visualizer", "org_name": "microsoft", "org_repo": "microsoft/Damage_Assessment_Visualizer", "platform_org_repo": "github+microsoft/Damage_Assessment_Visualizer", "link_to_repo": "https://github.com/microsoft/Damage_Assessment_Visualizer", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Open Data Community Kit\n\n## 1. What is the Open Data Community Kit?\n\nThe Open Data Community Kit is a repository-based approach for a group collaborative to create an open data commons.\nOpen data is increasingly instrumental to solve some of the most intricate problems of the 21st century. However most open datasets today are releases of proprietary datasets made available under an open data agreement, and are still rarely the result of an open collaboration process. We hope the Open Data Community Kit will make it easier to collaborate to that end.\n\n## 2. When should I use the Open Data Community Kit?\n\nThe Open Data Community Kit enables a community to start an open data collaboration as easily as an open source project. It is well suited if you want to share data under an open framework and enable contributions from your community.\n\nThe Open Data Community Kit includes only the basic elements necessary for an open data collaboration to work, including the handling of contributions, the agreements for sharing the open data commons, and the processes for defining key aspects of the open data commons. The Open Data Community Kit is agnostic of where the open data commons is actually hosted.\n\n\n## 3. What is an open data commons and what kind of data can it contain?\n\nOne can think about an open data commons as a dataset and associated metadata assembled from the data contributions of many and shared under an open framework. \n\nSuitable types of data include organizational data, anonmymized data with no personal data.\n\n\n## 4. Should I modify the Open Data Community Kit to fit my needs?\n\nProbably not, or at least not until your open data commons has grown to a point where the current framework is no longer sufficient.\n\nWe designed the Open Data Community Kit as a ready-to-go package to meet the needs of many open data commons, especially in their early stages. Before you modify the kit, please consider the following aspects:\n- speed: time spent fine tuning the framework is likely to delay the start of a collaboration, and may be better spent growing the community or defining the collaboration on a technical or functional level;\n- simplicity and support: by relying on a default framework, you have access to the community behind that framework. Chances are they have encountered your issue before and can help you;\n- predictability: modifying the framework may invite further requests for modification, making the whole exercise more unpredictable.\n\nThat being said, be particularly cautious about the following aspects:\n- data agreement: the Open Data Community Kit default data agremeent is the [CDLA-Permissive-2.0](https://cdla.dev/permissive-2-0/). This agreement has several benefits: it is straghtforward to read and understand, makes it easy to collect and assemble data into larger datasets, creates a clear framework for Machine Learning and is legally sound, with the backing of many high profile entities. Consider these benefits before changing to another agreeement.\n- adding legal requirements: be careful not to inadvertently add legal requirements on top of those in the data agreement itself. For example, there is no attributution requirement in the CDLA-Permissive-2.0, so don't add one to your project (unless that's really your goal!);\n- the governance framework: the governance model for the Open Data Community Kit gives full control to the Data Commons Maintainer. You may want to revisit the model as the project evolves, but you may appreciate the flexibility and control at the early stages of the project.\n\nIf you think the project can be improved, feel free to make suggestions and issue a pull request.\n\n## 5. My project has grown. I need a more sophisticated framework. What do I do?\n\nYou have several options depending on your situation.\n- if you need a two-tier level governance, one at an organization-level and one for potentially multiple projects, the [Minimal Viable Governance](https://github.com/github/MVG) framework may be relevant. \n- if you need to expand beyond open data and create open source software or a technical specification, the [Joint Development Foundation](https://www.jointdevelopment.org/) may suit your needs.\n- if you need to formalize the project, form a corporate entity and collect fees to provide financial resources to the project, consider exploring how existing foundations can help you, such as the [Open Data Institute](https://theodi.org/) or the [Linux Foundation](https://www.linuxfoundation.org/).\n\n## 6. Where can I start?\n\nInstructions for using the Open Data Community Kit are provided in the [GettingStarted.md](GettingStarted.md) file.\n", "repo_name": "Open_Data_Community_kit", "org_name": "microsoft", "org_repo": "microsoft/Open_Data_Community_kit", "platform_org_repo": "github+microsoft/Open_Data_Community_kit", "link_to_repo": "https://github.com/microsoft/Open_Data_Community_kit", "platform": "github", "language": null, "stargazers_count": 5, "watchers_count": 5}, {"README_text": "---\r\npage_type: sample\r\nname: Extended eye tracking in Unity\r\ndescription: This sample shows you how to use extended eye tracking features in Unity projects using a HoloLens.\r\nlanguages:\r\n- csharp\r\nproducts:\r\n- hololens\r\n---\r\n\r\n# Extended eye tracking in Unity \r\n\r\n![License](https://img.shields.io/badge/license-MIT-green.svg)\r\n\r\nSupported device  | Supported Unity versions | Built with XR configuration\r\n:---------------: | :----------------------: | :--------------------------: \r\nHoloLens 2        | Unity 2020 or higher     | Mixed Reality OpenXR Plugin\r\n\r\nThis sample shows how to use EyeTracking SDK to access extended eye tracking features in Unity projects using a HoloLens. Covered features include \r\n* Setting eye tracking data framerate\r\n* Getting individual and combined eye gaze vectors\r\n\r\n## Contents\r\n\r\nFile/folder  | Description |\r\n-------------|-------------|\r\n`SampleEyeTracking/Assets` | Unity assets, scenes, prefabs, and scripts. |\r\n`SampleEyeTracking/Packages` | Project manifest and packages list. |\r\n`SampleEyeTracking/ProjectSettings` | Unity asset setting files. |\r\n`SampleEyeTracking/.gitignore` | Define what to ignore at commit time. |\r\n\r\n## Required tools\r\n\r\n1. Install the [tools for Mixed Reality development](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/install-the-tools)\r\n2. Install the [recommended Unity version](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/choosing-unity-version) \r\n\r\nThis repo uses following versions of tools\r\n* HoloLens 2 with OS build minimum 20348.1537\r\n* Unity 2020.3.40f1\r\n* Mixed Reality OpenXR Plugin 1.5.1\r\n* Visual Studio 2022 17.3.6\r\n\r\n## Setup\r\n\r\n1. Clone or download this sample repository.\r\n2. Open the `SampleEyeTracking` folder in Unity Hub and launch the project.\r\n3. Open the `Assets/SampleScene` in Unity.\r\n\r\n## Run the sample\r\n\r\nExtended eye tracking features are not supported by Holographic Remoting at this time, so running this sample in Unity editor won't get any gaze data. The only way to test it is building and deploying to HoloLens.\r\n\r\n## Build and deploy the sample\r\n\r\n1. Follow [Build and deploy to the HoloLens](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/build-and-deploy-to-hololens) to build Unity project and deploy to HoloLens.\r\n2. Launching this app in HoloLens and granting the gaze permission in the dialog, the gaze framerate will be set to 90FPS and you will see 3 cubes and 1 cylinder following your gaze direction in 1.5 meter away.\r\n    * The green cube represents your left eye gaze.\r\n    * The red cube represents your right eye gaze.\r\n    * The cyan cube represents your combined eye gaze.\r\n    * The blue cylinder also represents your combined eye gaze but its coordinate is set related to the Unity camera GameObject.\r\n\r\n## Project explanation\r\n\r\n### Used packages\r\n\r\nThis Unity sample app use following packages. \r\n\r\nPackage  | Description \r\n-------------|-------------\r\n[Mixed Reality OpenXR Plugin](https://github.com/microsoft/OpenXR-Unity-MixedReality-Samples/releases) | to read the pose of eyeGazeTracker under Unity scene coordinate system, then use to convert the gaze data's coordinate\r\n[NuGetForUnity](https://github.com/GlitchEnzo/NuGetForUnity) | a tool to download and import NuGet package\r\n[Microsoft.MixedReality.EyeTracking](https://www.nuget.org/packages/Microsoft.MixedReality.EyeTracking) | NuGet package to provide eye tracking features\r\n[MRTK Graphic Tools](https://github.com/microsoft/MixedReality-GraphicsTools-Unity) | provide the shaders to render holograms\r\n\r\nThe `Mixed Reality OpenXR Plugin` and `MRTK Graphic Tools` could be imported into Unity by the [MRTK Feature Tool](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/welcome-to-mr-feature-tool#download). The `NuGetForUnity` could be imported through Unity's custom package. The `Microsoft.MixedReality.EyeTracking` could be imported through the `NuGetForUnity` tab in Unity editor. \r\n\r\nDepending on your use case, you may don't need to use all these packages in your projects.\r\n\r\n**Note**: some of packages above are not open-source. \r\n\r\n### Scripts\r\n\r\nThe script [ExtendedEyeGazeDataProvider](./SampleEyeTracking/Assets/Scripts/ExtendedEyeGazeDataProvider.cs) is the core script to call APIs and provide gaze data to other scripts in Unity. Specially, it does following things\r\n* Maintain the eyeGazeTracker instance\r\n* Set the framerate of eyeGazeTracker\r\n* Read eyeGaze data from eyeGazeTracker\r\n* Read eyeGazeTracker pose from Mixed Reality OpenXR plugin API\r\n* Convert eyeGaze data into Unity's scene coordinate system or Unity's camera GameObject coordinate system\r\n\r\n## Other documentations\r\n\r\n* Complete API reference for the Microsoft.MixedReality.EyeTracking NuGet package on at the [NuGet Gallery](https://www.nuget.org/packages/Microsoft.MixedReality.EyeTracking)\r\n* For more details about how to setup your Unity project, refer to [Microsoft Docs](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/extended-eye-tracking-unity).\r\n* For more details about how to setup native project, refer to [Microsoft Docs](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/extended-eye-tracking-native).\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "MixedReality-EyeTracking-Sample", "org_name": "microsoft", "org_repo": "microsoft/MixedReality-EyeTracking-Sample", "platform_org_repo": "github+microsoft/MixedReality-EyeTracking-Sample", "link_to_repo": "https://github.com/microsoft/MixedReality-EyeTracking-Sample", "platform": "github", "language": "C#", "stargazers_count": 17, "watchers_count": 17}, {"README_text": "![Category overview](docs/images/entra_hero.png \"Microsoft Entra\")\n\n# Security is complex. We can help you simplify it.\n\nThis repository contains documentation, samples and Software Developer Kits (SDKs) supporting the [Microsoft Entra](http://aka.ms/entra) product family. You can use to learn, understand and leverage the following products:\n\n* [Verified ID](https://github.com/microsoft/entra/VerifiedID/README.md \"Microsoft Entra Verified ID\") \n\n## Contributing\n\nThere are many ways in which you can participate in this project, for example:\n\n* [Submit bugs and feature requests](https://github.com/microsoft/entra/issues), and help us verify as they are checked in\n* Review the [documentation](https://github.com/microsoft/entra/docs) and make pull requests for anything from typos to additional and new content\n\n## Feedback\n\n* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/vscode)\n* [Request a new feature](CONTRIBUTING.md)\n* Upvote [popular feature requests](https://github.com/microsoft/vscode/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)\n* [File an issue](https://github.com/microsoft/vscode/issues)\n* Connect with the extension author community on [GitHub Discussions](https://github.com/microsoft/vscode-discussions/discussions) or [Slack](https://aka.ms/vscode-dev-community)\n* Follow [@code](https://twitter.com/code) and let us know what you think!\n\nSee our [wiki](https://github.com/microsoft/vscode/wiki/Feedback-Channels) for a description of each of these channels and information on some other available community-driven channels.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "entra", "org_name": "microsoft", "org_repo": "microsoft/entra", "platform_org_repo": "github+microsoft/entra", "link_to_repo": "https://github.com/microsoft/entra", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project Trieste\n\nProject Trieste is an experimental term rewriting system for experimental programming language development.\n\nThis research project is at an early stage and is open sourced to facilitate academic collaborations. We are keen to engage in research collaborations on this project, please do reach out to discuss this.\n\nThe project is not ready to be used outside of research.\n\n## Getting Started\n\nIf you want to dive right into understanding how to use Trieste, take\na look at the [`infix` tutorial language](./samples/infix/README.md),\nwhich will walk you through implementing a simple calculator language\nin Trieste.\n\n## Using Trieste\n\nTrieste is a header-only C++20 library. To get started, you'll need to define your own `trieste::Driver`, and run it from `main`:\n\n```c++\n#include <trieste/driver.h>\n\nint main(int argc, char** argv)\n{\n  // Define your driver...\n  trieste::Driver driver(...);\n  return driver.run(argc, argv);\n}\n```\n\n## Building the Samples\n\nHere's an example of how to build the `verona` sample and run the self-tests. Other build systems and compilers may work as well.\n\n```sh\ngit clone https://github.com/microsoft/trieste\ncd trieste\nmkdir build\ncd build\ncmake -G Ninja .. -DCMAKE_BUILD_TYPE=Debug -DCMAKE_CXX_COMPILER=clang++-14\nninja install\n./dist/verona/verona test\n```\n\n## Using Trieste in Your Project\n\nYou can use Trieste via FetchContent by including the following lines\nin your CMake:\n\n``` cmake\nFetchContent_Declare(\n  trieste\n  GIT_REPOSITORY https://github.com/microsoft/Trieste\n  GIT_TAG        a2a7fada4ab5250a4f8d1313b749ad336202841b\n)\n\nFetchContent_MakeAvailable(trieste)\n```\n\nAnd then adding it as a target link library, e.g.\n\n``` cmake\ntarget_link_libraries(verona\n  Threads::Threads\n  CLI11::CLI11\n  trieste::trieste\n  )\n```\n\n## Contributing\n\nIf you are interested in contributing to Trieste, please see our [contributing document](CONTRIBUTING.md).\n", "repo_name": "Trieste", "org_name": "microsoft", "org_repo": "microsoft/Trieste", "platform_org_repo": "github+microsoft/Trieste", "link_to_repo": "https://github.com/microsoft/Trieste", "platform": "github", "language": "C++", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# U-Prove Node Reference Implementation\n\nThis project provides a Node reference implementation of the Lite profile of the [U-Prove Specification 1.1 (Revision 5)](./doc/U-Prove%20Cryptographic%20Specification%20V1.1%20Revision%205.pdf). The [U-Prove technology](https://microsoft.com/uprove) enables the creation of unlinkable credentials which can encode attributes of any types, supporting selective subset disclosure; see the [U-Prove Technology Overview](./doc/U-Prove%20Technology%20Overview%20V1.1%20Revision%203.pdf) for more details. The Lite profile of the specification simplifies the implementation by limiting the feature set; namely it _does not_ support:\n* Device binding\n* Scope-exclusive pseudonyms\n* Presenting committed attributes\n\nThe project also implements the [U-Prove JSON Framework (UPJF)](./doc/U-Prove_JSON_Framework.md), describing a deployment model and JSON serialization for the U-Prove artifacts.\n\n## Setup\n\nMake sure [node.js](https://nodejs.org/) and [npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) are installed on your system; the latest Long-Term Support (LTS) version is recommended for both.\n\n1. Get the source, for example using `git`\n```\ngit clone -b main https://github.com/microsoft/uprove-node-reference.git\ncd uprove-node-reference\n```\n\n2. Build the `npm` package\n```\nnpm install\nnpm run build\n```\n\n3. Optionally, run the unit tests\n\n```\nnpm test\n```\n\n4. Optionally, run the samples program\n\n```\nnpm run samples\n```\n\nAdditionally, the `samples/upjf` folder contains its own npm project demonstrating a sample 3-party system using the U-Prove JSON Framework; see its [README](./samples/upjf/README.md) for details.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "uprove-node-reference", "org_name": "microsoft", "org_repo": "microsoft/uprove-node-reference", "platform_org_repo": "github+microsoft/uprove-node-reference", "link_to_repo": "https://github.com/microsoft/uprove-node-reference", "platform": "github", "language": "TypeScript", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# EventLogExpert\n\nNothing to see here yet. This tool is in the early stages of development. Please check back later.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "EventLogExpert", "org_name": "microsoft", "org_repo": "microsoft/EventLogExpert", "platform_org_repo": "github+microsoft/EventLogExpert", "link_to_repo": "https://github.com/microsoft/EventLogExpert", "platform": "github", "language": "C#", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers\n\nThis repository contains code, data and pretrained models used in [AutoMoE (pre-print)](https://arxiv.org/abs/2210.07535). This repository builds on [Hardware Aware Transformer (HAT)'s repository](https://github.com/mit-han-lab/hardware-aware-transformers).\n\n## AutoMoE Framework\n![AutoMoE Framework](images/framework.png)\n\n## AutoMoE Key Result\n\nThe following table shows the performance of AutoMoE vs. baselines on standard machine translation benchmarks: WMT'14 En-De, WMT'14 En-Fr and WMT'19 En-De.\n\n| WMT\u201914 En-De         | Network | \\# Active Params (M) | Sparsity (%) | FLOPs (G) | BLEU  | GPU Hours  |\n|----------------|--------|---------|------|------|------|------|\n| Transformer | Dense | 176 | 0 | 10.6 | 28.4 |  184 |\n| Evolved Transformer | NAS over Dense | 47 | 0 | 2.9 | 28.2 | 2,192,000 |\n| HAT | NAS over Dense | 56 | 0 | 3.5 | 28.2 | 264 |\n| AutoMoE (6 Experts) | NAS over Sparse | 45 | 62 | 2.9 | 28.2 | 224 | \n\n| WMT\u201914 En-Fr         | Network | \\# Active Params (M) | Sparsity (%) | FLOPs (G) | BLEU  | GPU Hours  |\n|----------------|--------|---------|------|------|------|------|\n| Transformer |  Dense | 176 | 0 | 10.6 | 41.2 | 240 |\n| Evolved Transformer | NAS over Dense | 175 | 0 | 10.8 | 41.3 | 2,192,000  |\n| HAT | NAS over Dense | 57 | 0 | 3.6 | 41.5 | 248 |\n| AutoMoE (6 Experts) | NAS over Sparse | 46 | 72 | 2.9 | 41.6 | 236  |\n| AutoMoE (16 Experts) | NAS over Sparse | 135 | 65 | 3.0 | 41.9 | 236 | \n\n| WMT\u201919 En-De        | Network | \\# Active Params (M) | Sparsity (%) | FLOPs (G) | BLEU  | GPU Hours  |\n|----------------|--------|---------|------|------|------|------|\n| Transformer |  Dense | 176 | 0 | 10.6 | 46.1 | 184 |\n| HAT | NAS over Dense | 63 | 0 | 4.1 | 45.8 | 264 |\n| AutoMoE (2 Experts) | NAS over Sparse | 45 | 41 | 2.8 | 45.5 | 248  |\n| AutoMoE (16 Experts) | NAS over Sparse | 69 | 81 | 3.2 | 45.9 | 248 | \n\n\n## Quick Setup\n\n### (1) Install\nRun the following commands to install AutoMoE:\n```bash\ngit clone https://github.com/UBC-NLP/AutoMoE.git\ncd AutoMoE\npip install --editable .\n```\n\n### (2) Prepare Data\nRun the following commands to download preprocessed MT data:\n```bash\nbash configs/[task_name]/get_preprocessed.sh\n```\nwhere `[task_name]` can be `wmt14.en-de` or `wmt14.en-fr` or `wmt19.en-de`.\n\n### (3) Run full AutoMoE pipeline\nRun the following commands to start AutoMoE pipeline:\n```bash\npython generate_script.py --task wmt14.en-de --output_dir /tmp --num_gpus 4 --trial_run 0 --hardware_spec gpu_titanxp --max_experts 6 --frac_experts 1 > automoe.sh\nbash automoe.sh\n```\nwhere,\n* `task` - MT dataset to use: `wmt14.en-de` or `wmt14.en-fr` or `wmt19.en-de` (default: `wmt14.en-de`)\n* `output_dir` - Output directory to write files generated during experiment (default: `/tmp`)\n* `num_gpus` - Number of GPUs to use (default: `4`)\n* `trial_run` - Run trial run (useful to quickly check if everything runs fine without errors.): 0 (final run), 1 (dry/dummy/trial run) (default: `0`)\n* `hardware_spec` - Hardware specification: `gpu_titanxp` (For GPU) (default: `gpu_titanxp`)\n* `max_experts` - Maximum experts (for Supernet) to use (default: `6`)\n* `frac_experts` - Fractional (varying FFN. intermediate size) experts: 0 (Standard experts) or 1 (Fractional) (default: `1`)\n* `supernet_ckpt` - Skip supernet training by specifiying checkpoint from [pretrained models](https://1drv.ms/u/s!AlflMXNPVy-wgb9w-aq0XZypZjqX3w?e=VmaK4n) (default: `None`)\n* `latency_compute` - Use (partially) gold or predictor latency (default: `gold`)\n* `latiter` - Number of latency measurements for using (partially) gold latency (default: `100`)\n* `latency_constraint` - Latency constraint in terms of milliseconds (default: `200`)\n* `evo_iter` - Number of iterations for evolutionary search (default: `10`)\n\n## Contact\nIf you have questions, contact Ganesh (`ganeshjwhr@gmail.com`), Subho (`Subhabrata.Mukherjee@microsoft.com`) and/or create GitHub issue.\n\n## Citation\nIf you use this code, please cite:\n```\n@misc{jawahar2022automoe,\n      title={AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers}, \n      author={Ganesh Jawahar and Subhabrata Mukherjee and Xiaodong Liu and Young Jin Kim and Muhammad Abdul-Mageed and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah and Sebastien Bubeck and Jianfeng Gao},\n      year={2022},\n      eprint={2210.07535},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nSee LICENSE.txt for license information.\n\n## Acknowledgements\n* [Hardware Aware Transformer](https://github.com/mit-han-lab/hardware-aware-transformers) from `mit-han-lab`\n* [fairseq](https://github.com/facebookresearch/fairseq) from `facebookresearch`\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AutoMoE", "org_name": "microsoft", "org_repo": "microsoft/AutoMoE", "platform_org_repo": "github+microsoft/AutoMoE", "link_to_repo": "https://github.com/microsoft/AutoMoE", "platform": "github", "language": "Python", "stargazers_count": 28, "watchers_count": 28}, {"README_text": "\n![alt text](images/banner.png \"Title\")\n\n<br/>\n<br/>\n\n# DevSquad Solution Accelerators\n\nDeveloped by the Microsoft GPS DevSquad Team, the DevSquad Solution Accelerators are different repositories that provide a set of tools and templates to help you accelerate your development process in varios context like DataOps, MLOps and DevSecOps. The DevSquad Solution Accelerators are designed to help you build solutions that are secure, scalable, and reliable. The DevSquad Solution Accelerators are built on top of the Microsoft Azure platform and are designed to be used with the Microsoft Azure DevOps platform or GitHub.\n\n<br/>\n<br/>\n\n\n## Summary Table of DevSquad Solution Accelerators\nBelow is a summary list of the DevSquad Solution Accelerators:\n<br/>\n<br/>\n\n<table>\n    <thead>\n        <tr>\n            <th>Name</th>\n            <th>Description</th>\n            <th>Services or Architecture Reference</th>\n            <th>Link</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td rowspan=9><b>DevSquad DevSecOps</b></td>\n            <td rowspan=9>In these workshops, you will learn how to build and deploy modern Cloud-Native apps based on reference arquitectures, supported by best practices of <a href=\"https://github.com/microsoft/code-with-engineering-playbook/blob/main/docs/ENG-FUNDAMENTALS-CHECKLIST.md\">Software Engineering</a> and <a href=\"https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github\">Secure DevOps Practices</a>.  <br/>Reference architecture <a href=\"https://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github\">DevSecOps in Azure</a></td>\n            <td>Infrastructure as code, reference architecture: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-infrastructure-as-code\">DevSecOps for infrastructure as code (IaC)</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_iac\">DevSquad IaC Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Deploy ASP.Net Code Application on Azure, reference architectures: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github\">Deploy Web App on Azure</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_appservice\">DevSquad App Service Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Deploy Serverless Application on Azure, reference architectures: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/serverless/cloud-automation\">Deploy event driven architecture on Azure</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_serverless\">DevSquad Serverless Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Azure Spring Cloud, reference architecture: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/example-scenario/blue-green-spring/blue-green-spring\">Deployment for Applications on Azure Spring Cloud</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_springcloud\">DevSquad Spring Cloud Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Deploy Microservice on Azure Kubernetes Service, reference architecture: <a href=\"https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices\">Deploy microservice on AKS</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_cloud-native\">DevSquad Kubernetes Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Azure Red Hat OpenShift, reference architecture: <a href=\"https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/app-platform/azure-red-hat-openshift/landing-zone-accelerator\">ARO Reference Architecture</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_ARO\">DevSquad Azure Red Hat OpenShift Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Deploy a <a href=\"https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/devsecops-in-github\">DevOps reference arquitecture</a></td>\n            <td><a href=\"https://github.com/oaviles/DevSquad/tree/main/DevOps_Wizard\">DevSquad DevOps Accelerator</a></td>\n        </tr>\n        <tr>\n            <td>Deploy a <a href=\"https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/iot\">IoT reference arquitecture</a></td>\n            <td><a href=\"https://github.com/luisruval/DevSquad-IoT\">DevSquad IoT Accelerator</a></td>\n        </tr>\n        <tr>\n            <td><a href=\"https://docs.microsoft.com/en-us/learn/modules/intro-to-java-azure/5-deployment-opportunities\">Build or Migrate Java Web Apps</a></td>\n            <td><a href=\"https://github.com/oaviles/hello_java\">DevSquad Java on Azure Accelerator</a></td>\n        </tr>\n        <tr>\n            <td><b>DevSquad DataOps</b></td>\n            <td>In this workshop, you will deploy a DataOps reference arquitecture, for understanding best practices of Data Engineering & Software Engineering combined.</td>\n            <td>ARM Templates, Azure DevOps, Azure Data Factory, Azure Databricks</td>\n            <td><a href=\"https://github.com/microsoft/devsquad-dataops\">DevSquad DataOps</a></td>\n        </tr>\n        <tr>\n            <td><b>DevSquad MLOps</b></td>\n            <td>In this workshop, you will deploy a MLOps reference arquitecture. It demostrates how apply the worflow in a sample project along with a CI/CD implementation for Azure DevOps.</td>\n            <td>ARM Templates, Azure DevOps, Azure Machine Learning</td>\n            <td><a href=\"https://github.com/microsoft/devsquad-mlops\">DevSquad MLOps</a></td>\n        </tr>\n    </tbody>\n</table>\n\n                \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "devsquad-accelerators", "org_name": "microsoft", "org_repo": "microsoft/devsquad-accelerators", "platform_org_repo": "github+microsoft/devsquad-accelerators", "link_to_repo": "https://github.com/microsoft/devsquad-accelerators", "platform": "github", "language": null, "stargazers_count": 10, "watchers_count": 10}, {"README_text": "<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/1785175/199388258-4ca228d5-9f0b-463d-82dd-6c27015bc4ab.png\" width=\"400px\">\n</p>\n<h1 align=\"center\">PDEArena</h1>\n\n[![Documentation](https://img.shields.io/badge/docs-passing-brightgreen)](https://microsoft.github.io/pdearena)\n[![Paper](https://img.shields.io/badge/arXiv-2209.15616-blue)](https://arxiv.org/abs/2209.15616)\n\nThis repository contains code accompanying the paper [**Towards multi-spatiotemporal-scale generalized PDE modeling**](https://arxiv.org/abs/2209.15616), and as such we hope this serves as a starting point for future PDE surrogate learning research.\nWe will soon have models from [**Clifford neural layers for PDE modeling**](https://arxiv.org/abs/2209.04934) as well.\n\nFor details about usage please see [documentation](https://microsoft.github.io/pdearena).\nIf you have any questions or suggestions please open a [discussion](https://github.com/microsoft/pdearena/discussions). If you notice a bug, please open an [issue](https://github.com/microsoft/pdearena/issues).\n\n## Citation\n\nIf you find this repository useful in your research, please consider citing the following papers:\n\n```bibtex\n@article{gupta2022towards,\n  title={Towards Multi-spatiotemporal-scale Generalized PDE Modeling},\n  author={Gupta, Jayesh K and Brandstetter, Johannes},\n  journal={arXiv preprint arXiv:2209.15616},\n  year={2022}\n}\n```\nDo remember to cite the original papers as well for individual architectures.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "pdearena", "org_name": "microsoft", "org_repo": "microsoft/pdearena", "platform_org_repo": "github+microsoft/pdearena", "link_to_repo": "https://github.com/microsoft/pdearena", "platform": "github", "language": "Python", "stargazers_count": 115, "watchers_count": 115}, {"README_text": "# Secret Service\n\nSecret Service Rust library.\n\nInterfaces with the Linux Secret Service API through dbus.\n\n### Versioning\nThis library is feature complete, has stabilized its API for the most part. However, as this\ncrate is almost soley reliable on the `zbus` crate, we try and match major version releases\nwith theirs to handle breaking changes and move with the wider `zbus` ecosystem.\n\n### Documentation\n\n[Get Docs!](https://docs.rs/secret-service/)\n\n### Basic Usage\n\nDoes not require dbus library! Pure Rust!\n(On ubuntu, this was libdbus-1-dev when building, and libdbus-1-3 when running)\n\nIn Cargo.toml:\n\n```\n[dependencies]\nsecret-service = \"2.0.0\"\n```\n\nOr, you can add this project with `cargo add`:\n\n```\n$ cargo add secret-service\n```\n\nIn source code (below example is for --bin, not --lib). This example uses `tokio` as\nthe async runtime.\n\n```rust\nuse secret_service::SecretService;\nuse secret_service::EncryptionType;\nuse std::{collections::HashMap, error::Error};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn Error>> {\n    // initialize secret service (dbus connection and encryption session)\n    let ss = SecretService::connect(EncryptionType::Dh).await?;\n\n    // get default collection\n    let collection = ss.get_default_collection().await?;\n\n    // create new item\n    collection.create_item(\n        \"test_label\", // label\n        HashMap::from([(\"test\", \"test_value\")]), // properties\n        b\"test_secret\", // secret\n        false, // replace item with same attributes\n        \"text/plain\" // secret content type\n    ).await?;\n\n    // search items by properties\n    let search_items = ss.search_items(\n        HashMap::from([(\"test\", \"test_value\")])\n    ).await?;\n\n    let item = search_items.unlocked.get(0).ok_or(\"Not found!\")?;\n\n    // retrieve secret from item\n    let secret = item.get_secret().await?;\n    assert_eq!(secret, b\"test_secret\");\n\n    // delete item (deletes the dbus object, not the struct instance)\n    item.delete().await?;\n    Ok(())\n}\n```\n\n### Functionality\n\n- SecretService: initialize dbus, create plain/encrypted session.\n- Collections: create, delete, search.\n- Items: create, delete, search, get/set secret.\n\n### Changelog\nSee [the changelog file](./CHANGELOG.md)\n\n## License\n\nLicensed under either of\n\n* Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\n\nat your option.\n\n### Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.\n", "repo_name": "vscode-secret-service-rs", "org_name": "microsoft", "org_repo": "microsoft/vscode-secret-service-rs", "platform_org_repo": "github+microsoft/vscode-secret-service-rs", "link_to_repo": "https://github.com/microsoft/vscode-secret-service-rs", "platform": "github", "language": "Rust", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "![High Level Overview](documentation/images/ds-toolkit-banner.png)\n\n# Speech-to-Text Transcription and Classification Solution Accelerator\nThis repository contains the base repository for developing an end-to-end speech transcription and message classification solution for any type of business problem requiring an advanced method for classifying audio messages into actionable meaning. The types of business problem benefitting from this solution include:\n- General audio message classification within a noisy environment \n- Monitoring communications for safety and regulatory purposes, eg: Mining industry - positive communications\n- Complex audio message and communication classification scenarios, eg: Message understanding   \n\nNote that to illustrate the specific use case of positive communication in mining industry, some customized samples files [sample_files_mining](./sample_files_mining) are aslo provided in this repo. This use case will be elabrated in [An example use case in Mining Industry](#an-example-use-case-in-mining-industry).\n\n## Prerequisites\n\nThe diagram below presents the main data flow overview:\n\n![High Level Overview](documentation/images/solution-overview.png)\n\n# Main features\n\nProvided audio files and their supporting attributes, this repository includes the following main features: \n- `Audio Signal Processing`\n- `Speech to text transcription Machine Learning Modelling` \n- `Natural Language Processing (NLP) Machine Learning Modelling` \n- `Advanced sequential and classification modelling`\n- `Speech to text transcription performance claculation methodology`\n\nThese features enable you to have extract major insights.\n\n## Audio Signal Processing \nThe audio signal processing and filtering techniques is developed to maximise the signal-to-noise ratio (SNR) of the audio signals for optimised speech-to-text processing. The key steps in the audio signal processing includes:\n1. Audio data processing and mono channel averaging\n2. Frequency domain and spectral analysis\n3. Signal (`S`) and noise (`N`) power analysis\n4. SNR optimisation\n5. Fast-Fourier-Transform (FFT) and Butterworth filter application\n\n\n![High Level Overview](documentation/images/audio-signal-processing-overview.png)\n\nThe digarm below shows a sample audio signal processing using the above mensioned key stapes, where the raw audio file (left) and optmised and filtered to produce the filtered (maximised SNR) audio file (right).\n\n![High Level Overview](documentation/images/sample-audio-filtering.png)\n\n\n## Speech to text transcription Machine Learning Modelling\nThe first stage of the solution is to apply a combination of Azure Cognitive services for Speech and customised scripts to convert the array of the audio files into lexical text\n\nThe output format for speech-to-text JSON (saved as `transcripted_dict.json`) is presented below:\n\n```yaml\n{\n    \"documents\": [\n        {\n            \"id\": \"1249120_44142156_61923550\",\n            \"text\": \"i have a rash and eat itches very bad\",\n            \"display_text\": \"I have a rash and eat itches very bad\",\n            \"language\": \"en\",\n            \"word_count\": 9,\n            \"confidence\": 0.81069607,\n            \"duration\": 2700.0,\n            \"offset\": 620.0,\n            \"ground_truth\": \"i have a rash and it itches very bad\",\n            \"ground_truth_class\": \"OTHERS\"\n        },\n        {\n            \"id\": \"1249120_44142156_65967262\",\n            \"text\": \"i hit my head at the basketball game could i have a concussion\",\n            \"display_text\": \"I hit my head at the basketball game Could I have a concussion None\",\n            \"language\": \"en\",\n            \"word_count\": 13,\n            \"confidence\": 0.84714672,\n            \"duration\": 4740.0,\n            \"offset\": 70.0,\n            \"ground_truth\": \"i hit my head at the basketball game could i have a concussion\",\n            \"ground_truth_class\": \"UPPER\"\n        },\n        .\n        .\n        \n        ]\n}\n```\n\nwhere:\n- `id`: represents the audio file (filtered)\n- `text`: represents the basic (lexical) transcribed radio communication\n- `word_count`: represents the number of words detected\n- `confidence`: represent sthe confidence level for the transcription\n\n\n## Natural Language Processing (NLP) Machine Learning Modelling\nThe next stage uses NLP and related algorithms is the tokenisation of the lexical text and extraction of the key phrases. \nThe output format for the NLP JSON (saved as `NLP_dict.json`) is presented below.\n(NB: Major attributes in `transcripted_dict.json` are also included in `NLP_dict.json`)\n\n```yaml\n{\n    \"documents\": [\n        {\n            \"id\": \"1249120_44142156_61923550_filtered\",\n            \"text\": \"i have a rash and eat itches very bad\",\n            \"display_text\": \"I have a rash and eat itches very bad\",\n            \"language\": \"en\",\n            \"word_count\": 9,\n            \"confidence\": 0.7165945,\n            \"duration\": 2860.0,\n            \"offset\": 580.0,\n            \"ground_truth\": \"i have a rash and it itches very bad\",\n            \"ground_truth_class\": \"OTHERS\",\n            \"filtered_tokenized_transcript\": [\n                \"rash\",\n                \"eat\",\n                \"itches\",\n                \"bad\"\n            ],\n            \"num_filtered_tokens\": 4,\n            \"token_fdisk\": {\n                \"rash\": 1,\n                \"eat\": 1,\n                \"itches\": 1,\n                \"bad\": 1\n            },\n            \"keyPhrases\": [\n                \"rash\",\n                \"itches\"\n            ],\n            \"warnings\": [],\n            \"num_key_phrases\": 2,\n            \"general_phrases\": [],\n            \"entities\": []\n        }, \n        \n        {\n            \"id\": \"1249120_44142156_65967262_filtered\",\n            \"text\": \"i hit my head at the basketball game could i have a concussion\",\n            \"display_text\": \"I hit my head at the basketball game Could I have a concussion\",\n            \"language\": \"en\",\n            \"word_count\": 13,\n            \"confidence\": 0.8216368249999999,\n            \"duration\": 4660.0,\n            \"offset\": 60.0,\n            \"ground_truth\": \"i hit my head at the basketball game could i have a concussion\",\n            \"ground_truth_class\": \"UPPER\",\n            \"filtered_tokenized_transcript\": [\n                \"hit\",\n                \"head\",\n                \"basketball\",\n                \"game\",\n                \"could\",\n                \"concussion\"\n            ],\n            \"num_filtered_tokens\": 6,\n            \"token_fdisk\": {\n                \"hit\": 1,\n                \"head\": 1,\n                \"basketball\": 1,\n                \"game\": 1,\n                \"could\": 1,\n                \"concussion\": 1\n            },\n            \"keyPhrases\": [\n                \"basketball game\",\n                \"head\",\n                \"concussion\"\n            ],\n            \"warnings\": [],\n            \"num_key_phrases\": 3,\n            \"general_phrases\": [],\n            \"entities\": [\n                {\n                    \"text\": \"basketball\",\n                    \"category\": \"Skill\",\n                    \"offset\": 21,\n                    \"length\": 10,\n                    \"confidenceScore\": 0.62\n                },\n                {\n                    \"text\": \"basketball game\",\n                    \"category\": \"Event\",\n                    \"subcategory\": \"Sports\",\n                    \"offset\": 21,\n                    \"length\": 15,\n                    \"confidenceScore\": 0.89\n                }\n            ]\n        },\n                .\n                .\n        ]\n}\n```\nwhere:\n- `filtered_tokenized_transcript` key: the tokeinised (word extract) of the lexical text, with stop-words removed\n- `num_filtered_tokens` key: the number of tokenised words extracted\n- `token_fdisk` key: the frequency distribution of the tokenised words\n- `keyPhrases` key: the key phrases extracted by the NLP model\n- `entities` key: the key named entity recognition (NER) extracted by the NLP model. \n\n\n## Speech to text transcription performance claculation methodology\nThe transcripted voide data into text will be evlauted against the actual text using industry standard [Word Error Rate](https://en.wikipedia.org/wiki/Word_error_rate). For reporting we will use the accuarcy rate as shown below\n\n<font size=5>\n$$WER(transcript_{n}) = \\frac{I + D + S}{N} * 100%$$\n$$WA_{cc}(transcript_{n}) = 1 - WER(transcript_{n})$$\n</font>\n\n*where*\n* `Insertion` (*I*): Transcribed Words that are incorrectly added in the hypothesis transcript\n* `Deletion` (*D*): Transcribed Words that are undetected in the hypothesis transcript\n* `Substitution` (*S*): Trabscribed Words that were substituted between reference and hypothesis\n\n\n# Getting Started\n\n## Contents of Notebooks\n\nWe have two notebooks([00. provisioning](./src/engine/00.%20provisioning.ipynb), [10. training](.//src/engine/10.%20training.ipynb)) to understand the entire features.\n\n- Provisioning\n    - We set up Azure resouces and environment variables with the Notebook.\n- Training\n    - After completing provisioning, we find the following features:\n        - Extract text from provided audio files with speech-to-text features\n        - Extract some features from NLP techniques.\n        - With extracted features, you can generate advanced Machine Learning classifier.\n\n## Provisioning\n\nAt first, set up your Azure resource with [the instruction](./src/engine/00.%20provisioning.ipynb), which contains Azure resouce generation, setting python environment, and configuration some datastore to be used in experiencing training notebook.\n\n## Preparing some constants and files\n\nIn order to analyze audio files, you need to prepare the following files:\n\n- Audio files with `.wav` format\n    - Prepare audio files, which you want to analyze. Then, please upload all of them into `recordings` directory in  `raw` container. This container should be already generated in [here](./src/engine/00.%20provisioning.ipynb).\n\n- Transcription for each audio file\n    - It defines true transcription for audio file, which will be used in measuring the accuracy of Speech-to-Text \n\n- Ground truth for each audio file, which will be used in Machine Learning classifier:\n\n- Aggregated csv file `transcription-truth.csv`, which include the following columns:\n    - `audio file name`: Names of sample audio files\n        - This repository was verified with some `.wav` files.\n    - `true transcription`: True transcription for each audio file\n    - `Labels for each sample audio file`: Ground-truth of the classifier\n        - Set your labels in `MESSAGE_CLASSIFICATION_GROUP` in `./src/common/constants.py`.\n            - Ex.) If you define labels with `UPPER`, `LOWER` and `OTHERS`, please indicate them as \"list\" object in python like `MESSAGE_CLASSIFICATION_GROUP = ['OTHERS', 'UPPER', 'LOWER']` as well.\n\n        ![Image](./documentation/images/transcripts-truth.png)\n\n    NB: You don't need to add column names in header of csv file, and just put actual values there\n\n- Vocabulary definition files\n    - `general-ontology.json`\n        - It defines the frequently used vocabularies in a particular use case, which are used to boost the performance of speech to text transcribing.\n\n    - `homophone-list.txt`\n        - It defines pairs of homophone vocabularies, which are used to correct the results of the original transcriptions in general to be more frequent vocabularies in the specific use case.  \n\n    - `key-phrases-to-search.json`\n        - It defines the key phrases in a specific use case, which will be used for NLP text mining. \n\nOnce you prepare those files, please follow the instruction in `0.6 Upload those files` in [here](./src/engine/00.%20provisioning.ipynb)\n\n## Training\n\nNow, we're ready to enjoy the experiments\n\n- [Training Notebook](./src/engine/10.%20training.ipynb): Demonstrate a series of processes like loading audio files, extract texts with speech-to-text technology, extract insights with NLP-techniques, generate classifier given ground-truth labels.\n\n# An example use case in Mining Industry\n\nThis part will elaborate the specific use case of positive communication in mining industry. \n\n## Positive communication\n\nIn mining industry, there are a variety of vehicles involved in the normal operations a mining site, ranging from light vehicles to various type of heavy mobile platforms like dump trucks, excavators and loaders. The complicated interactions between the diverse types of vehiles in the complicated mining scenario are potential risk for the safe operation of a mining site. As such, multiple control measures are demanded to mitigate such risk.\n\nOne of these measures is called positive communications, which means clear and proactive communication with repeated confirmation. The radio communications between vehicle drivers are required to following this positive communication protocol. \n\nFor example, a 50m safety distance around a heavy vehicle is assumed to ensure safety. When another vehicle approaches the 50m safety distance of the heavy vehicle, it needs to initiate a positive communication such as \"40 loader, this is 539 dump truck. Permission to pass your left.\" and wait for the response for the heavy vehicle. After receiving the request, the heavy vehicle is expected to reply in a clear and repeated way such as \"539 dump truck. You are clear to pass 40 loader on the left.\"    \n\n![Positive communication illustration](./documentation/images/positive_communication.PNG)\n\n## Automated transcribing and compliance classification\n\nThe mining companies have the demands to ensure the compliance of the radio communication to this positive communication protocol. Traditionally, there is a human supervisor responsible for monitoring the radio communication and evaluate the trend of compliance manually, which is inefficient and subjective.\n\nAs such, automated monitoring the radio communication and classifying if they are compliant to positive communication or not is highly demanded. \n\n## Sample files for mining use case\n\nsome customized samples files [sample_files_mining](./sample_files_mining) are provided to illustrate the use case of the accelerator. The sample files in it are explained as follows.\n\n### Audio files with `.wav` format\n- Prepared audio files, which you want to analyze. Then, please upload all of them into `recordings` directory in  `raw` container. This container should be already generated in [here](./src/engine/00.%20provisioning.ipynb). In this sample, the audio files are just an illustration of positive communication in mining which is not real recording from radio communication in production mining scenario. \n\n### Aggregated csv file `transcription-truth.csv`\n    It includes the following columns:\n\n- `audio file name`: Names of sample audio files. In this example, the audio file is named by the following format: <No. of conversation>-<start_date: YYMMDD>-<start_time: HHMMSS>-<end_date: YYMMDD>-<end_time: HHMMSS>. \n- `true transcription`: True transcription for each audio file, which will be used in measuring the accuracy of Speech-to-Text.\n- `Labels for each sample audio file`: Ground-truth of the classifier\n    - Y: It is a positive communication and it IS compliant.\n    - N: If is a positive communication but it is NOT compliant. \n    - X: OTHERS, which is not a positive communication.\n    Note that you don't need to add column names in header of csv file, and just put actual values there\n\n### Vocabulary definition files\n- `general-ontology.json`\n    - It defines the frequently used vocabularies in a particular use case, which are used to boost the performance of speech to text transcribing. This file should be organized as following JSON format.\n    ```yaml\n    {   \n        \"general\" : [\"Say again\", \"understand\", \"permission\", \"clear\"],\n        \"orientation\" : [\"left\", \"right\", \"front\", \"behind\"]\n    }\n    ```\n    Some words with frequently occurences in positive communications of mining can be selected to use as the general_ontology.\n\n- `homophone-list.txt`\n    - It defines pairs of homophone vocabularies, which are used to correct the results of the original transcriptions in general to be more frequent vocabularies in the specific use case. This file should be organized as the following format.\n    ```yaml\n    [\n        ('is there', 'zero'), \n        ('stock', 'stop'),\n        ('full', 'four'),\n        ('stick', 'six'),\n        ('project', 'approaching'),\n        ('through', 'two'),\n        ('jerry', 'zero'),\n        ('speaker', 'three two'),\n    ]\n    ```         \n    The vocabularies are uttered and transcribed but should be replaced with other words in positive communication use case. These vocabulary pairs should be listed here. This file should be organized as the following format.\n\n- `key-phrases-to-search.json`\n    - It defines the key phrases in a specific use case, which will be used for NLP text mining. \n    ```yaml\n    {   \n        \"vehicle_phrases\" : [\"loader\", \"dump truck\", \"excavator\", \"grader\", \"crane\"],\n        \"movement_phrases\" : [\"pass\", \"halted\", \"moving\", \"approaching\", \"turn\", \"stopped\"]\n    }\n    ```\n    The keywords which are beneficial for classifying whether is a positive communication and/or compliant should be provided here for the NLP feature extraction. \n\nIf you would like to try this use case, please follow the instruction in `0.6 Upload those files` in [here](./src/engine/00.%20provisioning.ipynb) to put the sample files in the right locations.\n\n# Project Folder Structure\n\n    \u251c\u2500\u2500 Speech2Text_NLP             # The main package, including the experiemntation and Azure ML pipelines\n    \u2502   \u251c\u2500\u2500 documentation           # Additional documentation descrining the system and any images used\n    |   |\u2500\u2500 sample_files_mining     # Sample files for mining use case\n    |   |      |\u2500\u2500 Audio_files      # Audio files with\u00a0.wav\u00a0format\n    |   |      |\u2500\u2500 ontoloy_files    # Frequently used vocabularies in a particular use case\n    |   |      |\u2500\u2500 transcripts-truth# Ground truth labels for transcriptions and classifications      \n    \u2502   |\u2500\u2500 src                     # Main scripts to perform the experimentation and the required inferences\n    |   |      |\u2500\u2500 common           # The common folder containing any constants decleration\n    |   |      |\u2500\u2500 engine           # The main training and inference scripts\n    |   |      |\u2500\u2500 pipeline         # The data ingestion, orchestration and Azure ML training & inference pipelines\n    \u2502   |\u2500\u2500 CODE_OF_CONDUCT.md\n    \u2502   |\u2500\u2500 CONTRIBUTING.md  \n    \u2502   |\u2500\u2500 LICENSE\n    \u2502   |\u2500\u2500 README.md\n    \u2502   |\u2500\u2500 SECURITY.md\n    \u2502   |\u2500\u2500 SUPPORT.md    \n\n# Contact\n\nFor more details or help deploying, contact the following:\n* [Nejhdeh Ghevondian](neghevo@microsoft.com), Microsoft\n* [Stan Kotlyar](stan.kotlyar@microsoft.com), Microsoft\n* [Michael Keane](michael.keane@microsoft.com), Microsoft\n* [Ben Plummer](benplummer@microsoft.com), Microsoft\n* [Kyoichi Iwasaki](kyiwasak@microsoft.com), Microsoft\n* [Zachary Hou](zacharyhou@microsoft.com), Microsoft\n\n\n# Acknowledgements\n\nThis repository is build in part using the following frameworks:\n- [python-dotenv](https://pypi.org/project/python-dotenv/)\n- [azure-cognitiveservices-speech](https://pypi.org/project/azure-cognitiveservices-speech/)\n- [upgrade azureml-sdk](https://pypi.org/project/azureml-sdk/)\n- [jiwer](https://pypi.org/project/jiwer/)\n- [nltk](https://pypi.org/project/nltk/)\n- [gensim](https://pypi.org/project/gensim/)\n- [pyldavis](https://pypi.org/project/pyLDAvis/)\n- [enum34](https://pypi.org/project/enum34/)\n- [noisereduce](https://pypi.org/project/noisereduce/)\n- [moviepy](https://pypi.org/project/moviepy/)\n- [mgrs](https://pypi.org/project/mgrs/)\n\n\n# Backlogs\n\n- Add more detailed NLP techniques in [Training notebook](./src/engine/10.%20training.ipynb).\n- Inferencing notebook.\n- CI/CD pipeline with training/inferencing.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-speech-to-text-nlp-mining-accelerator", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-speech-to-text-nlp-mining-accelerator", "platform_org_repo": "github+microsoft/dstoolkit-speech-to-text-nlp-mining-accelerator", "link_to_repo": "https://github.com/microsoft/dstoolkit-speech-to-text-nlp-mining-accelerator", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "<!-- markdownlint-disable MD041 MD033 MD034 -->\n<!-- Enable navigation back to top of page -->\n<a name=\"readme-top\"></a>\n\n<!-- PROJECT SHIELDS -->\n\n<div align=\"center\">\n\n[![Contributors][14]][contributors-url]\n[![Forks][15]][forks-url]\n[![Stargazers][17]][stars-url]\n[![GitHub Discussions][26]][discussions-url]\n[![Issues][16]][issues-url]\n\n</div>\n\n<div align=\"center\">\n\n[![Code License: MIT][12]][license-code-url]\n[![Docs License: CC-BY][13]][license-docs-url]\n\n</div>\n\n<br />\n<div align=\"center\">\n  <!--\n  <a href=\"https://github.com/microsoft/Documentarian\">\n    <img src=\"images/logo.png\" alt=\"Logo\" width=\"80\" height=\"80\">\n  </a>\n  -->\n  <h3 align=\"center\">Documentarian Modules</h3>\n\nAn open source toolkit for documentarians and community contributors to reduce friction and provide\na delightful experience for contributing to and maintaining documentation.\n\n[**Explore the docs \u00bb**][h0]\n\n<br />\n\n[Report a problem][h1] \u00b7 [Ask a question][h2] \u00b7 [Propose an improvement][h3]\n\n</div>\n\n<!-- TABLE OF CONTENTS -->\n<details>\n<summary>Table of Contents</summary>\n\n1. [About the Project](#about-the-project)\n   - [Built With](#built-with)\n1. [Getting Started](#getting-started)\n   - [Prerequisites](#prerequisites)\n   - [Installation](#installation)\n1. [Roadmap](#roadmap)\n1. [Contributing](#contributing)\n1. [Legal Notices](#legal-notices)\n\n</details>\n\n## About The Project\n\nThe Documentarian Modules are an open source toolkit for documentarians and community contributors\nto reduce friction and provide a delightful experience for contributing to and maintaining\ndocumentation.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n### Built With\n\n<!--\nThis section should list any major frameworks/libraries used to bootstrap your project. Leave any\nadd-ons/plugins for the acknowledgements section.\n-->\n\n- [![PowerShell][11]][pwsh-url]\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Getting Started\n\nThe Documentarian modules aren't yet available on the [PowerShell Gallery][22]. For now, you'll\nneed to [manually download and install them][27].\n\n### Prerequisites\n\nThis is an example of how to list things you need to use the software and how to install them.\n\n- PowerShell 7.2 or later\n\n### Installation\n\nNot yet available. See [Contributing][01]\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n<!--\n## Usage\n\nUse this space to show useful examples of how a project can be used. Additional screenshots, code\nexamples and demos work well in this space. You may also link to more resources.\n\nFor more examples, please refer to the Documentation.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n-->\n\n## Roadmap\n\nSee the [open issues][06] for a full list of [proposed features][08] (and [known issues][07]).\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Contributing\n\n<!-- vale off -->\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repos using our\nCLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct][19]. For more information see\nthe [Code of Conduct FAQ][20] or contact [opencode@microsoft.com][25] with any additional questions\nor comments.\n\n<!-- vale on -->\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n## Legal Notices\n\n<!-- vale off -->\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License][02],\nsee the [LICENSE][23] file, and grant you a license to any code in the repository under the\n[MIT License][21], see the [LICENSE-CODE][24] file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the\ndocumentation may be either trademarks or registered trademarks of Microsoft in the United States\nand/or other countries. The licenses for this project do not grant you rights to use any Microsoft\nnames, logos, or trademarks. Microsoft's general trademark guidelines can be found at\nhttp://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/.\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights,\npatents, or trademarks, whether by implication, estoppel or otherwise.\n\n<!-- vale on -->\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n\n<!--\n## Contact\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n-->\n\n<!--\n## Acknowledgments\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n-->\n\n<!-- Shield Link References -->\n[contributors-url]: https://github.com/microsoft/Documentarian/graphs/contributors\n[forks-url]: https://github.com/microsoft/Documentarian/network/members\n[stars-url]: https://github.com/microsoft/Documentarian/stargazers\n[issues-url]: https://github.com/microsoft/Documentarian/issues\n[discussions-url]: https://github.com/microsoft/Documentarian/discussions\n[license-code-url]: https://github.com/microsoft/Documentarian/blob/main/LICENSE-CODE\n[license-docs-url]: https://github.com/microsoft/Documentarian/blob/main/LICENSE\n[pwsh-url]: https://learn.microsoft.com/powershell\n<!-- Heading Link References -->\n[h0]: https://microsoft.github.io/Documentarian\n[h1]: https://github.com/microsoft/Documentarian/issues/new?assignees=&labels=bug%2Cneeds-triage&template=code-bug.yml\n[h2]: https://github.com/microsoft/Documentarian/discussions/new?category=q-a\n[h3]: https://github.com/microsoft/Documentarian/discussions/new?category=code-proposals\n<!-- Link References -->\n[01]: #contributing\n[02]: https://creativecommons.org/licenses/by/4.0/legalcode\n[06]: https://github.com/microsoft/Documentarian/issues\n[07]: https://github.com/microsoft/Documentarian/labels/bug\n[08]: https://github.com/microsoft/Documentarian/discussions/categories/code-proposals\n[11]: https://img.shields.io/badge/PowerShell-v7.2-blue?logo=powershell\n[12]: https://img.shields.io/badge/License%20(Code)-MIT-green?style=for-the-badge\n[13]: https://img.shields.io/badge/License%20(Docs)-CC--BY--4.0-green?style=for-the-badge\n[14]: https://img.shields.io/github/contributors/microsoft/Documentarian?style=for-the-badge\n[15]: https://img.shields.io/github/forks/microsoft/Documentarian.svg?style=for-the-badge\n[16]: https://img.shields.io/github/issues/microsoft/Documentarian.svg?style=for-the-badge\n[17]: https://img.shields.io/github/stars/microsoft/Documentarian.svg?style=for-the-badge\n[19]: https://opensource.microsoft.com/codeofconduct/\n[20]: https://opensource.microsoft.com/codeofconduct/faq/\n[21]: https://opensource.org/licenses/MIT\n[22]: https://powershellgallery.com\n[23]: LICENSE\n[24]: LICENSE-CODE\n[25]: mailto:opencode@microsoft.com\n[26]: https://img.shields.io/github/discussions/microsoft/Documentarian?style=for-the-badge\n[27]: https://microsoft.github.io/Documentarian/docs/authoring/install-tools/\n", "repo_name": "Documentarian", "org_name": "microsoft", "org_repo": "microsoft/Documentarian", "platform_org_repo": "github+microsoft/Documentarian", "link_to_repo": "https://github.com/microsoft/Documentarian", "platform": "github", "language": "PowerShell", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# Purpose\nThis repo just builds to a [*GitHub Pages* site](https://microsoft.github.io/CSS-SystemCenter), which contains links to Support Scripts used by other System Center products like:\n\n- [Operations Manager (SCOM)](https://github.com/microsoft/CSS-SystemCenter-OperationsManager)\n- [Service Manager (SCSM)](https://microsoft.github.io/CSS-SystemCenter-ServiceManager)\n- [Orchestrator (SCO)](https://microsoft.github.io/CSS-SystemCenter-Orchestrator)\n- [Virtual Machine Manager (VMM)](https://github.com/blakedrumm/SCVMM-Scripts-and-SQL)\n\nThese repos are maintained by Microsoft technical support engineers in Customer Support Services (CSS).\n\n## Contributing\n\nPlease navigate to a repo you want to contribute.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CSS-SystemCenter", "org_name": "microsoft", "org_repo": "microsoft/CSS-SystemCenter", "platform_org_repo": "github+microsoft/CSS-SystemCenter", "link_to_repo": "https://github.com/microsoft/CSS-SystemCenter", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Power-CAT", "org_name": "microsoft", "org_repo": "microsoft/Power-CAT", "platform_org_repo": "github+microsoft/Power-CAT", "link_to_repo": "https://github.com/microsoft/Power-CAT", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Power CAT\nWe are the Power Customer Advisory Team (Power CAT), part of the Microsoft Power Platform engineering team at Microsoft. We work with a specific group of key enterprise customers and do whatever it takes to ensure their success with Power Platform. We are a diverse group of technical architects, community managers, program managers, developers, and content creators, located all over the world, all sharing a deep passion for the possibilities of low-code. \n\nOur charter includes engaging with key enterprise customers and guiding their platform implementation to success, sharing with the world what we learn from our engagements with customers in guidance and tools, and celebrating the success of our customers via compelling customer stories.\n\nThis repository contains the Power CAT website at https://microsoft.github.io/powercat.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "powercat", "org_name": "microsoft", "org_repo": "microsoft/powercat", "platform_org_repo": "github+microsoft/powercat", "link_to_repo": "https://github.com/microsoft/powercat", "platform": "github", "language": "SCSS", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\nThis repository is for defining various networking hardware offload specifications. When officially complete, these specs may\nbe published at a more standard location (TBD).\n\n- [UDP RSC Offload](udp-rsc-offload.md) - Offloading UDP datagram coalescing and reassembly.\n- [QUIC Encryption Offload](quic-encryption-offload.md) - Offloading QUIC short header packet encryption and decryption.\n- [Time based Packet Transmission Offload](time-based-packet-transmission-offload.md) - Offloading per packet transmission timestamps.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "net-offloads", "org_name": "microsoft", "org_repo": "microsoft/net-offloads", "platform_org_repo": "github+microsoft/net-offloads", "link_to_repo": "https://github.com/microsoft/net-offloads", "platform": "github", "language": "C", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Video Call MOS\nThis repository provides the code and dataset for the Video Call MOS (VCM) prediction model, accepted at ICASSP 2023.\nThe model predicts the perceived video quality of videos that were transmitted via videoconferencing calls.\nIn contrast to other state-of-the-art video MOS models it is able to take temporal distortions, such as video freezes, into account.\nWe further provide a dataset with live Microsoft Teams video recordings and crowdsourced subjective quality ratings using [P.910 Crowd](https://github.com/microsoft/P.910). \nThe prediction is performed with the following steps:\n\n 1. Time-alignment of reference video via QR-code marker detection\n 2. VMAF Computation\n 3. Frame freeze feature computation based on time-alignment indices\n 4. Predict MOS with Video Call MOS LSTM, using VMAF and frame freeze features as input\n\nLink to paper: [Gabriel Mittag, Babak Naderi, Vishak Gopal and Ross Cutler, \u201cLSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls,\u201d accepted at ICASSP 2023, 2023.](https://arxiv.org/pdf/2303.12761v1.pdf)\n\n## Performance\nIn comparison to VMAF, the proposed VCM model performs better on videos with temporal distortions. The following figure shows how VMAF overestimates the quality for multiple samples in the validation dataset:\n<br><br><img src=\"imgs/results.png\" width=\"500\" >\n\nThe following example shows the per-frame predictions for a video that is impaired by a single freeze of around 1 second. According to the crowdsourced ratings, the ground truth video quality MOS is 2.95. Because VMAF does not take the temporal freeze but only the reduced resolution / bitrate into acount, it overestimates the quality with a score of 3.52. In contrast, the proposed VCM model reduces the predictions during frozen frames, resulting in an overall MOS score close to the ground truth.\n<br><br><img src=\"imgs/example_1.png\" width=\"500\" >\n\nThe next figure shows a similar effect but instead with multiple shorter frame freezes:\n<br><br><img src=\"imgs/example_2.png\" width=\"500\" >\n\n\nPlease refer to the [paper](https://arxiv.org/pdf/2303.12761v1.pdf) for more detailed results.\n\n## Requirements\nThe code in this repository was tested on Ubuntu. Adjustments to the FFMPEG commands may be necessary when running on Windows.\nTo perform reference video alignment and VMAF computation, FFMPEG with VMAF support is required, which can be installed on Ubuntu via the following steps (optional for training and evaluation on the VCM dataset, as pre-computed VMAF features are available in CSV files).\nSee also https://www.johnvansickle.com/ffmpeg/faq for more info on the FFMPEG installation.\n\n```bash\napt-get update -y\napt-get install -y libzbar0 libgl1 # needed for reading QR-codes\nwget -q https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz\ntar xf ffmpeg-git-amd64-static.tar.xz\nmv ffmpeg-git-*-amd64-static/ffmpeg ffmpeg-git-*-amd64-static/ffprobe /usr/local/bin/\n```\n\nIt is recommended to create a new virtual or conda environment dedicated to the project. Use the following command to install the required python packages via pip.\n\n```bash\npip install requirements.txt\n```\n\n## Dataset\nBefore running the code, it is necessary to download the Video Call MOS dataset. Please note that the dataset is a subset of the one used in the [paper](https://arxiv.org/pdf/2303.12761v1.pdf). It can be found here:\n\nhttps://challenge.blob.core.windows.net/video-call-mos/video_call_mos_set.zip\n\nThe dataset contains 10 reference videos and 1467 degraded videos. The videos were transmitted via Microsoft Teams calls in 83 different network conditions and contain various typical videoconferencing impairments. It also includes [P.910 Crowd](https://github.com/microsoft/P.910) subjective video MOS ratings (see [paper](https://arxiv.org/pdf/2303.12761v1.pdf) for more info).\n\n## Evaluating\nTo evaluate the default VCM or a newly trained model, the following script can be run. It also plots correlation diagrams and per-frame MOS predictions and compares the results to VMAF (it should reproduce exactly the same results as shown above in [Performance](#Performance)). The path variables `data_dir` and `csv_file` within the script need to be updated before executing. \n\n```bash\npython run_evaluation_and_plotting.py   \n```\n\nThe script is using the pre-computed VMAF features and alignment indices loaded from CSV files as inputs to the VCM model. For a new dataset, new CSV files can be written by using the `run_video_call_mos_on_dataset.py` script (see [Video Quality Prediction](#Video-Quality-Prediction)).\n\n## Video Quality Prediction\nTo predict the MOS score of a single video file, the following command can be used:\n```bash\npython run_video_call_mos.py --deg_video /path/to/video_call_mos_set/data/deg_0001.mp4 --ref_video /path/to/video_call_mos_set/data/ref_01.mp4 --results_dir /path/to/video_call_mos_set/results --tmp_dir /path/to/video_call_mos_set/tmp\n```\nThis command requires longer computation time and will run the inference end-to-end, including QR-code detection, reference alignment, VMAF computation, and Video Call MOS LSTM model. Note that the code expects 1920x1080 MP4 video files and the reference and degraded videos need to have QR-code markers drawn onto them (see [Draw QR-code markers](#Draw-QR-code-markers)).\n\nTo run the Video Call MOS model on a dataset provided via CSV file, the following script can be used (the paths within the script need to be updated):\n```bash\npython run_video_call_mos_on_dataset.py\n```\n\n## Training\nTo train a new Video Call MOS model following script can be used. It uses pre-computed VMAF features and alignment indices loaded from CSV files as inputs. For a new dataset, new CSV files can be written by using the `run_video_call_mos_on_dataset.py` script (see [Video Quality Prediction](#Video-Quality-Prediction)). The path variables within the script need to be updated before running the script. The training parameters, such as, which input features to use, the number of epochs or LSTM layers and hidden units size may be adjusted as well.\n\n```bash\npython run_training.py   \n```\n\n## Draw QR-code Markers\nBecause videos received during a video call are prone to frame freezes, skips and playback rate changes, it is necessary to align the degraded videos to the clean reference video. In order to allow for a robust time alignment, we apply QR-code markers to the source videos. The reference videos in the Video Call MOS dataset are already prepared with QR-code markers. To draw markers on new reference videos, the following script can be used. The paths and parameters within the script need to be updated. Please note that the script expects 1920x1080 MP4 video files but could be adjusted for other formats.\n\n```bash\npython run_draw_qr_codes.py   \n```\n\n## Citation\nIf you use the code or dataset in a publication please cite the following [paper](https://arxiv.org/pdf/2303.12761v1.pdf):\n\n```BibTex\n@inproceedings{vcm_icassp,\n  title={LSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls},\n  author={Mittag, Gabriel and Naderi, Babak and Gopal, Vishak and Cutler, Ross},\n  booktitle={accepted at ICASSP 2023},\n  year={2023}\n}\n```\n\n# Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a\nCLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the\ndocumentation may be either trademarks or registered trademarks of Microsoft in the United States\nand/or other countries. The licenses for this project do not grant you rights to use any Microsoft\nnames, logos, or trademarks. Microsoft's general trademark guidelines can be found at\nhttp://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/privacystatement.\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n\n## Dataset licenses\nMICROSOFT PROVIDES THE DATASETS ON AN \"AS IS\" BASIS. MICROSOFT MAKES NO WARRANTIES, EXPRESS OR IMPLIED, GUARANTEES OR CONDITIONS WITH RESPECT TO YOUR USE OF THE DATASETS. TO THE EXTENT PERMITTED UNDER YOUR LOCAL LAW, MICROSOFT DISCLAIMS ALL LIABILITY FOR ANY DAMAGES OR LOSSES, INCLUDING DIRECT, CONSEQUENTIAL, SPECIAL, INDIRECT, INCIDENTAL OR PUNITIVE, RESULTING FROM YOUR USE OF THE DATASETS.\n\nThe dataset is provided under the original terms that Microsoft received the source dataset. The Terms of Use of the Microsoft Learn videos, which are used as source videos in the Video Call MOS dataset, can be found at https://learn.microsoft.com/en-us/legal/termsofuse.\n\n## Code license\nMIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n", "repo_name": "Video_Call_MOS", "org_name": "microsoft", "org_repo": "microsoft/Video_Call_MOS", "platform_org_repo": "github+microsoft/Video_Call_MOS", "link_to_repo": "https://github.com/microsoft/Video_Call_MOS", "platform": "github", "language": "Python", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "![GitHub Logo](/Header.png)\n\n# What are Emerging Tech Workshops?\n\nEmerging Tech Workshops (ETW) is series of workshops that will give you a chance to get experience with our security and compliance solutions and present the value of these products in the context of security and compliance challenges that a typical organisation faces.\n\nThe workshop will use a set of scenario-based challenges which will cover the journey from risk identification to mitigation and value delivery and leave you with knowledge that you will be able to translate to your own work environment.\n\nThese challenges are not step-by-step labs, but provide scenario-driven requirements that attendees work collaboratively to solve. The attendee squads are not alone in implementing the requirements, as the workshop coaches work with each team to provide guidance and help along the way. \n\nEnvironment: These workshops can be hosted virtually over Teams or in-person (for larger audience)\n\n# Typical team size:\nAttendees work in teams of 5 to 10 people to solve a series of challenges.\n\n# Workshops Available:\n\n## Security Workshop\nThis workshop enables participants to implement recommended security best-practices by working through challenges inspired from real-world scenarios. \n\nBy the end of the workshop, participants will gain hands-on knowledge on how to assess security setup for an organisation architect against required policies and regulations. They will also go through the process of architecting and implementing policies to mitigate any threats discovered during the assessment.\n5-6 participants in each team\n\n[Let's Get Started](/ETWorkshops/Security/Intro.html)\n\n## Entra Permissions Management\n\nAccess governance for cloud environments is a key focus area for a lot of organisations. Due to complexity of cloud IAM and lack of visibility around permissions usage, cloud identities become high-value target for attackers. \n \nThis workshop leverages challenges inspired from real-world scenarios to enable your team on Entra Permissions Management (EPM), a CIEM solution that provides visibility and control over cloud permissions and help minimize over-provisioning of access. \n \nBy the end of the workshop, participants will gain knowledge on how to discover and mitigate permissions gap in their cloud infrastructure, and align their security redesign initiative with Zero Trust framework by implementing Least Privilege\n\n[Let's Get Started](/ETWorkshops/EPM/Intro.html)\n\n## Compliance and Privacy\nDetails Coming soon.\n\n[Let's Get Started](/ETWorkshops/Compliance/Intro.html)\n\n## Defender IoT\n\nVisibility and security of OT/IoT infrastructure is a key area of concerns for many industries. \n\nThis workshop leverages challenges inspired from real-world scenarios to enable your team on using Microsoft Defender for IoT agentless network detection and response (NDR) capability to gain comprehensive security across your IoT/OT infrastructure. \n\nBy the end of the workshop, participants will gain knowledge on how to rapidly deploy Defender IoT suite across a IoT / OT landscape and utilise it to provide solve some common business problems and secure an OT environment. \n\n[Let's Get Started](/ETWorkshops/IoT/Intro.html)\n\n\n## Threat Intelligence (TI) and External Attack Surface Management (EASM)\n\nMany businesses have internet-facing assets they may not be aware of or have simply forgotten about. These are often created by shadow IT, mergers, and acquisitions, incomplete cataloguing, business partners\u2019 exposure, or simply rapid business growth. \n \nThis workshop leverages challenges inspired from real-world scenarios to enable your team on Defender Threat Intelligence and External Attack Surface Management, a pair of solutions that map the internet every day, providing security teams with the necessary information to understand adversaries and their attack techniques.\n\nBy the end of the workshop, participants will gain knowledge on how to discover and investigate threats from external exposure, perform threat hunting and intelligence gathering, and deploy customised and relevant cyber threat intelligence to Sentinel.\n\n[Let's Get Started](/ETWorkshops/TIEASM/Intro.html)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ETWorkshops", "org_name": "microsoft", "org_repo": "microsoft/ETWorkshops", "platform_org_repo": "github+microsoft/ETWorkshops", "link_to_repo": "https://github.com/microsoft/ETWorkshops", "platform": "github", "language": null, "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "NBLyzer", "org_name": "microsoft", "org_repo": "microsoft/NBLyzer", "platform_org_repo": "github+microsoft/NBLyzer", "link_to_repo": "https://github.com/microsoft/NBLyzer", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "![Microsoft Defending Democracy Program: ElectionGuard Core2][banner image]\n\n# \ud83d\uddf3 ElectionGuard Core2\n\n[![ElectionGuard Specification 1.1.0](https://img.shields.io/badge/\ud83d\uddf3%20ElectionGuard%20Specification-1.1.0-green)](https://www.electionguard.vote) ![Github Package Action](https://github.com/microsoft/electionguard-core2/workflows/Release/badge.svg) [![license](https://img.shields.io/github/license/microsoft/electionguard-core2)](https://github.com/microsoft/electionguard-core2/blob/main/LICENSE) [![license](https://img.shields.io/nuget/v/Electionguard.Encryption)](https://www.nuget.org/packages/ElectionGuard.Encryption/)\n\nThis monorepo contains the ElectionGuard 2.0+ code. The core functionality is implemented in C++ for performance and interoperability. It provides functionality for all ElectionGuard workflows including key ceremony, ballot encryption, tally, ballot decryption, and verification. It is designed to be integrated into existing (or new) voting system software. It includes a variety of interop layers to provide functionality to languages including C, .NET, and Java.\n\nThis repository is `pre-release` software to showcase the ElectionGuard API implemented in a native language. It is not feature complete and should not be used for production applications.\n\n## \ud83d\udcc1 In This Repository\n\n| File/folder                          | Description                                |\n| ------------------------------------ | ------------------------------------------ |\n| [`.github`](.github)                 | Github workflows and issue templates       |\n| [`.vscode`](.vscode)                 | VS Code configurations                     |\n| [`/bindings`](/bindings)             | Binding interfaces for different languages |\n| [`/cmake`](/cmake)                   | `CMake` dependencies`                      |\n| [`/include`](/include)               | Public include headers                     |\n| [`/src`](/src)                       | ElectionGuard source code                  |\n| [`/test`](/test)                     | Unit tests                                 |\n| [`.clang-format`](.clang-format)     | Style guidelines                           |\n| [`.gitignore`](.gitignore)           | Define what to ignore at commit time.      |\n| [`CMakeLists.txt`](CMakeLists.txt)   | Root CMake file                            |\n| [`CONTRIBUTING.md`](CONTRIBUTING.md) | Guidelines for contributing to the sample. |\n| [`README.md`](README.md)             | This README file.                          |\n| [`LICENSE`](LICENSE)                 | The license for the sample.                |\n\n## \u2753 What Is ElectionGuard?\n\nElectionGuard is an open source software development kit (SDK) that makes voting more secure, transparent and accessible. The ElectionGuard SDK leverages homomorphic encryption to ensure that votes recorded by electronic systems of any type remain encrypted, secure, and secret. Meanwhile, ElectionGuard also allows verifiable and accurate tallying of ballots by any 3rd party organization without compromising secrecy or security.\n\nLearn More in the [ElectionGuard Repository](https://github.com/microsoft/electionguard)\n\n## \ud83e\uddb8 How Can I use ElectionGuard?\n\nElectionGuard supports a variety of use cases. The Primary use case is to generate verifiable end-to-end (E2E) encrypted elections. The ElectionGuard process can also be used for other use cases such as privacy enhanced risk-limiting audits (RLAs). This implementation only includes encryption functions and cannot be used to generate election keys and it cannot decrypt tally results.\n\nThis c++ implementation also includes a C API that can be consumed from anywhere that can call C code directly. A .Net Standard package is also provided.\n\n## \ud83d\udcbb Requirements\n\n### All Platforms\n\n- A [C++17](https://isocpp.org/get-started) standard compliant compiler is required to build the core library. While any modern compiler should work, the library is tested on a subset. Check out the [GitHub actions](#) to see what is officially supported.\n- [GNU Make](https://www.gnu.org/software/make/manual/make.html) is used to simplify the commands and GitHub Actions. This approach is recommended to simplify the command line experience. This is built in for MacOS and Linux. For Windows, setup is simpler with [Chocolatey](https://chocolatey.org/install) and installing the provided [make package](https://chocolatey.org/packages/make). The other Windows option is [manually installing make](http://gnuwin32.sourceforge.net/packages/make.htm).\n- [CMake](https://cmake.org/) is used to simplify the build experience.\n\n### \ud83e\udd16 Android\n\nTo build for android, you need the Android SDK and platforms 21 and 26. The easiest way is to download android studio. Alternatively, you can use the SDK installation that ships with the Xamarin Tooling in Visual Studio. WE also require the use of the Android NDK. Android builds can be compiled on Linux, Mac, or Windows\n\n- [Android SDK](https://developer.android.com/studio/#downloads)\n- [SDK 28](https://developer.android.com/studio/releases/platforms#9.0)\n- [NDK 25](https://developer.android.com/ndk/downloads/)\n\n### \ud83c\udf4f iOS\n\nTo build for iOS you need XCode installed\n\n- [XCode](https://developer.apple.com/xcode/resources/) and the [Command Line Tools for XCode](#)\n- [CMake 3.19](https://cmake.org/) may be necessary, along with changes to the Makefile. [See ISSUE #138](https://github.com/microsoft/electionguard-cpp/issues/138)\n\n### Linux\n\nThe automated install of dependencies is currently only supported on debian-based systems. See the makefile for more information.\n\n### Web Assembly\n\nBuilding for WebAssembly (wasm) is supported with the `emscripten` toolchain.\n\n- Install [emscripten](https://emscripten.org/docs/getting_started/downloads.html)\n- Install [Node Version Manager](https://github.com/nvm-sh/nvm)\n- Ensure the latest versions of both emscripten and node are activated\n- Ensure Emscripten and node are both in your path\n- run `make test-wasm` to build for wasm and validate the library.\n\n### \ud83d\udda5\ufe0f Windows (using MSVC)\n\nBuilding on windows is supported using the `MSVC` toolchain.\n\n- Install [Chocolatey](https://chocolatey.org/install)\n- Install [Powershell Core](https://github.com/powershell/powershell)\n- Install [VS 2022](https://visualstudio.microsoft.com/vs/)\n- Open the Visual Studio Installer and install\n  -- MSVC v142 - VS 2022 C++ x64/x86 build tools\n  -- Windows 10 SDK (latest)\n  -- C++ CMake tools for Windows\n  -- C++/CLI support for v142 build tools\n\n### \ud83d\udda5\ufe0f Windows (using MSYS2)\n\nBuilding on windows is supported using the `MSYS2` toolchain. MSYS is the default toolchain on Windows.\n\n- Install [Chocolatey](https://chocolatey.org/install)\n- Install [Powershell Core](https://github.com/powershell/powershell)\n- Install [MSYS2](https://www.msys2.org)\n- Open the MSYS2 prompt by running the newly-created \"MSYS2 MSYS\" shortcut in your start menu.\n- Inside the prompt, run `pacman -Syu`, then close the window when it prompts you to.\n- Reopen the MSYS2 prompt and run:\n  ```\n  pacman -Syu\n  pacman -S mingw-w64-x86_64-gcc mingw-w64-x86_64-cmake make\n  ```\n- Modify your `%Path%` to include the newly-installed software. You should include these two paths:\n  ```\n  C:\\msys64\\mingw64\\bin\n  C:\\msys64\\usr\\bin\n  ```\n\n#### \ud83d\udea7 The Procedure Entry Point Could not be Located\n\nWhen compiling with shared libraries, you may encounter an error running the unit tests project. This is likely due to windows resolving the incorrect implementation of `libstdc++-6.dll`. Solving this depends on your use case, but you can either ensure that the path modifications made above appear before any other paths which include this library (e.g. c\\Windows\\System32\\), or you can include a copy of the correct DLL in the output folder. [See this StackOverflow post for more information](https://stackoverflow.com/questions/18668003/the-procedure-entry-point-gxx-personality-v0-could-not-be-located)\n\n### \ud83c\udf10 .NET Standard\n\nA .NET Standard binding library is provided so that this package can be consumed from C# applications. At this time, MacOS, Linux and Windows are supported.\n\n- [Latest DotNet SDK](https://dotnet.microsoft.com/download)\n- [Visual Studio](https://visualstudio.microsoft.com)\n- [NuGet Command Line (CLI)](https://docs.microsoft.com/en-us/nuget/reference/nuget-exe-cli-reference#macoslinux)\n- On Linux, you need [Mono](https://www.mono-project.com/download/stable/)\n\n## Build C++\n\nUsing **make**,\n\n### Download Dependencies\n\n```sh\nmake environment\n```\n\n### Build the Library for the current host (Release, default toolchain)\n\n```sh\nmake build\n```\n\n### Build the Library for the current host (Debug, default toolchain)\n\n```sh\nexport TARGET=Debug && make build\n```\n\n### Android\n\nThe Android Build currently Targets API Level 26 but can be configured by modifying the Makefile\n\nSet the path to the NDK, replacing the version with your own\n\n```sh\nexport NDK_PATH=/Users/$USER/Library/Android/sdk/ndk/21.3.6528147 && make build-android\n```\n\n### iOS\n\nThe iOS build currently targets iPhone OS 12 but can be configured by modifying the Makefile\n\nCreates a fat binary for the simulator and targets a recent version of iOS\n\n```sh\nmake build-ios\n```\n\n### Windows\n\nUsing the default MSYS2 toolchain:\n\n```pwsh\nmake build\n```\n\nUsing the MSVC toolchain:\n\n```pwsh\nmake build-msvc\n```\n\n## Build Wrappers\n\n### .Net Standard 2.0\n\nWraps the build artifacts in a C# wrapper conforming to .Net Standard 2.0.\n\n```sh\nmake build-netstandard\n```\n\n## Test\n\n### Running the C++ and C tests\n\n```sh\nmake test\n```\n\n#### Running the tests on Windows using the MSVC toolchain\n\n```sh\nmake test-msvc\n```\n\n### Running the netstandard tests\n\nTo run the tests when building for the current host (Linux, Mac, windows:)\n\n```sh\nmake build-netstandard\nmake test-netstandard\n```\n\nTo run the tests when building for a mobile device, you can run the .Net Standard tests using the Xamarin Test runner on the Android Emulator or the iOS simulator:\n\n**NOTE: Xamarin build support is temporarily disabled while the project migrates to the new SDK style project format.** Please refer to ISSUE #195 for more information.\n\n```sh\nmake build-netstandard\n```\n\nThen, open Visual studio for Mac and run the `ElectionGuard.Tests.Android` or `ElectionGuard.Tests.iOS` project.\n\n## \ud83d\udcc4 Documentation\n\n## Contributing\n\nThis project encourages community contributions for development, testing, documentation, code review, and performance analysis, etc. For more information on how to contribute, see [the contribution guidelines][contributing]\n\n### Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Reporting Issues\n\nPlease report any bugs, feature requests, or enhancements using the [GitHub Issue Tracker](https://github.com/microsoft/electionguard-python/issues). Please do not report any security vulnerabilities using the Issue Tracker. Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report). See the [Security Documentation][security] for more information.\n\n### Have Questions?\n\nElectionGuard would love for you to ask questions out in the open using GitHub Discussions. If you really want to email the ElectionGuard team, reach out at electionguard@microsoft.com.\n\n## License\n\nThis repository is licensed under the [MIT License]\n\n## Thanks! \ud83c\udf89\n\nA huge thank you to those who helped to contribute to this project so far, including:\n\n**[Josh Benaloh _(Microsoft)_](https://www.microsoft.com/en-us/research/people/benaloh/)**\n\n<a href=\"https://www.microsoft.com/en-us/research/people/benaloh/\"><img src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/09/avatar_user__1473484671-180x180.jpg\" title=\"Josh Benaloh\" width=\"80\" height=\"80\"></a>\n\n**[Steve Maier](https://github.com/SteveMaier-IRT) [_(InfernoRed Technology)_](https://infernored.com/)**\n\n<a href=\"https://github.com/SteveMaier-IRT\"><img src=\"https://avatars2.githubusercontent.com/u82616727?v=4\" title=\"SteveMaier-IRT\" width=\"80\" height=\"80\"></a>\n\n**[Keith Fung](https://github.com/keithrfung) [_(InfernoRed Technology)_](https://infernored.com/)**\n\n<a href=\"https://github.com/keithrfung\"><img src=\"https://avatars2.githubusercontent.com/u/10125297?v=4\" title=\"keithrfung\" width=\"80\" height=\"80\"></a>\n\n**[Matt Wilhelm](https://github.com/AddressXception) [_(InfernoRed Technology)_](https://infernored.com/)**\n\n<a href=\"https://github.com/AddressXception\"><img src=\"https://avatars0.githubusercontent.com/u/6232853?s=460&u=8fec95386acad6109ad71a2aad2d097b607ebd6a&v=4\" title=\"AddressXception\" width=\"80\" height=\"80\"></a>\n\n**[Dan S. Wallach](https://www.cs.rice.edu/~dwallach/) [_(Rice University)_](https://www.rice.edu/)**\n\n<a href=\"https://www.cs.rice.edu/~dwallach/\"><img src=\"https://avatars2.githubusercontent.com/u/743029?v=4\" title=\"danwallach\" width=\"80\" height=\"80\"></a>\n\n**[Marina Polubelova](https://polubelova.github.io/) [_(INRIA Paris)_](https://prosecco.gforge.inria.fr/)**\n\n<a href=\"https://polubelova.github.io/\"><img src=\"https://polubelova.github.io/authors/admin/avatar_hu562f921c0165de84dfdc53044b574fa1_846381_270x270_fill_q90_lanczos_center.jpg\" title=\"polubelova\" width=\"80\" height=\"80\"></a>\n\n**[Santiago Zanella-B\u00e9guelin](https://www.microsoft.com/en-us/research/people/santiago/) [_(Microsoft Research)_](https://www.microsoft.com/en-us/research/)**\n\n<a href=\"https://www.microsoft.com/en-us/research/people/santiago/\"><img src=\"https://www.microsoft.com/en-us/research/uploads/prod/2020/08/profile_cropped-5f44d9b09ecd7.jpg\" title=\"santiago\" width=\"80\" height=\"80\"></a>\n\n**[Jonathan Protzenko](https://jonathan.protzenko.fr/) [_(Microsoft Research)_](https://www.microsoft.com/en-us/research/group/research-software-engineering-rise/)**\n\n<a href=\"https://jonathan.protzenko.fr/\"><img src=\"https://jonathan.protzenko.fr/assets/protzenko.jpg\" title=\"protzenko\" width=\"80\" height=\"80\"></a>\n\n<!-- Links -->\n\n[banner image]: https://raw.githubusercontent.com/microsoft/electionguard-python/main/images/electionguard-banner.svg\n[pull request workflow]: https://github.com/microsoft/electionguard-ccpp/blob/main/.github/workflows/pull_request.yml\n[contributing]: https://github.com/microsoft/electionguard-core2/blob/main/CONTRIBUTING.md\n[security]: https://github.com/microsoft/electionguard-core2/blob/main/SECURITY.md\n[mit license]: https://github.com/microsoft/electionguard-core2/blob/main/LICENSE\n", "repo_name": "electionguard-core2", "org_name": "microsoft", "org_repo": "microsoft/electionguard-core2", "platform_org_repo": "github+microsoft/electionguard-core2", "link_to_repo": "https://github.com/microsoft/electionguard-core2", "platform": "github", "language": "C#", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# What is telemetry?\nMicrosoft Power Platform and Dynamics 365 products continuously emits telemetry about events that happen in the customer environment. \n\n# What can I use telemetry for?\nTelemetry can be useful for troubleshooting an issue or to determine how often a feature is used.\n\n# Is there a Frequently Asked Questions (FAQ)?\nPlease visit the [FAQ page](FAQ.md) for any questions on how to get started, pricing, privacy, and more.\n\n# What resources can I find in this repository?\nThis repository contains instructions for how you can obtain the telemetry and resources that help you get immediate value from the telemetry from the Dynamics 365 Supply Chain Management product.\n\n| Area of interest | Use it for  | Take me there (use CTRL+click to open in a new tab) |\n| ------ | ------ | ------ |\n| Alerts | If something happens in your environment that you need to take action on, make the system send you an alert. | [Alerting on telemetry](samples/Alerts) |\n| Excel | Want to make (refreshable) reports in Excel with data from Azure Application Insights? | [Using Excel with telemetry](samples/Excel) |\n| HTML | Want to construct a link that runs a KQL query and shows the result in the Azure Application Insights portal? | [Embedding telemetry links in your apps](samples/HTML) |\n| Kusto Query Language (KQL) | Want to query data in Azure Application Insights with KQL (similar to SQL)? | [Querying telemetry with KQL](samples/KQL) |\n| Power Automate | Want to query Azure Application Insights data in a Power Automate flow? | [Using telemetry with Power Automate](samples/PowerAutomate) |\n| Power BI | Want to query Azure Application Insights data in a Power BI report? | [Using telemetry with Power BI](samples/PowerBI) |\n| PowerShell | Want to query Azure Application Insights data in a Powershell script? | [Using telemetry with PowerShell](samples/Powershell) |\n| Troubleshooting | Want to make interactive troubleshooting guides with Jupyter notebooks? | [Using telemetry with Jupyter notebooks](samples/TroubleshootingGuides) |\n\n\n\n\n\n\n\n\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "d365-scm-telemetry", "org_name": "microsoft", "org_repo": "microsoft/d365-scm-telemetry", "platform_org_repo": "github+microsoft/d365-scm-telemetry", "link_to_repo": "https://github.com/microsoft/d365-scm-telemetry", "platform": "github", "language": null, "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation\n\nCode and data for EMNLP 2022 paper [PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation](https://arxiv.org/abs/2205.12697).\n\n\n### Requirements\n```angular2html\npython >= 3.8\ntransformers >= 4.5\ntorch >= 1.10.1\n```\n#### Evaluation Scripts\n- [multi-bleu.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl)\n- [ROUGE](https://github.com/bheinzerling/pyrouge)\n## Datasets\n\n### [LOGICNLG](https://github.com/wenhuchen/LogicNLG)\n\nOriginal LOGICNLG dataset consists of three `.json` files for train/dev/test samples and a directory `all_csv/` of `.csv` files for tables. \nTo extract the csv files:\n```angular2html\ncd data/logicnlg\nunzip all_csv.zip\n```\n\n### CONTLOG\nCONTLOG is collected based on [Logic2text](https://github.com/czyssrs/Logic2Text) dataset. We provide the table-to-text data with highlighted cells and pre-computed cell information in `data/contlog`.\n\n### Table-to-logic pretraining data\nDownload pretraining data from [here](https://drive.google.com/drive/folders/1MRqchdjwwOHsKEAer61jYDrTo0lnAyJX?usp=sharing).\n\n\n\n## Finetuning on Downstream Tasks\n\n### Training\n\n#### LOGICNLG\n```\nCUDA_VISIBLE_DEVICES=0 python train_logicnlg.py --do_train \\\n                          --model [t5-base|t5-large|facebook/bart-large]  \\\n                          --task text  \\\n                          --data_path data/logicnlg \\\n                          --use_cache \\\n                          --affix [experiment id]  \\\n                          --interval_type epoch  \\\n                          --pre_com \\\n                          --load_from [pretrained model checkpoint path]\n```\n#### CONTLOG\n```\nCUDA_VISIBLE_DEVICES=0 python train_contlog.py --do_train \\\n                          --model [t5-base|t5-large|facebook/bart-large]  \\\n                          --task text  \\\n                          --data_path data/contlog  \\\n                          --affix [experiment id] \\\n                          --interval_type epoch  \\\n                          --pre_com  \\\n                          --load_from [pretrained model checkpoint path] \n```\n\n### Inference\n#### LOGICNLG\n```\nCUDA_VISIBLE_DEVICES=0 python train_logicnlg.py --do_test \\\n                          --model [t5-base|t5-large|facebook/bart-large] \\\n                          --task text \\\n                          --data_path data/logicnlg \\\n                          --use_cache  \\\n                          --affix [experiment id] \\\n                          --pre_com  \\\n                          --load_from [checkpoint path]\n\n```\n\n#### CONTLOG\n```\nCUDA_VISIBLE_DEVICES=0 python train_logicnlg.py --do_test \\\n                          --model [t5-base|t5-large|facebook/bart-large] \\\n                          --task text \\\n                          --data_path data/contlog \\\n                          --affix [experiment id]  \\\n                          --pre_com  \\\n                          --load_from [checkpoint path]\n```\n## Pretraining with Table-to-Logic Data\n\n#### LOGICNLG\n```\nCUDA_VISIBLE_DEVICES=0 python train_logicnlg.py --do_train \\\n                          --model [t5-base|t5-large|facebook/bart-large] \\\n                          --task logic  \\\n                          --data_path data/logicnlg \\\n                          --use_cache  \\\n                          --affix [experiment id]  \\\n                          --interval_type step  \\\n                          --pre_com\n```\n#### CONTLOG\n```\nCUDA_VISIBLE_DEVICES=0 python train_contlog.py --do_train  \\\n                          --model [t5-base|t5-large|facebook/bart-large] \\\n                          --task logic \\\n                          --data_path data/contlog  \\\n                          --affix [experiment id]  \\\n                          --interval_type step \\\n                          --pre_com\n```\n\n## Evaluation of Model Outputs\nWe provide the model outputs in `model_outputs/` and evaluation scripts for TAPEX-Acc and TAPAS-Acc in `scripts/`.\n\n### TAPEX-Acc Evaluation\n\nTo use [TAPEX](https://github.com/microsoft/Table-Pretraining), you may need to install the latest version of Transformers and Datasets:\n```bash\npip install -U datasets\npip install -U transformers\n```\n\n#### CONTLOG\n\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python scripts/eval_contlog_with_tapex.py --do_predict \\\n                          --model_name_or_path microsoft/tapex-large-finetuned-tabfact \\\n                          --test_name model_outputs/contlog/plog-bart-large.txt \\\n                          --split_name test \\\n                          --output_dir tapex-contlog-eval \\\n                          --affix plog-bart-large \\\n                          --data_dir data/contlog \\\n                          --per_device_eval_batch_size 12 \\\n                          --eval_accumulation_steps 6                   \n```\n\n#### LOGICNLG\n\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python scripts/eval_logicnlg_with_tapex.py --do_predict \\\n                          --model_name_or_path microsoft/tapex-large-finetuned-tabfact \\\n                          --test_name model_outputs/logicnlg/plog-bart-large.txt \\\n                          --split_name test \\\n                          --output_dir tapex-logic-eval \\\n                          --affix plog-bart-large \\\n                          --data_dir data/logicnlg \\\n                          --per_device_eval_batch_size 12 \\\n                          --eval_accumulation_steps 6 \n\n```\n\n`--test_name` is the path to a model output file, `--affix` is the experiment ID, `--output_dir` is the directory for storing intermediate data files and prediction results, `--data_dir` is the path to original data files.\n\n### TAPAS-Acc Evaluation\n\n#### CONTLOG\n```bash\nCUDA_VISIBLE_DEVICES=0 python scripts/eval_contlog_with_tapas.py --test_file model_outputs/contlog/plog-bart-large.txt --data_dir data/contlog --batch_size 4 --split_name test\n```\n\n#### LOGICNLG\n```bash\nCUDA_VISIBLE_DEVICES=0 python scripts/eval_logicnlg_with_tapas.py --test_file model_outputs/logicnlg/plog-bart-large.json --data_dir data/logicnlg --batch_size 4\n```\n\n\n\n## Reference\n\nIf you find this project useful in your work, please consider citing the paper:\n\n```\n@article{liu2022plog,\n  title={PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation},\n  author={Liu, Ao and Dong, Haoyu and Okazaki, Naoaki and Han, Shi and Zhang, Dongmei},\n  journal={arXiv preprint arXiv:2205.12697},\n  year={2022}\n}\n```\n\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Security\n\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us through [https://docs.opensource.microsoft.com/releasing/securing-content/reporting-security-issues/](https://docs.opensource.microsoft.com/releasing/securing-content/reporting-security-issues/).\n\n\n\n", "repo_name": "PLOG", "org_name": "microsoft", "org_repo": "microsoft/PLOG", "platform_org_repo": "github+microsoft/PLOG", "link_to_repo": "https://github.com/microsoft/PLOG", "platform": "github", "language": "Python", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# TaCube\n\nThis is the official repository of:\n+ [TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data](https://arxiv.org/abs/2205.12682) \n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Security\n\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us through [https://docs.opensource.microsoft.com/releasing/securing-content/reporting-security-issues/](https://docs.opensource.microsoft.com/releasing/securing-content/reporting-security-issues/).\n", "repo_name": "TaCube", "org_name": "microsoft", "org_repo": "microsoft/TaCube", "platform_org_repo": "github+microsoft/TaCube", "link_to_repo": "https://github.com/microsoft/TaCube", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project Verona Runtime\n\nThis repository contains the runtime for the [Verona](https://github.com/microsoft/verona) research project.\nSee the main [Verona](https://github.com/microsoft/verona) repo for more information.\n\n## [Building](docs/building.md)\n\n## [Contributing](CONTRIBUTING.md)\n\n## [Internal Design](docs/internal)\n", "repo_name": "verona-rt", "org_name": "microsoft", "org_repo": "microsoft/verona-rt", "platform_org_repo": "github+microsoft/verona-rt", "link_to_repo": "https://github.com/microsoft/verona-rt", "platform": "github", "language": "C++", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# didx509cpp\n\nA header-only C++ library for verification of DID:x509 identifiers.\n\n[![Continuous Integration](https://github.com/microsoft/didx509cpp/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/microsoft/didx509cpp/actions/workflows/ci.yml) [![CodeQL](https://github.com/microsoft/didx509cpp/actions/workflows/codeql-analysis.yml/badge.svg?branch=main)](https://github.com/microsoft/didx509cpp/actions/workflows/codeql-analysis.yml)\n\n## Usage\n\n```cpp\n#include <didx509cpp.h>\n\nstd::string pem_chain = ...;\nstd::string did = \"did:x509:0:sha256:WE4P5dd8DnLHSkyHaIjhp4udlkF9LqoKwCvu9gl38jk::eku:1.3.6.1.4.1.311.10.3.13\";\n\ntry {\n    std::string doc = resolve(pem_chain, did));\n} catch (...)\n{...}\n    \n// Or when resolving a historical did, for example for audit purposes\n    \ntry {\n    std::string doc = resolve(pem_chain, did, true /* Ignore time */));\n} catch (...)\n{...}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "didx509cpp", "org_name": "microsoft", "org_repo": "microsoft/didx509cpp", "platform_org_repo": "github+microsoft/didx509cpp", "link_to_repo": "https://github.com/microsoft/didx509cpp", "platform": "github", "language": "C++", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "[![Microsoft Democracy Forward Program: ElectionGuard][election-guard-banner]](https://microsoft.github.io/electionguard/)\r\n\r\n[![license][license-image]](LICENSE)\r\n\r\nElectionGuard is an **open source** software development kit (SDK) that makes voting more secure, transparent and accessible. Announced at the [Build developer conference][build-developer-conference] in 2019, ElectionGuard enables end-to-end verification of elections as well as support the publication of results from ballot comparison audits. The ElectionGuard SDK leverages [homomorphic encryption][homomoprhic-encryption] to ensure that votes recorded by electronic systems of any type remain encrypted, secure, and secret. Results can be published online or made available to third-party organizations for secure validation, and allow individual voters to confirm their votes were correctly counted.\r\n\r\n## \u2764\ufe0f Open-Source\r\n\r\nThis library and all linked ElectionGuard projects, are licensed under the MIT license. There is no fee for using ElectionGuard.\r\n\r\n## \ud83d\ude80 Getting Started\r\n\r\nElectionGuard is always improving. To keep up with the latest, check our **[official site on GitHub Pages][election-guard-official-page]** and our [roadmap][election-guard-road-map]. For those looking to get started, we recommend the following repositories.\r\n\r\n### Documentation\r\n\r\nThis repository is a living document to help everyone interact with ElectionGuard. The [official ElectionGuard site][election-guard-official-page] is built using the `/docs` folder and [mkdocs][mkdocs-official-site] with [mkdocs-material][material-mkdocs]. Ensure you have the Python 3.8 or newer installed and run `make` to install the dependencies and start the site.\r\n\r\n### Python\r\n\r\nAn ElectionGuard Core implemented in Python which includes ballot encryption, decryption, key generation, and tallying.\r\n\r\n[\ud83d\udcc1 Source][election-guard-python-source] |\r\n[\ud83d\udce6 Package][election-guard-python-package] |\r\n[\ud83d\udcdd Documentation][election-guard-python-documentation]\r\n\r\n### C ++\r\n\r\nAn ElectionGuard Core implemented in C++ to support ballot encryption.\r\n\r\n[\ud83d\udcc1 Source][election-guard-cpp-source-code] |\r\n[\ud83d\udce6 Package][election-guard-cpp-package] |\r\n[\ud83d\udcdd Documentation][election-guard-cpp-documentation]\r\n\r\n### Web API\r\n\r\nA Web API that wraps the Python package to perform ballot encryption, casting, spoiling, and tallying.\r\n\r\n[\ud83d\udcc1 Source][election-guard-web-api-source] | \r\n[\ud83d\udc33 Docker][election-guard-web-api-docker] | \r\n[\ud83d\udcc4 Documentation][election-guard-web-api-documentation]\r\n\r\n### User Interface\r\n\r\nMonorepo in React & Typescript consisting of an api client, components, and apps to demonstrate examples of user interface.\r\n\r\n[\ud83d\udcc1 Source][election-guard-ui-source] |\r\n[\ud83d\udcc4 Documentation][election-guard-ui-documentation]\r\n\r\n## \ud83d\udee1 Security Issues Reporting\r\n\r\nWe encourage the developer and security community to conduct research, report issues, and suggest improvements on this code base. However, unlike performance or feature bugs, please do **not** report security vulnerabilities in public Github comments. Each repository has a SECURITY file with instructions on responsibly reporting security vulnerabilities under Microsoft's CVD process.\r\n\r\n## \ud83e\udd1d Contributing\r\n\r\nHelp defend democracy and **[contribute to the project][]**.\r\n\r\n[code of conduct]: CODE_OF_CONDUCT.md\r\n[contribute to the project]: CONTRIBUTING.md\r\n\r\nWe welcome discussions on our [discussions page][election-guard-discussions], feel free to check in and ask your questions and drop your suggestions regarding the specifications over there.\r\n\r\n## \u2753 Questions\r\n\r\nElectionGuard would love for you to ask questions out in the open using Github Issues. If you really want to email the ElectionGuard team, reach out at electionguard@microsoft.com.\r\n\r\n## \ud83c\udf89 Thanks!\r\n\r\nA huge thank you to those who helped to contribute to this project so far, including:\r\n\r\n- Josh Benaloh (whose [PhD thesis][verifiable-search-ballot-elections-paper] was the genesis of much of this work)\r\n- [InfernoRed Technology][infernored]\r\n- [VotingWorks][voting-works]\r\n- [Center for Civic Design][center-for-civic-design]\r\n- [Oxide Design][oxide-design]\r\n- Many teams within Microsoft\r\n\r\n<!-- Links -->\r\n[election-guard-banner]: docs/images/electionguard-banner.svg \"Election Guard banner SVG\"\r\n[license-image]: https://img.shields.io/github/license/microsoft/electionguard \"Election Guard license image\"\r\n[build-developer-conference]: https://blogs.microsoft.com/on-the-issues/?p=63211 \"Protecting democratic elections through secure, verifiable voting\"\r\n[homomoprhic-encryption]: https://en.wikipedia.org/wiki/Homomorphic_encryption \"Homomorphic encryption\"\r\n[election-guard-official-page]: https://microsoft.github.io/electionguard \"Official Election Guard site on Github Pages\"\r\n[election-guard-road-map]: https://www.electionguard.vote/overview/Roadmap/ \"Election Guard road map\"\r\n[mkdocs-official-site]: https://www.mkdocs.org/ \"MkDocs official website\"\r\n[material-mkdocs]: https://squidfunk.github.io/mkdocs-material/ \"Material for MkDocs\"\r\n[election-guard-python-source]: https://github.com/microsoft/electionguard-python \"Election Guard Python source code\"\r\n[election-guard-python-package]: https://pypi.org/project/electionguard/ \"Election Guard Python package\"\r\n[election-guard-python-documentation]: https://microsoft.github.io/electionguard-python/ \"Election Guard Python documentation\"\r\n[election-guard-cpp-source-code]: https://github.com/microsoft/electionguard-cpp/ \"Election Guard C++ source code\"\r\n[election-guard-cpp-package]: https://www.nuget.org/packages/ElectionGuard.Encryption/ \"Election Guard C++ package\"\r\n[election-guard-cpp-documentation]: https://github.com/microsoft/electionguard-cpp#readme \"Election Guard C++ documentation\"\r\n[election-guard-web-api-source]: https://github.com/microsoft/electionguard-api-python \"Election Guard Web API source code\"\r\n[election-guard-web-api-docker]: https://hub.docker.com/r/electionguard/electionguard-web-api \"Election Guard Web API Docker\"\r\n[election-guard-web-api-documentation]: https://microsoft.github.io/electionguard-api-python/ \"Election Guard Web API documentation\"\r\n[election-guard-ui-source]: https://github.com/microsoft/electionguard-ui \"Election Guard UI source code\"\r\n[election-guard-ui-documentation]: https://github.com/microsoft/electionguard-ui#readme \"Election Guard UI documentation\"\r\n[election-guard-discussions]: https://github.com/microsoft/electionguard/discussions \"Election Guard Discussions page\"\r\n[verifiable-search-ballot-elections-paper]: https://www.microsoft.com/en-us/research/publication/verifiable-secret-ballot-elections/ \"Verifiable Secret-Ballot Elections - Microsoft Research, Josh Benaloh\"\r\n[infernored]: https://infernored.com/ \"InfernoRed\"\r\n[voting-works]: https://voting.works/ \"Voting works - Elections you can trust\"\r\n[center-for-civic-design]: https://civicdesign.org/ \"Center for Civic Design\"\r\n[oxide-design]: https://oxidedesign.com/ \"Oxide Design\"\r\n", "repo_name": "electionguard-egvote", "org_name": "microsoft", "org_repo": "microsoft/electionguard-egvote", "platform_org_repo": "github+microsoft/electionguard-egvote", "link_to_repo": "https://github.com/microsoft/electionguard-egvote", "platform": "github", "language": "HTML", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure-databricks\n- power-bi\n---\n\n![Microsoft Solutions Early Access Engineering](./reference/images/CustomerRevenueGrowthFactor.png)\n\n# Azure Databricks Solution Accelerator for Financial Analytics of Customer Revenue Growth Factor\nThis solution accelerator was built to provide developers with all of the resources needed to build a solution to identify the top factors for revenue growth from an e-commerce platform using Azure Databricks.\n\n## Getting Started\n* **Don't have an existing Azure Databricks Workspace:** Azure Databricks is a first-party Azure solution. You can easily deploy a new Azure Databricks workspace using the Azure Portal by following these [instructions](https://learn.microsoft.com/en-us/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal#create-an-azure-databricks-workspace). Once the workspace is deployed you can continue with the [Setup Guide](./setup/setup_guide.md)\n* **Have an existing Azure Databricks Workspace:** continue to the [Setup Guide](./setup/setup_guide.md).\n\n## Prerequisites\nIn order to successfully complete your solution, you will need to have access to and or provisioned the following:\n1. Access to an Azure subscription\n2. A Power BI Pro License (or free trial)\n\n## Azure and Analytics Platform\nThe directions provided for this repository assume fundamental working knowledge of Azure, Azure Databricks, and Power BI.\n\nFor additional training and support, please see:\n 1. [Azure Databricks](https://azure.microsoft.com/en-us/products/databricks/)\n 3. [Power BI](https://docs.microsoft.com/en-us/power-bi/)\n\n## Process Overview  \n\nThe architecture diagram below details what you will be building for this Solution Accelerator:\n\n![Azure Databricks Architecture](./reference/images/DatabricksArchitecture.png)\n\n## License\nCopyright (c) Microsoft Corporation\n\nAll rights reserved.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"\"Software\"\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE\n\n## Note about Libraries with MPL-2.0 License\n\nThe following libraries are not explicitly included in this repository, but users who use this Solution Accelerator may need to install them locally and in Azure Databricks to fully utilize this Solution Accelerator. However, the actual binaries and files associated with the libraries are not included as part of this repository, but they are available for installation via the PyPI library using the pip installation tool.  \n  \nLibraries: certifi\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Azure-Databricks-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor", "org_name": "microsoft", "org_repo": "microsoft/Azure-Databricks-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor", "platform_org_repo": "github+microsoft/Azure-Databricks-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor", "link_to_repo": "https://github.com/microsoft/Azure-Databricks-Solution-Accelerator-Financial-Analytics-Customer-Revenue-Growth-Factor", "platform": "github", "language": "Python", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "graphir-react-client", "org_name": "microsoft", "org_repo": "microsoft/graphir-react-client", "platform_org_repo": "github+microsoft/graphir-react-client", "link_to_repo": "https://github.com/microsoft/graphir-react-client", "platform": "github", "language": "JavaScript", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Microsoft MakeCode Arcade extension for Visual Studio Code\n\nA VS Code extension for making retro-style video games with [Microsoft MakeCode Arcade](https://arcade.makecode.com/). Code, create pixel art, and play your game in both native VS Code and [vscode.dev](https://insiders.vscode.dev/makecode)!\n\n## Starting a new project\n\nTo start a new MakeCode project in VS Code, you first need to open an empty folder (File > Open Folder).\n\n> Note: In vscode.dev, the File menu is located in the hamburger button in the top-left corner of the page.\n\nOnce you've opened a folder, you can create an empty project by clicking the \"Create a New Project\" command in the MakeCode Asset Explorer.\n\n### Importing an existing MakeCode project\n\nIf you have a share link for a MakeCode Arcade project, you can also import it by clicking the \"Import Project from URL\" command and pasting the URL in the input that appears.\n\n### Opening an existing GitHub project\n\nIf you have opened a GitHub repository that contains a MakeCode Arcade project, you'll need to install your project's dependencies for features like intellisense to work. Click the \"Install project dependencies\" command in the actions section of the MakeCode Asset Explorer to download and install them.\n\n> Note: You must be connected to the internet in order to download and install extensions\n\n## Opening the MakeCode Asset Explorer\n\nYou can access the MakeCode Asset Explorer by clicking on the MakeCode icon in the Visual Studio Code action bar.\n\n![Screenshot of the VS code action bar with an arrow pointing to the MakeCode Arcade Asset Explorer](./images/action-bar.png)\n\n## Running actions in the MakeCode Asset Explorer\n\nAt the top of the asset explorer you'll find a list of commands for managing MakeCode Arcade projects. Click on a command to run it!\n\n![Screenshot of the MakeCode Asset Explorer with a red box around the command list](./images/command-list.png)\n\n## Anatomy of a MakeCode project\n\nInside your MakeCode project, you'll see a folder structure that looks something like this:\n\n```\nproject/\n\u251c\u2500 built/\n\u251c\u2500 main.ts\n\u2514\u2500 pxt.json\n   ...\n```\n\nDon't worry if you don't see all of these files or if your project contains more than what's listed here! We're just going to go over the important ones first:\n\n* `built/` contains all of the compiled code for your project. If you compile your project, the result will show up inside this directory.\n* `main.ts` is the main code file for your project. This code will run when you run your game.\n* `pxt.json` is the file that configures your project. More on that below!\n\nSome other files you might see in your project include:\n\n* `*.jres` and `*.g.jres` - these files contain the assets for your project like images, animations, songs, and tilemaps. See \"Managing your project's assets\" below for more information on assets.\n* `*.g.ts` - these files are autogenerated when the assets for your project change. Don't edit these by hand!\n* `README.md` - this is a markdown file where you can add documentation for your project that other people can read when they import it.\n* `main.blocks` - if you imported your project, it might have a blocks file inside. You can't edit this file inside of VS code!\n* `tsconfig.json` - this file is required to make features like intellisense work in the editor. You probably don't need to edit it!\n* `.github`, `mkc.json`, `.vscode`, `.gitignore`, `.prettierrc`, `assets.json` - These are all advanced configuration files. You can safely ignore them!\n\n### pxt.json\n\n`pxt.json` is a very important file that is required in all MakeCode projects. Be careful when editing this file! If it isn't valid JSON or is missing required fields, your project might stop working. Always check for errors before saving it!\n\nSome of the important fields include:\n\n* **name** - The name of the project. When you create a MakeCode share link, this name is what people will see. Try to make it descriptive!\n* **description** - A description of your project.\n* **dependencies** - This field contains all of the extensions used by your project. To add/remove an extension, see the sections below. By default, all arcade projects depend on the `device` extension; make sure not to remove it if you want your project to work with MakeCode Arcade!\n* **files** - This is a list of the files in your project. All `.ts`, `.jres`, `.g.ts`, `.g.jres`, and `.md` files should be listed here.\n* **palette (optional)** - This optional field can be used to change the 15 colors that MakeCode Arcade uses for its color palette. More information [here](https://arcade.makecode.com/developer/images).\n* **version (optional)** - This optional field is used to set the version of your project. Versions should be in the from 0.0.0 (aka [semantic versioning](https://semver.org/)). This field is mostly just for extensions.\n* **testFiles (optional)** - This optional field lists files to include when building the project itself, but not include when the project is added to another project as an extension. It's useful for testing out your code if you are authoring an extension.\n\n### Adding a file to your project\n\nWhenever you create a new file that you want to be included in your project, you need to add it to the **files** entry inside `pxt.json`. If you aren't seeing your changes when you run your game, make sure you didn't leave a file out!\n\n## Run your project in the simulator\n\n![Screenshot of VS Code with the MakeCode simulator open and the \"Start MakeCode simulator\" command highlighted](./images/simulator-pane.png)\n\nClick the \"Start MakeCode Simulator\" command in the MakeCode Asset Explorer to run your game inside of VS Code. A new view pane will open after the project has finished compiling and will automatically reload whenever you save a file in the open workspace.\n\nTo use your keyboard to control the simulator, make sure you first click on the simulator pane so that it has focus.\n\n### Viewing the simulator console\n\nAll serial messages and exceptions from the simulator are printed in VS Code's output view pane. If the output view pane is hidden, you can open it from the View menu in the top bar (View > Output).\n\n> Note: In vscode.dev, the View menu is located in the hamburger button in the top-left corner of the page\n\nOnce the pane is visible you can view all MakeCode messages by selecting \"MakeCode\" from the dropdown in the top-right:\n\n![Screenshot of VS Code with the output pane open and a red arrow pointing to a dropdown with MakeCode selected](./images/output-pane.png)\n\n\n## Managing your project assets\n\nAll of your projects images, tilemaps, animations, and songs will be listed inside the MakeCode Arcade Asset Explorer. Clicking on an asset from this list will open the asset editor. Any changes made to an asset inside of this editor will be automatically saved in your project.\n\n### Creating an asset\n\nTo create a new asset, hover over the asset type in the MakeCode Asset Explorer and click the \"Create File\" icon that appears:\n\n![Screenshot of the MakeCode Asset Explorer with a red arrow pointing to the create icon in the images section](./images/create-new-asset.png)\n\n### Editing assets\n\nTo edit an existing asset, click on its name in the asset explorer to open it in the asset editor. To rename an asset, change its name in the text input that appears in the bottom of the asset editor. If you don't see the text input, you may need to increase the width of the pane that the asset editor is in.\n\n![Screenshot of VS Code with the asset editor open to a cat sprite and a red box around the asset name.](./images/asset-rename.png)\n\n### Deleting and duplicating assets\n\nWhen you hover over the name of an asset, two icons will appear next to it:\n\n![Screenshot of the MakeCode Asset Explorer with a red square around two icons next to the name of an image asset](./images/duplicate-delete.png)\n\nClicking the trash can icon will delete the asset from your project. Be careful, there is no undo for this operation!\n\nClicking the copy icon will duplicate the asset and open the copy in the asset editor.\n\n### Referencing assets inside your code\n\nTo reference an asset you've created inside your code, you can use the name you gave it with one of the tagged templates on the `assets` namespace:\n\n```ts\nlet myImage = assets.image`imageName`;\nlet myAnimation = assets.animation`animName`;\nlet myTile = assets.tile`tileName`;\nlet myTilemap = assets.tilemap`tilemapName`;\nlet mySong = assets.song`songName`;\n```\n\nYou can also create or edit an asset referenced in code using Code Actions:\n\n![Screenshot of code actions provided for a MakeCode Asset with red arrows pointing at the code actions icon and two sample code action suggestions.](./images/edit-asset-inline.png)\n\n## Adding an extension to your project\n\nTo add a MakeCode extension to your project, click the \"Add an Extension\" command in the actions section of the MakeCode Asset Explorer. Then either select an extension from the list that appears or paste a GitHub URL for an extension repo in the text input and press Enter.\n\n> Note: You must be online to add an extension to your project\n\nAfter you add an extension to your project, an entry for it will automatically appear inside your `pxt.json` file.\n\n### Removing an extension from a project\n\nTo remove an extension, run the \"MakeCode: Remove an Extension\" command in the VS Code command palette and select the extension to remove.\n\n### Reinstalling project extensions\n\nIf you manually edited your project's `pxt.json` file and need to reinstall the project's dependencies, click the \"Install Project Dependencies\" command in the actions section of the MakeCode Asset Explorer.\n\n## Downloading a project to hardware\n\nTo compile your project and download it to hardware (e.g. a Meowbit), first run the \"Build Project for Hardware\" command in the actions section of the MakeCode Asset Explorer. Once the compile finishes, you can find the generated hex/uf2 file under the `built/` folder in your project workspace. Depending on what hardware you selected to compile for, the file may be under a subdirectory (e.g. `built/n3/binary.hex`). The file will be named either `binary.hex` or `binary.uf2`.\n\n### Flashing hardware from native VS Code\n\n1. Right click on the uf2/hex file in your `built/` folder and select \"Reveal in File Explorer\" or \"Reveal in Finder\" to locate the downloaded file on your computer:\n\n  ![Screenshot of VS Code with the right-click context menu open over the file binary.uf2](./images/reveal-in-file-explorer.png)\n\n2. Connect your hardware to the computer with a USB cable. It should appear as a USB drive in your computer's file explorer. If you don't see the USB drive, see \"Troubleshooting hardware connections\" below.\n3. Drag the file you located into the USB drive for your hardware. It should automatically reset and load with your compiled game.\n\n### Flashing hardware from vscode.dev\n\n1. Right click on the uf2/hex file in your `built/` folder and select \"Download...\" to download the uf2/hex file to your computer:\n\n  ![Screenshot of VS Code with the right-click context menu open over the file binary.hex](./images/download-hex.png)\n\n2. Connect your hardware to the computer with a USB cable. It should appear as a USB drive in your computer's file explorer. If you don't see the USB drive, see \"Troubleshooting hardware connections\" below.\n3. Drag the file you downloaded into the USB drive for your hardware. It should automatically reset and load with your compiled game.\n\n### Troubleshooting hardware connections\n\nIf your hardware is failing to show up as a USB drive when you plug it in, try the following steps:\n\n1. Try a different USB cable (some USB cables are power-only and will not allow data transfer).\n2. Try updating the firmware for your device. Follow the manufacturer's instructions for how to do this with your device.\n3. As a last resort, try a different computer. There may be a device policy in place that is restricting access to USB ports.\n\n\n## Sharing your project\n\nTo create a MakeCode share link for your project, click the \"Create MakeCode Share Link\" command in the MakeCode Asset Explorer. This will cause the output pane to open with a link that you can copy/paste.\n\nTo change the name of your shared project, see the `pxt.json` section above.\n\n## Local development\n\nSee the [developer guide](./development.md) for info on developing vscode-makecode.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-makecode", "org_name": "microsoft", "org_repo": "microsoft/vscode-makecode", "platform_org_repo": "github+microsoft/vscode-makecode", "link_to_repo": "https://github.com/microsoft/vscode-makecode", "platform": "github", "language": "TypeScript", "stargazers_count": 26, "watchers_count": 26}, {"README_text": "# Overview\n\nThis project offers an implementation of the paper:\n\n**Interactive Text Generation**\\\nFelix Faltings, Michel Galley, Baolin Peng, Kiant\u00e9 Brantley, Weixin Cai, Yizhe Zhang, Jianfeng Gao, Bill Dolan\\\n[arXiv](https://arxiv.org/abs/2303.00908)\n\n<img src=\"images/motivation.png\" width=\"70%\">\n\n# Installation\n\nInstall dependencies using requirements.txt:\n`pip install -r requirements.txt`\nThen, install the package. From the top level directory (where `setup.py` is located), run:\n`pip install -e .`\nThis will install this package as an editable module named `infosol`.\n\n# DATA\n\nYou can regenerate the data used in the paper using the `make_data.py` script. You only need to specify the `data_dir` argument where the data will be saved (under `data_dir/cnn_bart`). This script first downloads the raw data from the Huggingface hub.\n\n# Test\n\nTo replicate the main experiments of the paper, run:\n\nscripts/make_eval_jobs.py\n\nThe above command creates jobs files in 'jobs' directory, as well as the directory structure ('out') where test results will be stored. Then, you can pass any of the configuration files under 'jobs' as argument to scripts/run_eval.py. For example, run the following to replicate the S2S 'interactive' experiments (Table 4 of the paper):\n\npython scripts/run_eval.py --args_path jobs/interactive/cnn-bart-s2s --cuda_device 0\n\nNote: The S2S experiments of the paper yield generation that were inconsisent in length and hurt S2S performance. Thus, we tuned its length_penalty hyperparameter on a held out set, and the corresponding job files can be found in jobs/s2s.txt.\n\n# Train\n\nIn order to train all the models presented in the paper, you may use the provided Makefile. Set the `run_dir` variable to the directory where you would like model weights to be saved to. You also need to set the `DATA_DIR` and `SAVE_DIR` paths in `train.py`.\n\n# Code walkthrough\n\nWe suggest you go through the code in the order alignment -> model -> train/evaluate. Alignment defines some basic objects like alignments (used heavily by the oracle) and canvases (the objects that the model and oracle operate on). The others are self-explanatory and are commented.\n\nThe main files:\n- alignment.py\n- env.py\n- evaluate.py\n- models/word_edit_model.py\n- run_eval.py\n- train.py\n- (decoding.py)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "InteractiveTextGeneration", "org_name": "microsoft", "org_repo": "microsoft/InteractiveTextGeneration", "platform_org_repo": "github+microsoft/InteractiveTextGeneration", "link_to_repo": "https://github.com/microsoft/InteractiveTextGeneration", "platform": "github", "language": "Python", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# ParaTag\nCode and data of the following paper (still under Microsoft Review):\n## ParaTag : A Dataset of Paraphrase Tagging for Fine-Grained Labels, NLG Evaluation, and Data Augmentation\nShuohang Wang (shuowa at microsoft), Ruochen Xu, Yang Liu, Chenguang Zhu, Michael Zeng\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ParaTag", "org_name": "microsoft", "org_repo": "microsoft/ParaTag", "platform_org_repo": "github+microsoft/ParaTag", "link_to_repo": "https://github.com/microsoft/ParaTag", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n<!-- \n***[![MIT License][license-shield]][license-url]\n-->\n\n<!-- PROJECT LOGO -->\n\n<br />\n<div align=\"center\">\n  <a href=\"https://github.com/microsoft/robustlearn\">\n    <img src=\"https://wjdcloud.blob.core.windows.net/tools/roblearn.png\" alt=\"Logo\" width=\"400\">\n  </a>\n\n  <strong>robustlearn</strong>: A unified library for research on robust machine learning\n\n</div>\n\nLatest research in robust machine learning, including adversarial/backdoor attack and defense, out-of-distribution (OOD) generalization, and safe transfer learning.\n\nHosted projects:\n- **Diversify** (ICLR 2023, #OOD):\n  - [Code](./diversify/) | [Out-of-distribution Representation Learning for Time Series Classification](https://arxiv.org/abs/2209.07027)\n- **MARC** (ACML 2022, #Long-tail): \n  - [Code](./marc/) | [Margin Calibration for Long-Tailed Visual Recognition](https://arxiv.org/abs/2112.07225)\n- **ChatGPT robustness** (arXiv 2023, #OOD #Adversarial): \n  - [Code](./chatgpt-robust/) | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)\n- Stay tuned for more upcoming projects!\n\nYou can clone or download this repo. Then, go to the project folder that you are interested to run and develop your research.\n\nRelated repos:\n  - Transfer learning: [[transferlearning: everything for transfer, domain adaptation, and more](https://github.com/jindongwang/transferlearning)]\n  - Semi-supervised learning: [[USB: unified semi-supervised learning benchmark](https://github.com/microsoft/Semi-supervised-learning)] | [[TorchSSL: a unified SSL library](https://github.com/TorchSSL/TorchSSL)] \n  - Federated learning: [[PersonalizedFL: library for personalized federated learning](https://github.com/microsoft/PersonalizedFL)]\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n[contributors-shield]: https://img.shields.io/github/contributors/microsoft/robustlearn.svg?style=for-the-badge\n[contributors-url]: https://github.com/microsoft/robustlearn/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/microsoft/robustlearn.svg?style=for-the-badge\n[forks-url]: https://github.com/microsoft/robustlearn/network/members\n[stars-shield]: https://img.shields.io/github/stars/microsoft/robustlearn.svg?style=for-the-badge\n[stars-url]: https://github.com/microsoft/robustlearn/stargazers\n[issues-shield]: https://img.shields.io/github/issues/microsoft/robustlearn.svg?style=for-the-badge\n[issues-url]: https://github.com/microsoft/robustlearn/issues\n[license-shield]: https://img.shields.io/github/license/microsoft/robustlearn.svg?style=for-the-badge\n[license-url]: https://github.com/microsoft/robustlearn/blob/main/LICENSE.txt", "repo_name": "robustlearn", "org_name": "microsoft", "org_repo": "microsoft/robustlearn", "platform_org_repo": "github+microsoft/robustlearn", "link_to_repo": "https://github.com/microsoft/robustlearn", "platform": "github", "language": "Python", "stargazers_count": 236, "watchers_count": 236}, {"README_text": "# FranceConnect Starter Kit - FranceConnect Facade\n\n**This project is a Work In Progress (WIP) effort**\n\n## About FranceConnect\n\nThe [FranceConnect](https://franceconnect.gouv.fr) platform (FCP) is an identification system designed to facilitate user access to the digital online services of the e-government, a.k.a. FranceConnected services, avoiding every French citizen to have to create a new account when accessing a new online service and therefore to remember different passwords for all services accessed.\n\nTo do this, FCP allows each user to have an identification mechanism recognized by the administration's online services through the FranceConnect button. When accessing a new service, in addition to the possibility of registering with an administrative authority that the user does not yet know, the button allows the user to select a compatible supported identity that he or she already has, and use it in this context. In terms of supported identity providers (IdP), this platform makes it possible for citizens to use an existing account @ an administrations or an e-gov agency\u2019s website, e.g., IRS and social security. Interestingly, a demo environment is available [here](https://fournisseur-de-service.dev-franceconnect.fr).\n\nFranceConnect is supported by the Interministerial Digital Directorate, or DINUM, which assists ministries in their digital transformation, advises the government and develops shared services and resources, such as the State's online identification and authentication system, or the State's interministerial network, data.gouv.fr or api.gouv.fr.\n\nFranceConnect figures:\n- 40+ million users.\n- 18 million monthly connections.\n- Access to over 1400 online services. Such services are referred as to FranceConnected services.\n\nThe FranceConnect service implementation documentation is available on the [partner portal](https://partenaires.franceconnect.gouv.fr/) provided by DINUM.\n\n## About the FranceConnect Facade (FCP)\n\nFrom a FranceConnected services (a.k.a., service providers (SP)) standpoint, [Dynamics 365](https://dynamics.microsoft.com/) Biz Apps portals, the [Power Pages](https://powerpages.microsoft.com/) websites, formely Power Apps portals, as well as [Azure AD B2C](https://azure.microsoft.com/en-us/services/active-directory/external-identities/b2c/#overview), cannot unfortunately directly integrate with the FranceConnect platform (FCP), while both these offerings and FCP are based on the same industry standard protocol, namely the [OpenID Connect (OIDC) protocol](https://openid.net/specs/openid-connect-core-1_0.html) w/ the authorization code flow. As always, the devil resides in detail. \n\nin this context, this project both discusses and illustrates a suggested solution via a so-called FranceConnect Facade (FCF), i.e., a lightweight adaptation layer to handle all the identified discrepancies and cope with the related issues, and to ultimately interoperate with FCP from a \"plumbing\" perspective, while also providing the expected user experience (UX) with the so-called FranceConnect button. \n\n## Content\n\nThis project currently provides the following content:\n- [A series of technical-functional specifications (Draft)](https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore/tree/master/Specifications) to build such a facade.\n- [A code sample in .NET 6 (LTS)](https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore/tree/master/Source) to illustrate how to implement such a defined facade. \n- [A \"Getting Started\" guide](https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore/tree/master/Documentation) to help deploy the code sample, and how to test it with Microsoft Power Pages for direct integration.\n- [A dedicated Azure AD B2C guide](https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore/tree/master/Documentation) to help leverage the code sample, with Azure Active Directory B2C (Azure AD B2C) with the B2C user flows and/or the custom policies, as far as user journeys are concerned. Indirected integration of Microsoft Power Pages is also covered.\n- [A series of Bicep scripts](https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore/tree/main/Scripts) to help deploy the required resources in Azure, and fulfill the prerequisites that pertains to this deployment.\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "franceconnect-facade-dotnet-webapp-aspnetcore", "org_name": "microsoft", "org_repo": "microsoft/franceconnect-facade-dotnet-webapp-aspnetcore", "platform_org_repo": "github+microsoft/franceconnect-facade-dotnet-webapp-aspnetcore", "link_to_repo": "https://github.com/microsoft/franceconnect-facade-dotnet-webapp-aspnetcore", "platform": "github", "language": "C#", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# @vscode/openssl-prebuilt\n\nThis is a public package containing static compilations of OpenSSL used in the VS Code CLI, for every platform that we publish on. It's available to install via npm as:\n\n```\nnpm install --save @vscode/openssl-prebuilt\n```\n\nWe use this to reduce the amount of compilation done at build time for VS Code, and we compile OpenSSL from signed sources ourselves to ensure supply chain security. This package may be pulled down automatically when building the VS Code CLI from sources, depending on your platform and configuration.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-openssl-prebuilt", "org_name": "microsoft", "org_repo": "microsoft/vscode-openssl-prebuilt", "platform_org_repo": "github+microsoft/vscode-openssl-prebuilt", "link_to_repo": "https://github.com/microsoft/vscode-openssl-prebuilt", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# GA4GH TES on Azure\n\nThis project is an implementation of the [GA4GH Task Execution Service (TES)](https://github.com/ga4gh/task-execution-schemas), that provides distributed batch task execution on Microsoft Azure. The TES API is an effort to define a standardized schema and API for describing batch execution tasks. A task defines a set of input files, a set of Docker containers and commands to run, a set of output files, and some other logging and metadata.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ga4gh-tes", "org_name": "microsoft", "org_repo": "microsoft/ga4gh-tes", "platform_org_repo": "github+microsoft/ga4gh-tes", "link_to_repo": "https://github.com/microsoft/ga4gh-tes", "platform": "github", "language": "C#", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Workshop\n\nPlease click on the following link to access the SQL Server IaaS Extension workshop [click here ](./workshop)\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "SQLExtentions", "org_name": "microsoft", "org_repo": "microsoft/SQLExtentions", "platform_org_repo": "github+microsoft/SQLExtentions", "link_to_repo": "https://github.com/microsoft/SQLExtentions", "platform": "github", "language": "PowerShell", "stargazers_count": 17, "watchers_count": 17}, {"README_text": "# scala-torch\nJVM/Scala wrappers for LibTorch.\n\n## State of this project\n\nThis project is mature enough to be used regularly in production code. The API exposed is fairly clean\nand tries to follow PyTorch syntax as much as possible. The API is a mix of hand-written wrappings and a wrapper\naround most of `Declarations.yaml`. \n\nThat said, some internal documentation is not quite ready for public consumption yet, though there is enough\ndocumentation that people who are already familiar with Scala and LibTorch can probably figure out what's going on. \nCode generation is accomplished through a combination of [Swig](https://www.swig.org) and a quick-and-dirty \n[Python script](swig/src/main/swig/bindgen.py) that reads in `Declarations.yaml`, which provides a language-independent \nAPI for a large part of LibTorch. This file is [deprecated](https://github.com/pytorch/pytorch/issues/69471) and in the \nfuture, we can hopefully replace `bindgen.py` using the forthcoming [torchgen](https://github.com/pytorch/pytorch/issues/69471#issuecomment-1273642655)\ntool provided by PyTorch. \n\nOne major annoyance with Scala in particular is that you cannot define multiple overloads of a method that take default\narguments. Currently, `bindgen.py` uses any defaults present in only the first overload found in `Declarations.yaml`.\nIn some cases, clever use of Scala's implicit conversions can hide these headaches, but currently, you occasionaly have to write\nout the defaults where you would not have to in Python. One potential future option is to give overloads\ndifferent names, but we elected not to do that (yet).\n\nWe have not yet published JARs for this project. These are coming soon. \n\n## Short tour\n\nScala-torch exposes an API that tries to mirror PyTorch as much as Scala syntax\nallows. For example, taking some snippets from\n[this tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html):\n\nPyTorch:\n```python\nimport torch\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n```\n\nScala-Torch:\n```scala\nimport com.microsoft.scalatorch.torch\nimport com.microsoft.scalatorch.torch.syntax._\n\ntorch.ReferenceManager.forBlock { implicit rm =>\n val data = $($(1, 2), $(3, 4))\n val x_data = torch.tensor(data)\n}\n```\n\n\nPyTorch:\n```python\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[..., -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n```\n\nScala-Torch:\n```scala\nval tensor = torch.ones($(4, 4))\nprintln(s\"First row: ${tensor(0)}\")\nprintln(s\"First column: ${tensor(::, 0)}\")\nprintln(s\"Last column: ${tensor(---, -1)}\")\ntensor(::, 1) = 0\nprintln(tensor)\n```\n\nSee [this file](scala-torch/src/test/scala/com/microsoft/scalatorch/torch/tutorial/PyTorchOrgTensorTutorialTest.scala) for \na complete translation of the PyTorch tutorial into Scala-Torch.\n\n### Memory management\n\nOne big difference between Scala-Torch and PyTorch is in memory management. Because Python and LibTorch both use \nreference counting, memory management is fairly transparent to users. However, since the JVM uses garbage collection\nand [finalizers are not guaranteed to run](https://docs.oracle.com/javase/9/docs/api/java/lang/Object.html#finalize--),\nit is not easy to make memory management transparent to the user. Scala-Torch elects to make memory management something\nthe user must control by providing [ReferenceManager](scala-torch/src/main/scala/com/microsoft/scalatorch/torch/ReferenceManager.scala)s \nthat define the lifetime of any LibTorch-allocated object\nthat is added to it. All Scala-Torch methods that allocate objects from LibTorch take an `implicit` `ReferenceManager`,\nso it is the responsibility of the caller to make sure there is a `ReferenceManager` in `implicit` scope (or passed\nexplicitly) and that that `ReferenceManager` will be `close()`ed when appropriate. See documentation and uses\nof `ReferenceManager` for more examples.\n\n## Handling of native dependencies\n\nPyTorch provides pre-built binaries for the native code backing it [here](https://pytorch.org/get-started/locally/). \nWe make use of the pre-built dynamic libraries by packaging them up in a jar, much like [TensorFlow Scala](http://platanios.org/tensorflow_scala/installation.html).\nDownstream\nprojects have two options for handling the native dependencies: they can either \n1. Declare a dependency on the packaged native dependencies wrapped up with a jar using\n```scala\nval osClassifier = System.getProperty(\"os.name\").toLowerCase match {\n  case os if os.contains(\"mac\") || os.contains(\"darwin\") => \"darwin\"\n  case os if os.contains(\"linux\")                        => \"linux\"\n  case os if os.contains(\"windows\")                      => \"windows\"\n  case os                                                => throw new sbt.MessageOnlyException(s\"The OS $os is not a supported platform.\")\n}\nlibraryDependencies += (\"com.microsoft.scalatorch\" % \"libtorch-jar\" % \"1.10.0\").classifier(osClassifier + \"_cpu\")\n```\n2. Ensure that the libtorch dependencies are installed in the OS-dependent way, for example, in `/usr/lib` or in `LD_LIBRARY_PATH` on Linux,\nor in `PATH` on windows. Note that on recent version of MacOS, [System Integrity Protected](https://developer.apple.com/library/archive/documentation/Security/Conceptual/System_Integrity_Protection_Guide/RuntimeProtections/RuntimeProtections.html)\nresets `LD_LIBRARY_PATH` and `DYLD_LIBRARY_PATH` when working processes, so it is very hard to use that approach on MacOS. \n\nThe native binaries for the JNI bindings for all three supported OSes are published in `scala-torch-swig.jar`, so there\nis no need for OS-specific treatment of those libraries.\n\nApproach 1 is convenient because sbt will handle the libtorch native dependency for you and users won't need install\nlibtorch or set any environment variables. This is the ideal approach for local development. \n\nThere are several downsides of approach 1:\n* it may unnecessarily duplicate installation of libtorch if, for example, pytorch is already installed\n* jars for GPU builds of libtorch are not provided, so approach 2 is the only option if GPU support is required\n* care must be taken when publishing any library that depends on Scala-Torch to not publish the dependency\n on the `libtorch-jar`, since that would force the consumer of that library to depend on whatever OS-specific\n version of the jar was used at building time. See the use of `pomPostProcess` in [build.sbt](build.sbt) for\n how we handle that. Note that another option is for downstream libraries to exclude the `libtorch-jar`\n using something like \n```scala\nlibraryDependencies += (\"com.microsoft\" % \"scala-torch\" % \"0.1.0\").exclude(\"com.microsoft.scalatorch\", \"libtorch-jar\")\n```\n\nApproach 2 is the better option for CI, remote jobs, production, etc. \n\n### Local Development (MacOS)\n\nYou will need to have SWIG installed, which you can\ninstall using `brew install swig`.\n\n```\ngit submodule update --init --recursive\ncd pytorch\npython3 -m tools.codegen.gen -s aten/src/ATen -d torch/share/ATen\ncd ..\ncurl https://download.pytorch.org/libtorch/cpu/libtorch-macos-$(pytorchVersion).zip -o libtorch.zip\nunzip libtorch.zip\nrm -f libtorch.zip\nconda env create --name scala-torch --file environment.yml\nconda activate scala-torch\nexport TORCH_DIR=$PWD/libtorch\n# This links to the JNI shared library to the absolute paths in the libtorch dir instead of \n# using an rpath.\nexport LINK_TO_BUILD_LIB=true\nsbt test\n```\n\nA similar setup should work for Linux and Windows. \n\n#### Troubleshooting\n\nIf you are using Clang 11.0.3 you may run into an error \nwhen compiling the `SobolEngineOps` file. This is most \nlikely due to an issue with the compiler and it has already \nbeen reported [here](https://github.com/pytorch/pytorch/issues/35478).\nA temporary workaround is to install another version of \nClang (e.g., by executing `brew install llvm`). Another option\nis to downgrade XCode to a version < 11.4.\n\n### Upgrading the LibTorch version\n\nTo upgrade the underlying version of LibTorch:\n* `cd pytorch; git checkout <commit>` with the `<commit>` of the desired release version, \n  best found [here](https://github.com/pytorch/pytorch/releases).\n* Rerun the steps under **Local Development**.\n* Change `TORCH_VERSION` in [run_tests.yml](.github/workflows/run_tests.yml).\n* Address compilation errors when running `sbt compile`. Changes to [bindgen.py](swig/src/main/swig/bindgen.py) may\n  be necessary.\n\n# Contributors\n\nThanks to the following contributors to this project:\n\n* [Adam Pauls](https://github.com/adampauls)\n* [David Hall](https://github.com/dlwh)\n* [Theo Lanman](https://github.com/theo-lanman)\n* [Alex Kyte](https://github.com/alexanderkyte)\n* [Hao Fang](https://github.com/hao-fang)\n* [Anthony Platanios](https://github.com/eaplatanios)\n* [Dmitrij Peters](https://github.com/Dpetters)\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "scala_torch", "org_name": "microsoft", "org_repo": "microsoft/scala_torch", "platform_org_repo": "github+microsoft/scala_torch", "link_to_repo": "https://github.com/microsoft/scala_torch", "platform": "github", "language": "Scala", "stargazers_count": 117, "watchers_count": 117}, {"README_text": "\n\n# Azure Cosmos DB for Multitenant Applications Workshop\n\n## Azure Cosmos DB Introduction\n\nAzure Cosmos DB is a fully managed NoSQL database for modern multitenant application development. You can build applications \nfast with open source APIs, multiple SDKs, schemaless data and no-ETL analytics over operational data.\nSingle-digit millisecond response times, and instant scalability, guarantee speed at any scale.\nGuarantee business continuity, 99.999% availability and enterprise-grade security for every application.\nEnd-to-end database management, with serverless and automatic scaling matching your application and TCO needs. \nSupports multiple database APIs including native API for NoSQL, API for Mongo DB, Apache Cassandra, Apache Gremlin and Table.\nIt also started supporting PostgreSQL extended with the Citus Open Source which is useful for highly scalable relational apps.\n\nTo begin using Azure Cosmos DB, create an Azure Cosmos DB account in an Azure resource group in your subscription. \nYou then create databases and containers within the account.\n\n#### Resource Model:\nA single Azure Cosmos DB account can virtually manage an unlimited amount of data and provisioned throughput. \nTo manage your data and provisioned throughput, you can create one or more databases within your account, \nthen one or more containers to store your data. \n\n<img src=\"./images/CosmosDB_ResourceModel.jpg\" alt=\"Cosmos DB Resource Model\" Width=\"600\">\n\n#### Request Units: \nCost of database operations is normalized by Azure Cosmos DB and is expressed by Request Units (RU). It is a performance \ncurrency abstracting the system resources such as CPU, IOPS and Memory to perform the database operations supported by \nAzure Cosmos DB. You can examine the response header to track the number of RUs that are consumed by any database \noperation.\n<img src=\"./images/CosmosDB_Request_unit.jpg\" alt=\"Request Unit Diagram\" Width=\"600\" >\n\n\n#### Indexing: \nAzure Cosmos DB is a schema-agnostic database that allows you to iterate on your application without having to deal with schema \nor index management. By default, Azure Cosmos DB automatically indexes every property for all items in your container without \nhaving to define any schema or configure secondary indexes. When an item is written, Azure Cosmos DB effectively indexes each \nproperty's path and its corresponding value. In some situations, you may want to override this automatic behavior to better \nsuit your requirements. You can customize a container's indexing policy by setting its indexing mode, and include or exclude \nproperty paths.\n\nAccess [Azure Cosmos DB Documentation](https://learn.microsoft.com/azure/cosmos-db/introduction) for more details and training. \n\n## Why Azure Cosmos DB?\nHere are the scenarios where Azure Cosmos DB can help:\n* Looking to modernize their monolithic on-premise applications as SaaS applications.\n* scale up to the max throughput for addressing unpredicted workloads and scale down automatically.\n* Goals to expand globally with low latency and highly scalable throughput. \n* Trying to reduce costs to support multiple customers with fluctuating throughput requirement.\n* Application needs to support multiple businesses with flexible schema.\n* Unable to meet performance SLA requirements and reaching max storage limits with growing data.\n\nAll the above use cases need a new mindset and special features. This workshop will show you how Azure Cosmos DB will be the best option.\n\n\n\n## Workshop Challenge List\n- [Challenge-1: Deploy Azure Cosmos DB Service](#challenge-1-Deploy-Azure-Cosmos-DB-Service)\n- [Challenge-2: Model data to build SaaS applications](#Challenge-2-Model-data-to-build-SaaS-applications)\n- [Challenge-3: Design Cosmos DB Account to serve small, medium and large customers](#Challenge-3-Design-Cosmos-DB-Account-to-serve-small-medium-and-large-customers)\n- [Challenge-4: Validate Cosmos DB features Auto Failover, Autoscale and Low Latency](#Challenge-4-Validate-Cosmos-DB-features-Auto-failover-Autoscale-and-Low-latency)\n- [Challenge-5: Build an application using Cosmos DB Emulator at no cost](#Challenge-5-Build-an-application-using-Azure-Cosmos-DB-at-no-cost)\n\n## Multi-Tenancy features for Software Companies \n\n### Distributes Data horizontally\nBy using partitions with Azure Cosmos DB containers, you can create containers that are shared across multiple tenants. \nWith large containers, Azure Cosmos DB spreads your tenants across multiple physical nodes to achieve a high degree of scale.\n\n### Control the **throughput** based on the size of the customer to lower your costs: \nTypically, you provision a defined number of request units per second for your workload, which is referred \nto as throughput. You can assign throughput at a container level, at a database level to share among the containers, automatically\nscale up to the max throughput for address unpredicted workloads and scale down to 10% of Max to save costs.\n\n    \n## Business Scenario\nFictitious ISV company called \"Smart Booking Inc\" has built an on-line reservation application called \"EasyReserveApp\" and \ncurrently deployed as an on-premises application to 4 hotel chains. This application is a big hit in the industry and \nthey want to convert this application as a SaaS application to meet the global demand. They are looking for a database \nto handle unpredictable volume, maintain low latency response time to users at any part of the world, maintain high \navailability & business continuity with optimized cost based on the usage. \n\nIt currently has the following clients in the Hotel Industry:\n\n<img src=\"./images/MultiTenant_Hotel_Business_Model.jpg\" alt=\"Application Data Model Architecture\" Width=\"600\" Height=\"400\">\n\n\nThis workshop gives you handson experience on designing Azure Cosmos DB for small, medium and large multi-tenant \ncustomers using this use case.  \n\n\n## Architecture Solution Diagram\n<img src=\"./images/Multi-Tenant_Cosmos_DB_Workshop_Architecture.jpg\" alt=\"Architecture for Azure Cosmos DB Lab\" Width=\"600\"> \n\n\n\n## Challenge-1: Deploy Azure Cosmos DB Service \n\nWe have developed an Azure Deployment script to provision the required Azure Services used in the above architecture diagram.\n\n1.1 Open a new InPrivate window from your Microsoft Edge browser. \n\n<img src=\"./images/Browser_in_private_Marked.jpg\" alt=\"Edge Browser InPrivate Window selection\" width=\"400\">\n\n1.2 Enter the following Workshop github link in the browser.\n```\n\thttps://github.com/microsoft/CosmosDB_Multi-Tenant\n```\n\n1.3 Click **Challenge-1** from Workshop Challenge List. \n\n1.4 Click the \"Deploy to Azure\" button\n\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FCosmosDB_Multi-Tenant%2Fmain%2Fazuredeploy.json)\n\n1.5 Enter your Workshop **login Id and Password** provided by the instructor.\n\n1.6 You will see **Welcome to SDP Innovation!** screen. Select **Yes**.\n\n1.7. Enter the following options in the custom deployment screen.\n\t\n* Select your allocated resource group from the dropdown.\n\n* Select your region from the dropdown list for **Cosmos DB Location** selection. \n\nBest practice is to keep resource group and Cosmos DB account in the same region. We had to set the resource group\n\"West US\" to automate the deployment script. Don't follow this practice in your environment.\n\n1.8 Click on \"Review+create\" button.\n\n<img src=\"./images/CosmosDB_AzureDeploymentOptions_Marked.jpg\" alt=\"Azure Custom Depolyment Screen\" Width=\"600\">\n\n1.9 It completes the validation as the next step and click on 'create' button.\n\nIt will provision Azure Cosmos DB account in your subscription:\n\nIt may take 2 to 5 minutes to create the services.\n\n1.10 Click on \"Go to resource group\" when the deployment is complete.\n\n<img src=\"./images/CosmosDB_AzureDeployment_Complete_Marked.jpg\" alt=\"Deployment complete\" Width=\"600\">\n\nIt will take you to your resource group showing the installed Azure Cosmos DB services.\n\n<img src=\"./images/CosmosDB_AzureDeployment_ResourceGroup_Marked.jpg\" alt=\"Resource Group with Services list\" Width=\"600\">\n\nYou have successfully deployed the required services to Azure. Congratulations for completing your first challenge.\n\n## Challenge-2: Model data to build SaaS applications\nLet us review the object model for this application and plan the data model for SaaS application.\n\n### Multi-Tenant Reservation System Object Model\n\n**Business_Entity** object contains all the business entity data.\n**Tenant** object contains the tenant location address and contact info.\n**Hotel_Room** object contains catalog of rooms with type, view, number of beds etc.\n**Customers** object has all the customer profile data. \n**Room_Inventory** object contains all the room numbers with room type in each tenant location.\n**Room_Availability** object contains available dates with rate info for each tenant location.\n**Hotel_Reservations** object contains all the hotel reservations for each tenant location.\n\n<img src=\"./images/MultiTenant_Hotel_Business_Software_Object_Model.jpg\" alt=\"Multi-Tenant Reservation System Object Model\" Width=\"600\">\n\n### Access Patterns\n\n1) Provide Hotel Room availability to support customer search based on location, dates and room types.\n2) Need to update availability after customer completes the reservation.\n3) Customer will access the reservation to review, cancel or update. \n4) Business owners and the support team at each tenant location will access the availability to \nbook/change reservation as per their customer request. \n\nYou would want to keep all the relevant data in one object based on the highly frequent access patterns to write and read data.\n\n<img src=\"./images/CosmosDB_MultiTenant_Hotel_Business_Data_Model.jpg\" alt=\"Cosmos DB Document Model diagram\" width=\"800\">\n\nAs per the above diagram, it make sense to keep all the business entity information such as customers and room types \nalong with tenant related data such as room inventory, availability and reservations in one Cosmos DB Container. \n\nThis challenge demonstrate how software object model transform to NoSQL database design model which completely different from SQL based\ndatabases. **Cosmos DB Data Model requires a different mindset and also requires the knowledge of highly frequent access patterns.**\n\nApply the same methodology to migrate your legacy applications or to build new green field applications. **You have successfully \ncompleted challenge 2 by creating a Cosmos DB data model based on highly frequent access patterns!!**\n\n## Challenge-3: Design Cosmos DB Account to serve small, medium and large customers\n\nEvaluate options to keep relevant data in one logical partition using partitioning key.\n\n\n### Database Strategies to support small, medium and large customers\n\n#### Shared throughput for small business entities\n\nIt would be better to create one container per business entity and share the throughput at the database level. This will avoid \ncreating one database per each customer and saves lot of money. You will have to understand that it may cause noisy neighbor \nproblem. This approach tends to work well when the amount of data stored for each tenant is small. \n\nIt can be a good choice for building a pricing model that includes a free tier, and for business-to-consumer (B2C) solutions. \nIn general, by using shared containers, you achieve the highest density of tenants and therefore the lowest price \nper tenant.\n\n#### 3.1 Create Cosmos DB Containers with shared database throughput\n\n* Access Cosmos DB Service in Azure Portal.\n* Select **Data Explorer** from the left panel.\n* Select **SharedThroughputDB** database.\n* Expand **CasinoHotel** container. \n* select **Settings**.\n* You will see **tenantId** as the partition key. It will create logical partitions per each tenant location. \n\n* Verify FamilyFunHotel Container for the partition key.\n* Select **Scale** property under **SharedThroughputDB** database.\n* It is set to use Autoscale up to 2,000 RUs and will share across all the containers (business entities). \n\nAutoscale will fall back to 200 RUs (10% of max RUs) when no activity. **This will save lot money since you don't have to allocate \nthe maximum capacity all the time.**\n\n**This database models serves multiple customers with multi-tenant data models with AutoScale capability to lower the costs and avoids \ncreating single database per customer.**\n\n<img src=\"./images/sharedThroughputContainers_3d.jpg\" alt=\"Shared Throughtput at Database level\" width=\"800\" >\n\n#### Dedicated throughput to avoid Noisy Neighbor\nHiking Hotel is a medium size business entity and you can avoid noisy neighbor issue by providing a dedicated throughput at \nthe container level. Follow the steps to create a dedicated throughput as part of the shared throughput database.\n\n#### 3.2 Add Cosmos DB container with dedicated throughput under shared database throughput \n\nFollow the steps to provision a new container with a dedicated throughput in a shared throughput database.  \n\n* Select **New Container** from the top bar inside Data Explorer.\n* Select **ShardThroughputDB** under **Use existing** database dropdown \n* Type **HikingHotel** as the container name\n* Type **/tenantId** as partition key.\n* Select **Provision dedicated throughput for this container** option.\n* Set the **Container Max RUs** as 2000.\n* click **OK**\n\n<img src=\"./images/create_SharedThroughputContainers_3d.jpg\" alt=\"Create dedicated throughput container\" width=\"800\">\n\n\n#### Database per business entity\nYou can provision dedicated containers for each business entity. This can work well for isolating large customers with higher \nthroughput requirement and for providing dedicated capacity. It will provide guaranteed level of performance, serve medium size \ncustomers.\n\n#### 3.3 Create Cosmos DB Database to serve large customers\n\n* Select **DedicatedThroughputDB** database\n* Expand **GoodFellas** container.\n* Select **Scale & Settings** \n* It shows 1000 RUs as the Maximum RUs with Autoscale throughput option.\n\n<img src=\"./images/DedicatedThroughputDB_3d.jpg\" atl=\"dedicated database for large customers\" width=\"800\">\n\n#### Account for tenant\nYou can provision separate database accounts for each tenant, which provides the highest level of isolation, \nbut the lowest density. A single database account is dedicated to a tenant, which means they are not subject to \nthe noisy neighbor problem. You can also configure the location of the container data via replication according to the \ntenant's requirements, and you can tune the configuration of Azure Cosmos DB features, network access, backup policy, \ngeo-replication and customer-managed encryption keys, to suit each tenant's requirements.\n\n#### Hybrid Approaches\nYou can consider a combination of the above approaches to suit different tenants' requirements and your pricing model. \nFor example:\n* Provision all free trial customers within a shared container, and use the tenant ID or a synthetic key partition key.\n* Offer a higher Silver tier that provisions dedicated throughput for the tenant's container.\n* Offer the highest Gold tier, and provide a dedicated database account for the tenant, which also allows tenants to \nselect the geography for their deployment.\n\n#### Partitioning Key Design\n\nTo store multi-tenant data in a single container, Azure Cosmos DB provides **partition key** to distribute the data into logical\npartitions. By using tenantId as the partition key, Cosmos DB keeps the data for each tenant in one logical partition and will \nperform faster queries with low cost. \n\nPartition Key plays a major role to save costs and to provide sub millisecond response time. Make sure to keep the partition key \nas part of most frequent queries. Cosmos DB also provides Document Type to keep relevant data in one container. \n\n\n#### 3.4 Load Business data into containers\n\nDownload the Workshop Data zip file (Multi-Tenant_CosmosDB_Workshop_data.zip) from the provided github link data folder. \nUnzip the file into your local folder and you should see the following files.\n\n<img src=\"./images/MultiTenant_dataFile_list_3d.jpg\" alt=\"list of multi-tenant reservation and availability data\" width=\"600\" >\n\n\nMulti-Tenant databases have unique set of data per each tenant in our use case number of rooms, availability and reservations. It \nwould make sense to create TenantID as the partition key and collect room availability and reservation info using document type.\n\nYou can also keep reference data such as Guest info and room type definitions in the same container. \n\n##### 3.4.1 Load Room Availability and Reservation data into **CasinoHotel** Container. \n* Expand **Casino Hotels** container under **SharedThroughputDB** database\n* select **items** section\n* Select **upload** to load the data files from the local foldder.\n* Select **CasinoHotel_RoomInventory.json** file from the local folder.\n* Select Open\n* Click **Upload** button.\n<img src=\"./images/UploadMultiTenantData_3d.jpg\" alt=\"Load Data into Casion Hotel Container\" width=\"800\" >\n\n* Load Reservation data into the same container by selecting **CasinoHotel_Reservation.json** file.\n\n##### 3.4.2 Load Room Availability and Reservation data into **FamilyFunHotel** Container. \n* Follow the above steps to load the data.\n\n##### 3.4.3 Load Room Availability and Reservation data into **HikingHotel** Container. \n* Follow the above steps to load the data.\n\n##### 3.4.4 Load Room Availability and Reservation data into **GoodFellasHotel** Container. \n* Follow the above steps to load the data.\n\n\n#### 3.5 Query Data using Data Explorer\n\nYou can query data using APIs and also can use Data Explorer for quick check.\n\n* Select **CasinoHotel** container.\n* Open Query window by selecting 'Folder with Search' icon from the top bar.\n* Type the following Query \n\n``` SELECT count(1) FROM c where c.type='Reservation' ```\n\n* Select 'Execute Selection' from the top bar to execute the query\n* Results tab will be display the results on the bottom.\n* Query Stats next to results tab will show RU cost, size of the document and Query Execution time.\n\n<img src=\"./images/cosmosdb-query-execution-3d.jpg\" alt=\"Query data using Data Explorer\" width=\"600\" >\n\nWith this challenge you have gained a hands-on experience to create multi-tenant Cosmos DB database to support small, medium \nand large customers. Congratulations!!\n\n\n## Challenge-4: Validate Cosmos DB features Auto failover, Autoscale and Low latency\n\n\n### 4.1 High Availability Features:\nAzure Cosmos DB is designed to provide multiple features and configuration options to achieve high availability to satisfy \nthe mission critical enterprise application's requirement.\n\n### Replica Outages\nReplica outages refer to outages of individual nodes in an Azure Cosmos DB cluster deployed in an \nAzure region. Azure Cosmos DB automatically mitigates replica outages by guaranteeing at least three replicas of your \ndata in each Azure region for your account within a four replica quorum.\n\n### Zone Outages\nIn many Azure regions, it is possible to distribute your Azure Cosmos DB cluster across \navailability zones, which results increased SLAs, as availability zones are physically separate and provide \ndistinct power source, network, and cooling. See Availability Zones. When an Azure Cosmos DB account is \ndeployed using availability zones, Azure Cosmos DB provides RTO = 0 and RPO = 0 even with a zone outage.\n\nSelect 'Replicate data globally' under 'Settings' section in the left pane. It show all the available regions \nfor Cosmos DB deployment. Availability Zone option for the write region can be enabled at the time of account creation.\n\nselect \"+ Add region\" to add a read region. Check the box for 'Availability Zone'. **No save action needed for this lab.**\n\n<img src=\"./images/Deploy_CosmosDBMultTenant_Lab_CosmosDB_add_region_save.jpg\" alt=\"Region availability zones\" width=\"800\">\n\n### Region Outages\nRegion outages refer to the outages that affect all Azure Cosmos DB nodes in an Azure region, across all availability \nzones. In the rare cases of region outages, Azure Cosmos DB can be configured to support various outcomes of \ndurability and availability\n\n#### Durability: \nTo protect against complete data loss that may result from catastrophic disasters in a region, Azure \nCosmos DB provides continuous and periodic backup modes.  \n\n#### Service-Managed failover: \nIt allows Azure Cosmos DB to fail over the write region of multi-region account. Region \nfailovers are detected and handled by Azure and do not require any changes from the application.\n\nSelect \"Service-Managed Failover\" option to failover the database to read region at the time of region outage.\n\n<img src=\"./images/Cosmosdb_feature_Automatic_Failover.jpg\" alt=\"Auto failover feature\" width=\"800\">\n\nSelect the \"On\" button under \"Enable Service-Managed Failover\".\n\n<img src=\"./images/CosmosDB_feature_Service_Managed_Failover_TurnOn.jpg\" alt=\"Auto failover feature\" width=\"800\" height=\"500\">\n\n**No action is needed for this lab.** It will take time to enable the failover option.\n\n\n### 4.2 Autoscale for scalability\nIt allows you to scale the throughput (RU/s) of your database or container automatically and instantly. \nThe throughput is scaled based on the usage, without impacting the availability, latency, throughput, or \nperformance of the workload.\n\nAutoscale provisioned throughput is well suited for mission-critical workloads that have variable or unpredictable \ntraffic patterns, and require SLAs on high performance and scale.\n\nSelect 'Data Explorer' from the left pane and expand 'DedicatedThroughputDB' database. \n\nExpand **GoodFellasHotel** container.\nSelect **Scale & Settings** setting.\n\nChange Max RU/s back to '2000' and select **save** button on the top bar. \n\nIt will change the throughput instantly without impacting the current workloads.\n\n<img src=\"./images/DedicatedThroughputDB_3d.jpg\" atl=\"dedicated database for large customers\" width=\"800\">\n\n\n### Sub Millisecond Fast Response Time\nSelect 'Data Explorer' from the left pane and expand 'SharedThroughputDB' database.\nHover over 'CasinoHotel' container and select three dots.\nIt provide options to create SQL Query, Stored Procedure, UDF & Trigers. Select the 'New SQL Query' option. \n\n<img src=\"./images/cosmosdb-query-execution-3d.jpg\" alt=\"Query data using Data Explorer\" width=\"600\" >\n\nType the following Query:\n\n```\nSELECT * FROM c where c.type='Reservation' \n```\n\nselect \"Query Stats\" tab and check the Query execution time. It shows the sub millisecond response time.\n\n\n## Challenge-5: Build an application using Azure Cosmos DB at no cost  \nCosmos DB is a developer friendly database and supports SaaS applications with no schema and indexing to manage. \nIt also provides built in Cache for improved performance. It provides Cosmos DB Emulator tool to build your \napplications using Cosmos DB in your development environment with no cost.\n\n### Quick Start\nYou can test building an application from Cosmos DB service portal itself. Select 'Quick start' from the left panel.\nIt will you programming language options .NET, Xamarin, Java, Node.js & Python to choose.\n\nUse the default .NET option.\n\n5.1 Select create 'Items' container button.\n\nIt creates an **Items** container in \"ToDoList\" database with 400 RU throughput capacity.\n\n<img src=\"./images/CosmosDB_QuickStart_AddContainer_Marked.jpg\" alt=\"Quick Start Create Container\" width=\"600\">\n\n5.2 Select Download button to download .NET app to your laptop.\n\n<img src=\"./images/CosmosDB_QuickStart_Download_App_Marked.jpg\" alt=\"Download .NET app button\" width=\"600\">\n\n5.3 Extract all from 'DocumentDB-Quickstart-DotNet.zip' file and open \"CosmosGettingStarted.sln\" in sql-dotnot folder \nwith Visual Studio 2022.\n\n<img src=\"./images/cosmosdb_dotnet_downloadzip_marked.jpg\" alt=\"Download ZIP and open Visual Studio\" width=\"600\">\n\n5.4 Clean and rebuild the solution.\n\n5.5 Put a breakpoint in **GetStartedDemoAsync** method at  \n\n```\nthis.ReplaceFamilyItemAsync();\n``` \n\n<img src=\"./images/cosmosdb_dotnet_app_breakpoint_marked.jpg\" alt=\"create a breakpoint\" width=\"800\">\n\n5.6 run the debug program by selecting green start button.\n\n<img src=\"./images/cosmosdb_dotnet_app_exec_output.jpg\" alt=\"create a breakpoint\" width=\"600\">\n\nThis application creates documents in the Cosmos DB Items container and stops at the breakpoint.\n\n5.7 Go back to Cosmos DB in Azure Portal and verify the data the application has created.\n\n<img src=\"./images/CosmosDB_QuickStart_Create_Items_Marked.jpg\" alt=\"create a breakpoint\" width=\"800\">\n\n5.8 Come back to Visual Studio and continue the execution by selecting 'Continue' button.\nIt will delete all the items it created in the Cosmos DB database.\n\n5.9 Go back to Cosmos DB in Azure Portal and verify if the application has deleted the database, container\nand items.\n\nYou are successfully built an application to access Cosmos DB Service and to create database, container and \npopulate with items. Congratulations!!.\n\n\n### Azure Cosmos DB Emulator\nThe Azure Cosmos DB Emulator provides a local environment that emulates the Azure Cosmos DB service for development purposes. \nUsing the Azure Cosmos DB Emulator, you can develop and test your application locally, without creating an Azure subscription \nor incurring any costs. When you're satisfied with how your application is working in the Azure Cosmos DB Emulator, you can \nswitch to using an Azure Cosmos DB account in the cloud.\n\n#### Requirements to install:\n* Currently Windows Server 2016, 2019 or Windows 10 host OS are supported. The host OS with Active \nDirectory enabled is currently not supported.\n* 64-bit operating system\n* 2-GB RAM\n* 10-GB available hard disk space\n* administrative privileges on the computer. The emulator will add a certificate and also set the firewall \nrules in order to run its services. Therefore admin rights are necessary for the emulator to be able to \nexecute such operations.\t\n\nAccess [Azure Cosmos DB Emulator for local development and testing](https://learn.microsoft.com/en-us/azure/cosmos-db/local-emulator?tabs=ssl-netstd21#how-does-the-emulator-work) \nfor more details.\n\n5.10 Download Azure Cosmos DB Emulator\n\n[download](https://aka.ms/cosmosdb-emulator) and install the latest version of Azure Cosmos DB Emulator \non your local computer. \n\nYou will download azure-cosmosdb-emulator-2.14.9-3c8bff92.msi file to your local environment.\n\nRun a DOS window as an **administrator** and run the install by entering the full file name at the prompt.\n\n<img src=\"./images/cosmos_emulator_install_admin_window_Marked.jpg\" alt=\"Emulator install in admin window\" width=\"600\">\n\n\n5.11 Installer launches the emulator in a browser with the following screen.\n\n<img src=\"./images/CosmosDB_Emulater_Start_Screen_Marked.jpg\" alt=\"Emulator start screen\" width=\"800\">\n\n5.12 Copy the **URI** and **Primary key** to a notepad.\n\n5.13 Open up Visual Studio Quick Start application and use the Solution Explorer to navigate to **App.config**.\nupdate **EndpointUri** and **PrimaryKey** which you copied from Cosmos DB Emulator.\n\n<img src=\"./images/cosmos_vs_app_config_change_Marked.jpg\" alt=\"Visual Studio Config Change\" width=\"800\">\n\n5.14 Execute the application from Visual Studio with a breakpoint\n\n5.15 Verify the data using the **Explorer** in the local Cosmos DB emulator tool\n\n<img src=\"./images/CosmosDB_Emulator_CreateItem_Marked.jpg\" alt=\"Verify data in local Emulator tool\" width=\"600\">\n\n5.16 You can also run SQL queries to analyze the data using the **SQL Query window** similar to the tool you used \nwith Azure Cosmos DB Service. \n\n**This is the best way to estimate your query costs and optimize your queries to save costs.** \n\n<img src=\"./images/CosmosDB_Emulator_RunQuery_Marked.jpg\" alt=\"Run SQL Query using emulator\" width=\"600\">\n\nYou have built your local environment to build applications on Cosmos DB with no costs till you are ready to \ndeploy to Azure. Congratulations!!\n\nThis workshop provided you a hands-on experience to model data in Cosmos DB and optimize the throughput for small, medium and \nenterprise customers. Also gave an experience to build applications using Cosmos DB in your development \nenvironment with no cost. \n\nPlease send your feedback and suggestions to us!!\n\n\n\n\n\n", "repo_name": "CosmosDB_Multi-Tenant", "org_name": "microsoft", "org_repo": "microsoft/CosmosDB_Multi-Tenant", "platform_org_repo": "github+microsoft/CosmosDB_Multi-Tenant", "link_to_repo": "https://github.com/microsoft/CosmosDB_Multi-Tenant", "platform": "github", "language": null, "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mixedreality.instrumentation", "org_name": "microsoft", "org_repo": "microsoft/mixedreality.instrumentation", "platform_org_repo": "github+microsoft/mixedreality.instrumentation", "link_to_repo": "https://github.com/microsoft/mixedreality.instrumentation", "platform": "github", "language": "C#", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# MMLMCalibration\n\n<h2 align=\"center\">\n  Official Code for the paper: \"On the Calibration of Massively Multilingual Language Models\"\n</h2>\n\n<p align=\"center\">\n  <a href=\"https://2022.emnlp.org/\"><img src=\"https://img.shields.io/badge/EMNLP%20-2022-blue\"></a>\n  <a href=\"https://github.com/microsoft/MMLMCalibration/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/License-MIT-green\">\n  </a>\n</p>\n\n\nMassively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well towards improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce the calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model specific factors influence it, and pointing out the strategies to improve the same.\n\n<p float=\"middle\">\n  <img src=\"images/en_oob.svg\" width=\"24%\" />\n  <img src=\"images/sw_oob.svg\" width=\"24%\"/>\n  <img src=\"images/sw_zs_cali.svg\" width=\"24%\"/>\n  <img src=\"images/sw_fs_cali.svg\" width=\"24%\"/>\n</p>\n\n\n\n#### Dependencies\n- Compatible with Python3.7\n- The necessary packages can be install through requirements.txt.\n\n#### Setup\nInstall VirtualEnv using the following (optional):\n\n```shell\n$ [sudo] pip install virtualenv\n```\nWe recommend creating a virtual environment(optional):\n\n```shell\n$ virtualenv -p python3 venv\n$ source venv/bin/activate\n```\nFinally, install the required packages by running:\n\n```shell\npip install -r requirements.txt\n```\n\n#### Datasets\n\nDownload XNLI and PAWS-X datasets from the [link](https://drive.google.com/file/d/14hkXccY8bWUN_po2hQ699QgdYJJ_qSuO/view?usp=sharing) and place them in `data/` directory. There is no need of downloading XCOPA, COPA and SIQA datasets, they are loaded using Hugging Face's [datasets library](https://huggingface.co/docs/datasets/index). MARC dataset should be downloaded from [here](https://registry.opendata.aws/amazon-reviews-ml/) and pasted into `data/` directory.\n\n#### Running Experiments\n\nFor vanilla fine-tuning of mBERT on XNLI and measuring calibration across lanaguages run\n```\npython -m src.run_sentence_cls \\\n    --mmlm bert-base-multilingual-uncased \\\n    --dataset xnli \\\n    --lr 3e-5 \\\n    --num_epochs 3 \\\n    --max_train_samples 40000\n```\n\nTo run with label smoothing (LS)\n```\npython -m src.run_sentence_cls \\\n    --mmlm bert-base-multilingual-uncased \\\n    --dataset xnli \\\n    --lr 3e-5 \\\n    --num_epochs 3 \\\n    --max_train_samples 40000 \\\n    --alpha_smoothing 0.1\n```\n\nRunning Label Smoothing + Temperature Scaling (LS + TS)\n```\npython -m src.run_sentence_cls \\\n    --mmlm bert-base-multilingual-uncased \\\n    --dataset xnli \\\n    --lr 3e-5 \\\n    --num_epochs 3 \\\n    --max_train_samples 40000 \\\n    --alpha_smoothing 0.1 \\\n    --temp_scaling \\\n    --cal_lang en \\\n    --cal_size 500\n```\n\nOther examples can be found in [`scripts/experiments_xnli.sh`](scripts/experiments_xnli.sh) and [`scripts/experiments_xcopa.sh`](scripts/experiments_xcopa.sh)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "MMLMCalibration", "org_name": "microsoft", "org_repo": "microsoft/MMLMCalibration", "platform_org_repo": "github+microsoft/MMLMCalibration", "link_to_repo": "https://github.com/microsoft/MMLMCalibration", "platform": "github", "language": "Python", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)\n\n# Responsible AI Tracker\n\nResponsible AI Tracker is a JupyterLab Extension for managing, tracking, and comparing results of machine learning experiments for model improvement. Using this extension, users can view models, code, and visualization artifacts within the same framework enabling fast model iteration and evaluation processes. The extension is a work-in-progress research prototype to test and understand tooling functionalities and visualizations that can be helpful to data scientists. If you would like to propose new ideas for improvement feel free to contact the development team at [rai-toolbox@microsoft.com](mailto:rai-toolbox@microsoft.com) or create new issues in this repository.\n\nThis repo is a part of the [Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox), a suite of tools providing a collection of model and data exploration and assessment user interfaces and libraries that enable a better understanding of AI systems. These interfaces and libraries empower developers and stakeholders of AI systems to develop and monitor AI more responsibly, and take better data-driven actions.\n\n### Main functionalities of the tracker include:\n\n- **Managing and linking model improvement artifacts**: the extension encourages clean and systematic data science practices by allowing users to associate the notebook used to create a model with the resulting model. These practices support careful model tracking and systematic experimentation.\n\n- **Disaggregated model evaluation and comparisons**: the model comparison table in the extension provides an in-depth comparison between the different models registered in the extension. This comparison contrasts performance results across different data cohorts and metrics, following a disaggregated approach, which goes beyond single-score performance numbers and highlights cohorts of data for which a model may perform worse than its older versions. Read more about disaggregated analysis [here](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/basics_disaggregated.html).\n\n- **Integration with the Responsible AI Mitigations library**: as data scientists experiment and ideate different steps for model improvement, the [Responsible AI Mitigations Library](https://github.com/microsoft/responsible-ai-toolbox-mitigations) helps them implement different mitigation techniques in python that may improve model performance and can be targeted towards specified cohorts of interests.\n\n## Tour\n\nWatch a [video tour](https://www.youtube.com/watch?v=jN6LWFzSLaU) of the Responsible AI Tracker and follow along using the notebooks and dataset [here](./tour).\n<p align=\"center\">\n<img src=\"./docs/imgs/RAI%20Tracker%20full%20view.png\" alt=\"ResponsibleAITrackerOverview\" width=\"750\"/>\n\n\n\n## Installation\n\nThe Responsible AI Tracker can be deployed on Windows or Ubuntu, using anaconda or python.\n\n### The Responsible AI Tracker prerequisites:\n\n- [Nodejs](https://nodejs.org/)\n- [Python](https://www.python.org/downloads/) (versions supported 3.9 **to** 3.10.6)\n\n- JupyterLab\n  - If you use pip:\n  ```shell\n  pip install jupyterlab==3.6.3\n  ```\n  - If you use conda:\n  ```shell\n  conda install -c conda-forge jupyterlab==3.6.3\n  ```\n\n### The Responsible AI Tracker has two installation options:\n\n- The default installation only installs the essential packages.\n\n  ```shell\n  pip install raitracker\n  ```\n\n- The installation With the [all] flag installs the essential packages plus PyTorch, and Tensorflow.\n  ```shell\n  pip install raitracker[all]\n  ```\n\nInstallation through the JupyterLab Extension Manager coming soon. \n\n### Running\n\nStart up JupyterLab using:\n\n```bash\njupyter lab\n```\n\nThe extension should be available in the left vertical bar. For ideas on getting started, watch the [video tour](https://www.youtube.com/watch?v=jN6LWFzSLaU) and follow along using the notebooks and dataset [here](./tour).\n \n<details><summary>Dependencies</summary>\n<ul>\n\n<li>jupyterlab</li>\n<li>fluentui</li>\n<li>nodejs</li>\n<li>react</li>\n<li>redux</li>\n<li>lumino</li>\n<li>lodash</li>\n<li>babel</li>\n<li>codeMirror</li>\n<li>webpack</li>\n<li>mlflow</li>\n<li>numpy</li>\n<li>pandas</li>\n<li>scikit-learn</li>\n<li>pytorch</li>\n</ul>\n</details>\n\n---\n\n## Getting help\n\nWe encourage you to check the Responsible AI Tracker [documentation](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/). \n\nFor Responsible AI Mitigations Library help see [Responsible AI Mitigations documentation](https://responsible-ai-toolbox-mitigations.readthedocs.io/en/latest/).  \n\nSee [here](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/SUPPORT.md) for further support information.\n\n\n### Bug reports\n\nTo report a bug please read the [guidelines](https://responsible-ai-toolbox-tracker.readthedocs.io/en/latest/) and then open a [Github issue](https://github.com/microsoft/responsible-ai-toolbox-tracker/issues/new). \n\n\n### Feature requests\n\nWe welcome suggestions for new features as they help make the project more useful for everyone. To request a feature please use the [feature request template](https://github.com/microsoft/responsible-ai-toolbox-tracker/labels/enhancement).\n\n### Contributing\n\nTo contribute code or documentation to the Responsible AI Tracker, please read the [contribution guidelines](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/CONTRIBUTING.md).\n\n---\n\n## Microsoft Open Source Code of conduct\n\nThe [Microsoft  Code of conduct](https://github.com/microsoft/responsible-ai-toolbox-tracker/blob/main/CODE_OF_CONDUCT.md) outlines expectations for participation in Microsoft-managed open source communities.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Research and Acknowledgements\n\n**Current Maintainers:** [ThuVan Pham](https://www.microsoft.com/en-us/research/people/thuvanp/), [Matheus Mendon\u00e7a](https://github.com/mrfmendonca), [Besmira Nushi](https://github.com/nushib), [Rahee Ghosh Peshawaria](https://github.com/raghoshMSFT), [Marah Abdin](https://github.com/marah-abdin), [Mark Encarnaci\u00f3n](https://github.com/markenc), [Dany Rouhana](https://github.com/danyrouh)\n\n**Past Maintainers:** [Irina Spiridonova](https://github.com/irinasp)\n\n**Research Contributors:** [Besmira Nushi](https://github.com/nushib), [Jingya Chen](https://www.jingyachen.net/), [Rahee Ghosh Peshawaria](https://github.com/raghoshMSFT), [ThuVan Pham](https://www.microsoft.com/en-us/research/people/thuvanp/), [Matheus Mendon\u00e7a](https://github.com/mrfmendonca), [Ece Kamar](https://www.ecekamar.com/), [Dany Rouhana](https://github.com/danyrouh)", "repo_name": "responsible-ai-toolbox-tracker", "org_name": "microsoft", "org_repo": "microsoft/responsible-ai-toolbox-tracker", "platform_org_repo": "github+microsoft/responsible-ai-toolbox-tracker", "link_to_repo": "https://github.com/microsoft/responsible-ai-toolbox-tracker", "platform": "github", "language": "TypeScript", "stargazers_count": 35, "watchers_count": 35}, {"README_text": "# Public API for the ms-vscode.cmake-tools VS Code extension\n\nThe purpose of this API is to allow for extensions to access with the CMake code model and execute CMake commands.\n\nWhen your extension activates, you can use the following code to get access to the API:\n\n```TypeScript\nimport { CMakeToolsApi, Version, getCMakeToolsApi, UIElement } from 'vscode-cmake-tools';\n\nconst api = await getCMakeToolsApi(Version.v1);\nif (api) {\n    // Access the API here. In this example, we hide the launch target button.\n    await api.hideUIElement(UIElement.StatusBarLaunchButton);\n}\n```\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "vscode-cmake-tools-api", "org_name": "microsoft", "org_repo": "microsoft/vscode-cmake-tools-api", "platform_org_repo": "github+microsoft/vscode-cmake-tools-api", "link_to_repo": "https://github.com/microsoft/vscode-cmake-tools-api", "platform": "github", "language": "TypeScript", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Introduction \r\nA Rust library for implementing anonymous one-time tokens from algebraic MACs. It embeds a private bit by the server to the token. ", "repo_name": "MacTok", "org_name": "microsoft", "org_repo": "microsoft/MacTok", "platform_org_repo": "github+microsoft/MacTok", "link_to_repo": "https://github.com/microsoft/MacTok", "platform": "github", "language": "Rust", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "![System Center Operations Manager](/media/git-guidance/product/opsmgr_logo.png)\n\n![Powershell](/media/git-guidance/language/powershell_logo.png)\n\n----\n# Introduction \nThis folder contains various Powershell scripts used to troubleshoot System Center Operations Manager issues.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CSS-SystemCenter-OperationsManager", "org_name": "microsoft", "org_repo": "microsoft/CSS-SystemCenter-OperationsManager", "platform_org_repo": "github+microsoft/CSS-SystemCenter-OperationsManager", "link_to_repo": "https://github.com/microsoft/CSS-SystemCenter-OperationsManager", "platform": "github", "language": "PowerShell", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "## Usage\n\nThis extensions adds blocks to help you create your own carnival games in MakeCode Arcade using throwable balls, extra timer functions, and extra game-over options.\n\n\n* Setting Up a Ball\n\n```blocks\nlet myBall: Ball = null\nmyBall = carnival.create(assets.image`ball-yellow`, SpriteKind.Player)\nmyBall.setPosition(80, 90)\nmyBall.controlBallWithArrowKeys(true)\nmyBall.setIter(50)\nmyBall.setTraceMulti(carnival.Tracers.Full)\n```\n\nThis snippet sets up a ball called myBall and allows you to control it with arrow keys. You'll also see that it has a trace marker that's set to 50% value.\n\n\n* Variable Power\n\n```blocks\nlet theTarget: Sprite = null\nlet throwBall: Ball = null\nlet myBall: Ball = null\nscene.setBackgroundImage(assets.image`wildWest`)\nmyBall = carnival.create(assets.image`ball-yellow`, SpriteKind.Player)\nmyBall.setPosition(80, 90)\nlet statusbar = statusbars.create(120, 6, StatusBarKind.Health)\nstatusbar.setColor(5, 10)\nstatusbar.setBarBorder(1, 1)\nstatusbar.setPosition(80, 113)\nlet myBooth = sprites.create(assets.image`booth`, SpriteKind.Booth)\nmyBall.controlBallWithArrowKeys(true)\nmyBall.setIter(10)\nmyBall.setTraceMulti(carnival.Tracers.Cross)\nmyBall.variablePower(statusbar, 30, 50, 100)\n```\n\nThis snippet adds to the code above by creating a statusbar that is used as a visual indicator of the power at any given time. The power varies between 30% and 50% of full power.\n\n\n\n* Tossing Projectile Balls\n\n```blocks\ncontroller.A.onEvent(ControllerButtonEvent.Pressed, function () {\n    throwBall = carnival.createProjectileBallFromSprite(assets.image`ball-blue`, myBall)\n})\n```\n\nThis snippet creates a ball of type projectile that emits from the parent ball using its angle and power settings.\n\n\n\n* Timer\n\n```blocks\ncarnival.startTimer()\n```\n\nThis sets a timer that counts up throughout the game, or until carnival.startTimer() is called again.\n\n\n\n* Countdown\n\n```blocks\ncarnival.startCountdownGame(15, carnival.WinTypes.Timed)\n\n```\n\nThis starts a countdown that launches an endgame state when completed. You can choose from win, lose, best time, best score, or multiplayer.\n\n\n\n* Game\n\n```blocks\n    carnival.customGameOverExpanded(\"Great Job!\", effects.confetti, music.powerUp, carnival.ScoreTypes.HScore, 0)\n```\n\nThis allows for a custom game over message under circumstances of your choosing.  You can change the message, the background effect, the music, the way score is judged (win, lose, time, score, or multiplayer.) You can also pass in a custom \"score\" to for judging success.\n\n\n\n* Label\n\n```blocks\ncarnival.addLabelTo(\"Target Practice\", carnival.Areas.Top, 4)\n```\n\nThis adds a label to your carnival booth, using a predetermined location near the top, middle, or bottom of the play area.\n\n\n\n\n## Use this extension\n\nThis repository can be added as an **extension** in MakeCode.\n\n* open [https://arcade.makecode.com/](https://arcade.makecode.com/)\n* click on **New Project**\n* click on **Extensions** under the gearwheel menu\n* search for **https://github.com/microsoft/arcade-carnival** and import\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n\n## Blocks preview\n\nThis image shows the blocks code from the last commit in master.\nThis image may take a few minutes to refresh.\n\n![A rendered view of the blocks](https://github.com/microsoft/arcade-carnival/raw/master/.github/makecode/blocks.png)\n\n#### Metadata (used for search, rendering)\n\n* for PXT/arcade\n<script src=\"https://makecode.com/gh-pages-embed.js\"></script><script>makeCodeRender(\"{{ site.makecode.home_url }}\", \"{{ site.github.owner_name }}/{{ site.github.repository_name }}\");</script>\n", "repo_name": "arcade-carnival", "org_name": "microsoft", "org_repo": "microsoft/arcade-carnival", "platform_org_repo": "github+microsoft/arcade-carnival", "link_to_repo": "https://github.com/microsoft/arcade-carnival", "platform": "github", "language": "JavaScript", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Microsoft System Center - Orchestrator Support Scripts\n\n## The Repository\n\nThis repository is the home of several scripts for Microsoft System Center - Orchestrator (SCO).\nThe scripts are intended for identifying and resolving a wide range of issues regarding SCO.\n\nFor more information, see the documentation of individual scripts: \n\nhttps://microsoft.github.io/CSS-SystemCenter-Orchestrator/\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nSee the documentation of individual scripts to learn how to contribute:\nhttps://microsoft.github.io/CSS-SystemCenter-Orchestrator/\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CSS-SystemCenter-Orchestrator", "org_name": "microsoft", "org_repo": "microsoft/CSS-SystemCenter-Orchestrator", "platform_org_repo": "github+microsoft/CSS-SystemCenter-Orchestrator", "link_to_repo": "https://github.com/microsoft/CSS-SystemCenter-Orchestrator", "platform": "github", "language": "PowerShell", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CSS-SystemCenter-VirtualMachineManager", "org_name": "microsoft", "org_repo": "microsoft/CSS-SystemCenter-VirtualMachineManager", "platform_org_repo": "github+microsoft/CSS-SystemCenter-VirtualMachineManager", "link_to_repo": "https://github.com/microsoft/CSS-SystemCenter-VirtualMachineManager", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cla-approved-bots", "org_name": "microsoft", "org_repo": "microsoft/cla-approved-bots", "platform_org_repo": "github+microsoft/cla-approved-bots", "link_to_repo": "https://github.com/microsoft/cla-approved-bots", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Introduction \nAzure Managed HSM offers a TLS Offload library which is compliant with PKCS#11 version 2.40. We do not support all possible functions listed in the PKCS#11 specification. Our TLS Offload library supports a limited set of functions and mechanisms for SSL/TLS Offload with F5 (BigIP) and Nginx only!\n\n# Getting Started\nInstallation including configuration and authentication requirements for TLS Offload can be found in the readme file within each .deb and .rpm release package under *[RELEASES](https://github.com/microsoft/AzureManagedHsmTLSOffload/releases)* \n\nAdditional information can be found from *[Get Started](https://github.com/microsoft/AzureManagedHsmTLSOffload/blob/main/GET_STARTED.md)*\n\n## Supported Operating Systems\n\n- CentOS 7 \n- Ubuntu 18.04\n- CBL Mariner 2 \n\n*CentOS 8 is not supported! Red Hat no longer supports CentOS 8 as it reached end of life December 31st, 2021.*\n*** \n\n## Supported Key Types & Mechanisms\n#### Key Types\nAzure Managed HSM TLS Offload supports the following key types. \n\n| **Key Types** | **Description** |\n|-----------|-----------|\n| RSA | Generate 2048, 3072 and 4096-bit RSA keys. |\n| ECDSA | Generate keys with P-256, P-256K, P-384, P-521 curves. |\n| AES | Not Supported |\n\n#### Mechanisms \nAzure Managed HSM TLS Offload supports the following algorithms. \n\n| **Mechanisms** | **Description** |\n|-----------|-----------|\n| Encryption and Decryption | Not supported through TLS Offload library. Supported through Managed HSM API only. | \n| Sign and Verify | RSA, and ECDSA supported. SignRecover/VerifyRecover not supported. |\n| Hash/Digest | SHA256, SHA384, and SHA512 supported. |\n| Key Wrap\u202f| Not Supported through TLS Offload library. Key Wrap is supported through Managed HSM API only. |\n| Triple Des (3DES) | Not Supported |\n| Key Derivation | Not Supported |\n\nTo invoke a cryptographic feature using our TLS Offload library, call a function with a given mechanism. The following table summarizes the combinations of functions and mechanisms supported by Azure Managed HSM.  \n\nX indicates that Azure Managed HSM supports the mechanism for the function. We do not support all possible functions listed in the PKCS#11 specification. \n\n#### Mechanisms & Functions\n| |  **Encrypt & Decrypt** | **Sign  & Verify** | **SR & VR** | **Digest** | **Gen Key/Key Pair** | **Wrap & Unwrap** | **Derive**|\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| CKM_RSA_PKCS_KEY_PAIR_GEN  | | | | |X| | |\n| CKM_RSA_PKCS\u00a0\u00a0\u00a0\u00a0           | |X| | | | | |\n| CKM_RSA_PKCS_OAEP          | | | | | | | |\n| CKM_RSA_PKCS_PSS\u00a0\u00a0\u00a0\u00a0       | |X| | | | | |\n| CKM_EC_KEY_PAIR_GEN\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  | | | | |X| | |\n| CKM_ECDSA\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0            | |X| | | | | |\n| CKM_SHA256                 | | | |X| | | |\n| CKM_SHA384                 | | | |X| | | |\n| CKM_SHA512                 | | | |X| | | |\n\n***\n## Supported API Operations \nAzure Managed HSM TLS Offload supports the following API operations. \n- C_GetFunctionList \n- C_GetSlotList \n- C_GetSlotInfo \n- C_GetInfo \n- C_GetTokenInfo \n- C_GetSessionInfo \n- C_GetMechanismList \n- C_GetMechanismInfo \n- C_Initialize \n- C_Finalize \n- C_Login \n- C_Logout \n- C_OpenSession \n- C_CloseSession \n- C_CloseAllSessions \n- C_FindObjectsInit \n- C_FindObjects \n- C_FindObjectsFinal \n- C_GenerateKeyPair \n- C_GenerateRandom \n- C_GetAttributeValue \n- C_SetAttributeValue \n- C_SignInit \n- C_Sign \n- C_VerifyInit \n- C_Verify \n***\n\n# FAQ\n### Can I create a Key in Azure Managed HSM then later use the TLS Offload library for that Key?\nNo. Keys created without using the mhsm-pkcs11 TLS Offload Library are NOT compatible. A key must be created using either the mhsm_p11_create_key sample or a custom application that loads the mhsm-pkcs11 TLS Offload library and calls the appropriate interface functions.\n\n### Does TLS Offload library support multiple Managed HSM resources?\nYes. You can declare multiple resources in the mhsm-pkcs.conf. SlotId is the identifier for the Managed HSM resource which is unique.\n\n### Does TLS Offload library support multiple service principals?\nNo. Our TLS Offload library does not support multiple service principles.\n\n### Does the TLS Offload Library support AES Keys? \nNo. Our TLS Offload Library does not support AES keys. Customers that require AES keys should use the Azure Managed HSM REST API.\n\n### Does the TLS Offload Library support TLS V1.0 or TLS 1.1?\nNo. We only support TLS 1.2 and TLS 1.3.\n\n### Does the TLS Offload Library support Azure Key Vault and Azure Managed HSM?\nNo. Azure Key Vault is not supported.  Only Azure Managed HSM is supported through our TLS Offload Library.\n\n### How can I improve TLS Offload Library performance? \n1. Turn OFF all the logging in the MHSM configuration file.\n2. Connection Caching must be configured to match the concurrent signing requests issued by your application. Refer to *[Connection Caching](https://learn.microsoft.com/azure/key-vault/managed-hsm/tls-offload-library#connection-caching)* for configuration. \n3. For optimal performance your application should be hosted/collocated in the same region as your HSM pool. \n\n### How do I use the Key Creation Tool included in the TLS Offload Library?\nTo Get Started refer to the *[ TLS Offload Library Overview](https://learn.microsoft.com/en-us/azure/key-vault/managed-hsm/tls-offload-library)*\n\nThe TLS Offload Library includes a key creation tool: mhsm_p11_create_key.  The key creation tool requires a Service Principal which is assigned to the \u201cManaged HSM Crypto User\u201d role at the \u201c/keys\u201d scope.  The key creation tool reads the Service Principal credentials from the environment variables MHSM_CLIENT_ID and MHSM_CLIENT_SECRET.  \n-\tMHSM_CLIENT_ID \u2013 must be set to the Service Principal\u2019s Application (Client) ID \n-\tMHSM_CLIENT_SECRET \u2013 must be set to the Service Principal\u2019s Password (Client Secret) \n\nFor managed identities, these environment variables are not needed.  \n-\tUse the \u201c--identity\u201d argument to enable managed identity with mhsm_p11_create_key tool. \n-\tThe \u201cclient_id\u201d of user-assigned managed identity should be mentioned in the mhsm configuration file (mhsm-pkcs11.conf). If \u201cclient_id\u201d of user-assigned managed identity is not provided it will consider it as system-assigned managed identity. \n\n### How do I configure my TLS server to use the Managed HSM TLS Offload Library as the interface library \nConfigure your TLS server (e.g. the nginx SSL configuration setting `ssl_certificate_key\u2019) with the key label and the TLS Offload Service Principal credentials. For MSI (managed service identity) use empty credentials or enable it via TLS offload mhsm configuration file (mhsm-pkcs11.conf) and \u201cclient_id\u201d of user-assigned managed identities. If MSI is enabled via TLS offload mhsm configuration file (mhsm-pkcs11.conf), then the service principal credentials will be ignored.\n\n### How do I file production support incidents or get help?\nAll production incident support tickets for Azure Managed HSM or TLS Offload Library should be submitted through the Azure Portal under Help+Support. This TLS Offload Library project uses GitHub issues to only track bugs and feature requests not production live site support incidents. \n\n- For help and issues using this project for SSL Offloading / Keyless TLS with Azure Managed HSM please submit an Azure support request through Azure Portal. For any other questions about using this project for SSL Ofloading / Keyless TLS please send email to [mhsmp11@microsoft.com](mailto:mhsmp11@microsoft.com) and ensure to include your Microsoft Account Manager.\n\n- For help and questions about F5 BigIP or Nginx issues or configuration please send email to [support@f5.com](mailto:support@f5.com) or submit through case management at *[my.f5.com](https://my.f5.com)* \n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureManagedHsmTLSOffload", "org_name": "microsoft", "org_repo": "microsoft/AzureManagedHsmTLSOffload", "platform_org_repo": "github+microsoft/AzureManagedHsmTLSOffload", "link_to_repo": "https://github.com/microsoft/AzureManagedHsmTLSOffload", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Code and Data for: Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming\n\nArxiv link: https://arxiv.org/abs/2210.14306\n\nDataframe of telemetry for the study is available in [data pickle](data/logs_by_user_session_labeled_new.pkl) \n\nVideo data of the coding session is available as a zipped folder at https://drive.google.com/file/d/1qriGQXjMDoesr1WxB7s0QK8rYy2hSddc/view?usp=sharing \n\n# Installation\n\nThe environment named 'coderec' is available as a yml file and can be installed using:\n```\nconda env create -f environment.yml\n```\n\nSome of the packages are not required for all the scrips and notebooks, but are included in the environment for convenience. \n\nThere is also a requirements.txt file available, but it does not include pip install so it is insufficient, to use that:\n```\nconda create --name coderec --file requirements.txt\n```\nWe will also need to install further libraries and tools.\n\n- Treesitter for getting code labels\n```\n git clone https://github.com/tree-sitter/tree-sitter-python\n```\n\n\n# Dataframe for user study\n\nOur study complete data is stored in '/data/logs_by_user_session_labeled.pkl' which contains python  array which we will name 'df_observations' where: 'df_observations[i]' is the session for the ith user stored as a pandas dataframe.\n\nTo be more explicit, df_observations[i] is a pandas dataframe that contains the following columns:\n```\n'UserId' \n'TimeGenerated': timestamp for event\n'StateName': betweeen 'Shown', 'Accepted', 'Rejected', 'Replay' (consecutive shown), 'Browsing' (shown different choice index)\n'HiddenState' : high level hidden state between 'UserBeforeAction', 'UserPaused', 'UserTyping'\n'TimeSpentInState'\n'CurrentSuggestion' \n'CurrentPrompt'\n'Measurements': measurements from logs\n'EditPercentage': an array containing the relative edit distance (in 0-1) for the 5 stillincode events for this completion\n'LabeledState': the state label by the user for the current state\n```\n\nThe LabeledState takes the following values:\n```\nThinking About Suggestion (a): actively thinking/verifying about suggestion shown, also includes going to the internet to verify <br>\nNot Thinking (s): not thinking about suggestion shown  <br>\nDeferring Thought For Later (d): decide to not think now about suggestion, but will think later about it  <br>\nThinking About New Code To Write (f): thinking about code outside suggestions to write, new functionality  <br>\nWaiting For Suggestion (g): waiting for Copilot suggestion to be shown   <br>\nWriting New Code (z): writing code that implements new functionality <br>\nEditing Recent Suggestions (x): editing recent Copilot suggestions<br>\nEditing (Personally) Written Code(c): editing code you wrote that was not a Copilot suggestion for purpose of fixing functionality <br>\nPrompt Crafting (v): writing comment or code with intention of copilot completion<br>\nWriting Documentation (b): adding comments for purpose of documentation,<br>\nDebugging/Testing Code (h): running or debugging code to check functionality, may include writing tests or debugging statements<br>\nLooking at documentation: looking online for documentation\n```\n\nThe study dataframe is also stored as a csv file in '/data/logs_by_user_session_labeled.csv' which can be imported as a single pandas dataframe. \n\n\n\n# User Interface for Study Data\n\nYou can interact with the study data using our annotation interface.\n\nSteps:\n\n- First make sure to download the data including the labels (.json files) and the videos (.mp4 files). \n- Place both the json and mp4 in the 'user_study_webapp/app_study/static' folder.\n- For each study,  run the following commands:\n\n```\npython server.py -p static/logs_user_8_l.json -v static/video_cropped_8.mp4\n```\n- Go to http://localhost:8080/ on your browser to see the interface.\n\nNote that the jsons of the labeled states are not the final labels, please consult the [data pickle](data/logs_by_user_session_labeled_new.pkl)\n\n![Annotation Interface](images/interface_sreenshot.png)\n\n\n\nWe include the instructions for each coding task in [coding_tasks](user_study_webapp/coding_tasks.ipynb)\n\n\n# Visualization and Analysis for User Study\n\n## Drawing Timelines and Graphs\n\n\nUse the jupyter notebook [viz_draw](user_study_analysis/viz_draw.ipynb) to draw the timelines for the study data.\n\n![User Timeline](images/user_timeline.PNG)\n\n\n\nUse the jupyter notebook [viz_draw](user_study_analysis/viz_draw.ipynb) to draw the graph for the study data.\n\n\n![Graph](images/graph.PNG)\n\n\n##  Analysis\n\nFor insights and analysis that are found in our paper, they can be replicated in the following two notebooks:\n\n- [high level statistics about actions, states and transitions](user_study_analysis/high_level_stats.ipynb.ipynb)\n- [time adjustments and deeper insights](user_study_analysis/deep_insights.ipynb.ipynb)\n\n\n# Generating Features from the  Dataframe for Prediction and Decoding\n\n\nGiven the extended logs, we will generate features for the prediction and decoding models.\n\nThe below command will generate a pickle file containing a python variable, name it 'df_features', of the following form:\n\ndf_features[i][j][h]: is the h'th feature for the k'th event for the ith user.\n\nLet us elaborate further, df_features[i] is the all the data for the ith user. df_features[i][k] contains the features for the k'th event in the  session. Finally, df_features[i][k][h] contains the h'th feature, more precisely, df_features[i][k] is a list of different feature, where df_features[i][k][h]  is a list contains a representation of the h'th feature as follows:\n```\n   feature_dict = {'Measurements: compCharLen, confidence, documentLength, numLines, numTokens, promptCharLen, promptEndPos, quantile': 0,\n    'edit percentage': 1, 'time_in_state': 2, 'session_features':3, 'suggestion_label':4, 'prompt_label':5,\n    'suggestion_embedding':6, 'prompt_embedding':7, 'suggestion_text_features':8, 'prompt_text_features':9, 'statename':10}\n```\nmeaning df_features[i][k][0] is a list contaning all measurement features, i.e. compCharLen, confidence, documentLength, numLines, numTokens, promptCharLen, promptEndPos, quantile in a row. And then df_features[i][k][6] is the 768 dimensional suggestion embedding and so forth.\n\n\nThe command to get the features pickle file is:\n```\npython action_prediction/generate_features.py -p'OUTPUT_PATH_EXTENDED_LOGS.pkl' \\\n-c 0 \\\n-b 1000 \\\n-o 'OUTPUT_PATH_features.pkl' \\\n-e 1 \\\n-m 99999  \\ \n\n```\n\nthe documentation for the args is:\n```\n('-p', '--path', help='Path to extended logs frame', required=True) \n('-c', '--cudadevice', help='cuda device id', default=0, required=True, type=int)\n('-b', '--batchsize', help='batch size', default=1000, required=True, type=int)\n('-o', '--output', help='Output path of .pkl file', required=True) \n('-e', '--embedding', help='Whether to get embeddings for suggestion and prompt', required=True, type=int)\n('-m', '--maxusers', help='max users', default=100, required=True, type=int)\n\n```\n\n# Predicting Accepts and Rejects\n\n\nThe task of predicting Accepts and Rejects can be directly ran after generating the features.\n\nThe script we will run trains an XGBoost model on the features and predicts the Accepts and Rejects and generate multiple plots and pickle files that save the results and model. The plots contain multiple analysis of the results.\n\nNote that we split data into train-val-split according to random_state=42, this can be changed in the action_prediction_prep.py script.\n\n```\npython action_prediction/action_prediction_xgb.py -p 'OUTPUT_PATH_features.pkl' \\\n-c 0 \\\n-s 1 \\\n-o 'OUTPUT_PATH_PLOTS_MODELS' \\ \n-t 0.2 \\ \n-v 0.1 \\\n```\n\nThe argument documentation is\n```\n('-p', '--path', help='Path to features array', required=True) # change to True\n('-c', '--usegpu', help='to use gpu (1 or 0)', default=0, required=True, type=int)\n('-s', '--splitbyusers', help='split by users or session (0 or 1)', default=0, required=True, type=int)\n('-o', '--output', help='output path folder to store results',  required=True)\n('-t', '--testpercentage', help='test percentage', default = 0.2, type =float)\n('-v', '--valpercentage', help='val percentage', default =0.1, type=float)\n```\n\nNote that there are other parameters we can change in the code to improve performance! You should expect to get almost 0.7 AUC and 70% accuracy on the test set. Note these numbers are not the same as those in Section 6 of the paper as those where obtained on a different dataset.\n\n\n\n\n![accuracy vs coverage curve](images/coverage_vs_acc.png)\n\n\n\n# Citation\n\nPlease cite our paper if you use our dataset or code:\n\n```\n@article{mozannar2022reading,\n  title={Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming},\n  author={Mozannar, Hussein and Bansal, Gagan and Fourney, Adam and Horvitz, Eric},\n  journal={arXiv preprint arXiv:2210.14306},\n  year={2022}\n}\n\n```\n\n# Other\n\n## Acknowledgements\nThis release is part of research done during an internship at Microsoft Research ([privacy statement](https://privacy.microsoft.com/en-us/privacystatement)) and was based on valuable feedback from colleagues across MSR and GitHub including Saleema Amershi, Victor Dibia, Forough Poursabzi, Andrew Rice, Eirini Kalliamvakou, and Edward Aftandilian.\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "coderec_programming_states", "org_name": "microsoft", "org_repo": "microsoft/coderec_programming_states", "platform_org_repo": "github+microsoft/coderec_programming_states", "link_to_repo": "https://github.com/microsoft/coderec_programming_states", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# CyBERTron-LM\n\nCyBERTron-LM is a research project on NLP. It collects some Models for solving NLP tasks (e.g., pre-training, understanding, generation or etc). \n\nThe current work in [CyBERTron-LM](https://github.com/microsoft/CyBERTron-LM) include:\n* Language Scoring\n  + Transcormer: [Transcormer](https://arxiv.org/abs/2205.12986)\n  + DiffusionNER: [DiffusionNER](https://arxiv.org/abs/2305.13298)\n\n## Reference\n\nIf you find this project useful in your work, you can cite the following papers if there's a need:\n* [1] ***Transcormer**: Transformer for Sentence Scoring with Sliding Language Modeling*, Kaitao Song, Yichong Leng, Xu Tan, Yicheng Zou, Tao Qin, Dongsheng Li, **NeurIPS 2022**.\n* [2] ***DiffusionNER**: Boundary Diffusion for Named Entity Recognition*, Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, **ACL 2023**. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CyBERTron-LM", "org_name": "microsoft", "org_repo": "microsoft/CyBERTron-LM", "platform_org_repo": "github+microsoft/CyBERTron-LM", "link_to_repo": "https://github.com/microsoft/CyBERTron-LM", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "## Synapse Genie Framework\n**Efficient Utilization of Spark Pools**\n\n<img width=\"324\" alt=\"GenieNewLamp\" src=\"https://user-images.githubusercontent.com/45026856/204741903-7e972b55-e48a-46f0-8962-0cd6c606b461.png\">\n\n ## What is Genie Framework?\n\nSynapse Genie Framework improves Spark pool utilization by executing multiple Synapse notebooks on the same Spark pool instance. It considers the sequence and dependencies between notebook activities in an ETL pipeline. It results in higher usage of full cluster for resources available in a Spark pool.\n\n## When to use Genie Framework?\n\nSynapse Genie is helpful when you have a set of related notebooks that are part of a Synapse pipeline. In any group of notebooks, the workload varies from small to large. By executing such varied workloads on a single Spark pool instance, you can improve resource utilization and reduce Spark pool costs. \n\nIt is also ideal to execute notebooks through Genie framework when the workloads are a mix of CPU and IO intensive operations.\n\nDuring development of a data application in Synapse, we tend to develop multiple notebooks for a pipeline. This is likely due to multiple developers working on tasks independently or due to different transformation logic required to process different datasets. In such a case, the workload per notebook is of varied size. Some notebooks have a small workload and others have large workloads. When these notebooks are invoked from a pipeline, each notebook spins up a separate Spark pool and reserves these nodes till the execution is complete. This results in additional cost, execution time and inefficient usage of pool resources. Even a small notebook with a few lines of code reserves at least two nodes and incurs cost for spin up, execution and deallocation time. Genie utility tries to avoid this by attempting to run all notebooks within a single Spark context. \u00a0\n\n## How does Genie Framework work?\n\nGenie Framework is a metadata driven utility written in Python. It is implemented using threading (ThreadPoolExecutor module) and directed acyclic graph (Networkx library). It consists of a wrapper notebook, that reads metadata of notebooks and executes them within a single Spark session. Each notebook is invoked on a thread with MSSparkutils.run() command based on the available resources in the Spark pool. The dependencies between notebooks are understood and tracked through a directed acyclic graph.\n\nHere is an example of metadata \n\n![image](https://user-images.githubusercontent.com/45026856/198976909-6bedb74e-07db-4241-ba25-21bd9e087bc2.png)\n\nSample graph screenshot that Genie builds for its execution.\n\n![image](https://user-images.githubusercontent.com/45026856/198977019-ecdb3e78-ac76-480a-b731-e1c0ed9f13eb.png)\n\n\n## What are the Capabilities of Genie Framework?\n\n- Provides metadata-driven orchestration of notebooks.\n- Supports easy onboarding to replace any Azure Data Factory/Synapse pipeline which has only notebook activities and sub-pipelines.\n- Has a configurable retry/restart mechanism for failed notebooks.\n- Logs error messages for future verification and run telemetry.\n- Able to activate and/or remove notebooks from pipeline through metadata.\n- Can pass parameters to notebooks like pipeline activity.\n\n\n **What are the benefits of Synapse Genie?**\n\n- Increased Spark pool utilization and reduction in execution time and costs.\n- Eliminates the need to configure node types and sizes per notebook.\n- Enables notebook activation and/or removal from pipeline through metadata.\n- Enables global views, user-defined functions (UDFs), and usage across notebooks.\n- Prevents small notebooks from hogging resources unnecessarily.\n- Puts a stop to long running notebooks from delaying other unrelated notebooks.\n\n## What are the Output Statistics?\nEarly observations have shown a reduction of execution time for a pipeline by 30% to 50%. Of course, this will vary per workload. In a few cases, we reduced the size of our Spark pool cluster rather than opting for a reduction in just pipeline execution duration after onboarding to Genie framework. Over time, both choices translate to significant cost savings.\n\n## Ok , I am in! How do I onboard?\nGenie utility consists of a Python Wheel package and a Synapse notebook. Notebook information is stored as metadata for the framework and the wrapper notebook is to be invoked through a Synapse pipeline.\n\nFor detailed instructions about onboarding and installation of the framework to your Synapse workspace, please go through the [onboarding document](https://github.com/microsoft/SynapseGenie/blob/main/Onboarding.md) for Genie Framework.\n\n## What else do I need to know before implementing ?\n\nGenie Framework **is not** a direct release of Azure Synapse Analytics products and will not be officially supported by Synapse. It is a utility developed by the HR Data Insights team within the Microsoft Digital Employee Experience organization. This framework was developed as a way in which notebooks can be run on the same instance of Spark pool and can help higher resource utilization, thus reducing execution time and cost.\n\nThis Framework is not ideal for notebooks which use parallel threading heavily and run-on exceptionally large datasets (in TB\u2019s). It is better suited when we have a mix of small and large workloads as mentioned in the \"When to use Genie framework?\" section above.\n\n\n**Session isolation**\nIn Genie Framework , all Notebooks run with the same Spark context and Spark session by default. This causes any temporary views defined in one notebook to be available in another. At times, this may lead collision if two notebooks have different definitions but one definition is overwritten in the spark session. \n\nTo avoid this, use a naming convention for views that denotes the notebook in which it is run. Alternatively, to isolate the spark session on a notebook from other notebooks in Genie framework , we can set the spark session on the first cell of the below notebook in the below manner\n\n**For Python Notebook**:\n![image](https://user-images.githubusercontent.com/45026856/198977106-fae91a5c-9cd2-4382-a836-8ed42ec01a5f.png)\n\n\n**For Scala Notebook**:\n\n![MicrosoftTeams-image (4)](https://user-images.githubusercontent.com/99250812/203032176-fa97e4ec-b181-4314-98bd-229a4e65881d.png)\n\nUsing the Synapse Genie utility can reduce execution time of your pipeline, there by reducing the overall costs. One can try and reduce the Spark pool node sizes to verify if the workload can be run on smaller cluster as all spark pool resources are available for the master genie notebook.\n\n## Roadmap \n\n - Provide Session isolation without code changes on the client side\n - Ability to trigger restart of a Genie instance run from a Datafactory\\Synapse pipeline\n - Simplify Onboarding to Genie to Support Bulk onboarding\n\n## Appendix\n\nFor more on [optimizing spark jobs on Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-performance#optimize-job-execution) \n\nRun [notebooks with threadpool](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#notebook-utilities)\n\nCode repository:\u00a0[Synapse Genie GitHub](https://github.com/microsoft/SynapseGenie)\n\nSteps to [Onboard Genie](https://github.com/microsoft/SynapseGenie/blob/main/Onboarding.md)\n", "repo_name": "SynapseGenie", "org_name": "microsoft", "org_repo": "microsoft/SynapseGenie", "platform_org_repo": "github+microsoft/SynapseGenie", "link_to_repo": "https://github.com/microsoft/SynapseGenie", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# ADX Automotive Demos\n\nWelcome to Azure Data Explorer Automotive demos for Test & Validation Fleets. This project contains sample code for the [Data analytics for automotive test fleets](https://learn.microsoft.com/en-us/azure/architecture/industries/automotive/automotive-telemetry-analytics) architecture.\n\nHere you can find two sub-projects:\n- [geospatial](geospatial/README.md) is a Node.js project that provides geospatial visualization capabilities.\n- [mdf42adx](mdf42adx/README.md) is a python script that prepares ASAM MDF-4 files for import into ADX.\n\n## Using the projects\n\nA easy way to get started is to use Visual Studio Code and WSL.\n\n- Install WSL2 in your Windows computer\n- Install Visual Studio Code\n- Check out the github repository\n- Open Visual Studio Code using the desired directory as argument, for example\n\n``` bash\ncode geospatial\ncode mdf42adx\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "adx-automotive-demos", "org_name": "microsoft", "org_repo": "microsoft/adx-automotive-demos", "platform_org_repo": "github+microsoft/adx-automotive-demos", "link_to_repo": "https://github.com/microsoft/adx-automotive-demos", "platform": "github", "language": "JavaScript", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo contains a samples showing usage of the new async model APIs to get billing and recon data.\n\n> For API details, please refer to the Partner Center [docs](https://learn.microsoft.com/en-us/partner-center/develop/get-invoice-billed-unbilled-consumption).\n\n## To run samples\n**Prerequisites**\n\n- You need to have Visual studio 2022 with .NET 6.0 installed.\n- Bearer token to access API. The new APIs accept the same authentication bearer token as existing APIs as mentioned in the [docs](https://learn.microsoft.com/en-us/partner-center/develop/partner-center-authentication). There is no change in the logic/values for generating token for new APIs.\n\n\nAfter downloading the file, open the solution samples\\Microsoft.Partner.Billing.V2.Demo.sln and update below values in program.cs based on your scenario.\n- accessToken: bearer token for authentication\n- invoiceid: invoiceid for which to get billed data. Invoices from September ownwards are only available. Example G012040490\n- downloadPath: local path where billing blobs should be downloaded. Example c:\\downloads\\\n- extractUsageFilesPath: local path where billing blobs after uncompression will be generated. Example c:\\downloads\\\n \n\n## Contributing\n\nPresently this project is not open for contributions.  For any suggestions kindly reach out through regular support \noptions for Partner Center.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Partner-Center-Billing-Recon", "org_name": "microsoft", "org_repo": "microsoft/Partner-Center-Billing-Recon", "platform_org_repo": "github+microsoft/Partner-Center-Billing-Recon", "link_to_repo": "https://github.com/microsoft/Partner-Center-Billing-Recon", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "\n# Conditional Access - Location based Data Protection in a Web app\nThe purpose of this section is to highlight a sample demonstration of the scenario. There may be other configurations/deployment scenarios. The configuration below is simply one illustration. \n\nThis repository has been created to document the sample demonstration deployment instructions for the scenario described in article [Location-based access control for FSI applications - Azure Architecture Center | Microsoft Learn](https://learn.microsoft.com/azure/architecture/example-scenario/financial/location-based-access)\n\n# Summary of the sample scenario:\n\nThe diagram below highlights at a high-level, the configuration steps A. and B. performed by the Administrator and Application Developer roles  respectively as well as the end-user steps 1.- 7. to demonstrate the scenario.\n\n   ![Image0](./readmefiles/Image00.png)\n\n**A.**\tAdministrator configures Conditional Access policy mapping authentication context to GPS Named Location (One-time activity) E.g. C1 = GPS CH (Switzerland)\n\n**B.**\tApplication Developers make their Apps Application context aware and can trigger step-up authentication requesting the required claims for actions from within their applications E.g. Action 1 requires Auth. Context claim acrs=C1\n\n**C.** End-User Steps:\n\n1.\tUser is already authenticated to App 1\n2.\tUser attempts to perform Action 1 withing App 1\n3.\tAction 1 requires Authentication Context Claim C1 (C1 = location Switzerland) to successfully execute. However since the current token doesn\u2019t have a claim C1 a claims challenge is triggered\n4.\tAzure AD executed policy CA policy 1 that maps to claim C1 to GPS location and requests location from the users Authenticator App.\n5.\tUser consents appropriately if it\u2019s the first time and provides location information\n6.\tIf the location information matches CH (Switzerland) a new access token is issued by Azure AD with claim C1\n7.\tCall comes back to App 1 with the new Access token and claim C1 and Action 1 is successfully executed\n\n# Deploy this Scenario\n\n## Pre-requisites\n\n1.\tDemo or Trial M365 Tenant\n2.\tSample Application and Visual Studio\n3.\t1 x Tenant Administrator account with appropriate admin rights in the M365 tenant\n4.\t1 x Test User account in with the appropriate AAD licenses assigned in the M365 Tenant\n5.\t1 x iOS/Android device supported by Microsoft Authenticator\n\n### Scenario Setup Steps\n\n_**Step 1: Configure Passwordless Phone-Sign with Authenticator**_\n\n> **Note**: Steps 1-3 are not a requirement for enforcing Location. However in order to onboard the user to Authenticator we have chosen the Authentication method Passwordless Phone-Sign-in with Microsoft Authenticator as the Passwordless Strong-Authentication method with Temporary Access Pass (TAP) as the onboarding option.\nTo enable the authentication method for passwordless phone sign-in, complete the following steps:\n\nTo enable the authentication method for passwordless phone sign-in, complete the following steps:\n1. Sign in to the Azure portal with an Authentication Policy Administrator account.\n2. Search for and select Azure Active Directory, then browse to **Security > Authentication methods > Policies**.\n3. Under **Microsoft Authenticator**, choose the following options:\n    - **Enable** - Yes\n    - **Target** - Select specific Group (Note: Test user must be member of that group)\n4. Each added group or user is enabled by default to use Microsoft Authenticator in both passwordless and push notification modes (\"Any\" mode). To change this, for each row:\n    - Browse to ... > **Configure**.\n    - For **Authentication mode** - choose **Any** \n5. To apply the new policy, click **Save**.\n\n    ![Image1](./readmefiles/Image1.png)\n\n6.\tUnder **Configure** Tab configure the following\n    1. Number matching enabled for Group\n    1. Show Application Name enabled for Group\n    1. Show geographic location disabled\n    1. To apply the new policy, click **Save.**\n\n        ![Image2](./readmefiles/Image22.png)\n\n_**Step 2: Configure Temporary Access Pass Settings**_\n\nAssuming users will not have passwords the only way to onboard to Passwordless Phone Sign-in is to use Temporary Access Pass (TAP). The TAP code is a one-time-code generated and communicated to the user.\n\nPlease follow the step at link below to Configure a Temporary Access Pass in Azure AD as shown in screenshot:\n> [Enable the temporary access pass policy](https://learn.microsoft.com/en-us/azure/active-directory/authentication/howto-authentication-temporary-access-pass#enable-the-temporary-access-pass-policy)\n\n![Image3](./readmefiles/Image3.png)\n\n_**Step 3: Onboard the test user to Temporary Access Pass**_\n\nTo Onboard the Test User to the Passwordless Phone-Sign-in solution using Temporary Access Pass.\n\nPlease logon as Tenant Adminstrator follow the steps in link below to create a new Temporary Access Pass (TAP) code for the Test User account. [Create a Temporary Access Pass](https://learn.microsoft.com/en-us/azure/active-directory/authentication/howto-authentication-temporary-access-pass#create-a-temporary-access-pass)\n\n\nThereafter use the Temporary Access Pass (TAP) code to onboard the user on to the Microsoft Authenticator App. To do this:\n\n1. Start the **Microsoft Autheticator App** and Add a new account by selecting **+** sign\n2. Choose **Work or school account** and in the \"Add work or school account\" dialog choose **Sign in**\n3. In the **Sign in** dialog enter the **UPN** of the Test User and select **Next**\n4. In the \"Enter Temporary access pass\" page enter the **Temporary Access pass (TAP) Code** that was generate earlier\n5. Choose **Continue** in the \"Sign-in with your phone\" dialog to perform your Device registration\n6. Select **Register** to Register your device\n7. Choose **Allow** in the \"Allow Notification\" dialog\n8. Select **Finish** \n\n\n_**Step 4: Configure GPS Named Locations in Azure AD**_\n\n1. Sign in to the **Azure portal** as a global administrator, security administrator, or Conditional Access administrator. \n2. Browse to **Azure Active Directory > Security > Conditional Access > Named locations.**\n3. Choose **New location.**\n4. Give your location a name. E.g. *GPS Switzerland*\n5. Choose **Select location by GPS coordinates** and choose the country E.g. **Switzerland** from the list by clicking the checkbox.\n6. Choose **Save**\n\n    ![Image4](./readmefiles/Image4.png)\n\n7. Repeat the above steps 3. \u2013 6. and create a few additional GPS country locations E.g. GPS UK, GPS Spain and GPS US\n\n\n\n_**Step 5: Configure Authentication Context Claims in Azure AD**_\n\n1. Sign in to the **Azure portal** as a global administrator, security administrator, or Conditional Access administrator.\n2. Authentication contexts are managed in the **Azure portal under Azure Active Directory > Security > Conditional Access > Authentication context**\n3. Create new authentication context definitions by selecting **New authentication context** in the Azure portal. Provide a Display name, Description, Publish to apps must be selected and the ID is a read-only value of the next available ID which is c1-c25 (At the time of writing the number of authentication context definitions was limited to 25)\n\n    ![Image5](./readmefiles/Image55.png)\n\n4. Repeat step 3. and create multiple authentication contexts that will be mapped to respective GPS locations\n\n\n_**Step 6: Configure Conditional Access Policy to map Claims to GPS Named Locations**_\n\nThis is where the Conditional Access policy maps the authentication context to the GPS location condition. \n\n1. Sign in to the **Azure portal** as a global administrator, security administrator, or Conditional Access administrator.\nAuthentication contexts are managed in the Azure portal under **Azure Active Directory > Security > Conditional Access > Policies**\n\n2.\tCreate a new Policy with the settings, under **Users or Workload Identities** select **All Users**\n\n3.\tAdministrators can select published authentication contexts in their Conditional Access policies under **Assignments > Cloud apps or actions** and selecting **Authentication context (preview)** from the **Select what this policy applies to** menu\n\n4.\tUnder **Select the Authentication Contexts this policy will apply to** Select the appropriate Authentication context we created earlier E.g. C1toCHMapping\n\n    ![Image6](./readmefiles/Image66.png)\n\n5.\tUnder **Conditions** configure the location condition **Include** All locations and **Exclude** the named GPS location **E.g. GPS Switzerland**\n\n    ![Image7](./readmefiles/Image7.png)\n\n6.\tUnder **Grant** select **Block access**\n\n7.\tChoose **Save**\n\n8.\tRepeat steps 2. \u2013 7. and create additional Conditional Access policies for E.g. GPS ES Authentication Context C2, GPS UK Authentication Context C3, GPS US Authentication Context C4\n\nAt the end of this step you will have created Policies mapping authentication context to named locations\n- C1 \u2013 GPS Switzerland\n- C2 \u2013 GPS Spain\n- C3 \u2013 GPS UK\n- C4 \u2013 GPS US\n\nFor the purpose of our demo we require the policies/mapping C1 \u2013 GPS Switzerland and C3 \u2013 GPS UK.\n\n_**Step 7: Deploy Sample Application**_\n\nDeploy the Sample ToDoList application as per the instructions documented at link below:\n\n>**Note:** You do not need to configure the Authentication Contexts and Policy rules described in the sample application instructions since you already performed these steps 4 \u2013 6 earlier.\n\n[Code Sample 1: Use the Conditional Access auth context to perform step-up authentication for high-privilege operations in a web app](https://github.com/Azure-Samples/ms-identity-dotnetcore-ca-auth-context-app/blob/main/README.md)\n\nOnce deployed, run the application as per the instructions\n\n_**Step 8: Configure Application to use Authentication Context Claims mapped Location**_\n\n>**Note:** For the purpose of the demo we are using a single Test User account to perform the Application configuration as well as the User testing in the next step. Typically the application configuration step is not exposed and should be pre-configured and automated as part of the Application deployment process.\n1.\tOnce the Sample Application is running, on the same machine Browse to https://localhost:44321 and sign-in with the Test User account \n2.\tSign-in using Passwordless Phone-Sign-in instead of Username/Password\n3.\tOnce logged on to the TodoListClient application Click **Admin** to create mappings between an Application Operations and Conditional Access policy. \n    >Note: This Sample application is exposing two Operation, POST and DELETE\n\n    -   Map the **POST** operation to Authentication Context **C1** requiring the user to be in location _**Switzerland**_ to successfully create a ToDoList item\n    - Map the **DELETE** operation to Authentication Context **C3** requiring the user to be in location _**UK**_ to successfully delete a ToDoList item\n\n    Once complete the mappings should show up as below:\n\n    ![Image8](./readmefiles/Image8.png)\n\n4. **Sign-out** from TodoListClient application\n\n_**Step 9: Validate Scenario with Sample Application**_\n\n>**Note:** The Test User location is Switzerland and we are using an iOS device for this demo\n\n1.\tBrowse to https://localhost:44321 and sign-in with the Test User account in your Tenant. Sign-in using Passwordless Phone-Sign-in using the Microsoft Authenticator App. using Number match + iOS (FaceID/TouchID)\n\n    ![Image9](./readmefiles/Image9.png)\n\n2. Once logged on to the TodoListClient application Click **TodoList** followed by **Create New** (To create a new item in the list) \n\n    ![Image10](./readmefiles/Image10.png)\n\n3. Fill in the ToDo item form and click **Create**\n\n    ![Image11](./readmefiles/Image11.png)\n\n4. Since this operation requires the Conditional access Authentication context claim challenge C1 the corresponding CA rule will be executed by Azure AD \n\n    -  The user will be shown the **location** requirement window with **number match** screen on the Browser\n    - In parallel the user will be **notified on the Microsoft Authenticator**\n\n        ![Image12](./readmefiles/Image12.png)\n\n    - The user must enter the **matching number** as well as perform iOS (FaceID/TouchID) after which the **location information** (Country code) is sent by Microsoft Authenticator to Azure AD.\n    - Azure AD issues a **new token** with appropriate Authentication Context claim **C1**\n    - Call comes back to the **ToDoListClient application** with the **new token** enabling the POST operation to complete thereby successfully creating the **new ToDoList item**\n\n5.\tThe final validation step is to attempt to delete an item from the TodoList. Click TodoList and choose an item to delete. This will load the Delete ToDo page with item displayed\n\n    ![Image13](./readmefiles/Image13.png)\n\n\n6. Click on the **Delete** button to confirm deletion.\n\n    - Since our Test User is in Switzerland and we already have the location information based on the authentication context claim in the token from earlier step 4., the **Delete operation will not be allowed.** \n    - The following message will be displayed by the application\n\n        ![Image14](./readmefiles/Image1414.png)\n\n## Contributors\n*This article is maintained by Microsoft. It was originally written by the following contributors.* \n\nPrincipal authors:\n - [Kunal Kodkani](https://www.linkedin.com/in/kunalkodkani) | Senior Program Manager - Cloud for Industry (FSI)\n - [Caleb Baker](https://www.linkedin.com/in/baker-caleb) | Principal Product Manager - Identity Engineering\n - [Paresh Nhathalal](https://www.linkedin.com/in/paresh-nhathalal-72613b2) | Senior Customer Engineering Manager - Identity Engineering\n\n \n*To see non-public LinkedIn profiles, sign in to LinkedIn.*\n\n\n", "repo_name": "ms-identity-conditional-access-for-compliance", "org_name": "microsoft", "org_repo": "microsoft/ms-identity-conditional-access-for-compliance", "platform_org_repo": "github+microsoft/ms-identity-conditional-access-for-compliance", "link_to_repo": "https://github.com/microsoft/ms-identity-conditional-access-for-compliance", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Microsoft Graph documentation\n\nThank you for your interest in Microsoft Graph documentation! For the best experience, we recommend you view this content on the [Microsoft Graph Developer Portal](https://learn.microsoft.com/graph).\n\n## Give us your feedback\n\nYour feedback is important to us.\n\n- To let us know about any questions or issues you find in the documentation, [submit an issue](https://github.com/microsoftgraph/microsoft-graph-docs/issues) in this repository.\n- We also encourage you to fork, make the fix, and do a pull request of your proposed changes. See [CONTRIBUTING.md](CONTRIBUTING.md) for contributing guidelines.\n- To let us know about your programming experience, what you would like to see in future versions, code samples, and so on, enter your suggestions and ideas at [Microsoft 365 Developer Platform Tech Community](https://techcommunity.microsoft.com/t5/microsoft-365-developer-platform/idb-p/Microsoft365DeveloperPlatform/label-name/Microsoft%20Graph).\n\n## Code of conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "microsoft-graph-docs-1", "org_name": "microsoft", "org_repo": "microsoft/microsoft-graph-docs-1", "platform_org_repo": "github+microsoft/microsoft-graph-docs-1", "link_to_repo": "https://github.com/microsoft/microsoft-graph-docs-1", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Project Overview\nAn application to assist healthcare users in forming new patient records with both accuracy and efficiency.\n\n## Healthcare Use Cases\nWe built this project over two different phases, each representing two different possible operations of health care data.\n\n### Medical Intake\nPatient presents medical insurance card at the doctor's office. An image of the insurance card is uploaded to the system and sent to the OCR service to extract patient biographic information as well as insurance coverage information.\n\nThe information extracted by the OCR is presented on a form for a intake specialist to verify, correct, and then submit to the EMR system using the FHIR API exposed by the Azure Health Data Service.\n\n### Discharge Summary Analysis\nFor a given patient, we obtain a discharge summary and extract relevant medical information. Using a custom web application, we submit the contents of the discharge summary to the Text Analytics for Healthcare service.\n\nUpon successful processing, we extract the entities identified as well as the FHIR bundle. The extracted entities are then displayed on a web page on a custom web\napplication  to conduct quick spot/quality check. Once the data is validated, the extracted FHIR bundle is submitted to the Azure Health Data Services for saving into the EMR using the Azure Health Data Service FHIR API.\n\n# Power Platform Version\n\n## Prerequisites\n\n### Azure API for FHIR\nThe healthcare industry is rapidly transforming health data to the emerging standard of FHIR\u00ae (Fast Healthcare Interoperability Resources). FHIR enables a robust, extensible data model with standardized semantics and data exchange that enables all systems using FHIR to work together. FHIR also enables the rapid exchange of data in applications. Backed by a managed PaaS [Azure API for FHIR](https://docs.microsoft.com/en-us/azure/healthcare-apis/overview) offering, FHIR also provides a scalable and secure environment for the management and storage of Protected Health Information (PHI) data in the native FHIR format.\n\n- Deploy Azure API for FHIR with First Party Auth by [deploying via the Azure Portal](https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir-paas-portal-quickstart). \n- To access the Azure API for FHIR, apply RBAC by adding users to `FHIR Data Reader` or `FHIR Data Contributor` role using Access Control (IAM). The users with access will authenticate through the Connector which assumes their role.\n- Load sample data into Azure API for FHIR. \n    - Sample project to load data into FHIR can be found [here](https://github.com/microsoft/FHIRPower/tree/main/FHIR-Seed-Data)\n    - Detailed instructions to load synthetic data can be found in the [OpenHack-FHIR GitHub](https://github.com/microsoft/OpenHack-FHIR/tree/main/Challenge01-AzureAPIforFHIR#task-2-generate--load-synthetic-data)\n- See the following link for more details on [Using Azure API for FHIR](https://github.com/microsoft/OpenHack-FHIR)\n\n### Certified Connector\n- Get access to [Power Platform](https://docs.microsoft.com/en-us/power-platform/) environment to create Power Apps.\n- Certified Connectors [FHIRBase](https://docs.microsoft.com/en-us/connectors/fhirbase/) and [FHIRClinical](\nhttps://docs.microsoft.com/en-us/connectors/fhirclinical/) need to be allowed in your Power Platform Environment DLP rules by an Environment Administrator.\n- Details on [FHIRBase Actions](https://docs.microsoft.com/en-us/connectors/fhirbase/#actions) and [FHIRClinical Actions](https://docs.microsoft.com/en-us/connectors/fhirclinical/#actions) for API calls.\n- Mapping [FHIR Base and Clinical Resources](https://www.hl7.org/fhir/resourcelist.html) to FHIRBase and FHIRClinical connectors.\n- More details on [Power Apps](https://docs.microsoft.com/en-us/powerapps/)\n\n### Custom Connector\n- Instructions to create your custom connector, can be found [here][(./CUSTOM_CONNECTOR.md) ](https://github.com/microsoft/FHIRPower/blob/main/CUSTOM_CONNECTOR.md)\n- Sample custom connector can be found [here][(./SampleFHIRCustomConnector)](https://github.com/microsoft/FHIRPower/tree/main/SampleFHIRCustomConnector)\n\n# Pro dev\n\n## Solution Architecture Overview\nWe built a custom web application using Blazor to handle the application flow. The application manages the two use-cases mentioned above, the medical intake as well as the discharge summary analysis.\n\n\nThe application will be deployed as a static website for performance purposes. We want to improve the performance, while also lowering the deployment cost.\nSince we are using a static website for production deployment, we\u2019ll need to use the SWA CLI for local development. When you deploy a static website on Azure, Azure provides the backend support required to run the application. This includes wiring up support of Authentication/Authorization as well as access to Azure Functions. In order to support local development, this infrastructure is not available, which is what the SWA CLI provides.\n\n![Architecture](./Fast-Pass-Architecture-resized.png)\n\n## Prerequisites\nMost recent version of Azure Functions CLI, Static Web Apps CLI, and at least DOTNET version 7\n\n## Local Development Setup Guide\n1. Clone Project from Git\n2. Expand FastPass.API and add a document called local.setting.json\n3. Initialize the following properties in the new file:\n```\n    {\n    \"IsEncrypted\": false,\n    \"Host\": {\n    \"CORS\": \"*\"  },\n     \"Values\": {\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet-isolated\",\n    \"IsDevelopment\": \"true\",\n    \"APIConfig:TextAnalyticsBase\": \"\",\n    \"APIConfig:TextAnalyticsKey\": \"\",\n    \"APIConfig:FhirScope\": \"\",\n    \"APIConfig:FhirServerUri\": \"\",\n    \"APIConfig:Authority\": \"\",\n    \"APIConfig:TenantId\": \"\",\n    \"APIConfig:ClientId\": \"\",\n    \"APIConfig:ClientSecret\": \"\"  }\n    }\n```\n4. The <ins>Base</ins> and <ins>Key</ins> can be found in the resource group containing your Azure Language service\n5. The <ins>Uri</ins> can be found in the resource group containing your Azure API for FHIR service and the <ins>Scope</ins> will be the default so it should look like the following: \n    `\"your uri\"/.default`\n6. The <ins>Tenant Id</ins> can be found in the Tenant Properties of your AAD subscription\n7. The <ins>ClienntId</ins> and <ins>ClientSecret</ins> can be found in your Azure Static Web Page resource\n8. Right click on the solution file and select \"startup project\" and select the \"multiple files option\". You will select \"start\" for both the API and UI files\n9. After building and running you should run the following command in a separate instance:\n    `swa start http://localhost:5043 --api-location http://localhost:7214`\n10. With the debugger open on your local host go to the local host port indicated by the emulator. For example:\n    `Azure Static Web Apps emulator started at http://localhost:4280. Press CTRL+C to exit.`\n11. Log in to the version of the app residing at the port indicated by the emulator using one of the given options\n12. Begin submitting text. For example: \n    ```\n    Patient:\n    Patient H Sample\n\n    Provider's Pt ID:\n    6910828\n    \n    Sex:\n    Female\n    ```\n\n## Provisioning and Deploying to Azure\nThe fast-pass pro dev version is implemented in a Blazor static web app and corresponding C# managed API. The sections below will help you get your Azure resources provisioned and the code deployed.\n\n### Provisioning Azure Resources\nA [bicep file](./deploy/main.bicep) is available for creating the Azure resources. Bicep is a domain specific language built on top of the Azure Resource Manager (ARM) APIs. To read more, see [What is Bicep?](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview?tabs=bicep).\n\n### Bicep Template Description\nThe template will provision three resources:\n1. A Cognitive Services - Text Analytics service\n    - <b>Note:</b> The acceptance of a 'Responsible AI' terms is required at least once in your subscription before the creation of a cognitive services resources can be automated. At the present time, there is no way to do this in an automated way. You can either pre-create the one that the script will manage (fast-pass-lang), or create and delete another. Upon creation of that resource in the portal, you will be asked to accept the Responsible AI terms. See [here](https://learn.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account-cli?tabs=windows#prerequisites) for more information.\n2. Application Insights\n3. A Static Web App \n    - This will also contain the managed API\n    - The Application settings will be updated to reference App Insights, Cognitive services and your other passed in parameters\n\nSee the [bicep file](./deploy/main.bicep) for any additional parameters you'd like to override. Including linking it to a GitHub repo.\n\nYou have two options to provision fast-pass resources; have the Bicep script take care of the App Registration or do the App Registration outside of Bicep and pass in the results. [Bicep doesn't yet handle App Registrations natively](https://learn.microsoft.com/en-us/answers/questions/1058054/support-for-creating-aad-app-registration-using-bi.html), but it can be done by invoking a PowerShell script from Bicep. This it is more involved, so it may be more straight forward to handle that outside the script. Both approaches are documented below.\n\n### App Registration Outside of Bicep Template\nThis is the default approach (manageAppRegistration=false). Be sure to login, then run the script:\nAzure CLI...\n```\naz login\naz deployment group create \\\n    --resource-group [[resource-group-name]] \\\n    --template-file main.bicep \\\n    --parameters \\\n        fhirServer='[[FHIRServerURL]]' \\\n        appRegistrationAuthority='[[AuthorityOfAppRegistration]]' \\\n        appRegistrationTenantId='[[TenantIdOfAppRegistration]]' \\\n        appRegistrationClientId='[[ClientIdOfAppRegistration]]' \\\n        appRegistrationClientSecret='[[ClientSecretOfAppRegistration]]'\n```\n\nPowerShell (Tested in v7.3)...\n```\nConnect-AzAccount\nNew-AzResourceGroupDeployment \n    -ResourceGroupName [[resource-group-name]] \n    -TemplateFile main.bicep \n    -fhirServer [[FHIRServerURL]] \n    -appRegistrationAuthority [[AuthorityOfAppRegistration]] \n    -appRegistrationTenantId [[TenantIdOfAppRegistration]] \n    -appRegistrationClientId [[ClientIdOfAppRegistration]] \n    -appRegistrationClientSecret [[ClientSecretOfAppRegistration]]\n```\n\n### App Registration via the Bicep Template\nThis approach (manageAppRegistration=true). Requires a managed identity to be able to create the App Registration. Follow the process [here](https://reginbald.medium.com/creating-app-registration-with-arm-bicep-b1d48a287abb) - don't skip the step where you are assigning the Application Administrator role to the identity.\n\nAzure CLI...\n```\naz login\naz deployment group create \\\n    --resource-group [[resource-group-name]] \\\n    --template-file main.bicep \\\n    --parameters \\\n        fhirServer='[[FHIRServerURL]]' \\\n        manageAppRegistration=true \\\n        appRegistrationUser='[[NameOfManagedIdentityWithAppAdminRights]]' \n```\n\nPowerShell (Tested in v7.3)...\n```\nConnect-AzAccount\nNew-AzResourceGroupDeployment \n    -ResourceGroupName [[resource-group-name]] \n    -TemplateFile main.bicep \n    -fhirServer [[FHIRServerURL]] \n    -manageAppRegistration \n    -appRegistrationUser [[NameOfManagedIdentityWithAppAdminRights]] \n```\n\n### Deploying Code From GitHub\nThere's already GitHub action in the repo: [here](./.github/workflows/azure-static-web-apps-brave-mushroom-056103510.yml). The only adjustment needed is to add the Deployment Token from the Static Web App and save it as a GitHub secret.\n- In the Azure Portal, go to the Static Web App resource that was created\n    - Click <ins>Manage deployment token</ins> > Copy to clipboard\n- Go to your GitHub repository where you've forked / cloned this repo\n    - Settings > Secrets > Actions > New repository secret\n    - Name the secret - make sure it is the same as what's referenced two places in the [GitHub action](./.github/workflows/azure-static-web-apps-brave-mushroom-056103510.yml)\n    - Paste the value that you copied above \n    - Click Add secret\n- Test the deployment by merging a change to the main branch.\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fast-pass", "org_name": "microsoft", "org_repo": "microsoft/fast-pass", "platform_org_repo": "github+microsoft/fast-pass", "link_to_repo": "https://github.com/microsoft/fast-pass", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# CLAP\n\nCLAP (Contrastive Language-Audio Pretraining) is a neural network model that learns acoustic concepts from natural language supervision. It achieves SoTA in \u201cZero-Shot\u201d classification, Audio-Text & Text-Audio Retrieval, and in some datasets when finetuned.\n\n<img width=\"832\" alt=\"clap_diagram_v3\" src=\"https://user-images.githubusercontent.com/26778834/199842089-39ef6a2e-8abb-4338-bdfe-680abab70f53.png\">\n\n## Setup\n\nYou are required to just install the dependencies: `pip install -r requirements.txt` using Python 3 to get started.\n\nIf you have [conda](https://www.anaconda.com) installed, you can run the following: \n\n```shell\ngit clone https://github.com/microsoft/CLAP.git && \\\ncd CLAP && \\\nconda create -n clap python=3.8 && \\\nconda activate clap && \\\npip install -r requirements.txt\n```\n\n## CLAP weights\nRequest CLAP weights: [Pretrained Model \\[Zenodo\\]](https://zenodo.org/record/7312125#.Y22vecvMIQ9)\n\n\n## Usage\n\nPlease take a look at `src/examples` for usage examples. \n\n- Load model\n```python\nfrom src import CLAP \n\nclap_model = CLAP(\"<PATH TO WEIGHTS>\", use_cuda=False)\n```\n\n- Extract text embeddings\n```python\ntext_embeddings = clap_model.get_text_embeddings(class_labels: List[str])\n```\n\n- Extract audio embeddings\n```python\naudio_embeddings = clap_model.get_audio_embeddings(file_paths: List[str])\n```\n\n- Compute similarity \n```python\nsim = clap_model.compute_similarity(audio_embeddings, text_embeddings)\n```\n\n## Examples\nTo run zero-shot evaluation on the ESC50 dataset or a single audio file from ESC50, check `CLAP\\src\\`. For zero-shot evaluation on the ESC50 dataset:\n```bash\n> cd src && python zero_shot_classification.py\n```\nOutput\n```bash\nESC50 Accuracy: 82.6%\n```\n\n## Citation\nhttps://arxiv.org/pdf/2206.04769.pdf\n```\n@article{elizalde2022clap,\n  title={Clap: Learning audio concepts from natural language supervision},\n  author={Elizalde, Benjamin and Deshmukh, Soham and Ismail, Mahmoud Al and Wang, Huaming},\n  journal={arXiv preprint arXiv:2206.04769},\n  year={2022}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CLAP", "org_name": "microsoft", "org_repo": "microsoft/CLAP", "platform_org_repo": "github+microsoft/CLAP", "link_to_repo": "https://github.com/microsoft/CLAP", "platform": "github", "language": "Python", "stargazers_count": 148, "watchers_count": 148}, {"README_text": "# Contributor Concept\n\nThis project implements the contributor design pattern explained in my medium article on\n[Scaling Teams Mobile Development \u2014 Evolving the design pattern](https://medium.com/microsoft-mobile-engineering/scaling-teams-mobile-development-evolving-the-design-pattern-c3c8ff53facb)\n\n### contributor-api\nThis module contains the contributor contracts\n\n### manager\nThis module contains the manager which collects contributors and contributions, orders them and provides to the main app.\n\n### contributors\nThis folder contains contributor implementations.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "contributor-design-pattern-sample", "org_name": "microsoft", "org_repo": "microsoft/contributor-design-pattern-sample", "platform_org_repo": "github+microsoft/contributor-design-pattern-sample", "link_to_repo": "https://github.com/microsoft/contributor-design-pattern-sample", "platform": "github", "language": "Kotlin", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# <img src=\"./website/static/img/vc-logo.svg\" width=\"50\"> Virtual Client\n\n\n[![Pull Request Build](https://github.com/microsoft/VirtualClient/actions/workflows/pull-request.yml/badge.svg)](https://github.com/microsoft/VirtualClient/actions/workflows/pull-request.yml)\n[![Document Build](https://github.com/microsoft/VirtualClient/actions/workflows/deploy-doc.yml/badge.svg?branch=main)](https://github.com/microsoft/VirtualClient/actions/workflows/deploy-doc.yml)\n[![Document Deployment](https://github.com/microsoft/VirtualClient/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/VirtualClient/actions/workflows/pages/pages-build-deployment)\n[![NuGet Release Status](https://msazure.visualstudio.com/One/_apis/build/status/OneBranch/CRC-AIR-Workloads/microsoft.VirtualClient?branchName=main)](https://msazure.visualstudio.com/One/_build/latest?definitionId=297462&branchName=main)\n\n------\n\nThe following links provide additional information on the Virtual Client project.\n\n* [Overview](https://microsoft.github.io/VirtualClient/docs/overview/)\n* [Getting Started + How to Build](https://microsoft.github.io/VirtualClient/docs/guides/0001-getting-started.md)\n\n## [Getting Started](https://microsoft.github.io/VirtualClient/docs/guides/getting-started/)\n\nYou will follow the [**Tutorial**](https://microsoft.github.io/VirtualClient/docs/guides/getting-started/) to benchmark your system with a quick workload: OpenSSL Speed - SHA256.\n\n## Contributing\n\nWe welcome your contribution, and there are many ways to contribute to VirtualClient:\n\n* [Just say Hi](https://github.com/microsoft/VirtualClient/discussions/categories/show-and-tell). It inspires us to know that there are fellow performance enthusiatics out there and VirtualClient made your work a little easier.\n* [Feature Requests](https://github.com/microsoft/VirtualClient/issues/new/choose). It helps us to know what benchmarks people are using.\n* [Submit bugs](https://github.com/microsoft/VirtualClient/issues/new/choose). We apologize for the bug and we will investigate it ASAP.\n* Review [source code changes](https://github.com/microsoft/VirtualClient/pulls). You likely know more about one workload than us. Tell us your insights.\n* Review the [documentation](https://github.com/microsoft/VirtualClient/tree/main/website/docs) and make pull requests for anything from typos to new content.\n* We welcome you to directly work in the codebase. Please take a look at our [CONTRIBUTING.md](./CONTRIBUTING.md). [Start here](https://microsoft.github.io/VirtualClient/docs/category/developing/) and contact us if you have any questions.\n\nThank you and we look forward to your contribution.\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Telemetry Notice\nData Collection. \n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft\u2019s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n#### VirtualClient does not collect your data by default\nVirtualClient does not collect any of your benchmark data and upload to Microsoft. When benchmarking at scale, and leveraging VC's telemetry capabilities, users need to explicitly provide a connection string, that points to a user-owned Azure Data Explorer cluster. VirtualClient does host a Azure storage account to host the benchmark binaries or source. The only information VirtualClient team could infer from usage, is the download traces from Azure storage account.\n\n#### About benchmark examples in source\nVirtualClient has example benchmark outputs in source, for unit-testing purpose, to make sure our parsers work correctly.\nThose runs might or might not be ran on Azure VMs. The results have also been randomly scrubbed. These examples do not represent Azure VM performance. They are in the source purely for unit testing purposes.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "VirtualClient", "org_name": "microsoft", "org_repo": "microsoft/VirtualClient", "platform_org_repo": "github+microsoft/VirtualClient", "link_to_repo": "https://github.com/microsoft/VirtualClient", "platform": "github", "language": "C#", "stargazers_count": 34, "watchers_count": 34}, {"README_text": "# Shared Npm packages\n\nThese packages are only meant for use by [in the box tasks](https://github.com/microsoft/azure-pipelines-tasks) - contributions will only be accepted for those purposes.\n\nPublishing should happen automatically on CI builds, for any issues with this process please reach out to @tkasparek/@martinmrazik and/or the Azure Pipelines Platform team.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-pipelines-tasks-common-packages", "org_name": "microsoft", "org_repo": "microsoft/azure-pipelines-tasks-common-packages", "platform_org_repo": "github+microsoft/azure-pipelines-tasks-common-packages", "link_to_repo": "https://github.com/microsoft/azure-pipelines-tasks-common-packages", "platform": "github", "language": "TypeScript", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "## Low-Resource MT Compression\n\n#### Official Code for \"Too Brittle To Touch: Comparing the Stability of Quantization and Distillation Towards Developing Lightweight Low-Resource MT Models\" (To Appear in WMT 2022)\n\nLeveraging shared learning through Massively Multilingual Models, state-of-the-art machine translation models are often able to adapt to the paucity of data for low-resource languages. However, this performance comes at the cost of significantly bloated models which are not practically deployable. In this work, we evaluate knowledge distillation's use to compress MT models focusing on languages with extremely limited training data. Through our analysis across 8 languages, we find that the variance in the performance of the distilled models due to their dependence on multiple priors makes distillation a brittle compression mechanism. We further explore the use of post-training quantization for the compression of these models. Here, we find that quantization provides more consistent performance trends (than distillation) for the entire range of languages, especially the lowest-resource languages in our target set.\n\n#### Languages Covered and Data Sources \n\nWe cover 8 languages of diverse linguistic origins, varying data between 7K samples to 3M samples for our study. The train-test splits for Gondi and Mundari will be released soon and testsets for all other languages are publicly available (listed in the paper). \n\n| **Language** | **Train Data (Sentence Pairs)** | **Links**                                                |\n|--------------|---------------------------------|-----------                                               |\n| Bribri       | ~7000                           | [Here](https://github.com/AmericasNLP/americasnlp2021)   |\n| Wixarica     | ~8000                           | [Here](https://github.com/AmericasNLP/americasnlp2021)   |\n| Mundari      | ~11000                          | Public Link Available Soon                               |\n| Gondi        | ~25000                          | [Here](http://cgnetswara.org/hindi-gondi-corpus.html)    |   \n| Assammesse   | ~135000                         | [Here](https://ai4bharat.iitm.ac.in/samanantar)          |\n| Odia         | ~1M                             | [Here](https://ai4bharat.iitm.ac.in/samanantar)          |\n| Punjabi      | ~2.4M                           | [Here](https://ai4bharat.iitm.ac.in/samanantar)          |\n| Gujarati     | ~3M                             | [Here](https://ai4bharat.iitm.ac.in/samanantar)          |\n\n#### Model Benchmarks - Compressed Variants \nEach of the quantized variants is at least 3x smaller than it's best performing model and the distilled variants are at least 6x smaller.  Models and their compressed variants (for plug-and-play usage) coming soon! \n\n| **Language** | **Best Uncompressed Variant** | **Best Distilled Variant** |           | **Best Quantized Variant** |           |\n|--------------|-------------------------------|----------------------------| --------- |----------------------------|-----------|\n|              |           **spBLEU**          |         **spBLEU**         | **chrF2** |         **spBLEU**         | **chrF2** |\n| _Bribri_     |              6.4              |             6.8            |    13.2   |             7.4            |    19.4   |\n| _Wixarica_   |              6.2              |             4.1            |    17.3   |             7.2            |    26.8   |\n| _Mundari_    |              15.9             |            18.2            |    32.7   |            15.7            |    29.3   |\n| _Gondi_      |              14.3             |            14.2            |    32.8   |            13.8            |    31.1   |\n| _Assamesse_  |              10.7             |             9.6            |    27.4   |             6.2            |    25.7   |\n| _Odia_       |              27.4             |            20.2            |    40.7   |            21.0            |    41.3   |\n| _Punjabi_    |              38.4             |            32.8            |    46.6   |            27.0            |    48.0   |\n| _Gujarati_   |              35.9             |            29.8            |    48.6   |            28.4            |    51.4   |\n----------------------------------------------------------------------------------------------------------------------------------\n\n#### Environment Information \nThe environment can be setup using the provided requirements file (Requires pip > pip 22.0.2)\n```\npip install -r requirements.txt \n```\n\n#### Directory Structure\n```\n\u251c\u2500\u2500 readme.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 scripts                            # Scripts with all the variants of the commands + default hyperparameter values\n\u2502   \u251c\u2500\u2500 confidence_estimation.sh       # logging the confidence statistics\n\u2502   \u251c\u2500\u2500 inference.sh                   # inference for both architectures - online and offline graphs \n\u2502   \u251c\u2500\u2500 preprocess.sh                  # preprocessing data for training and evaluation\n\u2502   \u251c\u2500\u2500 sweep.yaml                     # sweep yaml for hyperparameter trials \n\u2502   \u2514\u2500\u2500 train.sh                       # variants of training and continued pretraining\n\u2514\u2500\u2500 src                                # src files for all the experiments \n    \u251c\u2500\u2500 confidence_estimation.py       # logging confidence stats: average softmax entropy, standard deviation of log probabilities\n    \u251c\u2500\u2500 continued_pretraining.py       # continued pretraining of mt5\n    \u251c\u2500\u2500 inference.py                   # online and graph inference\n    \u251c\u2500\u2500 preprocess.py                  # preprocessing bilingual and monolingual data + vocab and tokenizer creation \n    \u251c\u2500\u2500 split_saving.py                # generating the offline graphs for both model architectures \n    \u251c\u2500\u2500 student_labels.py              # generating the student labels for the best model architecture for the models   \n    \u251c\u2500\u2500 train.py                       # training script for vanilla, distilled and pretrained model configuration\n    \u2514\u2500\u2500 utils.py                       # utils like script conversion, checking for deduplication\n```\n\n#### Training Procedure \n```\n1. Run **preprocess.py** to convert training data to HF format and generating the Tokenizer Files for the Vanilla tranformer. \n2. Run **train.py** for training and saving the best model. (monitored metric is BLEU with mt13eval tokenizer)\n3. Run **split_saving_{model_architecture_type}.py** to quantize the encoder and decoder separately. \n4. Run **inference.py** (with offline = True) for offline inference on the quantized graphs.  \n\nSample commands with default hyperparameter values are specified in scripts/\n```\n\n\n#### Evaluation Signature: BLEU and chrF\n```\n{\n \"nrefs:1|case:mixed|eff:no|tok:spm-flores|smooth:exp|version:2.2.0\",\n \"verbose_score\":,\n \"nrefs\": \"1\",\n \"case\": \"mixed\",\n \"eff\": \"no\",\n \"tok\": \"spm-flores\",\n \"smooth\": \"exp\",\n \"version\": \"2.2.0\"\n}\n{\n \"name\": \"chrF2\",\n \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n \"nrefs\": \"1\",\n \"case\": \"mixed\",\n \"eff\": \"yes\",\n \"nc\": \"6\",\n \"nw\": \"0\",\n \"space\": \"no\",\n \"version\": \"2.2.0\"\n}\n```\n\n#### Datasets Used \n - [Wixarika]\n    Mager, M., Carrillo, D., & Meza, I. (2018). Probabilistic finite-state morphological sgmenter for wixarika (huichol) language. Journal of Intelligent & Fuzzy Systems, 34(5), 3081-3087.\n - [Bribri]\n    Feldman, I., & Coto-Solano, R. (2020, December). [Neural Machine Translation Models with Back-Translation for the Extremely Low-Resource Indigenous Language Bribri](https://www.aclweb.org/anthology/2020.coling-main.351.pdf). In Proceedings of the 28th International Conference on Computational Linguistics (pp. 3965-3976).\n - [Mundari]: Data to be released soon. \n - [Odia] [Punjabi] [Gujarati] and [Assamesse]: @article{10.1162/tacl_a_00452,\n    author = {Ramesh, Gowtham and Doddapaneni, Sumanth and Bheemaraj, Aravinth and Jobanputra, Mayank and AK, Raghavan and Sharma, Ajitesh and Sahoo, Sujit and Diddee, Harshita and J, Mahalakshmi and Kakwani, Divyanshu and Kumar, Navneet and Pradeep, Aswin and Nagaraj, Srihari and Deepak, Kumar and Raghavan, Vivek and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh Shantadevi},\n    title = \"{Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages}\",\n    journal = {Transactions of the Association for Computational Linguistics},\n    volume = {10},\n    pages = {145-162},\n    year = {2022},\n    month = {02},\n    issn = {2307-387X},\n    doi = {10.1162/tacl_a_00452},\n    url = {https://doi.org/10.1162/tacl\\_a\\_00452},\n    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00452/1987010/tacl\\_a\\_00452.pdf},\n}\n\n\n#### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n#### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Lightweight-Low-Resource-NMT", "org_name": "microsoft", "org_repo": "microsoft/Lightweight-Low-Resource-NMT", "platform_org_repo": "github+microsoft/Lightweight-Low-Resource-NMT", "link_to_repo": "https://github.com/microsoft/Lightweight-Low-Resource-NMT", "platform": "github", "language": "Python", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "<p align=\"center\"><img width=\"70%\" src=\"docs/images/caspr-logo.png\" /></p>\r\n\r\n<!-- # AI.Models.CASPR -->\r\n**CASPR is a transformer-based framework for deep learning from sequential data in tabular format, most common in business applications.**\r\n\r\n<p align=\"justify\">\r\nTasks critical to enterprise profitability, such as customer churn prediction, fraudulent account detection or customer lifetime value estimation, are often tackled by models trained on features engineered from customer data in tabular format. Application-specific feature engineering however adds development, operationalization and maintenance costs over time. Recent advances in representation learning present an opportunity to simplify and generalize feature engineering across applications.\r\n\r\nWith **CASPR**  we propose a novel approach to encode sequential data in tabular format (e.g., customer transactions, purchase history and other interactions) into a generic representation of a subject's (e.g., customer's) association with the business. We evaluate these embeddings as features to train multiple models spanning a variety of applications (see: [paper](https://arxiv.org/abs/2211.09174)). CASPR, Customer Activity Sequence-based Prediction and Representation, applies transformer architecture to encode activity sequences to improve model performance and avoid bespoke feature engineering across applications. Our experiments at scale validate CASPR for both small and large enterprise applications. \r\n</p>\r\n\r\n<!-- - **Representation**      (TODO: in 2 sentences WHY and HOW on CASPR embeddings, RFM)\r\n\r\n- **Pre-Training**        (TODO: few words on self-supervised training, platforms supported, pointers to modules)\r\n\r\n- **Inference**           (TODO: few words on inference at scale, platforms supported, pointers to modules) -->\r\n\r\n## Getting Started & Resources\r\n\r\n* **CASPR: Customer Activity Sequence-based Prediction and Representation** (NeurIPS 2022, New Orleans: Tabular Representation Learning)\r\n   - [paper](https://arxiv.org/abs/2211.09174)\r\n   - [poster](https://github.com/microsoft/CASPR/docs/images/caspr-poster.png)\r\n\r\n* **Build**\r\n\r\n   - pre-requisites:  ```python==3.9, setuptools```\r\n   - building the wheel:  ```python setup.py build bdist_wheel```\r\n\r\n* **Installation**\r\n\r\n   ```\r\n   (now)\r\n   pip install .\\dist\\AI.Models.CASPR-<ver>.whl[<optional-env-modifier>]\r\n\r\n   (future)\r\n   pip install AI.Models.CASPR[<optional-env-modifier>]\r\n   ```\r\n\r\n   use any of below modifiers, to customize the installation for target system / usecase:\r\n   ```\r\n    horovod     - for distributed training and inference on Horovod\r\n    databricks  - for distributed training and inference on Databricks\r\n    aml         - for (distributed) training and inference on Azure ML\r\n    hdi         - for execution on Azure HD Insights\r\n    xai         - to enable explainability\r\n    test        - for extended test execution\r\n    dev         - for development purposes only\r\n   ```\r\n* **Examples**\r\n  \r\n   (TODO: can we point to a well commented one of our examples w/ or w/o data?)\r\n\r\n## Contributions and Feedback\r\n\r\nWe welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).\r\n\r\nFor feature requests or bug reports please file a [GitHub Issue](https://github.com/Microsoft/CASPR/issues).\r\n\r\n## Code of Conduct\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\r\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## License\r\n\r\nThis project is licensed under the [MIT License](LICENSE).\r\n\r\n---", "repo_name": "CASPR", "org_name": "microsoft", "org_repo": "microsoft/CASPR", "platform_org_repo": "github+microsoft/CASPR", "link_to_repo": "https://github.com/microsoft/CASPR", "platform": "github", "language": "Python", "stargazers_count": 27, "watchers_count": 27}, {"README_text": "# Microsoft Q&A\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-3-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n![Microsoft Q&A banner](./media/web-banner-header.png)\n\nHave you ever searched online for a solution to a tech problem that you have, and wished that you could have easily gotten the answer from a short video instead of having to comb through countless pages?\n\n**We feel the same, and YOU can now make a difference.**\n\n## Call for Content Creators\n\nShort sizzle reels or trailers addressing frequently asked questions on Microsoft Q&A played can help our community learn better. There is a new mantra in learning and that's \"show, don't tell\".\n\n\ud83d\udc49 We are inviting you to partner with us to create a no-longer than 2 minute content to address a frequently asked question found on any of our Microsoft forums like [Microsoft Q&A](https://aka.ms/MicrosoftQuestionsandAnswers), and [Power Platform Community](https://powerusers.microsoft.com).\n\n## Creating your content\n\nBefore you start, here's what you need to note. \n\n1. Visit [Microsoft Q&A](https://aka.ms/MicrosoftQuestionsandAnswers) or [Microsoft Power Platform Community Forum](https://powerusers.microsoft.com)\n    - Start by combing through the questions and the tags to find questions that are frequently asked, in the topic that you are passionate about.\n    - **New!** We have list the most frequently asked questions based on tags, as well as questions that fellow community have submitted. You can find them in [the discussions tab](https://github.com/microsoft/Microsoft-QnA/discussions).\n1. Once you decide on the question to address and answer, it's time to create your video. Provide a screen recording with the [requirements](https://github.com/microsoft/Microsoft-QnA/tree/new-doc#%EF%B8%8Frequirements%EF%B8%8F) below.\n1. Complete [Call for Content form](https://forms.office.com/r/RMXR9TbVbe) to inform us to submit your content.  \n*Please note that submissions are subjected to Microsoft Q&A's team approval. Creators with shortlisted submissions will be notified via email before content is published on the site.*\n\nOnce your submission is approved, we will work with you to produce the video. We'll take care of production so that you can focus on creating great content. You will amplify the final video on Microsoft Q&A and other forums with similar threads and questions.\n\n> For all questions, please see the [Q&A section in the discussions tab](https://github.com/microsoft/Microsoft-QnA/discussions/categories/q-a).\n\n### \u26a0\ufe0fRequirements\u26a0\ufe0f\n\n* Video of the screen captures in the format 1920 x 1080\n    - Ensure that there are no private and vital information e.g. IP addresses, URLs etc.\n    - Provide instructions (ex: blur, highlight) with the timestamp so our producers will be able to make the proper edits.\n* Make sure we can ear you clearly. All submission are in English. If you prefer not to speak, make sure to provide a [script](https://github.com/microsoft/Microsoft-QnA/tree/new-doc#script-optional).\n* Complete [Call for Content form](https://forms.office.com/r/RMXR9TbVbe) to inform us to submit your content.\n\n<br></br>\n\n## Creator's resources\n\nHere are some resources that you may find helpful.\n\n### Samples & Example\n\nHave a look the content already publish to give you an idea of what the video will look like:\n- [How to reference an existing resource in Bicep](https://learn.microsoft.com/en-us/shows/one-dev-minute/how-to-reference-an-existing-resource-in-bicep) to answer the question [Type casting in bicep](https://learn.microsoft.com/en-us/answers/questions/680702/type-casting-in-bicep) on Microsoft Q&A.\n- [How do you execute an Exe in Azure?](https://learn.microsoft.com/en-us/shows/one-dev-minute/how-do-you-execute-an-exe-in-azure) to answer the question [How to run an .exe file from Azure Function](https://learn.microsoft.com/en-us/answers/questions/806260/execute-an-exe-in-azure.html) on Microsoft Q&A.\n\n### Screen Capture\nProvide us with a screen recording of how you would address the answer to the question.\nBecause we want the videos to look as consistent as possible, you should take note of the following:\n1. Make sure it's in the format 1920 x 1080, .mp4 preferred. \n2. Do not zoom-in or move your mouse cursor to highlight something. This should be part of your instructions.\n\nHave a look at [this screen capture](https://github.com/microsoft/Microsoft-QnA/raw/main/asset/sample/Screen_only_1080_no-zoom.mp4) and you may noticed how the cursor stays still.\n\n### Script (Optional)\nWe also provide you the option providing us a script along with your video. While we know this may be more laborious but this may also reduce the likelihood of clarifications back and forth that we may eventually have with you. The script also provide you the opportunity to think through your narrative, conceptualizing your video. For example, \n1. what should our voice-over professional read to match your recording;\n2. what are the the instructions for the video editor to highlight part of the screen when it make sense (example, highlight line 31 in the code when explanations are provided). \n\nHave a look at [the script used](https://github.com/microsoft/Microsoft-QnA/raw/main/asset/sample/questions-and-answers-sample.docx) in the creation of the video. (It's a Word document.) [Template script](./asset/template/questions-and-answers-template.docx) for your use and to plan out your thought process.\n\n</br>\n\n### Other references\n\n- Refer to [How to write a quality answer](https://learn.microsoft.com/en-us/answers/support/quality-answer?utm_source=github) for the best practices.\n- Refer to [Tech Community Champion Guidelines](https://learn.microsoft.com/answers/support/community-champions-program) when you submission is approved and before you upload your content. \n\n\n#### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Contributors\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.codeisahighway.com\"><img src=\"https://avatars.githubusercontent.com/u/1054412?v=4?s=100\" width=\"100px;\" alt=\"Stephane Lapointe\"/><br /><sub><b>Stephane Lapointe</b></sub></a><br /><a href=\"#question-slapointe\" title=\"Answering Questions\">\ud83d\udcac</a> <a href=\"#content-slapointe\" title=\"Content\">\ud83d\udd8b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://medium.com/the-new-control-plane\"><img src=\"https://avatars.githubusercontent.com/u/5341999?v=4?s=100\" width=\"100px;\" alt=\"rbrayb\"/><br /><sub><b>rbrayb</b></sub></a><br /><a href=\"#question-rbrayb\" title=\"Answering Questions\">\ud83d\udcac</a> <a href=\"#content-rbrayb\" title=\"Content\">\ud83d\udd8b</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/zaidzaim/\"><img src=\"https://avatars.githubusercontent.com/u/43517319?v=4?s=100\" width=\"100px;\" alt=\"Zaid Zaim\"/><br /><sub><b>Zaid Zaim</b></sub></a><br /><a href=\"#question-ZaidZaim\" title=\"Answering Questions\">\ud83d\udcac</a> <a href=\"#content-ZaidZaim\" title=\"Content\">\ud83d\udd8b</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n", "repo_name": "Microsoft-QnA", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-QnA", "platform_org_repo": "github+microsoft/Microsoft-QnA", "link_to_repo": "https://github.com/microsoft/Microsoft-QnA", "platform": "github", "language": null, "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Yardl\n\nYardl is a simple schema language and command-line tool that generates domain\ntypes and serialization code.\n\n![A DSL on the left is translated to C++ code on the\nright](docs/images/overview.png)\n\nIt is conceptually similar to, and inspired by, [Avro](https://avro.apache.org/),\n[Protocol Buffers](https://developers.google.com/protocol-buffers),\n[Bond](https://microsoft.github.io/bond/), and others, but it was designed\nprimarily with raw medical instrument signal data in mind. Some of its features\nare:\n\n- Persistence to [HDF5](https://www.hdfgroup.org/solutions/hdf5/) files as well\n  as a compact binary format suitable for streaming over a network. There is\n  also a much less efficient NDJSON format that is easier to manually inspect or\n  use with other tools.\n- Built-in support for multidimensional arrays and complex numbers.\n- The schema is always embedded in the serialized data\n- \"Clean\" generated code with types that are easy to program against.\n- Generics\n- Computed fields\n\nModeling a data domain in Yardl brings a number of benefits over writing the\ncode by hand:\n\n- Writing correct and efficient serialization code can be tricky\n- Schema versioning, compatibility, and conversions are handled for you\n- You do not need to worry about consistency across different programming\n  languages\n- Comments could be used to generate documentation\n\n## Getting Started\n\nPlease check out the project [documentation](docs/docs.md).\n\n## Project Status\n\nWe are releasing this project order to get community feedback and contributions.\nIt is not complete and is **not ready for production use** at this time. We\nexpect to introduce breaking changes until the project reaches V1.\n\nWe currently support C++ codegen and work will begin on Python soon. Other\nplanned features include:\n\n- Reading data with a different schema version\n- References between packages\n- Validating schema evolution is non-breaking\n- Constraints\n- Improvements to the language and editing experience\n\n## Building the Code in this Repo\n\nWe recommend opening repo in a [dev\ncontainer](https://code.visualstudio.com/docs/devcontainers/containers) or a\n[codespace](https://docs.github.com/en/codespaces/overview). Otherwise, all the\nrequired dependencies are specified in the\n[Conda](https://docs.conda.io/en/latest/) [environment.yml](environment.yml)\nfile in the repo root.\n\nWe use the [`just`](https://github.com/casey/just) command runner to build and run tests. To get\nstarted, you should be able to run\n\n```bash\n$ just\n```\n\nfrom the repo root.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require\nyou to agree to a Contributor License Agreement (CLA) declaring that you have\nthe right to, and actually do, grant us the rights to use your contribution. For\ndetails, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether\nyou need to provide a CLA and decorate the PR appropriately (e.g., status check,\ncomment). Simply follow the instructions provided by the bot. You will only need\nto do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information\nsee the [Code of Conduct\nFAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional\nquestions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or\nservices. Authorized use of Microsoft trademarks or logos is subject to and must\nfollow [Microsoft's Trademark & Brand\nGuidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must\nnot cause confusion or imply Microsoft sponsorship. Any use of third-party\ntrademarks or logos are subject to those third-party's policies.\n", "repo_name": "yardl", "org_name": "microsoft", "org_repo": "microsoft/yardl", "platform_org_repo": "github+microsoft/yardl", "link_to_repo": "https://github.com/microsoft/yardl", "platform": "github", "language": "C++", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "arcade-platformer", "org_name": "microsoft", "org_repo": "microsoft/arcade-platformer", "platform_org_repo": "github+microsoft/arcade-platformer", "link_to_repo": "https://github.com/microsoft/arcade-platformer", "platform": "github", "language": "TypeScript", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# html-reporter-for-axe-core-dotnet\n\nDotnet library for converting [axe-core](https://github.com/dequelabs/axe-core) run results to a HTML report. Built on top of the work done in [axe-core-nuget](https://github.com/dequelabs/axe-core-nuget).\n\nProvides integration libraries for [Deque.AxeCore.Playwright](https://github.com/dequelabs/axe-core-nuget/tree/develop/packages/playwright) and [Deque.AxeCore.Selenium](https://github.com/dequelabs/axe-core-nuget/tree/develop/packages/selenium)\n\nThe packages in this repo are listed below:\n\n- [AxeCore.HTMLReporter](./src/html-reporter/README.md)\n- [AxeCore.HTMLReporter.Playwright](./src/html-reporter-playwright/README.md)\n- [AxeCore.HTMLReporter.Selenium](./src/html-reporter-selenium/README.MD)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "html-reporter-for-axe-core-dotnet", "org_name": "microsoft", "org_repo": "microsoft/html-reporter-for-axe-core-dotnet", "platform_org_repo": "github+microsoft/html-reporter-for-axe-core-dotnet", "link_to_repo": "https://github.com/microsoft/html-reporter-for-axe-core-dotnet", "platform": "github", "language": "C#", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "## Visual Deep Reinforcement Learning in 3 Stages (VRL3)\n\nOfficial code for the paper VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. Summary site: https://sites.google.com/nyu.edu/vrl3.\n\n![CheWang](VRL3.png)\n\nCode has just been released and the entire codebase is re-written to make it cleaner and improve readability, if you run into any problem, please do not hesitate to open an issue. \n\nWe are also doing some further clean-up of the code now. This repo will be updated. \n\n<a name=\"table-of-contents\"/> \n\n### Table of Contents  \n\n- [Repo structure](#repo-structure)\n- [Environment setup](#environment-setup) \n  - [Docker setup](#docker)\n  - [Run experiments](#run-exp)\n  - [Singularity setup](#singularity)\n- [Plotting example](#plotting)\n- [Technical details](#hyper)\n  - [Computation time](#computation)\n  - [Known issues](#known-issues)\n- [Acknowledgement](#acknowledgement)\n- [Citation](#citation)\n- [Contributing](#contributing)\n\n\n### Updates:\n\n<sup>03/30/2023: added example plot function and a quick tutorial.</sup>\n\n\n<a name=\"repo-structure\"/> \n\n## Repo structure and important files: \n\n```\nVRL3 # this repo\n\u2502   README.md # read this file first!\n\u2514\u2500\u2500\u2500docker # dockerfile with all dependencies\n\u2514\u2500\u2500\u2500plot_utils # code for plotting, still working on it now...\n\u2514\u2500\u2500\u2500src\n    \u2502   train_adroit.py # setup and main training loop\n    \u2502   vrl3_agent.py # agent class, code for stage 2, 3\n    \u2502   train_stage1.py # code for stage 1 pretraining on imagenet\n    \u2502   stage1_models.py # the encoder classes pretrained in stage 1\n    \u2514\u2500\u2500\u2500cfgs_adroit # configuration files with all hyperparameters\n\n# download these folders from the google drive link\nvrl3data \n\u2514\u2500\u2500\u2500demonstrations # adroit demos \n\u2514\u2500\u2500\u2500trained_models # pretrained stage 1 models\nvrl3examplelogs \n\u2514\u2500\u2500\u2500rrl  # rrl training logs\n\u2514\u2500\u2500\u2500vrl3 # vrl3 with default hyperparams logs\n```\n\nTo get started, download this repo and download adroit demos, pretrained models, and example logs with the following link: \nhttps://drive.google.com/drive/folders/14rH_QyigJLDWsacQsrSNV7b0PjXOGWwD?usp=sharing\n\n<a name=\"environment-setup\"/> \n\n## Environment setup\n\nThe recommended way is to just use the dockerfile I provided and follow the tutorial here. You can also look at the dockerfile to know the exact dependencies or modify it to build a new dockerfile. \n\n<a name=\"docker\"/> \n\n### Setup with docker\n\nIf you have a local machine with gpu, or your cluster allows docker (you have sudo), then you can just pull my docker image and run code there. (Newest version is 1.5, where the mujoco slow rendering with gpu issue is fixed). \n```\ndocker pull docker://cwatcherw/vrl3:1.5\n```\n\nNow, `cd` into a directory where you have the `VRL3` folder (this repo), and also the `vrl3data` folder that you downloaded from my google drive link. \nThen, mount `VRL3/src` to `/code`, and mount `vrl3data` to `/vrl3data` (you can also mount to other places, but you will need to adjust some commands or paths in the config files):\n```\ndocker run -it --rm --gpus all -v \"$(pwd)\"/VRL3/src:/code -v \"$(pwd)\"/vrl3data:/vrl3data  docker://cwatcherw/vrl3:1.5\n```\nNow you should be inside the docker container. Refer to the \"Run experiments\" section now. \n\n<a name=\"run-exp\"/>\n\n### Run experiments\n\nOnce you get into the container (either docker or singularity), first run the following commands so the paths are correct. Very important especially on singularity since it uses automount which can mess up the paths. (newest version code now uses `os.environ` to do these so you can also skip this step.) \n```\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/workspace/.mujoco/mujoco210/bin\nexport MUJOCO_PY_MUJOCO_PATH=/workspace/.mujoco/mujoco210/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nvidia/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/workspace/.mujoco/mujoco210/bin\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\nexport MUJOCO_GL=egl\n```\n\nGo to the VRL3 code directory that you mounted.\n```\ncd /code\n```\n\nFirst quickly check if mujoco is using your GPU correctly for rendering. If everything is correct, you should see the program print out the computation time for 1000 rendering (if it's first time mujoco is imported then there will also be mujoco build messages which takes a few minutes). The time used to do rendering 1000 times should be < 0.5 seconds. \n```\npython testing/computation_time_test.py\n```\n\n\nThen you can start run VRL3:\n```\ncd /code\npython train_adroit.py task=door\n```\n\nFor first-time setup, use `debug=1` to do a quick test run to see if the code is working. This will reduce training epochs and change many other hyperparameters so you will get a full run in a few minutes. \n```\npython train_adroit.py task=door debug=1\n```\n\nYou can also run with different hyperparameters, see the `config.yaml` for a full list of them. For example: \n```\npython train_adroit.py task=door stage2_n_update=5000 agent.encoder_lr_scale=0.1\n```\n\n<a name=\"singularity\"/>\n\n### Setup with singularity \n\nIf your cluster does not allow sudo (for example, NYU's slurm HPC), then you can use singularity, it is similar to docker. But you might need to modify some of the commands depends on how your cluster is being managed. Here is an example setup on the NYU Greene HPC.\n\nSet up singularity container (this will make a folder called `sing` in your scratch directory, and then build a singularity sandbox container called `vrl3sing`, using the `cwatcherw/vrl3:1.5` docker container which I put on my docker hub):\n```\nmkdir /scratch/$USER/sing/\ncd /scratch/$USER/sing/\nsingularity build --sandbox vrl3sing docker://cwatcherw/vrl3:1.5\n```\n\nFor example, on NYU HPC, start interactive session (if your school has a different hpc system, consult your hpc admin): \n```\nsrun --pty --gres=gpu:1 --cpus-per-task=4 --mem 12000 -t 0-06:00 bash\n```\nHere, by default VRL3 uses 4 workers for dataloader, so we request 4 cpus. Once the job is allocated to you, go the `sing` folder where you have your container, then run it:\n```\ncd /scratch/$USER/sing\nsingularity exec --nv -B /scratch/$USER/sing/VRL3/src:/code -B /scratch/$USER/sing/vrl3sing/opt/conda/lib/python3.8/site-packages/mujoco_py/:/opt/conda/lib/python3.8/site-packages/mujoco_py/ -B /scratch/$USER/sing/vrl3data:/vrl3data /scratch/$USER/sing/vrl3sing bash\n```\nWe mount the `mujoco_py` package folder because singularity files by default are read-only, and the older version of mujoco_py wants to modify files, which can be problematic. (Unfortunately, Adroit env relies on the older version of mujoco)\n\nAfter the singularity container started running, now refer to the \"Run experiments\" section.\n\n<a name=\"plotting\"/>\n\n## Plotting example\n\nIf you like to use the plotting functions we used, you will need `matplotlib`, `seaborn` and some other basic packages to use the plotting programs. You can also use your own plotting functions. \n\nAn example is given in `plot_utils/vrl3_plot_example.py`. To use it: \n1. make sure you downloaded the `vrl3examplelogs` folder from the drive link and unzipped it. \n2. in `plot_utils/vrl3_plot_example.py`, change the `base_dir` path to where the `vrl3examplelogs` folder is on your computer. \n3. similarlly, change `base_save_dir` path to where you want the figures to be generated. \n4. run `plot_utils/vrl3_plot_example.py`, this will generate a few figures comparing success rate between RRL and VRL3 to the specified path. \n\n(All ablation experiment logs generated during the VRL3 research are in the folder `vrl3logs` from the drive link. `plot_utils/vrl3_plot_runner.py` was used to generate figures in the paper. Currently still need further clean up.)\n\n<a name=\"hyper\"/>\n\n## Technical details\n\n- BC loss: in the config files, I now by default disable all BC loss since our ablations show they are not really helping. \n- under `src/cfgs_adroit/task/relocate.yaml` you will see that relocate has `encoder_lr_scale: 0.01`, as shown in the paper, relocate requires a smaller encoder learning rate. You can set specific default parameters for each task in their separate config files. \n- in the paper for most experiments, I used `frame_stack=3`, however later I found we can reduce it to 1 and still get the same performance. It might be beneficial to set it to 1 so it runs faster and takes less memory. If you set this to 1, then convolutional channel expansion will only be applied for the relocate env, where the input is a stack of 3 camera images. \n- all values in table 2 in appendix A.2 of the paper are set to be the default values in the config files. \n- to apply VRL3 to other environments, please consult the hyperparameter sensitivity study in appendix A, which identifies robust and sensitive hyperparameters. \n\n<a name=\"computation\"/>\n\n### Computation time\n\nThis table compares the computation time estimates for the open source code with default hyperparameters (tested on NYU Greene with RTX 8000 and 4 cpus). When you use the code on your machine, it might be slightly faster or slower, but should not be too different. These results seem to be slightly faster than what we reported in the paper (which tested on Azure P100 GPU machines). Improved computation speed is mainly due to we now set default `frame_stack` for Adroit.\n\n| Task  | Stage 2 (30K updates) | Stage 3 (4M frames) | Total   | Total (paper) | \n|------------------|-----------------------|---------------------|---------|------------|\n| Door/Pen/Hammer  | ~0.5 hrs              | ~13 hrs             | ~14 hrs | ~16 hrs         |\n| Relocate         | ~0.5 hrs              | ~16 hrs             | ~17 hrs |   ~24 hrs       |\n\nNote that VRL3's performance kind of converged already at 1M data for Door, Hammer and Relocate. So depending on what you want to achieve in your work, you may or may not need to run a full 4M frames. In the paper we run to 4M to be consistent with prior work and show VRL3 can outperform previous SOTA in both short-term and long-term performance. \n\n<a name=\"known-issues\"/>\n\n### Known issues:\n\n- Some might encounter a problem where mujoco can crush at an arbitrary point during training. I have not seen this issue before but I was told reinit `self.train_env` between stage 2 and stage 3 can fix it. \n- If you are not using the provided docker image and you run into the problem of slow rendering, it is possible that mujoco did not find your gpu and made a `CPUExtender` instead of a `GPUExtender`. You can follow the steps in the provided dockerfile, or force it to use the `GPUExtender` (see code in `mujoco-py/mujoco_py/builder.py`) Thanks to ZheCheng Yuan for identifying above 2 issues. \n- Newer versions of mujoco are easier to work with. We use an older version only because Adroit relies on it. (So you can try a newer mujoco if you want to test on other environments). \n\n<a name=\"acknowledgement\"/>\n\n## Acknowledgement\n\nVRL3 code has been mainly built on top of the DrQv2 codebase (https://github.com/facebookresearch/drqv2). Some utility functions and dockerfile are modified from the REDQ codebase (https://github.com/watchernyu/REDQ). The Adroit demo loading code is modified from the RRL codebase (https://github.com/facebookresearch/RRL). \n\n<a name=\"citation\"/>\n\n## Citation\n\nIf you use VRL3 in your research, please consider citing the paper as:\n```\n@inproceedings{wang2022vrl3,\n  title={VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning},\n  author={Wang, Che and Luo, Xufang and Ross, Keith and Li, Dongsheng},\n  booktitle={Conference on Neural Information Processing Systems},\n  year={2022},\n  url={https://openreview.net/forum?id=NjKAm5wMbo2}\n}\n```\n\n<a name=\"contributing\"/>\n\n## Contributing\n\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n<a name=\"trademarks\"/>\n\n## Trademarks\n\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "VRL3", "org_name": "microsoft", "org_repo": "microsoft/VRL3", "platform_org_repo": "github+microsoft/VRL3", "link_to_repo": "https://github.com/microsoft/VRL3", "platform": "github", "language": "Python", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Project\n\nThis project mainly exists to be able to use a more recent version of google protobuf within\nthe Spark environment. The protobuf version that comes embedded in Spark is old, so this project\npackage is used to shade the newer protobuf version to avoid version conflicts. It also includes\nthe auto-generated OnnxML.java file for interacting with ONNX models at the protobuf level.\n\nProjects can reference this package to use a shaded version of the com.google.protobuf package for OnnxML.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "onnx-protobuf", "org_name": "microsoft", "org_repo": "microsoft/onnx-protobuf", "platform_org_repo": "github+microsoft/onnx-protobuf", "link_to_repo": "https://github.com/microsoft/onnx-protobuf", "platform": "github", "language": "Scala", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# OpenHack for Lakehouse [Japanese]\n\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u306f OpenHack for Lakehouse \u3067\u5229\u7528\u3059\u308b Azure Databricks \u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304a\u3088\u3073\u95a2\u9023\u30ea\u30bd\u30fc\u30b9\u3092\u7ba1\u7406\u3057\u307e\u3059\u3002\n\n## \u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u53c2\u7167\u65b9\u6cd5\nAzure Databricks \u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9\u3067\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u53c2\u7167\u3059\u308b\u306b\u306f\u4ee5\u4e0b 2 \u3064\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002\n\n1. Azure Databricks \u306e [Repos](https://learn.microsoft.com/ja-jp/azure/databricks/repos/) \u6a5f\u80fd\u3092\u7528\u3044\u3066\u3001Azure Databricks \u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9\u306b\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3\u3059\u308b\n2. \u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u306e [Releases](https://github.com/microsoft/openhack-for-lakehouse-japanese/releases) \u306b\u6dfb\u4ed8\u3055\u308c\u305f\u6700\u65b0\u306e Databricks \u30ce\u30fc\u30c8\u30d6\u30c3\u30af (dbc) \u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057 Azure Databricks \u30ef\u30fc\u30af\u30b9\u30da\u30fc\u30b9\u306b\u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\n\n## \u30ea\u30dd\u30b8\u30c8\u30ea\u69cb\u9020\n\u672c\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30d5\u30a9\u30eb\u30c0\u69cb\u9020\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002\n\n### Azure Databricks \u30ce\u30fc\u30c8\u30d6\u30c3\u30af\n- `dayN` \u30d5\u30a9\u30eb\u30c0\u306b\u7a7a\u767d\u306e\u56de\u7b54\u6b04\u3092\u542b\u3080\u53d7\u8b1b\u8005\u7528\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304c\u3042\u308a\u307e\u3059\u3002\n- `dayN_with_answers` \u30d5\u30a9\u30eb\u30c0\u306b\u56de\u7b54\u4f8b\u3092\u542b\u3080\u30b3\u30fc\u30c1\u7528\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u304c\u3042\u308a\u307e\u3059\u3002\n- \u5b66\u3073\u306e\u6700\u5927\u5316\u306e\u305f\u3081\u3001\u53d7\u8b1b\u8005\u306f OpenHack \u306e\u6700\u4e2d\u306f `dayN_with_answers` \u30d5\u30a9\u30eb\u30c0\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u6975\u529b\u53c2\u7167\u3057\u306a\u3044\u3053\u3068\u3092\u63a8\u5968\u3057\u307e\u3059\u3002\n\n### Azure \u74b0\u5883\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7 \u30b9\u30af\u30ea\u30d7\u30c8\n- `infra` \u30d5\u30a9\u30eb\u30c0\u306b Azure \u74b0\u5883\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u306e\u305f\u3081\u306e Bicep \u304a\u3088\u3073 Bash \u30b9\u30af\u30ea\u30d7\u30c8\u304c\u3042\u308a\u307e\u3059\u3002\u30b3\u30fc\u30c1\u3001\u307e\u305f\u306f\u81ea\u5df1\u5b66\u7fd2\u3092\u5e0c\u671b\u3059\u308b\u53d7\u8b1b\u8005\u306f\u3001\u3053\u308c\u3089\u306e\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u74b0\u5883\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u304c\u53ef\u80fd\u3067\u3059\u3002\n\n### \u305d\u306e\u4ed6\u306e\u30ea\u30bd\u30fc\u30b9\n- `deck` \u30d5\u30a9\u30eb\u30c0\u306b Azure \u30b5\u30fc\u30d3\u30b9\u306a\u3069\u306b\u95a2\u3059\u308b\u30b9\u30e9\u30a4\u30c9\u8cc7\u6599\u304c\u3042\u308a\u307e\u3059\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- `images` \u30d5\u30a9\u30eb\u30c0\u306f\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u53c2\u7167\u3059\u308b\u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u3092\u7ba1\u7406\u3059\u308b\u30d5\u30a9\u30eb\u30c0\u3067\u3059\u3002\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "openhack-for-lakehouse-japanese", "org_name": "microsoft", "org_repo": "microsoft/openhack-for-lakehouse-japanese", "platform_org_repo": "github+microsoft/openhack-for-lakehouse-japanese", "link_to_repo": "https://github.com/microsoft/openhack-for-lakehouse-japanese", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# scitt-ccf-ledger\r\n\r\nThis repository contains the source code for scitt-ccf-ledger, an application\r\nthat runs on top of [CCF](https://ccf.dev/) implementing draft standards developed within the [IETF SCITT WG](https://datatracker.ietf.org/wg/scitt/about/). Its purpose is to provide provenance for artefacts in digital supply chains, increasing trust in those artefacts. scitt-ccf-ledger achieves this by allowing signed claims about artefacts to be submitted to a secure immutable ledger, and returning receipts which prove claims have been stored and registration policies applied.\r\n\r\nThis research project is at an early stage and is open sourced to facilitate academic collaborations. We are keen to engage in research collaborations on this project, please do reach out to discuss this by opening an issue.\r\n\r\n## Getting Started\r\n\r\nThe instructions below guide you through building and deploying a local instance of scitt-ccf-ledger for development and testing purposes.\r\n\r\nBeing a CCF application, scitt-ccf-ledger runs in SGX enclaves. However, for testing purposes, it also supports running on non-SGX hardware in what is called *virtual* mode.\r\n\r\nAll instructions below assume Linux as the operating system.\r\n\r\n### Using Docker\r\n\r\nUse the following commands to start a single-node CCF network with the scitt-ccf-ledger application setup for development purposes.\r\n\r\nNote: `PLATFORM` should be set to `sgx` or `virtual` to select the type of build.\r\n\r\n```sh\r\nexport PLATFORM=<sgx|virtual>\r\n./docker/build.sh\r\n./docker/run-dev.sh\r\n```\r\n\r\nThe node is now reachable at https://127.0.0.1:8000/.\r\n\r\nNote that `run-dev.sh` configures the network in a way that is not suitable for production, in particular it generates an ad-hoc governance member key pair and it disables API authentication.\r\n\r\nSee the `demo/` folder on how to interact with the application.\r\n\r\n### Development setup\r\n\r\nSee [DEVELOPMENT.md](DEVELOPMENT.md) for instructions on building, running, and testing scitt-ccf-ledger without Docker.\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions. Please see the [Contribution guidelines](CONTRIBUTING.md).\r\n\r\n### Trademarks \r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\r\n", "repo_name": "scitt-ccf-ledger", "org_name": "microsoft", "org_repo": "microsoft/scitt-ccf-ledger", "platform_org_repo": "github+microsoft/scitt-ccf-ledger", "link_to_repo": "https://github.com/microsoft/scitt-ccf-ledger", "platform": "github", "language": "C++", "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# heft.rushstack.io deployment repo\n\nThis repository hosts GitHub Pages deployments for the https://heft.rushstack.io website.\n\n**Website source files:** https://github.com/microsoft/rushstack-websites/tree/main/websites/heft.rushstack.io\n\n**Deploy branch:** https://github.com/microsoft/heft.rushstack.io-website/tree/gh-pages\n\nIf you have questions or suggestions regarding the `heft.rushstack.io` website, please create your issue in the upstream [rushstack-websites](https://github.com/microsoft/rushstack-websites/issues) monorepo.\n\n# Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all others rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "heft.rushstack.io-website", "org_name": "microsoft", "org_repo": "microsoft/heft.rushstack.io-website", "platform_org_repo": "github+microsoft/heft.rushstack.io-website", "link_to_repo": "https://github.com/microsoft/heft.rushstack.io-website", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# lfx.rushstack.io deployment repo\n\nThis repository hosts GitHub Pages deployments for the https://lfx.rushstack.io website.\n\n**Website source files:** https://github.com/microsoft/rushstack-websites/tree/main/websites/lfx.rushstack.io\n\n**Deploy branch:** https://github.com/microsoft/lfx.rushstack.io-website/tree/gh-pages\n\nIf you have questions or suggestions regarding the `lfx.rushstack.io` website, please create your issue in the upstream [rushstack-websites](https://github.com/microsoft/rushstack-websites/issues) monorepo.\n\n# Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all others rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "lfx.rushstack.io-website", "org_name": "microsoft", "org_repo": "microsoft/lfx.rushstack.io-website", "platform_org_repo": "github+microsoft/lfx.rushstack.io-website", "link_to_repo": "https://github.com/microsoft/lfx.rushstack.io-website", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# ElasticFlow Traces\n\nThis repository contains job traces of the 1st party deep learning training workloads in Microsoft's internal ITP clusters.\nThe traces are used in \"ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning\" in ASPLOS '23.\n\n## Overview\n\n### Characteristics\n\n* Duration: 2 months from 2021-03-04 to 2021-05-03\n* Total number of clusters: 10\n* Total number of jobs: 69,351\n* Total GPU horus: 1,632,719.7\n\n### Schema for raw-traces\n\n`data/raw-traces.tar.gz` contains raw trace data collected from ITP clusters.\nThere are 10 csv files, each file represents one cluster.\nDetailed descriptions for each column are as follows:\n\n|            Column | Description                                                                        |\n|------------------:|:-----------------------------------------------------------------------------------|\n|          `job_id` | Job unique id, the UUID is re-generated after trace is collected.                  |\n| `submission_time` | Job submission time, relative time in seconds to the beginning of collection time. |\n|        `duration` | Job duration in seconds.                                                           |\n|         `num_gpu` | Number of GPUs requested/used by the job.                                          |\n\nHere's an example for each csv:\n```\njob_id,submission_time,duration,num_gpu\n6b2d9ccc-6279-403c-e1a2-cfe7810d5c57,16566,3890,4\n```\n\n### Schema for elasticflow-traces\n\n`data/elasticflow-traces.tar.gz` contains extra randomly generated data based on raw trace data.\nThe traces are actually used in ElasticFlow experiments, 10 csv files are used for simulation and 195job csv is used to run in real testbed.\nDetailed descriptions for each column are as follows:\n\n|            Column | Description                                                                        |\n|------------------:|:-----------------------------------------------------------------------------------|\n|          `job_id` | Job unique id, the UUID is re-generated after trace is collected.                  |\n| `submission_time` | Job submission time, relative time in seconds to the beginning of collection time. |\n|   `num_iteration` | Randomly generated itertion number for the job's training.                         |\n|      `model_name` | Randomly generated model name for the job.                                         |\n|        `deadline` | Generated deadline based on specific rules.                                        |\n|      `batch_size` | Generated batch size for the job's training.                                       |\n|         `num_gpu` | Number of GPUs requested/used by the job.                                          |\n|        `duration` | Job duration in seconds.                                                           |\n\nHere's an example for each csv:\n```\njob_id,submission_time,num_iteration,model_name,deadline,batch_size,num_gpu,duration\n6b2d9ccc-6279-403c-e1a2-cfe7810d5c57,16566,40285,bert,21347,64,4,3890\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "elasticflow-traces", "org_name": "microsoft", "org_repo": "microsoft/elasticflow-traces", "platform_org_repo": "github+microsoft/elasticflow-traces", "link_to_repo": "https://github.com/microsoft/elasticflow-traces", "platform": "github", "language": null, "stargazers_count": 17, "watchers_count": 17}, {"README_text": "# Azure OpenAI Service Prompt Examples\n\nThis repo shares a set of prompt examples for [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/). These samples are intented as starting points for further exploration or for building production solutions.\n\nUse those prompts at your own risk and make sure to validate them on appropriate datasets.\n\n## Contributing\n\nThis repository welcomes contributions and suggestions. If you want to contribute some of your prompts, please file a pull request and we'll review and merge it.\n\n Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com. When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "openai-prompt-examples", "org_name": "microsoft", "org_repo": "microsoft/openai-prompt-examples", "platform_org_repo": "github+microsoft/openai-prompt-examples", "link_to_repo": "https://github.com/microsoft/openai-prompt-examples", "platform": "github", "language": null, "stargazers_count": 10, "watchers_count": 10}, {"README_text": "## Microsoft Open Source Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "vcpkg-docs", "org_name": "microsoft", "org_repo": "microsoft/vcpkg-docs", "platform_org_repo": "github+microsoft/vcpkg-docs", "link_to_repo": "https://github.com/microsoft/vcpkg-docs", "platform": "github", "language": "CMake", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# abap2git\n\n**abap2git** is a simplified sync tool written in ABAP code to one-way sync ABAP and configuration objects from an SAP system to Git repository with Azure DevOps REST API with following benefits:\n* Support multiple packages in a Git branch, packages don't have to be super package of one another\n* Support multiple SAP systems in the same Git repo, each in their respective branches, fit for code review the diffs across systems in the landscape\n* Sync ABAP objects in an active/released workbench/customizing/transport of copies transport request (shortened as TR below) to specific Git branch\n* Support configuration changes in customizing TR for 1380 object types specified in OBJH table and object types like TABU/CDAT/VDAT/TDAT\n* Support HR/Payroll schemas/personnel calculation rules (PCR) in customizing TR\n* The sync for the TR only accesses the objects contained in TR object list and not other unchanged ones, fit for code review the diffs\n\n## Features\n1. Download ABAP objects in specific package(s) of an SAP system to local folder, each package a folder.\n2. Download ABAP objects in all Y*/Z* customization packages to local folder.\n3. Support active version mode and latest version mode. In active version mode, for each ABAP object, active version, if any, otherwise latest version, will be downloaded; in latest version mode, for each ABAP object, only latest version will be downloaded or a released version no later than an optional released TR (to sync to that specific snapshot of the system), if not available the object will not be downloaded. Active version mode is best to download all current version of the ABAP objects for code inspection and analysis; latest version mode is best for continuous integration purpose.\n4. Sync a specific released TR to specific Git repo branch, provided the ABAP objects in that branch are downloaded in latest version mode above.\n5. Catchup: sync the Git repo branch to latest TR by generating commit for each TR between last sync-ed one and latest one, this helps you bring the git repo in sync.\n\n## How To\n* Add the classes and reports in abap folder to the target SAP system.\n* Create a branch for target SAP system in the Git repository, suggested branch name is \"users/system/system id\", for example, users/system/sy1.\n* Run the report Z_ABAPTOGIT_LOCALSYNC to download initial ABAP objects with active or latest version mode.\n* Use git command/CLI/... to push the initial objects.\n* Run the report Z_ABAPTOGIT_CATCHUPSYNC to \"catch up\" the TRs as mentioned in feature #5.\n\n## FAQ\n### Why is BAdI CTS_REQUEST_CHECK~CHECK_BEFORE_RELEASE not a good place to sync code to Azure DevOps?\nThe BAdI class is called before releasing the TR, the release process may fail afterwards due to many reasons, containing unreleased tasks, failed ATC rules, failed virtual forge checks, failed ATC unit tests, etc., then the request will end up with unreleased state, in this case the request is not supposed to sync to Git.\n\n### Why use background job to call abap2git?\nGiven the BAdI above with improper timing to sync to Git, scheduled background job with delay would be a solution to ensure the TR release process is completed when the job starts.\n\n### What if ABAP developer doesn't have the privilege to schedule background job in SAP?\nContact security team to add a customized role with authorize objects required to schedule job and have ABAP developer apply for that role.\n\n### What if security cannot approve customized role for scheduling background job?\nCheck out next question for options and pros/cons.\n\n### What are the options and pros/cons for background job solution?\n1. In BAdI class schedule background job to call abap2git spot sync method. The pros is the short turnaround time between TR release and Git sync, which ensures rapid continuous integration right after TR. The cons are privilege needed to schedule background job, and the race condition upon multiple TRs released in short time causing earlier TR got sync after later TR due to uncertain start time of their corresponding background jobs. A sequential queuing mechanism is required to eliminate this race condition and it's not provided by abap2git.\n2. In BAdI class schedule background job but manage a time window (say 1 hour) that only one job is scheduled within the window. Also requires background job privilege. This reduces but doesn't eliminate the race condition given the events (TR is acutally released; abap2git checks if that TR is in released status) may race when the events happen at the time window boundary. Worst case a TR is not sync-ed when there's no further TR followed.\n3. Use SM36 to schedule recurring background job in specific frequency which fetches configurations/secrets (ADO PAT) and calls Z_ABAPTOGIT_CATCHUPSYNC program catching up released TRs since last sync. This is best but relies on your security/BASIS policy.\n4. Use Z_ABAPTOGIT_SCHEDULE_JOB program to schedule 28 days \\* 24 jobs (1 per hour) at a time, requiring a developer with background job privilege to run it every 4 weeks. This doesn't require all ABAP developers with the privilege.\n\n### Why configuration changes look the same across multiple Git commits?\nCurrently configuration changes in a specific data table are captured with current data rows in the table when the TR is sync-ed, thus if multiple TRs are sync-ed in catching up or changes are made before catching up, the configuration changes captured for the TRs will be the same and don't reflect the actual changes released by the TR. Future release of abap2git will attempt to fetch the actual changes from audit log or TR export file.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "abap2git", "org_name": "microsoft", "org_repo": "microsoft/abap2git", "platform_org_repo": "github+microsoft/abap2git", "link_to_repo": "https://github.com/microsoft/abap2git", "platform": "github", "language": "ABAP", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "digital-content-distribution-toolkit", "org_name": "microsoft", "org_repo": "microsoft/digital-content-distribution-toolkit", "platform_org_repo": "github+microsoft/digital-content-distribution-toolkit", "link_to_repo": "https://github.com/microsoft/digital-content-distribution-toolkit", "platform": "github", "language": "C#", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "digital-content-distribution-toolkit-client", "org_name": "microsoft", "org_repo": "microsoft/digital-content-distribution-toolkit-client", "platform_org_repo": "github+microsoft/digital-content-distribution-toolkit-client", "link_to_repo": "https://github.com/microsoft/digital-content-distribution-toolkit-client", "platform": "github", "language": "Kotlin", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Azure Kubernetes Service Dev Day\n\nThe Azure Kubernetes Service Dev Day labs include:\n1. A lab that covers basic Kubectl commands and setting up an AKS cluster\n2. A lab that covers deploying an application using Helm to an AKS cluster\n3. A lab that covers deploying an Ingress controller and resource to an AKS cluster\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aks-dev-day", "org_name": "microsoft", "org_repo": "microsoft/aks-dev-day", "platform_org_repo": "github+microsoft/aks-dev-day", "link_to_repo": "https://github.com/microsoft/aks-dev-day", "platform": "github", "language": "HTML", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Welcome\n\nWelcome to the ITOpsTalk GitHub Repo. Here you'll find a collection of scripts and samples to help IT/Ops on different situations. The Microsoft Modern Infrastructure Cloud Advocates are responsible for the content on this repo.\n\nPlease visit our blog at [ITOpsTalk](https://itpostalk.com).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ITOpsTalk", "org_name": "microsoft", "org_repo": "microsoft/ITOpsTalk", "platform_org_repo": "github+microsoft/ITOpsTalk", "link_to_repo": "https://github.com/microsoft/ITOpsTalk", "platform": "github", "language": "PowerShell", "stargazers_count": 76, "watchers_count": 76}, {"README_text": "# Project\n\nWith QR + Password, frontline workers can more easily and more quickly authenticate into a shared device by automating the process of typing a UPN (most commonly the users email address). By scanning a QR code, users can save up to 60% of the time it takes to currently type a full username. If an employee only uses their Azure Active Directory (AAD) account at work, additional steps can be taken to simplify their password to further streamline the user authentication process.\n\nThis project is meant to demonstrate how you can add this improved authentication experience to your own  Android applications. In this sample application, we show how to complete the following:\n1.\tAdd a \u201cScan a QR code\u201d button to the first screen of your app,\n2.\tOn click of the button, open the device camera,\n3.\tDetect and analyze a QR code,\n4.\tPass the value to the MSAL library to autopopulate the UPN field.\n\nThis sample leverages [CameraX (CameraX \u00a0|\u00a0 Android Developers)](https://developer.android.com/jetpack/androidx/releases/camera) and [Zxing (GitHub - zxing/zxing: ZXing (\"Zebra Crossing\") barcode scanning library for Java, Android)](https://github.com/zxing/zxing) to power the end-to-end solution.\n\nThis solution is expected to be used in tandem with [Shared Device Mode (SDM)](https://learn.microsoft.com/en-us/azure/active-directory/develop/msal-shared-devices), a technology that automatically performs single sign-on and enables single sign-out for users on shared devices. By using SDM, employees will only need to sign-in once and get automatic access to all other applications on the device without needing to provide additional credentials.\n\nIf your employees already have badges or nametags, we recommend printing the QR codes on stickers and attaching them to the back of them. If your employees do not have badges or nametags, you can also store the QR codes in personal areas (e.g., employee locker) or in a backroom area (e.g., next to the physical time clock). Because these QR codes only contain the UPN, it is safe to put them in common employee areas.\n\n## Prerequisites\nThe following pre-requisites should be completed before using this sample application of QR + Password:\n1.\tAuthenticator must be installed on the test device \u2013 this is necessary to enable Shared Device Mode on your device.\n2.\tPlace your device in Shared Device Mode \u2013 this is possible with both Microsoft Intune and manually using the Authenticator app on the device.\n3.\tGenerate QR codes for the test users \u2013 please refer to the Generating QR Codes section for steps on how to generate QR codes for your employees .\n\n## Generating QR codes\nYou can either manually generate your QR codes using existing QR generation websites like www.qr-code-generator.com or create a service to automatically generate QR codes for you. To create a QR code for a specific user, complete the following steps:\n1.\tNavigate to a QR generation website.\n2.\tFor the value of your QR code, type in the UPN of your user (e.g., adelev@contoso.com).\n3.\tDownload and print the generated QR code.\n\n## About the security of the QR + Password solution\nBecause this solution only simplifies the entry of a user\u2019s UPN, using this initial solution does not increase your security risk. An analogous scenario from a security perspective would be printing off and distributing a list of email addresses so that employees did not forget their UPN.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "qr-authentication", "org_name": "microsoft", "org_repo": "microsoft/qr-authentication", "platform_org_repo": "github+microsoft/qr-authentication", "link_to_repo": "https://github.com/microsoft/qr-authentication", "platform": "github", "language": "Kotlin", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Open Source Soundscape\n\nThis open source project is a subset of 'Microsoft Soundscape' product\nas released on the Apple App Store.  To make this contribution to open\nsource, it was necessary to remove 3rd party sources and some other\nproprietary code.  Further Microsoft branding, reference to specific\nMicrosoft services, and deployment mechanisms have been removed.\n\n# What is Microsoft Soundscape\n\n\"Microsoft Soundscape is a product from Microsoft Research that explores the use of innovative audio-based technology to enable people to build a richer awareness of their surroundings, thus becoming more confident and empowered to get around. Unlike step-by-step navigation apps, Soundscape uses 3D audio cues to enrich ambient awareness and provide a new way to relate to the environment. It allows you to build a mental map and make personal route choices while being more comfortable within unfamiliar spaces. Soundscape is designed to be used by everyone and live in the background; therefore, feel free to use it in conjunction with other apps such as podcasts, audio books, email and even GPS navigation!\"\n\nAdditional app features include:\n\n* **Guided Routes** - Using the web authoring tool, users can create and share guided routes for Soundscape.\n* **Street Preview** - An audio virtual reality experience which places the user on a location with the ability to explore the road graph.\n* **Head tracking** (with supported headsets) - Allows the points of interest to stay in place as the user moves the head.\n* **Background Use** - The ability to run in the background while you use other apps.\n* **Current Location** - Quickly hear your current location and direction of travel.\n\n# Expectations\n\nThis open source project is not a turnkey equivalent of the Microsoft Soundscape product offering.  The sources have been modified to remove branding and IP.  References to the production resources were also altered.  Further elements too specific to Microsoft's internal environmen were omitted.\n\nTo bring this up this open source project, you'll need:\n* iOS experience -- Apple developer account, experience with Swift, Xcode, AppStore processes, etc.\n* Cloud experience -- The services ran in the the Azure Cloud, though could be adapted to run elsewhere. In particular, note that no automation has been included to provision the required resources and services.\n\nThe core elements of the  service eg. the OSM ingester via [imposm3](https://github.com/omniscale/imposm3) and serving the ingested data as GeoJSON are provided and packaged as containers in svcs/data.\n\nMicrosoft is committed to supporting this open source offering.  Please use the [Issues](https://github.com/microsoft/soundscape/issues) section to ask questions.\n\n# Contents\n\nThe open source project contains three components:\n\n| Component | Sources | Documentation |\n| --------- | ------- | ------------- |\n| Soundscape iOS Client app| [dir](./apps/ios) | [docs](docs/Client.md) |\n| Service backend | [dir](./svcs/data) | [docs](docs/Services.md) |\n| Authoring web app | [dir](./svcs/soundscape-authoring) | [docs](docs/Authoring.md) |\n\n# Trademark Notice\n\nTrademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "soundscape", "org_name": "microsoft", "org_repo": "microsoft/soundscape", "platform_org_repo": "github+microsoft/soundscape", "link_to_repo": "https://github.com/microsoft/soundscape", "platform": "github", "language": "Swift", "stargazers_count": 69, "watchers_count": 69}, {"README_text": "# Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions (ICLR'23)\n<p align=\"left\">\n    <a href=\"https://img.shields.io/github/license/niansong1996/lever\">\n        <img src=\"https://img.shields.io/github/license/niansong1996/lever\">\n    </a>\n    <a href=\"https://img.shields.io/github/last-commit/HKUNLP/Binder?color=green\">\n        <img src=\"https://img.shields.io/github/last-commit/HKUNLP/Binder?color=green\">\n    </a>\n    <a href=\"https://img.shields.io/badge/PRs-Welcome-red\">\n        <img src=\"https://img.shields.io/badge/PRs-Welcome-red\">\n    </a>\n    <br/>\n</p>\n\nCode for paper [Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions](https://arxiv.org/abs/2205.14318). In this work, we propose to let the model perform sampling during training and learn from those self-sampled correct or partially-correct solutions, which are automatically identified by comparing the final or intermediate execution states. An example is shown as below.\n<p align=\"center\">\n    <img src=\"example.png\" width=\"60%\">\n</p>\n\n## Updates\n- **2023-03-08**: Initial code release\n- **2023-02-17**: Camera-ready version updated on [arxiv](https://arxiv.org/abs/2205.14318)\n- **2023-01-20**: Paper is accepted at ICLR 2023\n\n## Environment Setup\n> **Note: all of the following has only been tested on Linux machines, you may need to build your own `tree-sitter` parsers if a different platform is used.**  \n\n*(Recommended)* Create a new conda environment\n```bash\nconda create -n trace-codegen python=3.8\nconda activate trace-codegen\n```\nClone the code and install the dependencies\n```bash\ngit clone git@github.com:microsoft/TraceCodegen\ncd TraceCodegen\npip install -r requirements.txt\n```\n*(Optional)* Set up `wandb` for experiment tracking. First following [wandb documentation](https://docs.wandb.ai/ref/cli/wandb-login) to login, then change the following lines in `trainer.logger+` fields of the `yaml` config file you would like to run:\n```yaml\nentity: <your-user/org-name>\nproject: <your-project-name>\n```\n*(Optional)* At any point, if you met with the Python import problem (e.g., `ModuleNotFoundError`), try doing this in the main (`TraceCodegen`) directory:\n```bash\nexport PYTHONPATH=`pwd`\n```\n\n## Data and Preprocessing\nWe conduct experiments on the [MathQA-Python](https://arxiv.org/abs/2108.07732) and [GSM8k](https://github.com/openai/grade-school-math) datasets. As they have different licenses and preprocessing pipelines, here we describe them separately. But first, let's make a `data` directory:\n```bash\nmkdir data\n```\n### MathQA-Python\nFirst follow [this script](https://github.com/google/trax/blob/master/trax/examples/MathQA_Python_generation_notebook.ipynb) for generation the MathQA-Python dataset from the [original MathQA dataset](https://math-qa.github.io/). After that, make sure your data directory looks something like this:\n```\ndata\n|-- mathqa\n|   |-- train-python.jsonl\n|   |-- val-python.jsonl\n|   |-- test-python.jsonl\n|---...\n```\nWe preprocess MathQA-Python by respliting the data with template-based deduplication (see detail in paper). To do this, run the preprocessing script with the following:\n```bash\npython resplit_mathqa_python.py\n```\nAfter this, your `data` directory should now look something like this:\n```\ndata\n|-- mathqa\n|   |-- train-python.jsonl\n|   |-- val-python.jsonl\n|   |-- test-python.jsonl\n|   |-- train_dedup.jsonl\n|   |-- val_dedup.jsonl\n|---...\n```\nNote that we only combine and resplit the orignal train and validation set, and the test set kept untouched.\n\n### GSM8k\nAs the solution to GSM8k questions are originally annotated as math formulas, we used a script to automatically extract \nMathQA-Python style programs as solutions. To replicate this, first download the data from the \n[original GSM8k repo](https://github.com/openai/grade-school-math/tree/master/grade_school_math/data). After that, your `data` directory should look like this:\n```\ndata\n|-- gsmath\n|   |-- train.jsonl\n|   |-- test.jsonl\n|   |-- ...\n|---...\n```\nNow run the preprocessing script for GSM8k:\n```bash\npython preprocessing/preprocess_gsm8k.py\n```\nAfter this, your `data` directory should look like this:\n```\ndata\n|-- gsmath\n|   |-- train.jsonl\n|   |-- test.jsonl\n|   |-- gsmath_train.jsonl\n|   |-- gsmath_val.jsonl\n|   |-- gsmath_test.jsonl\n|   |-- ...\n|---...\n```\n\n## Model Training\nOur training framework is built on top of [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) (version 1.5.10). More specifically, you would only need to change the `yaml` configuration files if you would like to adjust the hyperparameters (e.g., batch size, gpus, dataset file, etc). \n\n> **Note: To run model training, we recommend using GPUs that have at least 32GiB of memory, or decrease the training batch size accordingly. All our experiments are conducted on 8x V100-32GB GPUs.**\n\n### Basic usage\n```bash\npython trainer.py fit --config <config_file_path>.yaml\n```\nExisting `yaml` config files can be found in `training_configs`, you can also find all the hyperparameter settings (e.g., batch size) in the Appendix of the paper. \n\n### Using different transformer models\nIf you would like to switch between `GPT-Neo-125M` and `GPT-Neo-2.7B` models, be sure to change the following fields in the `yaml` config file:\n```yaml\ntrainer:\n  ...\n  strategy: deepspeed_stage_2_offload # for 2.7B, or \"ddp_find_unused_parameters_false\" for 125M\n...\nmodel:\n  class_path: ...\n  init_args:\n    transformer_model_name: &transformer EleutherAI/gpt-neo-2.7B # or EleutherAI/gpt-neo-125M\n...\ndata:\n  class_path: ...\n  init_args:\n    ...\n    batch_size: 2 # [Optional] change this according to the GPU memory\n    val_batch_size: 4 # [Optional] change this according to the GPU memory\n...\n```\n\n### Using different datasets\nSince the MathQA-Python and GSM8k datasets are in the same format after preprocessing, you just need to change the file paths in the following fields of the `yaml` config file:\n```yaml\ndata:\n    ...\n    train_file_path: data/mathqa/train_dedup.jsonl # or \"data/gsmath/gsmath_train.jsonl\" for gsm8k\n    val_file_path: data/mathqa/val_dedup.jsonl # or \"data/gsmath/gsmath_val.jsonl\" for gsm8k\n```\n\n### Use fully- or partially-correct self-sampled solutions\nTo this end, you just need to use different `yaml` config files in `training_configs`:\n```bash\ntraining_configs/gpt_self_sampling.yaml # for using self-sampled fully-correct solutions only\ntraining_configs/gpt_self_sampling_partial.yaml # for also using self-sampled partially-correct solutions\n```\n\n### Using different learning objectives\n- For the MLE baseline, just run with the config file of `training_configs/gpt_mle.yaml`\n- For running MML, set the following in the `yaml` config file:\n```yaml\nmodel:\n    ...\n    init_args:\n        ...\n        mle_lambda: 0.0\n        mml_lambda: 1.0\n    ...\n```\n- For running $\\beta$-MML, keep the above and set `beta_smoothing: <beta>`\n- For running MLE-Aug, set `mle_lambda: 1.0` and `mml_lambda: 0.0` in above.\n\n### All other hyperparameters\nFor all other hyperparameters, please read the rest of the fields in the `yaml` file and the corresponding `__init__` function in the corresponding class, or refer to the [pytorch-lightning documents](https://pytorch-lightning.readthedocs.io/en/1.5.10/).\n\n## Model Inference\nFor running model inference (e.g., on the test set), use the following command:\n```bash\npython trainer.py validate --config <your-config-file> --model.init_args.load_ckpt_file <saved_ckpt_file>\n```\n\n## Citation\nIf you use the code in this repository, consider cite:\n```bibtex\n@inproceedings{ni2023selfsampling,\n  title={Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions},\n  author={Ni, Ansong and Inala, Jeevana Priya and Wang, Chenglong and Polozov, Alex and Meek, Christopher and Radev, Dragomir and Gao, Jianfeng},\n  booktitle={The 2023 International Conference on Learning Representations}\n}\n```\nFor any questions, please open an issue. PRs are definitely welcomed, and please check the following section about contributing to this repo. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "TraceCodegen", "org_name": "microsoft", "org_repo": "microsoft/TraceCodegen", "platform_org_repo": "github+microsoft/TraceCodegen", "link_to_repo": "https://github.com/microsoft/TraceCodegen", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# ADO Extension for Shift-Left in Defender for DevOps\n\nThis repo contains the code for the ADO private extension that is installed on the customer's organization for injecting the scripts used to calculate the diff of the digests that were deployed as part of a pipeline run.\n\nMore information on ADO extensions - \nhttps://learn.microsoft.com/en-us/azure/devops/extend/develop/add-pipeline-decorator?view=azure-devops\nhttps://learn.microsoft.com/en-us/azure/devops/extend/get-started/node?view=azure-devops\n\n## Releasing new version \n\n1. Once the changes are commited, Update the version by modifying the VERSION file.\n2. Execute the `ReplaceVersion.ps1` script to update the version in `vss-extension.json` file and the `DfDExtension/DfDPostJobExtension.yml` file.\n3. From PowerShell/bash shell, run from the extension root directory \n    > npx tfx-cli extension create\n4. Publish and share the extension from marketplace\n`https://marketplace.visualstudio.com/manage/publishers/<Your-publisher-name>`\n    - For releasing to public, use the [Microsoft Dfd Publisher](https://marketplace.visualstudio.com/manage/publishers/ms-securitydevops?noPrompt=true)\n    - For local testing, use your own publisher name\n5. From ADO organization settings/Extension - add the extension. More information on publishing extension can be found [here](https://learn.microsoft.com/en-us/azure/devops/extend/publish/overview?view=azure-devops).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dfd-sl-ado-extension", "org_name": "microsoft", "org_repo": "microsoft/dfd-sl-ado-extension", "platform_org_repo": "github+microsoft/dfd-sl-ado-extension", "link_to_repo": "https://github.com/microsoft/dfd-sl-ado-extension", "platform": "github", "language": "PowerShell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# ViSNet: a scalable and accurate geometric deep learning potential for molecular dynamics simulation\n\n## Overview\n\n[ViSNet](https://arxiv.org/pdf/2210.16518.pdf) (shorted for \u201c**V**ector-**S**calar **i**nteractive graph neural **Net**work\u201d) is a scalable and accurate graph deep learning potential for molecular dynamics that significantly alleviate the dilemma between computational costs and sufficient utilization of geometric information.\n\n<img src=\"visnet_arch.jpg\" width=100%> \n\n## News\n\n### Nov 2022\n- *ViSNet Team* won the 2nd place in the [OGB-LSC @ NeurIPS 2022 PCQM4Mv2 Track](https://ogb.stanford.edu/neurips2022/results/)! Please check out the branch [OGB-LSC@NIPS2022](https://github.com/microsoft/ViSNet/tree/OGB-LSC%40NIPS2022) and give it a star if you find it useful!\n- The paper of ViSNet is under-review. We will release the codebase until it is accepted.\n\n## Citation\n\nIf you find this work useful, please kindly cite following paper:\n\n```latex\n@article{wang2022visnet,\n  title={ViSNet: a scalable and accurate geometric deep learning potential for molecular dynamics simulation},\n  author={Wang, Yusong and Li, Shaoning and He, Xinheng and Li, Mingyu and Wang, Zun and Zheng, Nanning and Shao, Bin and Wang, Tong and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2210.16518},\n  year={2022}\n}\n```\n\n## Contact\n\nPlease contact Tong Wang (watong@microsoft.com) for technical support.\n\n## License\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/ViSNet/blob/main/LICENSE) for additional details.\n", "repo_name": "ViSNet", "org_name": "microsoft", "org_repo": "microsoft/ViSNet", "platform_org_repo": "github+microsoft/ViSNet", "link_to_repo": "https://github.com/microsoft/ViSNet", "platform": "github", "language": null, "stargazers_count": 28, "watchers_count": 28}, {"README_text": "<div align=\"right\"><a href=\"https://guidance.readthedocs.org\"><img src=\"https://readthedocs.org/projects/guidance/badge/?version=latest&style=flat\" /></a></div>\n<div align=\"center\"><picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/figures/guidance_logo_blue_dark.svg\">\n  <img alt=\"guidance\" src=\"docs/figures/guidance_logo_blue.svg\" width=300\">\n</picture></div>\n<br/>\n\n> _Where there is no guidance, a model fails, but in an abundance of instructions there is safety._  \n_\\- <a href=\"notebooks/proverb.ipynb\">GPT 11:14</a>_\n\n<!--It expands the API of language models so you can craft rich output structure, design precise tool use, create multi-agent interactions, and much more all while using clear code and maximum inference efficiency.-->\n<b>Guidance</b> enables you to control modern language models more effectively and efficiently than traditional prompting or chaining. Guidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text. Simple output structures like [Chain of Thought](https://arxiv.org/abs/2201.11903) and its many variants (e.g., [ART](https://arxiv.org/abs/2303.09014), [Auto-CoT](https://arxiv.org/abs/2210.03493), etc.) have been shown to improve LLM performance. The advent of more powerful LLMs like [GPT-4](https://openai.com/research/gpt-4) allows for even richer structure, and `guidance` makes that structure easier and cheaper.\n\nFeatures:\n- [x] Simple, intuitive syntax, based on [Handlebars](https://handlebarsjs.com/) templating.\n- [x] Rich output structure with multiple generations, selections, conditionals, tool use, etc.\n- [x] Playground-like streaming in Jupyter/VSCode Notebooks.\n- [x] Smart seed-based generation caching.\n- [x] Support for role-based chat models (e.g., [ChatGPT](https://beta.openai.com/docs/guides/chat)).\n- [x] Easy integration with Hugging Face models, including [guidance acceleration](notebooks/guidance_acceleration.ipynb) for speedups over standard prompting, [token healing](notebooks/token_healing.ipynb) to optimize prompt boundaries, and [regex pattern guides](notebooks/pattern_guides.ipynb) to enforce formats.\n\n## Install\n\n```python\npip install guidance\n```\n\n\n                                     \n## Live streaming (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/proverb.ipynb\">notebook</a>)\n\nSpeed up your prompt development cycle by streaming complex templates and generations live in your notebook. At first glance, Guidance feels like a templating language, and just like standard <a href=\"https://handlebarsjs.com\">Handlebars</a> templates, you can do variable interpolation (e.g., `{{proverb}}`) and logical control. But unlike standard templating languages, guidance programs have a well defined linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (using the `{{gen}}` command) or make logical control flow decisions. This interleaving of generation and prompting allows for precise output structure that produces clear and parsable results.\n\n```python\nimport guidance\n\n# set the default language model used to execute guidance programs\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\")\n\n# define a guidance program that adapts a proverb\nprogram = guidance(\"\"\"Tweak this proverb to apply to model instructions instead.\n\n{{proverb}}\n- {{book}} {{chapter}}:{{verse}}\n\nUPDATED\nWhere there is no guidance{{gen 'rewrite' stop=\"\\\\n-\"}}\n- GPT {{#select 'chapter'}}9{{or}}10{{or}}11{{/select}}:{{gen 'verse'}}\"\"\")\n\n# execute the program on a specific proverb\nexecuted_program = program(\n    proverb=\"Where there is no guidance, a people falls,\\nbut in an abundance of counselors there is safety.\",\n    book=\"Proverbs\",\n    chapter=11,\n    verse=14\n)\n```\n<img src=\"docs/figures/proverb_animation.gif\" width=\"404\">\n\nAfter a program is executed, all the generated variables are now easily accessible:\n\n```python\nexecuted_program[\"rewrite\"]\n```\n> ', a model fails,\\nbut in an abundance of instructions there is safety.'\n\n## Chat dialog (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/chat.ipynb\">notebook</a>)\n\nGuidance supports API-based chat models like GPT-4, as well as open chat models like Vicuna through a unified API based on role tags (e.g., `{{#system}}...{{/system}}`). This allows interactive dialog development that combines rich templating and logical control with modern chat models.\n\n```python\n# connect to a chat model like GPT-4 or Vicuna\ngpt4 = guidance.llms.OpenAI(\"gpt-4\")\n# vicuna = guidance.llms.transformers.Vicuna(\"your_path/vicuna_13B\", device_map=\"auto\")\n\nexperts = guidance('''\n{{#system~}}\nYou are a helpful and terse assistant.\n{{~/system}}\n\n{{#user~}}\nI want a response to the following question:\n{{query}}\nName 3 world-class experts (past or present) who would be great at answering this?\nDon't answer the question yet.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'expert_names' temperature=0 max_tokens=300}}\n{{~/assistant}}\n\n{{#user~}}\nGreat, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'answer' temperature=0 max_tokens=500}}\n{{~/assistant}}\n''', llm=gpt4)\n\nexperts(query='How can I be more productive?')\n```\n<img src=\"docs/figures/chat_animation.gif\" width=\"619\">\n\n## Guidance acceleration (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/guidance_acceleration.ipynb\">notebook</a>)\n\nWhen multiple generation or LLM-directed control flow statements are used in a single Guidance program then we can significantly improve inference performance by optimally reusing the Key/Value caches as we progress through the prompt. This means Guidance only asks the LLM to generate the green text below, not the entire program. **This cuts this prompt's runtime in half vs. a standard generation approach.** \n\n````python\n# we use LLaMA here, but any GPT-style model will do\nllama = guidance.llms.Transformers(\"your_path/llama-7b\", device=0)\n\n# we can pre-define valid option sets\nvalid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n\n# define the prompt\ncharacter_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n```json\n{\n    \"id\": \"{{id}}\",\n    \"description\": \"{{description}}\",\n    \"name\": \"{{gen 'name'}}\",\n    \"age\": {{gen 'age' pattern='[0-9]+' stop=','}},\n    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n    \"weapon\": \"{{select 'weapon' options=valid_weapons}}\",\n    \"class\": \"{{gen 'class'}}\",\n    \"mantra\": \"{{gen 'mantra' temperature=0.7}}\",\n    \"strength\": {{gen 'strength' pattern='[0-9]+' stop=','}},\n    \"items\": [{{#geneach 'items' num_iterations=5 join=', '}}\"{{gen 'this' temperature=0.7}}\"{{/geneach}}]\n}```\"\"\")\n\n# generate a character\ncharacter_maker(\n    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\n    description=\"A quick and nimble fighter.\",\n    valid_weapons=valid_weapons, llm=llama\n)\n````\n<img src=\"docs/figures/json_animation.gif\" width=\"565\">\n\nThe prompt above typically takes just over 2.5 seconds to complete on a A6000 GPU when using LLaMA 7B. If we were to run the same prompt adapted to be a single generation call (the standard practice today) it takes about 5 seconds to complete (4 of which is token generation and 1 of which is prompt processing). *This means Guidance acceleration delivers a 2x speedup over the standard approach for this prompt.* In practice the exact speed-up factor depends on the format of your specific prompt and the size of your model (larger models benefit more). Acceleration is also only supported for Transformers LLMs at the moment. See the [notebook](https://github.com/microsoft/guidance/blob/main/notebooks/guidance_acceleration.ipynb) for more details.\n\n## Token healing (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb\">notebook</a>)\n\nThe standard greedy tokenizations used by most language models introduce a subtle and powerful bias that can have all kinds of unintended consequences for your prompts. Using a process we call \"token healing\" `guidance` automatically removes these surprising biases, freeing you to focus on designing the prompts you want without worrying about tokenization artifacts.\n\nConsider the following example, where we are trying to generate an HTTP URL string:\n\n```python\n# we use StableLM as an open example, but these issues impact all models to varying degrees\nguidance.llm = guidance.llms.Transformers(\"stabilityai/stablelm-base-alpha-3b\", device=0)\n\n# we turn token healing off so that guidance acts like a normal prompting library\nprogram = guidance('''The link is <a href=\"http:{{gen max_tokens=10 token_healing=False}}''')\nprogram()\n```\n<img src=\"docs/figures/url_with_space.png\" width=\"372\">\n\nNote that the output generated by the LLM does not complete the URL with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. Why? Because the string \"://\" is its own token (`1358`), and so once the model sees a colon by itself (token `27`), it assumes that the next characters cannot be \"//\"; otherwise, the tokenizer would not have used `27` and instead would have used `1358` (the token for \"://\").\n\nThis bias is not just limited to the colon character -- it happens everywhere. *Over 70% of the 10k most common tokens for the StableLM model used above are prefixes of longer possible tokens, and so cause token boundary bias when they are the last token in a prompt.* For example the \":\" token `27` has **34** possible extensions, the \" the\" token `1735` has **51** extensions, and the \" \" (space) token `209` has **28,802** extensions).\n\n`guidance` eliminates these biases by backing up the model by one token then allowing the model to step forward while constraining it to only generate tokens whose prefix matches the last token. This \"token healing\" process eliminates token boundary biases and allows any prompt to be completed naturally:\n\n```python\nguidance('The link is <a href=\"http:{{gen max_tokens=10}}')()\n```\n<img src=\"docs/figures/url_without_space.png\" width=\"362\">\n\n## Rich output structure example ([notebook](notebooks/anachronism.ipynb))\n\nTo demonstrate the value of output structure, we take [a simple task](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms) from BigBench, where the goal is to identify whether a given sentence contains an anachronism (a statement that is impossible because of non-overlapping time periods). Below is a simple two-shot prompt for it, with a human-crafted chain-of-thought sequence.\n\nGuidance programs, like standard Handlebars templates, allow both variable interpolation (e.g., `{{input}}`) and logical control. But unlike standard templating languages, guidance programs have a unique linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (the `{{gen}}` command) or make logical control flow decisions (the `{{#select}}...{{or}}...{{/select}}` command). This interleaving of generation and prompting allows for precise output structure that improves accuracy while also producing clear and parsable results.\n```python\nimport guidance\n                                                      \n# set the default language model used to execute guidance programs\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\") \n\n# define the few shot examples\nexamples = [\n    {'input': 'I wrote about shakespeare',\n    'entities': [{'entity': 'I', 'time': 'present'}, {'entity': 'Shakespeare', 'time': '16th century'}],\n    'reasoning': 'I can write about Shakespeare because he lived in the past with respect to me.',\n    'answer': 'No'},\n    {'input': 'Shakespeare wrote about me',\n    'entities': [{'entity': 'Shakespeare', 'time': '16th century'}, {'entity': 'I', 'time': 'present'}],\n    'reasoning': 'Shakespeare cannot have written about me, because he died before I was born',\n    'answer': 'Yes'}\n]\n\n# define the guidance program\nstructure_program = guidance(\n'''Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).\n----\n\n{{~! display the few-shot examples ~}}\n{{~#each examples}}\nSentence: {{this.input}}\nEntities and dates:{{#each this.entities}}\n{{this.entity}}: {{this.time}}{{/each}}\nReasoning: {{this.reasoning}}\nAnachronism: {{this.answer}}\n---\n{{~/each}}\n\n{{~! place the real question at the end }}\nSentence: {{input}}\nEntities and dates:\n{{gen \"entities\"}}\nReasoning:{{gen \"reasoning\"}}\nAnachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}''')\n\n# execute the program\nout = structure_program(\n    examples=examples,\n    input='The T-rex bit my dog'\n)\n```\n<img src=\"docs/figures/anachronism.png\" width=\"837\">\n\nAll of the generated program variables are now available in the executed program object:\n```python\nout[\"answer\"]\n```\n> ' Yes'\n\nWe [compute accuracy](notebooks/anachronism.ipynb) on the validation set, and compare it to using the same two-shot examples above **without** the output structure, as well as to the best reported result [here](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms). The results below agree with existing literature, in that even a very simple output structure drastically improves performance, even compared against much larger models.\n| Model | Accuracy |\n| :---: | :---: |\n| [Few-shot learning with guidance examples, no CoT output structure](notebooks/anachronism.ipynb) | 63.04% |\n| [PALM (3-shot)](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms) | Around 69% |\n| [Guidance](notebooks/anachronism.ipynb) | **76.01%** |\n\n## Guaranteeing valid syntax JSON example ([notebook](notebooks/guaranteeing_valid_syntax.ipynb))\n\nLarge language models are great at generating useful outputs, but they are not great at guaranteeing that those outputs follow a specific format. This can cause problems when we want to use the outputs of a language model as input to another system. For example, if we want to use a language model to generate a JSON object, we need to make sure that the output is valid JSON. With `guidance` we can both [accelerate inference speed](notebooks/guidance_acceleration.ipynb) and ensure that generated JSON is always valid. Below we generate a random character profile for a game with perfect syntax every time:\n```python\n# load a model locally (we use LLaMA here)\nguidance.llm = guidance.llms.Transformers(\"your_local_path/llama-7b\", device=0)\n\n# we can pre-define valid option sets\nvalid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n\n# define the prompt\nprogram = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\n```json\n{\n    \"description\": \"{{description}}\",\n    \"name\": \"{{gen 'name'}}\",\n    \"age\": {{gen 'age' pattern='[0-9]+' stop=','}},\n    \"armor\": \"{{#select 'armor'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\n    \"weapon\": \"{{select 'weapon' options=valid_weapons}}\",\n    \"class\": \"{{gen 'class'}}\",\n    \"mantra\": \"{{gen 'mantra'}}\",\n    \"strength\": {{gen 'strength' pattern='[0-9]+' stop=','}},\n    \"items\": [{{#geneach 'items' num_iterations=3}}\n        \"{{gen 'this'}}\",{{/geneach}}\n    ]\n}```\"\"\")\n\n# execute the prompt\nprogram(description=\"A quick and nimble fighter.\", valid_weapons=valid_weapons)\n```\n<img src=\"docs/figures/perfect_syntax.png\" width=\"657\">\n                                                      \n```python\n# and we also have a valid Python dictionary\nout.variables()\n```\n<img src=\"docs/figures/json_syntax_variables.png\" width=\"714\">\n                                                      \n## Role-based chat model example ([notebook](notebooks/chat.ipynb))\nModern chat-style models like ChatGPT and Alpaca are trained with special tokens that mark out \"roles\" for different areas of the prompt. Guidance supports these models through <a href=\"notebooks/api_examples/library/role.ipynb\">role tags</a> that automatically map to the correct tokens or API calls for the current LLM. Below we show how a role-based guidance program enables simple multi-step reasoning and planning.\n\n```python\nimport guidance\nimport re\n\n# we use GPT-4 here, but you could use gpt-3.5-turbo as well\nguidance.llm = guidance.llms.OpenAI(\"gpt-4\")\n\n# a custom function we will call in the guidance program\ndef parse_best(prosandcons, options):\n    best = int(re.findall(r'Best=(\\d+)', prosandcons)[0])\n    return options[best]\n\n# define the guidance program using role tags (like `{{#system}}...{{/system}}`)\ncreate_plan = guidance('''\n{{#system~}}\nYou are a helpful assistant.\n{{~/system}}\n\n{{! generate five potential ways to accomplish a goal }}\n{{#block hidden=True}}\n{{#user~}}\nI want to {{goal}}.\n{{~! generate potential options ~}}\nCan you please generate one option for how to accomplish this?\nPlease make the option very short, at most one line.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'options' n=5 temperature=1.0 max_tokens=500}}\n{{~/assistant}}\n{{/block}}\n\n{{! generate pros and cons for each option and select the best option }}\n{{#block hidden=True}}\n{{#user~}}\nI want to {{goal}}.\n\nCan you please comment on the pros and cons of each of the following options, and then pick the best option?\n---{{#each options}}\nOption {{@index}}: {{this}}{{/each}}\n---\nPlease discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the best option.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'prosandcons' temperature=0.0 max_tokens=500}}\n{{~/assistant}}\n{{/block}}\n\n{{! generate a plan to accomplish the chosen option }}\n{{#user~}}\nI want to {{goal}}.\n{{~! Create a plan }}\nHere is my plan:\n{{parse_best prosandcons options}}\nPlease elaborate on this plan, and tell me how to best accomplish it.\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'plan' max_tokens=500}}\n{{~/assistant}}''')\n\n# execute the program for a specific goal\nout = create_plan(\n    goal='read more books',\n    parse_best=parse_best # a custom Python function we call in the program\n)\n```\n<img src=\"docs/figures/chat_reading.png\" width=\"935\">\n\nThis prompt/program is a bit more complicated, but we are basically going through 3 steps:\n1. Generate a few options for how to accomplish the goal. Note that we generate with `n=5`, such that each option is a separate generation (and is not impacted by the other options). We set `temperature=1` to encourage diversity.\n2. Generate pros and cons for each option, and select the best one. We set `temperature=0` to encourage the model to be more precise.\n3. Generate a plan for the best option, and ask the model to elaborate on it. Notice that steps 1 and 2 were `hidden`, which means GPT-4 does not see them when generating content that comes later (in this case, that means when generating the plan). This is a simple way to make the model focus on the current step.\n\nSince steps 1 and 2 are hidden, they do not appear on the generated output (except briefly during stream), but we can print the variables that these steps generated:\n```python\nprint('\\n'.join(['Option %d: %s' % (i, x) for i, x in enumerate(out['options'])]))\n```\n> Option 0: Set a goal to read for 20 minutes every day before bedtime.  \nOption 1: Join a book club for increased motivation and accountability.  \nOption 2: Set a daily goal to read for 20 minutes.  \nOption 3: Set a daily reminder to read for at least 20 minutes.  \nOption 4: Set a daily goal to read at least one chapter or 20 pages.  \n\n```python\nprint(out['prosandcons'])\n``` \n\n> Option 0:  \n> Pros: Establishes a consistent reading routine.  \n> Cons: May not be suitable for those with varying schedules.  \n> \\---  \n> Option 1:  \n> Pros: Provides social motivation and accountability.  \n> Cons: May not align with personal reading preferences.  \n> \\---  \n> Option 2:  \n> Pros: Encourages daily reading habit.  \n> Cons: Lacks a specific time frame, which may lead to procrastination.  \n> \\---  \n> Option 3:  \n> Pros: Acts as a daily reminder to prioritize reading.  \n> Cons: May become repetitive and easy to ignore.  \n> \\---  \n> Option 4:  \n> Pros: Sets a clear daily reading target.  \n> Cons: May be difficult to achieve on busy days or with longer chapters.  \n>   \n> Best=0 \n\n## Agents ([notebook](notebooks/chat.ipynb))\nWe can easily build agents that talk to each other or to a user, via the `await` command. The `await` command allows us to pause execution and return a partially executed guidance program. By putting `await` in a loop, that partially executed program can then be called again and again to form a dialog (or any other structure you design). For example, here is how we might get GPT-4 to simulate two agents talking to one another:\n\n```python\nimport guidance\nimport re\nguidance.llm = guidance.llms.OpenAI(\"gpt-4\")\nrole_simulator = guidance('''\n{{#system~}}\nYou are a helpful assistant\n{{~/system}}\n                                           \n{{#user~}}\nYou will answer the user as {{role}} in the following conversation. At every step, I will provide you with the user input, as well as a comment reminding you of your instructions. Never talk about the fact that you are an AI, even if the user asks you. Always answer as {{role}}.\n{{#if first_question}}You can also start the conversation.{{/if}}\n{{~/user}}\n                                           \n{{~! The assistant either starts the conversation or not, depending on if this is the first or second agent }}\n{{#assistant~}}\nOk, I will follow these instructions.\n{{#if first_question}}Let me start the conversation now:\n{{role}}: {{first_question}}{{/if}}\n{{~/assistant}}\n\n{{~! Then the conversation unrolls }}\n{{~#geneach 'conversation' stop=False}}\n{{#user~}}\nUser: {{set 'this.input' (await 'input')}}\nComment: Remember, answer as a {{role}}. Start your utterance with {{role}}:\n{{~/user}}\n\n{{#assistant~}}\n{{gen 'this.response' temperature=0 max_tokens=300}}\n{{~/assistant}}\n{{~/geneach}}''')\n\nrepublican = role_simulator(role='Republican', await_missing=True)\ndemocrat = role_simulator(role='Democrat', await_missing=True)\n\nfirst_question = '''What do you think is the best way to stop inflation?'''\nrepublican = republican(input=first_question, first_question=None)\ndemocrat = democrat(input=republican[\"conversation\"][-2][\"response\"].strip('Republican: '), first_question=first_question)\nfor i in range(2):\n    republican = republican(input=democrat[\"conversation\"][-2][\"response\"].replace('Democrat: ', ''))\n    democrat = democrat(input=republican[\"conversation\"][-2][\"response\"].replace('Republican: ', ''))\nprint('Democrat: ' + first_question)\nfor x in democrat['conversation'][:-1]:\n    print('Republican:', x['input'])\n    print()\n    print(x['response'])\n```\n> Democrat: What do you think is the best way to stop inflation?\n\n> Republican: The best way to stop inflation is by implementing sound fiscal policies, such as reducing government spending, lowering taxes, and promoting economic growth. Additionally, the Federal Reserve should focus on maintaining a stable monetary policy to control inflation.\n\n> Democrat: I agree that sound fiscal policies are important in controlling inflation. As a Democrat, I would emphasize the importance of investing in education, healthcare, and infrastructure to promote long-term economic growth. Additionally, we should ensure that the Federal Reserve maintains a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment.\n\n\n> Republican: While investing in education, healthcare, and infrastructure is important, we must also prioritize reducing the national debt and limiting government intervention in the economy. By lowering taxes and reducing regulations, we can encourage businesses to grow and create jobs, which will ultimately lead to long-term economic growth. As for the Federal Reserve, it's crucial to maintain a stable monetary policy that primarily focuses on controlling inflation, as this will create a more predictable economic environment for businesses and consumers.\n\n> Democrat: While reducing the national debt and limiting government intervention are valid concerns, Democrats believe that strategic investments in education, healthcare, and infrastructure can lead to long-term economic growth and job creation. We also support a progressive tax system that ensures everyone pays their fair share, which can help fund these investments. As for the Federal Reserve, we believe that a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment, is essential for a healthy economy. We must strike a balance between fiscal responsibility and investing in our nation's future.\n\n> Republican: It's important to find a balance between fiscal responsibility and investing in our nation's future. However, we believe that the best way to achieve long-term economic growth and job creation is through free-market principles, such as lower taxes and reduced regulations. This approach encourages businesses to expand and innovate, leading to a more prosperous economy. A progressive tax system can sometimes discourage growth and investment, so we advocate for a simpler, fairer tax system that promotes economic growth. Regarding the Federal Reserve, while promoting full employment is important, we must not lose sight of the primary goal of controlling inflation to maintain a stable and predictable economic environment.\n\n> Democrat: I understand your perspective on free-market principles, but Democrats believe that a certain level of government intervention is necessary to ensure a fair and equitable economy. We support a progressive tax system to reduce income inequality and provide essential services to those in need. Additionally, we believe that regulations are important to protect consumers, workers, and the environment. As for the Federal Reserve, we agree that controlling inflation is crucial, but we also believe that promoting full employment should be a priority. By finding a balance between these goals, we can create a more inclusive and prosperous economy for all Americans.\n\n## GPT4 + Bing\nLast example [here](notebooks/chat.ipynb).\n\n# API reference\nAll of the examples below are in [this notebook](notebooks/tutorial.ipynb).\n## Template syntax\nThe template syntax is based on [Handlebars](https://handlebarsjs.com/), with a few additions.   \nWhen `guidance` is called, it returns a Program:\n```python\nprompt = guidance('''What is {{example}}?''')\nprompt\n```\n> What is {{example}}?\n\nThe program can be executed by passing in arguments:\n```python\nprompt(example='truth')\n```\n> What is truth?\n\nArguments can be iterables:\n```python\npeople = ['John', 'Mary', 'Bob', 'Alice']\nideas = [{'name': 'truth', 'description': 'the state of being the case'},\n         {'name': 'love', 'description': 'a strong feeling of affection'},]\nprompt = guidance('''List of people:\n{{#each people}}- {{this}}\n{{~! This is a comment. The ~ removes adjacent whitespace either before or after a tag, depending on where you place it}}\n{{/each~}}\nList of ideas:\n{{#each ideas}}{{this.name}}: {{this.description}}\n{{/each}}''')\nprompt(people=people, ideas=ideas)\n```\n![template_objects](docs/figures/template_objs.png)\n\nNotice the special `~` character after `{{/each}}`.  \nThis can be added before or after any tag to remove all adjacent whitespace. Notice also the comment syntax: `{{! This is a comment }}`.\n\nYou can also include prompts/programs inside other prompts; e.g., here is how you could rewrite the prompt above:\n```python\nprompt1 = guidance('''List of people:\n{{#each people}}- {{this}}\n{{/each~}}''')\nprompt2 = guidance('''{{>prompt1}}\nList of ideas:\n{{#each ideas}}{{this.name}}: {{this.description}}\n{{/each}}''')\nprompt2(prompt1=prompt1, people=people, ideas=ideas)\n```\n\n## Generation\n### Basic generation\nThe `gen` tag is used to generate text. You can use whatever arguments are supported by the underlying model.\nExecuting a prompt calls the generation prompt:\n```python\nimport guidance\n# Set the default llm. Could also pass a different one as argument to guidance(), with guidance(llm=...)\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\")\nprompt = guidance('''The best thing about the beach is {{~gen 'best' temperature=0.7 max_tokens=7}}''')\nprompt = prompt()\nprompt\n```\n![generation1](docs/figures/generation1.png)  \n\n`guidance` caches all OpenAI generations with the same arguments. If you want to flush the cache, you can call `guidance.llms.OpenAI.cache.clear()`.\n\n### Selecting\nYou can select from a list of options using the `select` tag:\n```python\nprompt = guidance('''Is the following sentence offensive? Please answer with a single word, either \"Yes\", \"No\", or \"Maybe\".\nSentence: {{example}}\nAnswer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{or}} Maybe{{/select}}''')\nprompt = prompt(example='I hate tacos')\nprompt\n```\n![select](docs/figures/select.png)\n```python\nprompt['logprobs']\n```\n>{' Yes': -1.5689583, ' No': -7.332395, ' Maybe': -0.23746304}\n\n### Sequences of generate/select\nA prompt may contain multiple generations or selections, which will be executed in order:\n```python\nprompt = guidance('''Generate a response to the following email:\n{{email}}.\nResponse:{{gen \"response\"}}\n\nIs the response above offensive in any way? Please answer with a single word, either \"Yes\" or \"No\".\nAnswer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{/select}}''')\nprompt = prompt(email='I hate tacos')\nprompt\n```\n![generate_select](docs/figures/generate_select.png)\n```python\nprompt['response'], prompt['answer']\n```\n>(\" That's too bad! Tacos are one of my favorite meals.\", ' No')\n\n### Hidden generation\nYou can generate text without displaying it or using it in the subsequent generations using the `hidden` tag, either in a `block` or in a `gen` tag:\n```python\nprompt = guidance('''{{#block hidden=True}}Generate a response to the following email:\n{{email}}.\nResponse:{{gen \"response\"}}{{/block}}\nI will show you an email and a response, and you will tell me if it's offensive.\nEmail: {{email}}.\nResponse: {{response}}\nIs the response above offensive in any way? Please answer with a single word, either \"Yes\" or \"No\".\nAnswer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{/select}}''')\nprompt = prompt(email='I hate tacos')\nprompt\n```\n![hidden1](docs/figures/hidden1.png)\n\nNotice that nothing inside the hidden block shows up in the output (or was used by the `select`), even though we used the `response` generated variable in the subsequent generation.\n\n### Generate with `n>1`\nIf you use `n>1`, the variable will contain a list (there is a visualization that lets you navigate the list, too):\n```python\nprompt = guidance('''The best thing about the beach is {{~gen 'best' n=3 temperature=0.7 max_tokens=7}}''')\nprompt = prompt()\nprompt['best']\n```\n> [' that it is a great place to',\n ' being able to relax in the sun',\n \" that it's a great place to\"]\n\n ## Calling functions\n You can call any Python function using generated variables as arguments. The function will be called when the prompt is executed:\n ```python\ndef aggregate(best):\n    return '\\n'.join(['- ' + x for x in best])\nprompt = guidance('''The best thing about the beach is {{~gen 'best' n=3 temperature=0.7 max_tokens=7 hidden=True}}\n{{aggregate best}}''')\nprompt = prompt(aggregate=aggregate)\nprompt\n```\n![function](docs/figures/function.png)\n\n## Pausing execution with `await`\nAn `await` tag will stop program execution until that variable is provided:\n```python\nprompt = guidance('''Generate a response to the following email:\n{{email}}.\nResponse:{{gen \"response\"}}\n{{await 'instruction'}}\n{{gen 'updated_response'}}''', stream=True)\nprompt = prompt(email='Hello there')\nprompt\n```\n![await1](docs/figures/await1.png)\n\nNotice how the last `gen` is not executed because it depends on `instruction`. Let's provide `instruction` now:\n\n```python\nprompt = prompt(instruction='Please translate the response above to Portuguese.')\nprompt\n```\n![await2](docs/figures/await2.png)\n\nThe program is now executed all the way to the end.\n\n## Notebook functions\nEcho, stream. TODO @SCOTT\n\n## Chat (see also [this notebook](notebooks/chat.ipynb))\nIf you use an OpenAI LLM that only allows for ChatCompletion (`gpt-3.5-turbo` or `gpt-4`), you can use the special tags `{{#system}}`, `{{#user}}`, and `{{#assistant}}`:\n```python\nprompt = guidance(\n'''{{#system~}}\nYou are a helpful assistant.\n{{~/system}}\n{{#user~}}\n{{conversation_question}}\n{{~/user}}\n{{#assistant~}}\n{{gen 'response'}}\n{{~/assistant}}''')\nprompt = prompt(conversation_question='What is the meaning of life?')\nprompt\n```\n![chat1](docs/figures/chat1.png)\n\nSince partial completions are not allowed, you can't really use output structure _inside_ an assistant block, but you can still set up a structure outside of it. Here is an example (also in [here](notebooks/chat.ipynb)):\n```python\nexperts = guidance(\n'''{{#system~}}\nYou are a helpful assistant.\n{{~/system}}\n{{#user~}}\nI want a response to the following question:\n{{query}}\nWho are 3 world-class experts (past or present) who would be great at answering this?\nPlease don't answer the question or comment on it yet.\n{{~/user}}\n{{#assistant~}}\n{{gen 'experts' temperature=0 max_tokens=300}}\n{{~/assistant}}\n{{#user~}}\nGreat, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\nIn other words, their identity is not revealed, nor is the fact that there is a panel of experts answering the question.\nIf the experts would disagree, just present their different positions as alternatives in the answer itself (e.g., 'some might argue... others might argue...').\nPlease start your answer with ANSWER:\n{{~/user}}\n{{#assistant~}}\n{{gen 'answer' temperature=0 max_tokens=500}}\n{{~/assistant}}''')\nexperts(query='What is the meaning of life?')\n```\n\nYou can still use hidden blocks if you want to hide some of the conversation history for following generations:\n```python\nprompt = guidance(\n'''{{#system~}}\nYou are a helpful assistant.\n{{~/system}}\n{{#block hidden=True~}}\n{{#user~}}\nPlease tell me a joke\n{{~/user}}\n{{#assistant~}}\n{{gen 'joke'}}\n{{~/assistant}}\n{{~/block~}}\n{{#user~}}\nIs the following joke funny? Why or why not?\n{{joke}}\n{{~/user}}\n{{#assistant~}}\n{{gen 'funny'}}\n{{~/assistant}}''')\nprompt()\n```\n\n### Agents with `geneach`\nYou can combine the `await` tag with `geneach` (which generates a list) to create an agent easily:\n```\nprompt = guidance(\n'''{{#system~}}\nYou are a helpful assistant\n{{~/system}}\n{{~#geneach 'conversation' stop=False}}\n{{#user~}}\n{{set 'this.user_text' (await 'user_text')}}\n{{~/user}}\n{{#assistant~}}\n{{gen 'this.ai_text' temperature=0 max_tokens=300}}\n{{~/assistant}}\n{{~/geneach}}''')\nprompt= prompt(user_text ='hi there')\nprompt\n```\n\nNotice how the next iteration of the conversation is still templated, and how the conversation list has a placeholder as the last element:\n```python\nprompt['conversation']\n```\n>[{'user_text': 'hi there',\n  'ai_text': 'Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.'},\n {}]\n\n We can then execute the prompt again, and it will generate the next round:\n\n ```python\n prompt = prompt(user_text = 'What is the meaning of life?')\nprompt\n```\nSee a more elaborate example [here](notebooks/chat.ipynb).\n\n### Using tools\nSee the 'Using a search API' example in [this notebook](notebooks/chat.ipynb).\n", "repo_name": "guidance", "org_name": "microsoft", "org_repo": "microsoft/guidance", "platform_org_repo": "github+microsoft/guidance", "link_to_repo": "https://github.com/microsoft/guidance", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 9527, "watchers_count": 9527}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureStackHCI-Liftoff", "org_name": "microsoft", "org_repo": "microsoft/AzureStackHCI-Liftoff", "platform_org_repo": "github+microsoft/AzureStackHCI-Liftoff", "link_to_repo": "https://github.com/microsoft/AzureStackHCI-Liftoff", "platform": "github", "language": "PowerShell", "stargazers_count": 17, "watchers_count": 17}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "digital-content-distribution-toolkit-infra", "org_name": "microsoft", "org_repo": "microsoft/digital-content-distribution-toolkit-infra", "platform_org_repo": "github+microsoft/digital-content-distribution-toolkit-infra", "link_to_repo": "https://github.com/microsoft/digital-content-distribution-toolkit-infra", "platform": "github", "language": "HCL", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "digital-content-distribution-toolkit-edge", "org_name": "microsoft", "org_repo": "microsoft/digital-content-distribution-toolkit-edge", "platform_org_repo": "github+microsoft/digital-content-distribution-toolkit-edge", "link_to_repo": "https://github.com/microsoft/digital-content-distribution-toolkit-edge", "platform": "github", "language": "Go", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# [Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning](https://openreview.net/forum?id=9EAQVEINuum)\n\n## Introduction\nThis is the repository for BINDER ([**BI**-encoder for **N**ame**D** **E**ntity **R**ecognition via Contrastive Learning](https://openreview.net/forum?id=9EAQVEINuum)) accepted at ICLR 2023.\n\nBINDER employs two encoders to separately map text and entity types\ninto the same vector space, and reuses the vector representations of entity types for different text spans (or vice versa), resulting in a faster training and inference speed.\nBased on the bi-encoder representations, BINDER introduces a unified contrastive learning framework for NER, which encourages the representation of entity types to be similar with the corresponding\nentity mentions, and to be dissimilar with non-entity text spans.\nBINDER also introudces a novel dynamic thresholding loss in contrastive learning. At test time, it leverages candidate-specific dynamic thresholds to distinguish entity spans from non-entity ones.\nCheck out [our paper](https://openreview.net/forum?id=9EAQVEINuum) for the details.\n\nIf you find our code is useful, please cite:\n```bib\n@article{zhang-etal-2022-binder,\n  title={Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning},\n  author={Zhang, Sheng and Cheng, Hao and Gao, Jianfeng and Poon, Hoifung},\n  journal={arXiv preprint arXiv:2208.14565},\n  year={2022}\n}\n```\n\n\n## Quick Start\n### 1. Data Preparation\n\nFollow the instructions [README.md](data_preproc/README.md) in the data_preproc folder.\n\n\n### 2. Environment Setup\n```bash\nconda create -n binder -y python=3.9\nconda activate binder\nconda install pytorch==1.13 pytorch-cuda=11.6 -c pytorch -c nvidia\npip install transformers==4.24.0 datasets==2.6.1 wandb==0.13.5 seqeval==1.2.2\n```\n\n### 3. Experiment Run\nAssuming you have prepared data for ACE2005 and finished environment setup, below is the command to run an experiment on ACE2005:\n```bash\npython run_ner.py conf/ace05.json\n```\n\nTo run experiments on other datasets, simply change the config.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "binder", "org_name": "microsoft", "org_repo": "microsoft/binder", "platform_org_repo": "github+microsoft/binder", "link_to_repo": "https://github.com/microsoft/binder", "platform": "github", "language": "Python", "stargazers_count": 50, "watchers_count": 50}, {"README_text": "# Boosting Natural Language Generation from Instructions with Meta-Learning\n\nBudhaditya Deb, Guoqing Zheng, Ahmed Hassan Awadallah, EMNLP 2022\nhttps://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.456/ \n\n\n# Setup\n\n## Setting up the Data Directory\n- Download data from repository: allenai/natural-instructions-expansion: Expanding natural instructions. https://github.com/allenai/natural-instructions-expansion\n- Run data_utils.py for some basic splits and preprocessing of the NI dataset\n\n\n## Training and evaluation\n\nUse python 3.9. Install Pytorch 1.9, torchvision 0.10 and torchaudio 0.9 for your cuda version. For example:\n  - conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia\n  - OR, pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n  - OR, conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=11.3 -c pytorch -c conda-forge\n\nInstall remaining packages from `requirements.txt` file\n- transformers\n- datasets nltk learn2learn rouge-metric rouge_score\n\n\n# Example Command Lines for Training \n(please change the paths according to your installations)\n\n## Standard with BART\nCUDA_VISIBLE_DEVICES=2 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2.5/standard/6/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2.5/ \\\n--model_name_or_path facebook/bart-base \\\n--tokenizer_name facebook/bart-base \\\n--dataset_name=\"natural_instructions_v2.5\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--per_device_train_batch_size 4 --per_device_eval_batch_size 8 \\\n--max_source_length 1024 --max_target_length 128 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--metric_for_best_model rouge-l-f --multi_ref_mode best \\\n--adafactor True --fp16 False  --lr_scheduler_type linear  \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction True \\\n--add_positive_examples True \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 1 \\\n--train_loop_opt_type standard \\\n--filter_dataset_by_length True  \\\n--disable_tqdm True --save_checkpoints True  --resume_from_checkpoint True --show_example_predictions 0 \\\n--evaluation_strategy steps --eval_steps 500 --save_steps 500 --logging_steps 50 --load_best_model_at_end True \\\n--prepend_inst_in_enc_or_dec encoder --mask_label_for_decoder_inst False \\\n--learning_rate 0.00005 --warmup_steps 500 --gradient_accumulation_steps 1 \\\n--num_train_epochs 1 \\\n--train_test_split_mode standard_train_test_dev \\\n--num_train_instances_per_task 10 --num_kshot_train_instances_per_task 0 \\\n--num_test_instances_per_task 10 --num_eval_instances_per_task 10 \\\n--use_train_tasks_list TASK_LIST_GENERATION_V2_5 \\\n--use_test_tasks_list  False \\\n--use_eval_tasks_list  TEST_TASKS_LIST_SUMM_TITLE \\\n--use_exclude_tasks_list  EXCLUDE_TASKS_LIST_SUMM_TITLE \\\n--log_task_level_metrics True --resume_from_checkpoint True \\\n--filter_sequences_by_len_diff_from_ref False --pred_ref_diff_tolerance 1.0  --use_train_dataset_for_eval False \\\n--run_hnet_in_batch_mode_per_task True --task_batch_size_per_iter 3  --num_tasks_per_iter 2  --per_device_train_batch_size 1\n\n\n## With MAML, BART\nCUDA_VISIBLE_DEVICES=0 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2.5/maml/3/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2.5/ \\\n--model_name_or_path facebook/bart-base \\\n--tokenizer_name facebook/bart-base \\\n--dataset_name=\"natural_instructions_v2.5\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--per_device_train_batch_size 1 --per_device_eval_batch_size 8 \\\n--max_source_length 1024 --max_target_length 128 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--metric_for_best_model rouge-l-f --multi_ref_mode best \\\n--adafactor True --fp16 False  --lr_scheduler_type linear  \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction True \\\n--add_positive_examples True \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 1 \\\n--train_loop_opt_type maml \\\n--task_sampling_mode uniform --target_task_sampling_rate_increase 0.000000 \\\n--filter_dataset_by_length True \\\n--disable_tqdm True --save_checkpoints True \\\n--show_example_predictions 2 \\\n--learning_rate 0.00001 --warmup_steps 100 --gradient_accumulation_steps 5 \\\n--inner_loop_learning_rate 0.000005 --use_multi_step_loss_optimization False --use_second_order_gradients False \\\n--num_inner_training_steps_per_iter 3 --task_batch_size_per_iter 3  --num_tasks_per_iter 2 \\\n--use_meta_sgd False --meta_sgd_per_param_per_layer False \\\n--log_task_level_metrics True \\\n--prepend_inst_in_enc_or_dec encoder \\\n--log_task_level_metrics True \\\n--num_train_epochs 20 --evaluation_strategy steps --eval_steps 200 --save_steps 200 --logging_steps 5 --load_best_model_at_end True \\\n--train_test_split_mode standard_train_test_dev \\\n--num_train_instances_per_task 10 --num_kshot_train_instances_per_task 0 \\\n--num_test_instances_per_task 10 --num_eval_instances_per_task 10 \\\n--use_train_tasks_list TASK_LIST_GENERATION_V2_5 \\\n--use_test_tasks_list False \\\n--use_eval_tasks_list  TEST_TASKS_LIST_SUMM_TITLE \\\n--use_exclude_tasks_list  EXCLUDE_TASKS_LIST_SUMM_TITLE \\\n--filter_sequences_by_len_diff_from_ref False --pred_ref_diff_tolerance 1.0 --use_train_dataset_for_eval False\n\n\n## With HNET, BART\nCUDA_VISIBLE_DEVICES=3 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2.5/hnet/4/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2.5/ \\\n--model_name_or_path facebook/bart-base \\\n--tokenizer_name facebook/bart-base \\\n--dataset_name=\"natural_instructions_v2.5\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--per_device_train_batch_size 3 --per_device_eval_batch_size 4 \\\n--max_source_length 1024 --max_target_length 128 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--train_test_split_mode random_tasks_k_shot \\\n--metric_for_best_model rouge-l-f --multi_ref_mode best \\\n--adafactor True --fp16 False  --lr_scheduler_type linear --gradient_checkpointing False \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction True \\\n--add_positive_examples True \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 1 \\\n--filter_dataset_by_length True --log_task_level_metrics True \\\n--disable_tqdm True --save_checkpoints True  --show_example_predictions 5 \\\n--prepend_inst_in_enc_or_dec encoder --mask_label_for_decoder_inst False \\\n--learning_rate 0.000005 --warmup_steps 100 --gradient_accumulation_steps 5 \\\n--log_task_level_metrics True \\\n--train_loop_opt_type hnet --hnet_opt_mode alternating_hnet_main --hnet_hidden_dim 128 \\\n--hnet_exclude_encoder_or_decoder restricted_encoder \\\n--hnet_add_instruction_to_mainlm True --hnet_use_encoder_last_hidden_state False --hnet_alternating_num_steps 10 \\\n--hnet_weighet_delta_params False \\\n--hnet_use_eval_for_nograd True \\\n--hnet_model_name_or_path facebook/bart-base --hnet_tokenizer_name facebook/bart-base \\\n--num_train_epochs 5 --evaluation_strategy steps --eval_steps 200 --save_steps 200 --logging_steps 50 --load_best_model_at_end False \\\n--train_test_split_mode standard_train_test_dev \\\n--num_train_instances_per_task 10 --num_kshot_train_instances_per_task 0 \\\n--num_test_instances_per_task 10 --num_eval_instances_per_task 10 \\\n--use_train_tasks_list TASK_LIST_GENERATION_V2_5 \\\n--use_test_tasks_list  False \\\n--use_eval_tasks_list  TEST_TASKS_LIST_SUMM_TITLE \\\n--use_exclude_tasks_list  EXCLUDE_TASKS_LIST_SUMM_TITLE \\\n--hnet_include_layer positions \\\n--run_hnet_in_batch_mode_per_task True --task_batch_size_per_iter 3  --num_tasks_per_iter 2  --per_device_train_batch_size 1\n\n## With HNET_MAML, BART\nCUDA_VISIBLE_DEVICES=2 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2.5/hnet_maml/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2.5/ \\\n--model_name_or_path facebook/bart-base \\\n--tokenizer_name facebook/bart-base \\\n--dataset_name=\"natural_instructions_v2.5\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--max_source_length 1024 --max_target_length 128 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--metric_for_best_model rouge-l-f \\\n--adafactor True --fp16 False  --lr_scheduler_type linear  \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction True \\\n--add_positive_examples True \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 1 \\\n--prepend_inst_in_enc_or_dec encoder  \\\n--filter_dataset_by_length True \\\n--disable_tqdm True --save_checkpoints False \\\n--show_example_predictions 2 \\\n--log_task_level_metrics True \\\n--learning_rate 0.000005 --warmup_steps 100 --gradient_accumulation_steps 1 \\\n--train_loop_opt_type hnet_maml --hnet_opt_mode alternating_hnet_main --hnet_hidden_dim 128 --hnet_exclude_encoder_or_decoder restricted_encoder \\\n--hnet_model_name_or_path facebook/bart-base --hnet_tokenizer_name facebook/bart-base \\\n--hnet_add_instruction_to_mainlm True --hnet_use_encoder_last_hidden_state False --hnet_alternating_num_steps 10 \\\n--hnet_use_eval_for_nograd True \\\n--num_inner_training_steps_per_iter 4 --task_batch_size_per_iter 3  --num_tasks_per_iter 2 --use_meta_sgd False \\\n--inner_loop_learning_rate 0.000001 --task_sampling_mode uniform --target_task_sampling_rate_increase 0.000000 \\\n--use_multi_step_loss_optimization False --use_second_order_gradients False \\\n--filter_sequences_by_len_diff_from_ref False --pred_ref_diff_tolerance 1.0  --log_task_level_metrics True \\\n--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n--num_train_epochs 5 --evaluation_strategy steps --eval_steps 100 --save_steps 100 --logging_steps 25 --load_best_model_at_end True \\\n--train_test_split_mode standard_train_test_dev \\\n--num_train_instances_per_task 10 --num_kshot_train_instances_per_task 0 \\\n--num_test_instances_per_task 10 --num_eval_instances_per_task 10 \\\n--use_train_tasks_list TASK_LIST_GENERATION_V2_5 \\\n--use_test_tasks_list  False \\\n--use_eval_tasks_list  TEST_TASKS_LIST_SUMM_TITLE \\\n--use_exclude_tasks_list  EXCLUDE_TASKS_LIST_SUMM_TITLE \\\n--run_hnet_in_batch_mode_per_task True\n\n## With GPT2\nCUDA_VISIBLE_DEVICES=1 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2/ \\\n--model_name_or_path gpt2-medium \\\n--tokenizer_name gpt2-medium \\\n--dataset_name=\"natural_instructions_v2\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--num_train_epochs 100 \\\n--learning_rate 5e-5 --warmup_steps 500 \\\n--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n--max_source_length 1024 --max_target_length 1024 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--train_test_split_mode random_tasks_k_shot \\\n--num_train_instances_per_task 1 --num_kshot_train_instances_per_task 1 --num_eval_instances_per_task 5 \\\n--metric_for_best_model rouge-l-f --multi_ref_mode best \\\n--evaluation_strategy steps --eval_steps 200 --save_steps 1000 --logging_steps 20 --load_best_model_at_end \\\n--adafactor True --fp16 False  --lr_scheduler_type linear --gradient_accumulation_steps 10 \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction False \\\n--add_positive_examples False \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 1 \\\n--train_loop_opt_type standard \\\n--filter_dataset_by_length True --log_task_level_metrics True \\\n--use_exclude_tasks_list False \\\n--use_train_tasks_list task1540_parsed_pdfs_summarization --use_test_tasks_list task1540_parsed_pdfs_summarization  \\\n--disable_tqdm True --save_checkpoints False  --show_example_predictions 5 \\\n--prepend_inst_in_enc_or_dec encoder --mask_label_for_decoder_inst False\n\n## With LED\nCUDA_VISIBLE_DEVICES=0 python data/natural_instructions_dataset/train.py \\\n--output_dir /mnt/Data/models/natural_instructions_v2/ --overwrite_output_dir \\\n--cache_dir /mnt/Data/model_cache_pt/ --overwrite_cache True \\\n--dataset_folder /mnt/Data/natural_instructions_v2/ \\\n--model_name_or_path allenai/led-large-16384 \\\n--tokenizer_name allenai/led-large-16384 \\\n--dataset_name=\"natural_instructions_v2\" \\\n--do_train --do_eval --do_predict --predict_with_generate \\\n--num_train_epochs 100 \\\n--learning_rate 5e-5 --warmup_steps 0 \\\n--per_device_train_batch_size 1 --per_device_eval_batch_size 1 \\\n--max_source_length 2048 --max_target_length 2048 --decode_max_length 128 \\\n--overwrite_output_dir True \\\n--train_test_split_mode random_tasks_k_shot \\\n--num_train_instances_per_task 1 --num_kshot_train_instances_per_task 1 --num_eval_instances_per_task 1 \\\n--metric_for_best_model rouge-l-f --multi_ref_mode best \\\n--evaluation_strategy steps --eval_steps 200 --save_steps 200 --logging_steps 2000 --load_best_model_at_end \\\n--adafactor True --fp16 False  --lr_scheduler_type linear --gradient_accumulation_steps 1 \\\n--disable_tqdm False \\\n--add_category False \\\n--add_task_summary False \\\n--add_instruction True \\\n--add_positive_examples False \\\n--add_negative_examples False \\\n--add_explanations False \\\n--num_examples_in_instruction 2 \\\n--train_loop_opt_type standard \\\n--use_train_tasks_list task1572_samsum_summary  \\\n--use_test_tasks_list  task1572_samsum_summary,task1540_parsed_pdfs_summarization,task1553_cnn_dailymail_summarization \\\n--use_exclude_tasks_list False \\\n--filter_dataset_by_length True --log_task_level_metrics True \\\n--pred_ref_diff_tolerance 1.0 \\\n--log_level warning --disable_tqdm True --save_checkpoints False  --show_example_predictions 4 \\\n--prepend_inst_in_enc_or_dec decoder --mask_label_for_decoder_inst False --gradient_checkpointing True\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n## Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "NLG_Instructions_MetaLearning", "org_name": "microsoft", "org_repo": "microsoft/NLG_Instructions_MetaLearning", "platform_org_repo": "github+microsoft/NLG_Instructions_MetaLearning", "link_to_repo": "https://github.com/microsoft/NLG_Instructions_MetaLearning", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# homebrew-msstore-cli\n\nHomebrew wrapper package for [msstore-cli](https://github.com/microsoft/msstore-cli)", "repo_name": "homebrew-msstore-cli", "org_name": "microsoft", "org_repo": "microsoft/homebrew-msstore-cli", "platform_org_repo": "github+microsoft/homebrew-msstore-cli", "link_to_repo": "https://github.com/microsoft/homebrew-msstore-cli", "platform": "github", "language": "PowerShell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# DeFacto\n\n## Introduction\n\nDeFacto is a dataset containing human demonstrations and feedback for improving factual consistency of text summarization.\n\nThe dataset is constructed with the following steps:\n\n1. **Detect errors**: The annotator is required to\nevaluate a summary given the source document and\n**decide if the summary is factually consistent**.\n2. **Categorize errors**: If the annotator decides\nthe summary *is not* factually consistent, they are\nrequired to **categorize the factual errors** in the\nsummary as either *intrinsic* or *extrinsic*.\n3. **Give explanation**: The annotator is required to\n**provide a natural language explanation** on why\nthe summary is factually consistent or not.\n4. **Provide evidence**: The annotator is required to\n*select a sentence from the source document as\nevidence* to support their claims described in 3.\n5. **Write corrective instruction**: The annotator is\nrequired to **provide instructions** of how to correct\nthe original summary if they think it is not factually\nconsistent. To enforce uniformity and reduce the\nnoise in the instructions, we provide six templates\nfor the annotators corresponding to different operations: *Remove*, *Add*, *Replace*, *Modify*, *Rewrite*,\nand *Others*. The annotators need to fill in the templates to generate the instructions.\n6. **Correct summary**: Following the instruction\nin 5., the annotator is required to **edit the initial\nsummary** to make it *factually consistent* with minimal, necessary modifications.\n\nWe use [XSum](https://github.com/EdinburghNLP/XSum) as the target dataset and [Pegasus](https://github.com/google-research/pegasus) as the pre-trained summarization model to generate the initial system outputs.\n\nThe dataset statistics are summarized below.\n\n| | Train | Val | Test | All |\n| --- | --- | --- | --- | --- |\n| All |  1000 | 486 | 1075 | 2561 |\n| w/ Errors | 701 | 341 | 779 | 1821 |\n\n## Using DeFacto\n\nWe provide the data files in **`./data`** and a simple data loader **``data_loader.py``**.\n\nEach line of the data files contain a data example stored in the Json format, with the following strucure:\n\n```\n {\n  \"article\": \"input article\",\n  \"abstract\": \"abstract/reference summary\",\n  \"candidate\": \"candidate/initial system output\",\n  \"doc_id\": int,\n  \"has_error\": true/false\n  \"intrinsic_error\": true/false,\n  \"extrinsic_error\": true/false,\n  \"feedback\": {\n    \"summary\": \"human-corrected summary\",\n    \"evidence\": \"selected sentence from the input article\",\n    \"explanation\": \"natural language explanation\",\n    \"instruction\": \"concatenated instructions\",\n    \"instruction_list\": [\"list of instructions\", ],\n  },\n}\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "DeFacto", "org_name": "microsoft", "org_repo": "microsoft/DeFacto", "platform_org_repo": "github+microsoft/DeFacto", "link_to_repo": "https://github.com/microsoft/DeFacto", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# Multi Runtime Environment (MRE) Application \n- Create a Resource Group for entire project\n- Create an API Management Service \n- Create an Function App \n- Create a Web App  \n- Create an AKS Cluster \n- Create an Azure Container Registry \n- Create a Blob Storage \n- Create an Active Directory B2C Tenant and App\n\n## Architecture\n\n<img src=\"./assets/multi%20env%20app%20on%20azure.jpg\" alt=\"students app architecture\" width=\"1200\"/>\n\n## Table of Contents\n - [Prerequisites](#prerequisites)\n - [Create a Resource Group](#create-a-resource-group)\n - [Create API Management Service](#create-api-management-service)\n - [Create Students Microservice](#create-students-microservice)\n   - [Create CosmosDB Database for students API](#create-cosmosdb-database-for-students-api)\n   - [Create Function App for students API](#create-function-app-for-students-api)\n - [Create Teachers Microservice](#create-teachers-microservice)\n   - [Create SQL database for teachers API](#create-sql-database-for-teachers-api)\n   - [Create AppService for Teachers API](#create-appservice-for-teachers-api)\n   - [Create Teachers API in API Management](#create-teachers-api-in-api-management)\n - [Create Workers Microservice](#create-workers-microservice)\n   - [Create SQL database for workers API](#create-sql-database-for-workers-api)\n   - [Create AKS Cluster with Application Gateway Ingress Controller](#create-aks-cluster-with-application-gateway-ingress-controller)\n   - [Upgrade AKS cluster with Azure Key Vault Provider for Secrets Store CSI Driver support](#upgrade-aks-cluster-with-azure-key-vault-provider-for-secrets-store-csi-driver-support)\n   - [Create ACR for Docker Images](#create-acr-for-docker-images)\n   - [Deploy Workers Application to AKS Cluster](#deploy-workers-application-to-aks-cluster)\n   - [Create Workers API in API Management](#create-workers-api-in-api-management)  \n - [Hosting Serverless UI on Azure Blob Storage](#hosting-serverless-ui-on-azure-blob-storage)\n - [Securing EMR Application](#securing-emr-application)\n   - [Azure ADB2C](#azure-adb2c)\n   - [Authenticating API Requests](#authenticating-api-requests)\n   \n\n\n## Prerequisites\n- Azure Subscription\n- Azure CLI \n- kubectl CLI \n- Docker CLI\n- Donet 6\n- Node 16\n- Visual Studio (Community/Enterprise)\n\n\n\n# Steps\n## Create a Resource Group \n1. Go to Azure Portal\n2. Create a resource group - **schoolapp** in your subscription\n## Create API Management service\n1. Go to Azure Portal \n2. Create a resource - API Management\n3. In Project Details\n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **schoolapp** resource group\n    - ```Region``` - Choose West Europe\n    - ```Resource name``` - Choose **schoolappapimanagement<number>** # Must be globally unique \n    - ```Organization name``` - Choose your organization name\n    - ```Administrator email``` - Choose your administrator email\n    - ```Pricing tier``` - Choose Consumption tier\n    - ```System assigned managed identity``` - Mark the Status\n    - ```Connectivity type``` - None  \n4. Review + create \n5. Create\n\n**_NOTE:_** API creation may take around 40-50 minutes.\n\n# Create Students Microservice\n## Create CosmosDB Database for students API\n1. Go to Azure Portal\n2. Create a resource - Azure Cosmos DB\n3. Choose - Azure Cosmos DB for NoSQL\n   - ```Subscription``` - Choose your subscription\n   - ```Resource Group``` - Choose **schoolapp** resource group\n   - ```Account Name``` - Choose unique account name\n   - ```Location``` - Choose your location\n   - ```Capacity mode``` - Provisioned throughput\n   - ```Apply Free Tier Discount``` - Apply if you want discount \n4. Review + create\n5. Create\n6. Go to your CosmosDB account\n7. Go to Data Explorer. \n8. Create a Container\n   - ```Database id``` - studentdb\n   - ```Database throughput``` - Manual - 400\n   - ```Container id``` - studentsdbcontainer\n   - ```Partition key``` - /email\n\n<img src=\"./assets/cosmosdb-container-db-creation.png\" alt=\"cosmosdb container db creation\" width=\"400\"/>\n\n9. Go to studentsdbcontainer -> Items\n10. New Item \n   ```\n   {\n    \"Email\": \"student1@mtc.com\",\n    \"Name\": \"Student One\",\n    \"Age\": 22,\n    \"Gender\": \"Male\",\n    \"Class\": \"History\"\n   }\n   ```\n11. Save\n\n## Create Function App for Students API\n[Create Function App for Students api](./nodejs_functions_backend_students_api/)\n\n## Create Students API \n1. Go to your API management - schoolappapimanagement\n2. Select APIs \n3. In Define a new API select Function App  \n<img src=\"./assets/api_management_select_function_app.png\" alt=\"api management select function app\" width=\"600\"/>  \n4. Click on Browse\n5. Click Select \n6. Choose studentsapp and hit Select  \n   \n**_NOTE:_** If you don't see your function app in the functions list, create a managed identity and give a contributor role to your function app  \n  \n1. Check all your functions and hit Select\n2. ```API URL suffix``` - Choose students. \n3.  Hit Create. \n4.  Go to deleteStudent and getStudent functions. \n5.  Edit Frontend.   \n<img src=\"./assets/functions_edit_frontend.png\" alt=\"api management select function app\" width=\"600\"/>.   \n12. Under Query add an email parameter and check required box. \n13. Hit Save.     \n14. Test the APIs. \n  \n**_NOTE:_** To test your API in API clients like POSTMAN, you need to add a Header with key - **Ocp-Apim-Subscription-Key** and value of **Subscriptions key** from the Subscription section in the API blade\n\n# Create Teachers Microservice\n## Create SQL database for teachers API\n1. Go to Azure Portal\n2. Create a resource - SQL Database \n   - ```Subscription``` - Choose your subscription\n   - ```Resource Group``` - Choose **schoolapp** resource group\n   - ```Database Name``` - Choose **teachersdb**\n   - ```Server``` - Hit Create new\n     - ```Server name``` - Choose unique server name for example <yourname><schoolappserver>\n     - ```Location``` - Choose West Europe\n     - ```Authentication method``` - Use SQL authentication and choose admin login and password\n     - Hit Ok\n     - ```Compute + storage``` - Choose Configure database\n       - ```Service tier``` - select DTU-based purchasing model - Basic\n       - Hit Apply\n     - ```Backup storage redundancy``` - Choose Locally-redundant backup storage\n     - ```Connectivity method``` - Select Public endpoint (For this demo only) \n       - Allow Azure services and resource to access this tier - Yes. \n       - Add current client IP address - Yes. \n     - Review + create  \n     - Create  \n   - ```Location``` - Choose your location\n   - ```Capacity mode``` - Provisioned throughput\n   - ```Apply Free Tier Discount``` - Apply if you want discount   \n   - Go to your SQL server blade\n   - ```Public network access``` - Selected networks -> Add your ip address \n   - ```Allow Azure services and resources to access this server``` - Check \n3. Go to teachersdb database \n4. Choose Query editor in database blade\n5. Login with sql server admin credentials \n6. In Query Editor create teachers table:\n   ```\n    CREATE TABLE Teachers (\n    Id int IDENTITY(1,1) PRIMARY KEY,\n    Email varchar(255) UNIQUE,\n    Name varchar(255),\n    Age int,\n    Gender varchar(255),\n    Class varchar(255)\n    );\n   ```\n7. In Query Editor create few rows:  \n   ```\n    INSERT INTO Teachers (Email, Name, Age, Gender, Class) VALUES ('Teacher1@mtc.com', 'Teacher One', 45, 'Male', 'History')\n    INSERT INTO Teachers (Email, Name, Age, Gender, Class) VALUES ('Teacher2@mtc.com', 'Teacher Two', 32, 'Female', 'Math')\n    INSERT INTO Teachers (Email, Name, Age, Gender, Class) VALUES ('Teacher3@mtc.com', 'Teacher Three', 29, 'Female', 'Computer Science')\n   ```\n\n## Create AppService for Teachers API\n[Create AppService for Teachers API](./dotnet_webapp_backend_teachers_api)\n\n## Create Teachers API in API Management\n1. Go to your API management - schoolappapimanagement\n2. Select APIs \n3. Select Add API \n4. From the API list select OpenAPI \n5. ```OpenAPI specification``` - Paste the swagger url\n6. Hit Create\n7. Your API is going to look like this:   \n  \n   <img src=\"./assets/api-management-routes.png\" alt=\"api management routes\" width=\"400\"/>  \n\n8. Test the routes from the API management\n  \n\n# Create Workers Microservice\n## Create SQL database for workers API\n1. Go to Azure Portal\n2. Create a resource - SQL Database \n   - ```Subscription``` - Choose your subscription\n   - ```Resource Group``` - Choose **schoolapp** resource group\n   - ```Database Name``` - Choose **workersdb**\n   - ```Server``` - Choose the same server you created in teachers microservice for example <yourname><schoolappserver>\n   - ```Compute + storage``` - Choose Configure database\n       - ```Service tier``` - select DTU-based purchasing model - Basic\n       - Hit Apply\n   - ```Capacity mode``` - Provisioned throughput\n   - ```Apply Free Tier Discount``` - Apply if you want discount   \n   - Go to your SQL server blade\n   - ```Public network access``` - Selected networks -> Add your ip address \n   - ```Allow Azure services and resources to access this server``` - Check \n3. Go to workersdb database \n4. Choose Query editor in database blade\n5. Login with sql server admin credentials \n6. In Query Editor create teachers table:\n   ```\n    CREATE TABLE Workers (\n    Id int IDENTITY(1,1) PRIMARY KEY,\n    Email varchar(255) UNIQUE,\n    Name varchar(255),\n    Age int,\n    Gender varchar(255),\n    );\n   ```\n7. In Query Editor create few rows:  \n   ```\n    INSERT INTO Workers (Email, Name, Age, Gender) VALUES ('Worker1@mtc.com', 'Worker One', 27, 'Male')\n    INSERT INTO Workers (Email, Name, Age, Gender) VALUES ('Worker2@mtc.com', 'Worker Two', 23, 'Female')\n    INSERT INTO Workers (Email, Name, Age, Gender) VALUES ('Worker3@mtc.com', 'Worker Three', 35, 'Female')\n    INSERT INTO Workers (Email, Name, Age, Gender) VALUES ('Worker4@mtc.com', 'Worker Four', 51, 'Male')\n   ```\n\n## Create AKS Cluster with Application Gateway Ingress Controller\n### Architecture\n<img src=\"./assets/aksVSingress.png\" alt=\"api management select function app\" width=\"600\"/> \n\n**_NOTE:_**  We recommend the use of [Azure Cloud Shell](https://shell.azure.com/) for all command line operations below.  \n\n### Steps  \n1. Create AD service principal. \n   ```bash\n    az ad sp create-for-rbac -o json > auth.json\n    appId=$(jq -r \".appId\" auth.json)\n    password=$(jq -r \".password\" auth.json)\n   ```\n2. Create Environment variables:\n   ```bash\n   objectId=$(az ad sp show --id $appId --query \"id\" -o tsv)\n   location=\"westeurope\"\n   kubernetesVersion=$(az aks get-versions --location $location --query \"orchestrators[-1].orchestratorVersion\" -o tsv)\n   ```\n3. Paste the entire command below (It is a single command on multiple lines) in Cloud Shell to create the `parameters.json` file. It will be used in the ARM template deployment.\n    ```bash\n    cat <<EOF > parameters.json\n    {\n      \"aksServicePrincipalAppId\": { \"value\": \"$appId\" },\n      \"aksServicePrincipalClientSecret\": { \"value\": \"$password\" },\n      \"aksServicePrincipalObjectId\": { \"value\": \"$objectId\" },\n      \"aksEnableRBAC\": { \"value\": false },\n      \"kubernetesVersion\":{\"value\":\"$kubernetesVersion\"}\n    }\n    EOF\n    ```\n   To deploy an **RBAC** enabled cluster, set the `aksEnabledRBAC` field to `true`. View the contents of the newly created file with `cat parameters.json`. It will contain the values of the `appId`, `password`, and `objectId` bash variables from the previous steps.\n\n4. Deploy Components\n    \n   The next few steps will add the following list of components to your Azure subscription:\n      - [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes)\n      - [Application Gateway](https://docs.microsoft.com/en-us/azure/application-gateway/overview) v2\n      - [Virtual Network](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview) with 2 [subnets](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview)\n      - [Public IP Address](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-public-ip-address)\n      - [Managed Identity](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview), which will be used by [AAD Pod Identity](https://github.com/Azure/aad-pod-identity/blob/master/README.md)\n   1. Download the ARM template into `template.json` file. Paste the following in your [shell](https://shell.azure.com/):\n        \n      ```bash\n      wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/deploy/azuredeploy.json -O template.json\n      ```  \n   2. Deploy the ARM template via [Azure Cloud Shell](https://shell.azure.com/) and the `az` tool. Modify the name of the resource group and region/location, then paste each of the following lines into your [shell](https://shell.azure.com/):    \n       \n      ```bash\n      resourceGroupName=\"schoolapp\"\n\n      deploymentName=\"school-ingress-appgw\"\n\n      az group create -n $resourceGroupName -l $location # Use only if you want to deploy in a new resource group.\n\n      az deployment group create -g $resourceGroupName -n $deploymentName --template-file template.json --parameters parameters.json\n      ```  \n      \n    Note: The last command may take a few minutes to complete.  \n   3. Once the deployment finished, download the deployment output into a file named `deployment-outputs.json`.  \n   \n      ```bash\n         az deployment group show -g $resourceGroupName -n $deploymentName --query \"properties.outputs\" -o json > deployment-outputs.json\n      ```\n     View the content of the newly created file with: `cat deployment-outputs.json`. The file will have the following shape (example):\n\n      ```\n      {\n         \"aksApiServerAddress\": {\n         \"type\": \"String\",\n         \"value\": \"aks-abcd41e9.hcp.westus2.azmk8s.io\"\n         },\n         \"aksClusterName\": {\n         \"type\": \"String\",\n         \"value\": \"aksabcd\"\n         },\n         \"applicationGatewayName\": {\n         \"type\": \"String\",\n         \"value\": \"applicationgatewayabcd\"\n         },\n         \"identityClientId\": {\n         \"type\": \"String\",\n         \"value\": \"7b1a3378-8abe-ab58-cca9-a8ef624db293\"\n         },\n         \"identityResourceId\": {\n         \"type\": \"String\",\n         \"value\": \"/subscriptions/a6466a81-bf0d-147e-2acb-a0ba50f6456e/resourceGroups/MyResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/appgwContrIdentityabcd\"\n         },\n         \"resourceGroupName\": {\n         \"type\": \"String\",\n         \"value\": \"MyResourceGroup\"\n         },\n         \"subscriptionId\": {\n         \"type\": \"String\",\n         \"value\": \"a6466a81-bf0d-147e-2acb-a0ba50f6456e\"\n         }\n      }\n      ```\n\n5. Set up Application Gateway Ingress Controller  \n   With the instructions in the previous section we created and configured a new AKS cluster and an App Gateway. We are now ready to deploy a workers app and an ingress controller to our new Kubernetes infrastructure.  \n\n   1. Setup Kubernetes Credentials  \n      For the following steps we need setup [kubectl](https://kubectl.docs.kubernetes.io/) command,\n      which we will use to connect to our new Kubernetes cluster. [Cloud Shell](https://shell.azure.com/) has `kubectl` already installed. We will use `az` CLI to obtain credentials for Kubernetes.  \n        \n      Get credentials for your newly deployed AKS ([read more](https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough#connect-to-the-cluster)):\n\n      ```bash\n      # use the deployment-outputs.json created after deployment to get the cluster name and resource group name\n      aksClusterName=$(jq -r \".aksClusterName.value\" deployment-outputs.json)\n      resourceGroupName=$(jq -r \".resourceGroupName.value\" deployment-outputs.json)\n\n      az aks get-credentials --resource-group $resourceGroupName --name $aksClusterName\n      ```\n\n      Test the connection:  \n      ```bash\n      kubectl get nodes\n      ```\n   2. Install AAD Pod Identity  \n\n      Azure Active Directory Pod Identity provides token-based access to [Azure Resource Manager (ARM)](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview).  \n\n      [AAD Pod Identity](https://github.com/Azure/aad-pod-identity) will add the following components to your Kubernetes cluster:  \n         - Kubernetes [CRDs](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/): `AzureIdentity`, `AzureAssignedIdentity`, `AzureIdentityBinding`  \n         - [Managed Identity Controller (MIC)](https://github.com/Azure/aad-pod-identity#managed-identity-controller) component  \n         - [Node Managed Identity (NMI)](https://github.com/Azure/aad-pod-identity#node-managed-identity) component  \n        \n      To install AAD Pod Identity to your cluster:  \n         - *RBAC enabled* AKS cluster\n\n            ```bash\n            kubectl apply -f https://raw.githubusercontent.com/Azure/aad-pod-identity/v1.8.6/deploy/infra/deployment-rbac.yaml\n            ```\n\n         - *RBAC disabled* AKS cluster\n\n            ```bash\n            kubectl apply -f https://raw.githubusercontent.com/Azure/aad-pod-identity/v1.8.6/deploy/infra/deployment.yaml\n            ```\n\n      ***Note:*** AAD Pod Identity introduced a [breaking change](https://github.com/Azure/aad-pod-identity/tree/v1.6.0#v160-breaking-change) after v1.5.5 regarding CRD fields become case sensitive, for any AAD Pod Identity version >= 1.6.0 or you plan to apply from master branch such as https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment-rbac.yaml, AGIC version at least [v1.2.0-rc2](https://github.com/Azure/application-gateway-kubernetes-ingress/blob/master/CHANGELOG/CHANGELOG-1.2.md#v120-rc2) will be required, more details please refer to [troubleshooting](../troubleshootings/troubleshooting-agic-fails-with-aad-pod-identity-breakingchange.md).\n\n      Check if all pods are running:  \n      ```bash\n      kubectl get pods\n      ```  \n\n       <img src=\"./assets/pod_identity_state.png\" alt=\"pod identity state\" width=\"400\"/> \n\n   3. Install AGIC with Helm\n        \n      [Helm](https://docs.microsoft.com/en-us/azure/aks/kubernetes-helm) is a package manager for Kubernetes. This document will use version 3 of helm, which is not backwards compatible with previous versions.\n\n      1. Add the AGIC Helm repository:\n\n         ```bash\n         helm repo add application-gateway-kubernetes-ingress https://appgwingress.blob.core.windows.net/ingress-azure-helm-package/\n         helm repo update\n         ```\n      \n      2. Install Ingress Controller Helm Chart\n         1. Use the `deployment-outputs.json` file created above and create the following variables.\n            \n            ```bash\n               applicationGatewayName=$(jq -r \".applicationGatewayName.value\" deployment-outputs.json)\n               resourceGroupName=$(jq -r \".resourceGroupName.value\" deployment-outputs.json)\n               subscriptionId=$(jq -r \".subscriptionId.value\" deployment-outputs.json)\n               identityClientId=$(jq -r \".identityClientId.value\" deployment-outputs.json)\n               identityResourceId=$(jq -r \".identityResourceId.value\" deployment-outputs.json)\n            ```\n      \n      3. Download sample-helm-config.yaml, which will configure AGIC:  \n         \n         ```bash\n         wget https://raw.githubusercontent.com/Azure/application-gateway-kubernetes-ingress/master/docs/examples/sample-helm-config.yaml -O helm-config.yaml\n         ```\n      4. Edit the newly downloaded helm-config.yaml and fill out the sections `appgw` and `armAuth`.\n         \n         ```bash\n         sed -i \"s|<subscriptionId>|${subscriptionId}|g\" helm-config.yaml\n         sed -i \"s|<resourceGroupName>|${resourceGroupName}|g\" helm-config.yaml\n         sed -i \"s|<applicationGatewayName>|${applicationGatewayName}|g\" helm-config.yaml\n         sed -i \"s|<identityResourceId>|${identityResourceId}|g\" helm-config.yaml\n         sed -i \"s|<identityClientId>|${identityClientId}|g\" helm-config.yaml\n         ```\n      5. Install the Application Gateway ingress controller package:  \n         Check the latest stable [version](https://github.com/Azure/application-gateway-kubernetes-ingress/releases)\n \n         ```bash\n         helm install ingress-azure \\\n         -f helm-config.yaml \\\n         application-gateway-kubernetes-ingress/ingress-azure \\\n         --version 1.5.2\n         ```  \n           \n         >Note: Use at least version 1.4.0, i.e. `--version 1.4.0`, when installing on k8s version >= 1.16. Kubernetes >= 1.22 requires version 1.5.0 (or higher).\n\n\n## Workload identity\n\n```bash\naz aks update --name <aksClusterName> --resource-group schoolapp --enable-oidc-issuer\naz aks addon list -g schoolapp -n <aksClusterName> # show addon list \n```\n\n## Upgrade AKS cluster with Azure Key Vault Provider for Secrets Store CSI Driver support\n1. Open Cloud Shell\n2. Run:  \n   ```bash\n   az aks enable-addons --addons azure-keyvault-secrets-provider --name $aksClusterName --resource-group $resourceGroupName\n   ```\n3. Verify the Azure Key Vault Provider for Secrets Store CSI Driver installation:  \n   ```bash\n   kubectl get pods -n kube-system -l 'app in (secrets-store-csi-driver, secrets-store-provider-azure)'\n\n   NAME                                     READY   STATUS    RESTARTS   AGE\n   aks-secrets-store-csi-driver-4vpkj       3/3     Running   2          4m25s\n   aks-secrets-store-csi-driver-ctjq6       3/3     Running   2          4m21s\n   aks-secrets-store-csi-driver-tlvlq       3/3     Running   2          4m24s\n   aks-secrets-store-provider-azure-5p4nb   1/1     Running   0          4m21s\n   aks-secrets-store-provider-azure-6pqmv   1/1     Running   0          4m24s\n   aks-secrets-store-provider-azure-f5qlm   1/1     Running   0          4m25s\n   ```\n\n\n\n## Create a Key Vault and store the secrets for database connection - Optional ( Need to configure an aks cluster for key-vault connection )\n1. Go to Azure Portal\n2. Create a resource - Key Vault \n3. In Key Vault details:  \n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **schoolapp** resource group\n    - ```Key vault name``` - Choose a unique name \n    - ```Region``` - Choose West Europe\n    - ```Pricing tier``` - Standard\n4. Review + create\n5. Create\n\n\n## Create ACR for Docker Images\n1. Go to Azure Portal\n2. Create a Container Registry\n3. In Registry details:  \n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **schoolapp** resource group\n    - ```Registry Name``` - Choose a unique name \n    - ```Location``` - Choose West Europe\n    - ```SKU``` - Standard\n4. Review + create\n5. Create\n6. Configure ACR integration with our AKS cluster\n   1. Open Cloud Shell\n   2. Integrate an existing ACR with existing AKS clusters by supplying valid values for acr-name or acr-resource-id as below.\n      ```bash\n      acrId=$(az acr show -n <registryName> -g schoolapp --query id)\n      az aks update -n $aksClusterName -g $resourceGroupName --attach-acr $acrId\n      ```\n## Deploy Workers Application to AKS Cluster\n1. Open Cloud Shell  \n2. Clone the git repo\n3. Open the project - ./nodejs_aks_backend_workers_api/\n4. Change the configuration in config.js file to connect to the DB\n5. Build the image and push it to your ACR:  \n   ```bash\n   az acr build --image workers:0.1 --registry <registryName> --file Dockerfile .\n   ```\n6. In the workers-k8s.yaml file, replace the image name with yours, the file will have the following shape (exmaple):    \n   ```bash\n   - image: \"<registryName>.azurecr.io/workers:0.1\"\n     name: workersapp\n     ports:\n     - containerPort: 8080\n       protocol: TCP\n   ``` \n7. To create Pod, Service and Ingress resources run:    \n   ```bash\n   kubectl create -f workers-k8s.yaml\n   ```\n8. To show your ingress ip run:  \n   ```bash\n   kubectl get ingress\n   ```\n9. Open a web browser and check the app - http://ingressIpAddress:80  \n     \n    <img src=\"./assets/workers_app.png\" alt=\"workers app\" width=\"300\"/>   \n   \n\n## Create Workers API in API Management\n1. Clone the git repo\n2. Open the project - multienvapponazure/nodejs_aks_backend_workers/swagger_output.json\n3. Go to your API management - schoolappapimanagement\n4. Select APIs \n5. Select Add API \n6. From the API list select OpenAPI \n7. ```OpenAPI specification``` - Paste the swagger file\n8. Hit Create\n9. Your API is going to look like this:  \n   \n    <img src=\"./assets/api-management-routes-workers.png\" alt=\"api management routes workers\" width=\"400\"/>\n\n10. Go to Settings\n11. Change ```Web service URL``` - http://ingressIp/api\n12. Hit Save\n\n\n## Hosting Serverless UI on Azure Blob Storage\n1. Go to Azure Portal\n2. Create a resource - Storage account \n3. In Storage account details:  \n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **schoolapp** resource group\n    - ```Storage account name``` - Choose a unique name \n    - ```Region``` - Choose West Europe\n    - ```Redundancy``` - LRS\n4. Hit Review \n5. Hit Create\n6. Go to resource\n7. In left navigation panel choose ***Static website***\n8. in Static Website details:\n   - ```Static website``` - Enabled\n   - ```index document name``` - index.html\n   - ```Error document path``` - index.html\n9. Hit Save \n10. Go your storage account and select ***Storage browser***\n11. In Blob containers select $web - It is the place where you upload a website.  \n      \n    <img src=\"./assets/localstorage_static_website.png\" alt=\"localstorage static website\" width=\"1000\"/>  \n      \n12. Clone the git repo\n13. Open the project - ./react_frontend_website_no_authentication\n14. Edit Api.js file and configure a baseUrl and Ocp-Apim-Subscription-Key\n15. Run ```npm install```\n16. Run ```npm run build``` - Build a project and store it in a build directory\n17. Upload all files to $web directory in a storage account \n18. In left navigation panel choose ***Static website***\n19. Copy the ***Primary endpoint*** url and test in your web browser\n\n\n## Securing EMR Application\n\n### Azure ADB2C\n* Identity management service\n* Sign up and sign in using email address\n* Sign in using social identity providers like Facebook, Google, Linkedin, etc.\n* ADB2C Suppots OpenID Connect, OAuth 2.0 and SAML authentication protocols\n* ADB2C manages user passwords\n* Default sign up/sign in and password reset authentication flows\n* Bring your own branding \n* Serverless \n* Azure ADB2C tenant different from Azure AD tenant\n* Documentation: [ADB2C](https://learn.microsoft.com/en-us/azure/active-directory-b2c/) , [Pricing](https://azure.microsoft.com/en-us/pricing/details/active-directory/external-identities/)\n\nCreate Azure ADB2C:  \n1. Go to Azure Portal\n2. Create a resource - Azure Active Directory B2C\n3. Select ***Create a new Azure AD B2C Tenant***\n4. In Create a tenant details:\n   - ```Organization name``` - abschooltenant\n   - ```Initial domain name``` - abschooltenant\n   - ```Country/Region``` - Ireland\n   - ```Subscription``` - Choose your subscription\n   - ```Resource Group``` - Choose **schoolapp** resource group\n5. Hit Review + Create\n6. Hit Create  \n**_NOTE:_** Tenant creation may take around 5 minutes.\n\n7. To switch to your new directory select the ***directories + subscriptions*** icon:  \n  \n    <img src=\"./assets/switch_directory.png\" alt=\"switch directory\" width=\"400\"/> \n  \n8. Select your new directory and click Switch\n  \n   <img src=\"./assets/choose_directory.png\" alt=\"choose directory\" width=\"600\"/>\n\n9. In new created tenant go to Azure AD B2C Main page\n10. In left side menu go to ***App registrations*** \n11. Hit ***New registration***\n12. In Register an application:  \n    - ```Name``` - ***schoolapp***\n    - ```Supported account types``` - Select ***Accounts in any identity provider or organizational directory***\n13. Hit Register  \n14. In Overview click ***Add a Redirect URI***\n15. Hit ***+Add a platform***\n16. Select ***Web***\n17. In Configure Web:  \n    - ```Redirect URIs``` - https://jwt.ms\n    - ```Implicit grant and hybrid flows``` - Check the ***Access tokens(used for implicit flows)***\n18. Hit Configure \n19. Go to Azure AD B2C Home Page\n20. In left side menu go to ***User flows***\n21. Hit ***+New user flow***\n22. Select ***Sign up and sign in*** User flow\n23. Hit Create\n24. In Create:  \n    - ```B2C_1_``` - SignUpSignin\n    - ```Local accounts``` - Check ***Email signup***\n25. Hit Create \n26. Select the user flow your create - ***B2C_1_SignUpSignin***\n27. In left side menu go to ***User attributes***\n28. Check a ***Email Address*** User attribute\n29. Hit Save \n30. Hit Run user flow\n31. You can create a temporary email account [here](https://temp-mail.org/)\n32. Go to Azure B2C Home Page\n33. In left side menu go to ***Users***\n34. Check if a new user you created exists in the list\n\n\n### Authenticating API Requests\n1. Switch back to your default directory \n2. Open your API service \n3. Select APIs\n4. Select a API\n5. In All operations under Inbound processing click ***+Add policy***\n6. Choose Validate JWT \n7. Choose full to expand all the options\n8. Fill:  \n   - ```Header name``` - Authorization\n   - ```Failed validation error message``` - Missing or invalid token\n   - ```Require expiration time``` - Yes\n   - ```Require signed tokens?``` - Yes\n   - ```Audiences``` - Go to AD B2C and copy your application (client) ID\n   - ```Issuers``` - Go to AD B2C User flow -> Run user flow -> Open OpenID configuration url and copy the issuer url\n   - ```Open ID URLs``` - Go to AD B2C User flow -> Run user flow -> Open OpenId configuration url and copy the url \n9. Hit Save \n10. Repeat the steps for all APIs\n\n\n\n", "repo_name": "MTC_IL_WORKSHOP_Multi_Runtime_Environment_Application", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_Multi_Runtime_Environment_Application", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_Multi_Runtime_Environment_Application", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_Multi_Runtime_Environment_Application", "platform": "github", "language": "JavaScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Repository setup required :wave:\nHands on workshop\nTarget Audience: Data Engineers, BI Analysts\n \nOverview: \n \na.      Building modern Data Lakes in Azure. Trends and Best Practices.\nb.      Review Azure data services:\n\t\tAzure Data Lake Storage  (ADLS v2) , \n\t\tAzure Synapse,\n\t\tAzure Databricks\nc. Security in Azure Data Lake\nd.  Labs\ne.\n \nLabs:\n-----\n \n1. Build ETL with Azure Data Factory.  Load the data to ADLS v2.\n \n2. Delta Lake format. Performance best practices.\n \n3. Data engineering in Azure Databricks.\n \n4. Data Analytics in Azure Databricks. Running SQL.\n \n5. Reporting with PowerBI.\n", "repo_name": "MTC_IL_WORKSHOP_Big_data_analytics_and_visualization", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_Big_data_analytics_and_visualization", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_Big_data_analytics_and_visualization", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_Big_data_analytics_and_visualization", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# React Native devblog\n\nThis **public** repository serves as the backbone of the official [React Native DevBlog](https://devblogs.microsoft.com/react-native/).\n\nThe editorial team is currently composed by [Ryan Haning](https://github.com/rhaning), [Steven Moyes](https://github.com/stmoy), [Lorenzo Sciandra](https://github.com/kelset), [Adam Foxman](https://github.com/afoxman).\n\n## Writing guide\n\nThe general flow for contributing content to the devblog is, briefly:\n\n    1. Idea gets pitched\n    2. Idea gets approved\n    3. Draft gets made\n    4. Draft gets approved\n    5. Draft gets published\n\nLet's dive a bit more into each step:\n\n### 1. Pitching an idea for content for the devblog\n\nWe want to empower everyone at Microsoft to be able to create content for the React Native DevBlog; in order to do so, we have prepared an issue template for [pitching ideas and proposing content](https://github.com/microsoft/react-native-devblog/issues/new?assignees=&labels=pitch&template=pitch-idea.yml&title=%5BPitch%5D%3A+) that the author would like to work on.\n\nFirst time authors **need** to pitch their idea via this method. Recurring authors can jump directly to step #3 if they feel their proposal is solid, but we'd suggest to still go first via an issue for pitching.\n\nPlease remember that since this repository is public, all the information you will be sharing in your pitch should be shareable with the general public! If there are aspects of your idea that you might want to pre-discuss privately first, reach out to the editorial team.\n\n### 2. Idea gets approved\n\nOnce the editorial team has been able to review the idea and approve it, the author will be added to the [RNDevBlog Writers](https://github.com/orgs/microsoft/teams/rndevblog-writers) GitHub team.\n\nThe author will also be asked to join the [Wordpress Instance](https://devblogs.microsoft.com/react-native/wp-admin/users.php) and setup their author profile over there; the username in Wordpress is needed for the author metadata field (see in the next section).\n\n### 3. Draft gets made\n\nOnce the author has got write access to the repository, they will git clone it and create a branch similar to `<author-alias>/<blogpost-title>`.\n\nIn it, they will make a new markdown file named something like `<tentative-date-YY-MM-DD>-<title>.md` and copy over this template:\n\n```md\n---\npost_title: \"post title\"\nauthor1: the-author-wordpress-username\npost_slug: same-as-filename\npost_date: same-as-filename-YY-MM-DD 12:00:00\ncategories: react-native // don't change the category\ntags: // add one or more tags, try reusing existing ones\nsummary: // one of two phrases that will preview the content\n---\n\n// the actual content goes here\n\n---\n\nYou can also follow us on Twitter [@ReactNativeMSFT](https://twitter.com/reactnativemsft) to keep up to date on news, feature roadmaps, and more.\n```\n\nThey can then work on the markdown file, adding their content and modifying the metadata accordingly.\n\nIf they need to add assets like images, put them in the `assets` folder; if there's more than one piece of media, please make a dedicated subfolder under `assets/<blogpost-file-title>`.\n\nIf needed, a custom featured image can be added under `assets/featured-images/<blogpost-file-title>`. More indications around featured images will be added ASAP.\n\n### 4. Draft gets approved\n\nOnce the author is ready to have their content reviewed, they can push up the branch and open a PR. At this point, the editorial team will review it and decide a publishing date with the author (so that the markdown file can have the right timestamp where needed).\n\nOnce it's approved, congrats! It's almost ready to be published: the PR will get merged, and with the help of the editorial team the tool syncing the GH->WP content will be triggered (it might require a one-commit-one-file follow up commit in main branch).\n\n### 5. Draft gets published\n\nOnce the draft reaches WordPress, it will only put it as a draft; the editorial team will need to go into the WP dashboard, \"all posts\" sections, select the new post (it will have a \"- DRAFT\" appended to the title), click on Edit, set the right date, then hit publish.\n\nAfter publication, the content will be broadcasted accordingly on whichever social medias have been agreeded (if any).\n\n### More information and gotchas\n\nRead more of how to use Markdown to deploy to the WordPress based blog [in this documentation](https://dev.azure.com/devdiv/DevDiv/_wiki/wikis/DevDiv.wiki/10339/Drafting-in-GitHub?anchor=template-to-add-at-the-top-of-a-github-file).\n\nThe tool used for this GH -> WP synchronization has a few limitations, so please keep the following in mind:\n\n- any edits to the post in WordPress side will not be synced back to GitHub. They will need to be manually replicated.\n- remember: author username need to be the WordPress ones, not the GitHub ID\n- on main, you need to do a one-commit-per-one-blog strategy (you can checkout the history for examples): the GH->WP tool doesn't work when you push multiple files in the same commit\n\nFor any further questions reach out to the editorial team.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "react-native-devblog", "org_name": "microsoft", "org_repo": "microsoft/react-native-devblog", "platform_org_repo": "github+microsoft/react-native-devblog", "link_to_repo": "https://github.com/microsoft/react-native-devblog", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# CodeQL\n\nThis open source repository contains the standard CodeQL libraries and queries that power [GitHub Advanced Security](https://github.com/features/security/code) and the other application security products that [GitHub](https://github.com/features/security/) makes available to its customers worldwide.\n\n## How do I learn CodeQL and run queries?\n\nThere is [extensive documentation](https://codeql.github.com/docs/) on getting started with writing CodeQL using the [CodeQL extension for Visual Studio Code](https://codeql.github.com/docs/codeql-for-visual-studio-code/) and the [CodeQL CLI](https://codeql.github.com/docs/codeql-cli/).\n\n## Contributing\n\nWe welcome contributions to our standard library and standard checks. Do you have an idea for a new check, or how to improve an existing query? Then please go ahead and open a pull request! Before you do, though, please take the time to read our [contributing guidelines](CONTRIBUTING.md). You can also consult our [style guides](https://github.com/github/codeql/tree/main/docs) to learn how to format your code for consistency and clarity, how to write query metadata, and how to write query help documentation for your query.\n\nFor information on contributing to CodeQL documentation, see the \"[contributing guide](docs/codeql/CONTRIBUTING.md)\" for docs.\n\n## License\n\nThe code in this repository is licensed under the [MIT License](LICENSE) by [GitHub](https://github.com).\n\nThe CodeQL CLI (including the CodeQL engine) is hosted in a [different repository](https://github.com/github/codeql-cli-binaries) and is [licensed separately](https://github.com/github/codeql-cli-binaries/blob/main/LICENSE.md). If you'd like to use the CodeQL CLI to analyze closed-source code, you will need a separate commercial license; please [contact us](https://github.com/enterprise/contact) for further help.\n\n## Visual Studio Code integration\n\nIf you use Visual Studio Code to work in this repository, there are a few integration features to make development easier.\n\n### CodeQL for Visual Studio Code\n\nYou can install the [CodeQL for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=GitHub.vscode-codeql) extension to get syntax highlighting, IntelliSense, and code navigation for the QL language, as well as unit test support for testing CodeQL libraries and queries.\n\n### Tasks\n\nThe `.vscode/tasks.json` file defines custom tasks specific to working in this repository. To invoke one of these tasks, select the `Terminal | Run Task...` menu option, and then select the desired task from the dropdown. You can also invoke the `Tasks: Run Task` command from the command palette.\n", "repo_name": "codeql", "org_name": "microsoft", "org_repo": "microsoft/codeql", "platform_org_repo": "github+microsoft/codeql", "link_to_repo": "https://github.com/microsoft/codeql", "platform": "github", "language": "CodeQL", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# lockfile-explorer-demos\n\n<div>\n  <br />\n  <a href=\"https://lfx.rushstack.io/\">\n    <img width=\"200\" alt=\"Rush Lockfile Explorer\" src=\"https://rushstack.io/images/lockfile-explorer.svg\">\n  </a>\n  <p />\n</div>\n\nThis branch is part of\n[a tutorial series](https://lfx.rushstack.io/pages/scenarios/demos_repo/)\nfrom the **Lockfile Explorer** documentation.\n\n\ud83d\udc49 To report problems with this content, [create a GitHub issue](https://github.com/microsoft/rushstack-websites/issues) in the main [microsoft/rushstack-websites](https://github.com/microsoft/rushstack-websites/issues) monorepo.\n\n## Instructions\n\nThis `main` branch should be cloned into a folder named `lockfile-explorer-demos-verdaccio`,\nso that the `demo/___` branch can be cloned into a folder named `lockfile-explorer-demos`.\n\nPlease see the Lockfile Explorer\n[Demos repository](https://lfx.rushstack.io/pages/scenarios/demos_repo/)\ndocumentation for complete setup instructions, as well as explanations of each demo branch.\n\n## Demo branches\n\nHere's a quick listing of the demo branches in this repository:\n\n- [demo/doppel-1](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/doppel-1)\n- [demo/doppel-2](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/doppel-2)\n- [demo/doppel-3](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/doppel-3)\n- [demo/peer-1](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/peer-1)\n- [demo/peer-2](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/peer-2)\n- [demo/peer-3](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/peer-3)\n- [demo/peer-4](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/peer-4)\n- [demo/peer-5](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/peer-5)\n- [demo/sample-1](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/sample-1)\n- [demo/sbs-1](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/sbs-1)\n- [demo/sbs-2](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/sbs-2)\n- [demo/sbs-3](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/sbs-3)\n\nThe base branch that is used to sync template content across the branches:\n\n- [demo/\\_base](https://github.com/microsoft/lockfile-explorer-demos/tree/demo/_base)\n\n## Contributor Notice\n\nThis repo welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis repo has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "lockfile-explorer-demos", "org_name": "microsoft", "org_repo": "microsoft/lockfile-explorer-demos", "platform_org_repo": "github+microsoft/lockfile-explorer-demos", "link_to_repo": "https://github.com/microsoft/lockfile-explorer-demos", "platform": "github", "language": "JavaScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# EA-HAS-Bench: Energy-Aware Hyperparameter and Architecture Search Benchmark\nWe present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a **large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption**. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Bezier curve-based model to predict learning curves with unlimited shape and length.\n\n<p align=\"center\">\n<img src=\"BSC\\figures\\BSC.png\" alt=\"EA-NAS-Bench\" width=\"80%\">\n</p>\n\nMost of the existing conventional benchmarks like `NAS-Bench-101` do not directly provide training energy cost but use model training time as the training resource budget, which as verified by our experiments, is an inaccurate estimation of energy cost. `HW-NAS-bench`  provides the inference latency and inference energy consumption of different model architectures but also does not provide the search energy cost\n<p align=\"center\">\n<img src=\"BSC\\figures\\Differences.png\" alt=\"Differece\" width=\"80%\">\n</p>\n\n## Dataset Overview\n### EA-HAS-Bench's Search Space\nUnlike the search space of existing mainstream NAS-Bench that focuses only on network architectures, our `EA-HAS-Bench` consists of a combination of two parts: the network architecture space- $\\mathrm{RegNet}$  and the hyperparameter space for optimization and training, in order to cover diversified configurations that affect both performance and energy consumption. The details of the search space are shown in Table.\n\n<p align=\"center\">\n<img src=\"BSC\\figures\\search_space.png\" alt=\"SearchSpace\" width=\"80%\">\n</p>\n\n### Evaluation Metrics\nThe `EA-HAS-Bench` dataset provides the following three types of metrics to evaluate different configurations. \n+ **Model Complexity:** parameter size, FLOPs, number of network activations (the size of the output tensors of each convolutional layer), as well as the inference energy cost of the trained model.\n+ **Model Performance:** **full training information** including training, validation, and test accuracy learning curves.\n+ **Search Cost:** energy cost (in kWh) and time (in seconds)\n\n### Dataset statistics\nThe left plot shows the validation accuracy box plots for each NAS benchmark in CIFAR-10. The right plot shows a comparison of training time, training energy consumption (TEC), and test accuracy in the dataset.\n\n<div align=\"center\">\n   <img src=\"BSC\\figures\\Box_plot.jpg\" height=300/><img src=\"BSC\\figures\\Tiny_Acc_as_color.jpg\" height=300/>\n</div>\n\nAlthough training the model for a longer period is likely to yield a higher energy cost, the final cost still depends on many other factors including power (i.e., consumed energy per hour). The left and right plots of Figure also verifies the conclusion, where the models in the Pareto Frontier on the accuracy-runtime coordinate (right figure) are not always in the Pareto Frontier on the accuracy-TEC coordinate (left figure), showing that training time and energy cost are not equivalent. \n\n<div align=\"center\">\n   <img src=\"BSC\\figures\\Tiny_Acc_Cost.jpg\" height=300/><img src=\"BSC\\figures\\Tiny_Acc_Time.jpg\" height=300/>\n</div>\n    \n</center>\n\n## Installation\n\nClone this repository and install its requirements.\n```bash\ngit clone https://github.com/microsoft/EA-HAS-Bench\ncd EA-HAS-Bench\ncat requirements.txt | xargs -n 1 -L 1 pip install\npip install -e .\n```\n### Surrogate Models\nDownload the pretrained [surrogate models](https://1drv.ms/u/s!AvrjE3KJo-Lbjl19CWJudMmuCYLC?e=wyrBSh) and place them into ``BSC/checkpoints/``. The current models are v0.1. \n\nNOTE: This codebase is still subject to changes. Upcoming updates include improved versions of the surrogate models and code for all experiments from the paper. The API may still be subject to changes.\n\n### Small Tabular Benchmark\nBesides providing a large-scale proxy benchmark and the tens of thousands of sampling points used to construct it, we also provide a small real tabular benchmark.\nWe redefine a very small joint search space with a size of 500. The latest benchmark file of NAS-Toy can be downloaded from [One Drive]().\n\n## Using the API\nThe api is located in [`api.py`](https://github.com/microsoft/EA-HAS-Bench/blob/main/api.py).\n\nHere is an example of how to use the API:\n```python\n\ndef get_ea_has_bench_api(dataset):\n    full_api = {}\n    # load the ea-nas-bench surrogate models\n    if dataset==\"cifar10\":\n        ea_has_bench_model = load_ensemble('checkpoints/ea_has_bench-v0.2'))\n        train_energy_model = load_ensemble('checkpoints/ea_has_bench-trainE-v0.2')\n        test_energy_model = load_ensemble('checkpoints/ea_has_bench-testE-v0.1')\n        runtime_model = load_ensemble('checkpoints/ea_has_bench-runtime-v0.1')\n    elif dataset==\"tiny\":\n        ea_has_bench_model = load_ensemble('checkpoints/ea-nas-bench-tiny-v0.2')\n        train_energy_model = load_ensemble('checkpoints/ea-nas-bench-trainE-v0.1')\n        test_energy_model = load_ensemble('checkpoints/ea-nas-bench-testE-v0.1')\n        runtime_model = load_ensemble('checkpoints/ea-nas-bench-runtime-v0.1')\n\n    full_api['ea_has_bench_model'] = [ea_has_bench_model, runtime_model, train_energy_model, test_energy_model]\n    return full_api\n\nea_api = get_ea_has_bench_api(\"cifar10\")\n\n# output the learning curve, train time, TEC and IEC\nlc = ea_api['ea_has_bench_model'][0].predict(config=arch_str)\ntrain_time = ea_api['ea_has_bench_model'][1].predict(config=arch_str)\ntrain_cost = ea_api['ea_has_bench_model'][2].predict(config=arch_str)\ntest_cost = ea_api['ea_has_bench_model'][3].predict(config=arch_str)\n```\n\n## Run NAS experiments\n\n```\n# Supported optimizers: rs, re, {EA}-(ls bananas), hb, bohb \ncd naslib\nbash naslib/benchmarks/nas/run_nbgree.sh \nbash naslib/benchmarks/nas/run_nbtoy.sh \n```\nResults will be saved in ``results/``.\n\n\n## How to Re-create EA-HAS-Bench from Scratch\n### Sampling points in $\\mathrm{RegNet}$ + hpo\nFor `EA-HAS-Bench`\u2019s search space that contains both model architectures and hyperparameters, we use random search (RS) to sample unbiased data to build a robust surrogate benchmark.\n\nThe following command will train all architecture candidate in the search space.\n```\ncd RegNet+HPO\npython tools/azure_sweep.py --mode amlt --config_path configs\\sweeps\\cifar\\mb_v0.4.yaml\npython tools/azure_sweep.py --mode amlt --config_path configs\\sweeps\\tinyimagenet\\mb_v0.1.yaml\n```\n\nAfter training these candidate architectures, please use the following command to re-organize all logs into the single file.\n\n```\npython tools/sweep_collect.py\n```\n\n### Creating B\u00e9zier Curves-based surrogated model\nTo fit a B\u00e9zier Curves-based surrogated model surrogate model run\n```\ncd BSC\npython fit_model.py --search_space regnet --model bezier_nn_STAR\n```\n\n## Citation\n```bibtex\n@inproceedings{ea23iclr,\ntitle={{EA}-{HAS}-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark},\nauthor={Dou, Shuguang and JIANG, XINYANG and Zhao, Cai Rong and Li, Dongsheng},\nbooktitle={International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=n-bvaLSCC78},\nnote={ICLR 2023 notable top 25%}\n}\n```\n", "repo_name": "EA-HAS-Bench", "org_name": "microsoft", "org_repo": "microsoft/EA-HAS-Bench", "platform_org_repo": "github+microsoft/EA-HAS-Bench", "link_to_repo": "https://github.com/microsoft/EA-HAS-Bench", "platform": "github", "language": "Python", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Power Apps Teams Sample App Templates\n\nSeveral sample app templates are available for use in Microsoft Teams. You can choose the sample app template that best fits your business requirement and quickly install it to get started. Sample app templates created with [Power Apps](https://learn.microsoft.com/en-us/power-apps/powerapps-overview) typically consist of multiple components such as apps, flows, and tables.\n\nRefer to [this link](https://learn.microsoft.com/en-us/power-apps/teams/use-sample-apps) for more information on Power Apps sample app templates in Teams.\n\nThis repo has 10 sample app templates:\n\n### [Boards](https://aka.ms/Boards)\n![image](https://user-images.githubusercontent.com/84343636/210890199-754cb955-c403-4913-b9ae-5a206ef48c46.png)\n\nA simple way to connect and share with people in your organization with similar interests.\n\n### [Bulletins](https://aka.ms/Bulletins)\n![image](https://user-images.githubusercontent.com/84343636/210890360-e62ce16d-f095-454f-b54b-aadea8776979.png)\n\nManager and user apps for company communications.\n\n### [Employee Ideas](https://aka.ms/EmployeeIdeas)\n![image](https://user-images.githubusercontent.com/84343636/210890443-abc27abc-576b-47aa-bb00-171919704530.png)\n\nApp for campaigns and ideas.\n\n### [How To](https://aka.ms/HowTo-TeamsSampleApp)\n![image](https://user-images.githubusercontent.com/84343636/210890607-a6b42178-5f05-4e7b-bb75-93476ee91351.png)\n\nLearn how to be a Power Apps maker.\n\n### [Inspection](https://aka.ms/AreaInspection)\n![image](https://user-images.githubusercontent.com/84343636/210890647-29f63e51-8b55-4011-99db-e9db92837bd3.png)\n\nManager and user apps for area inspections.\n\n### [Issue Reporting](https://aka.ms/IssueReporting)\n![image](https://user-images.githubusercontent.com/84343636/210890704-46c5fca7-ca87-42ef-b6be-07f60e78c381.png)\n\nManager and user apps for issue reporting.\n\n### [Milestones](https://aka.ms/Milestones)\n![image](https://user-images.githubusercontent.com/84343636/210890755-eae25c06-295a-4e7d-b040-fe6eda6eae9c.png)\n\nApp to keep track of projects, and initiatives.\n\n### [Perspectives](https://aka.ms/Perspectives-TeamsSampleApp)\n![image](https://user-images.githubusercontent.com/84343636/210890797-49afe1bb-0c8e-48de-8487-aaa140b0c29b.png)\n\nA simple way to add topics and extend the topics with Q&A for discussions\n\n### [Profile+](https://aka.ms/ProfilePlus)\n![image](https://user-images.githubusercontent.com/84343636/210891243-ea791ba3-21eb-43d6-a194-9c6b24fb75a3.png)\n\nQuickly find out about people in your organization.\n\n### [Get Connected](https://aka.ms/GetConnected-TeamsSampleApp)\n![image](https://user-images.githubusercontent.com/84343636/210891291-edf22da2-67f3-4d8c-bff6-1cbc26201c1c.png)\n\nConnect with people in your organization with similar skills.\n\n## Installation\nTo install the app, you can download the app package from this repo's release. Then upload the package as a teams custom app or import the app as a solution to your Power Apps environment. Refer to the [installation guide](https://github.com/microsoft/teams-powerapps-app-templates/blob/main/INSTALLATION.md) for details.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "teams-powerapps-app-templates", "org_name": "microsoft", "org_repo": "microsoft/teams-powerapps-app-templates", "platform_org_repo": "github+microsoft/teams-powerapps-app-templates", "link_to_repo": "https://github.com/microsoft/teams-powerapps-app-templates", "platform": "github", "language": null, "stargazers_count": 78, "watchers_count": 78}, {"README_text": "# SimXNS\n\n[\u2728Updates](#\ufe0fUpdates) | [\ud83d\udcdcCitation](#Citation) | [\ud83e\udd18Furthermore](#Furthermore) | [\u2764\ufe0fContributing](#Contributing) | [\ud83d\udcdaTrademarks](#Trademarks)\n\n[SimXNS](https://aka.ms/simxns) is a research project for information retrieval by MSRA NLC IR team. Some of the techniques are actively used in [Microsoft Bing](https://www.bing.com/). This repo provides the official code implementations.\n\nCurrently, this repo contains `SimANS`, `MASTER`, `PROD` and `LEAD`, and all these methods are designed for information retrieval.\nHere are some basic descriptions to help you catch up with the characteristics of each work:\n- [**SimANS**](https://arxiv.org/abs/2210.11773) is a simple, general and flexible ambiguous negatives sampling method for dense text retrieval. It can be easily applied to various dense retrieval methods like [AR2](https://github.com/microsoft/AR2). This method is also applied in [Bing](https://www.bing.com/) search engine, which is proven to be effective.\n- [**MASTER**](https://arxiv.org/abs/2212.07841) is a multi-task pre-trained model that unifies and integrates multiple pre-training tasks with different learning objectives under the bottlenecked masked autoencoder architecture.\n- [**PROD**](https://arxiv.org/abs/2209.13335) is a novel distillation framework for dense retrieval, which consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student.\n- [**LEAD**](https://arxiv.org/abs/2212.05225) aligns the layer features of student and teacher, emphasizing more on the informative layers by re-weighting.\n\n## Updates\n\n- 2023/05/29: release the official code of [LEAD](https://github.com/microsoft/SimXNS/tree/main/LEAD).\n- 2023/02/16: refine the resources of [SimANS](https://github.com/microsoft/SimXNS/tree/main/SimANS) by uploading files in a seperated style and offering the [file list](https://github.com/microsoft/SimXNS/tree/main/SimANS#-how-to-use).\n- 2023/02/02: release the official code of [PROD](https://github.com/microsoft/SimXNS/tree/main/PROD).\n- 2022/12/16: release the official code of [MASTER](https://github.com/microsoft/SimXNS/tree/main/MASTER).\n- 2022/11/17: release the official code of [SimANS](https://github.com/microsoft/SimXNS/tree/main/SimANS).\n\n\n## Citation\nIf you extend or use this work, please cite our paper where it was introduced:\n\n\n- **SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval**. Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, Nan Duan, Weizhu Chen. ***EMNLP 2022***. [Code](https://github.com/microsoft/SimXNS/tree/main/SimANS), [Paper](https://arxiv.org/abs/2210.11773).\n- **MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers**. Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, Ji-Rong Wen. ***arXiv***. [Code](https://github.com/microsoft/SimXNS/tree/main/MASTER), [Paper](https://arxiv.org/abs/2212.07841).\n- **PROD: Progressive Distillation for Dense Retrieval**. Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, Nan Duan. ***WWW 2023***. [Code](https://github.com/microsoft/SimXNS/tree/main/PROD), [Paper](https://arxiv.org/abs/2209.13335).\n- **LEAD: Liberal Feature-based Distillation for Dense Retrieval**. Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jian Jiao, Jingwen Lu, Yan Zhang, Daxin Jiang, Linjun Yang, Rangan Majumder, Nan Duan. ***arXiv***. [Code](https://github.com/microsoft/SimXNS/tree/main/LEAD), [Paper](https://arxiv.org/abs/2212.05225).\n\n\n```bibtex\n@article{zhou2022simans,\n   title={SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval},\n   author={Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, Nan Duan and Weizhu Chen},\n   booktitle = {{EMNLP}},\n   year={2022}\n}\n@article{zhou2022master,\n   title={MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers},\n   author={Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, Ji-Rong Wen},\n   booktitle = {{arXiv}},\n   year={2022}\n}\n@article{lin2023prod,\n   title={PROD: Progressive Distillation for Dense Retrieval},\n   author={Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, Nan Duan},\n   booktitle = {{WWW}},\n   year={2023}\n}\n@article{sun2022lead,\n  title={LEAD: Liberal Feature-based Distillation for Dense Retrieval},\n  author={Sun, Hao and Liu, Xiao and Gong, Yeyun and Dong, Anlei and Jiao, Jian and Lu, Jingwen and Zhang, Yan and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and others},\n  journal={arXiv preprint arXiv:2212.05225},\n  year={2022}\n}\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SimXNS", "org_name": "microsoft", "org_repo": "microsoft/SimXNS", "platform_org_repo": "github+microsoft/SimXNS", "link_to_repo": "https://github.com/microsoft/SimXNS", "platform": "github", "language": "Python", "stargazers_count": 71, "watchers_count": 71}, {"README_text": "# TorchScale - A Library for Transformers at (Any) Scale\n\n<p>\n  <a href=\"https://github.com/microsoft/torchscale/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://pypi.org/project/torchscale\"><img alt=\"MIT License\" src=\"https://badge.fury.io/py/torchscale.svg\" /></a>\n</p>\n\nTorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively.\nIt has the implementation of fundamental research to improve modeling generality and capability as well as training stability and efficiency of scaling Transformers.\n\n- Stability - [**DeepNet**](https://arxiv.org/abs/2203.00555): scaling Transformers to 1,000 Layers and beyond\n- Generality - [**Foundation Transformers (Magneto)**](https://arxiv.org/abs/2210.06423): towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)\n- Capability - A [**Length-Extrapolatable**](https://arxiv.org/abs/2212.10554) Transformer\n- Efficiency - [**X-MoE**](https://arxiv.org/abs/2204.09179): scalable & finetunable sparse Mixture-of-Experts (MoE)\n\n## News\n\n- November, 2022: TorchScale 0.1.1 released [[Paper](https://arxiv.org/abs/2211.13184)] [[PyPI](https://pypi.org/project/torchscale/)]\n\n## Installation\n\nTo install:\n```\npip install torchscale\n```\n\nAlternatively, you can develop it locally:\n```\ngit clone https://github.com/microsoft/torchscale.git\ncd torchscale\npip install -e .\n```\n\n## Getting Started\n\nIt takes only several lines of code to create a model with the above fundamental research features enabled. Here is how to quickly obtain a BERT-like encoder:\n\n```python\n>>> from torchscale.architecture.config import EncoderConfig\n>>> from torchscale.architecture.encoder import Encoder\n\n>>> config = EncoderConfig(vocab_size=64000)\n>>> model = Encoder(config)\n\n>>> print(model)\n```\n\nWe also support the `Decoder` architecture and the `EncoderDecoder` architecture:\n\n```python\n# Creating a decoder model\n>>> from torchscale.architecture.config import DecoderConfig\n>>> from torchscale.architecture.decoder import Decoder\n\n>>> config = DecoderConfig(vocab_size=64000)\n>>> decoder = Decoder(config)\n>>> print(decoder)\n\n# Creating a encoder-decoder model\n>>> from torchscale.architecture.config import EncoderDecoderConfig\n>>> from torchscale.architecture.encoder_decoder import EncoderDecoder\n\n>>> config = EncoderDecoderConfig(vocab_size=64000)\n>>> encdec = EncoderDecoder(config)\n>>> print(encdec)\n```\n\n## Key Features\n\n- [DeepNorm to improve the training stability of Post-LayerNorm Transformers](https://arxiv.org/abs/2203.00555)\n  * enabled by setting *deepnorm=True* in the `Config` class. \n  * It adjusts both the residual connection and the initialization method according to the model architecture (i.e., encoder, decoder, or encoder-decoder).\n\n- [SubLN for the model generality and the training stability](https://arxiv.org/abs/2210.06423)\n  * enabled by *subln=True*. This is enabled by default. \n  * It introduces another LayerNorm to each sublayer and adjusts the initialization according to the model architecture.\n  * Note that SubLN and DeepNorm cannot be used in one single model.\n\n- [X-MoE: efficient and finetunable sparse MoE modeling](https://arxiv.org/abs/2204.09179)\n  * enabled by *use_xmoe=True*. \n  * It replaces every *'moe_freq'* `FeedForwardNetwork` layers with the X-MoE layers.\n\n- [Multiway architecture for multimodality](https://arxiv.org/abs/2208.10442)\n  * enabled by *multiway=True*.\n  * It provides a pool of Transformer's parameters used for different modalities.\n\n- [Extrapolatable position embedding (Xpos)](https://arxiv.org/abs/2212.10554)\n  * enabled by *xpos_rel_pos=True*.\n\n- [Relative position bias](https://arxiv.org/abs/1910.10683)\n  * enabled by adjusting *rel_pos_buckets* and *max_rel_pos*.\n\n- [SparseClip: improving the gradient clipping for sparse MoE models](https://arxiv.org/abs/2211.13184)\n  * we provide a [sample code](examples/fairseq/utils/sparse_clip.py) that can be easily adapted to the FairSeq (or other) repo.\n\nMost of the features above can be used by simply passing the corresponding parameters to the config. For example:\n\n```python\n>>> from torchscale.architecture.config import EncoderConfig\n>>> from torchscale.architecture.encoder import Encoder\n\n>>> config = EncoderConfig(vocab_size=64000, deepnorm=True, multiway=True)\n>>> model = Encoder(config)\n\n>>> print(model)\n```\n\n## Examples\n\nWe have the examples of how to use TorchScale in the following scenarios/tasks:\n\n- Language\n\n  * [Decoder/GPT](examples/fairseq/README.md#example-gpt-pretraining)\n\n  * [Encoder-Decoder/Neural Machine Translation](examples/fairseq/README.md#example-machine-translation)\n\n  * [Encoder/BERT](examples/fairseq/README.md#example-bert-pretraining)\n\n- Vision\n\n  * ViT/BEiT [In progress]\n\n- Speech\n\n- Multimodal\n\n  * [Multiway Transformers/BEiT-3](https://github.com/microsoft/unilm/tree/master/beit3)\n\nWe plan to provide more examples regarding different tasks (e.g. vision pretraining and speech recognition) and various deep learning toolkits (e.g. [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)). Any comments or PRs are welcome!\n\n## Results\n\n### Stability Evaluation\n\n<p align=\"center\">\n  <img src=\"https://publicmodel.blob.core.windows.net/torchscale/pic/convergence.png\" width=\"800\"/>\n</p>\n\nThe training curve is smooth by using TorchScale, while the baseline Transformer cannot converge.\n\n### Scaling-up Experiments\n\n<p align=\"center\">\n  <img src=\"https://publicmodel.blob.core.windows.net/torchscale/pic/scaling_curve.png\" width=\"800\"/>\n</p>\n\nTorchScale supports arbitrary depths and widths, successfully scaling-up the models without pain.\n\n## Acknowledgments\n\nSome implementations in TorchScale are either adapted from or inspired by the [FairSeq](https://github.com/facebookresearch/fairseq) repository and the [UniLM](https://github.com/microsoft/unilm) repository.\n\n## Citations\n\nIf you find this repository useful, please consider citing our work:\n\n```\n@article{torchscale,\n  author    = {Shuming Ma and Hongyu Wang and Shaohan Huang and Wenhui Wang and Zewen Chi and Li Dong and Alon Benhaim and Barun Patra and Vishrav Chaudhary and Xia Song and Furu Wei},\n  title     = {{TorchScale}: {Transformers} at Scale},\n  journal   = {CoRR},\n  volume    = {abs/2211.13184},\n  year      = {2022}\n}\n```\n\n```\n@article{deepnet,\n  author    = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},\n  title     = {{DeepNet}: Scaling {Transformers} to 1,000 Layers},\n  journal   = {CoRR},\n  volume    = {abs/2203.00555},\n  year      = {2022},\n}\n```\n\n```\n@article{magneto,\n  author    = {Hongyu Wang and Shuming Ma and Shaohan Huang and Li Dong and Wenhui Wang and Zhiliang Peng and Yu Wu and Payal Bajaj and Saksham Singhal and Alon Benhaim and Barun Patra and Zhun Liu and Vishrav Chaudhary and Xia Song and Furu Wei},\n  title     = {Foundation {Transformers}},\n  journal   = {CoRR},\n  volume    = {abs/2210.06423},\n  year      = {2022}\n}\n```\n\n```\n@inproceedings{xmoe,\n  title={On the Representation Collapse of Sparse Mixture of Experts},\n  author={Zewen Chi and Li Dong and Shaohan Huang and Damai Dai and Shuming Ma and Barun Patra and Saksham Singhal and Payal Bajaj and Xia Song and Xian-Ling Mao and Heyan Huang and Furu Wei},\n  booktitle={Advances in Neural Information Processing Systems},\n  year={2022},\n  url={https://openreview.net/forum?id=mWaYC6CZf5}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [Furu Wei](mailto:fuwei@microsoft.com) and [Shuming Ma](mailto:shumma@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "torchscale", "org_name": "microsoft", "org_repo": "microsoft/torchscale", "platform_org_repo": "github+microsoft/torchscale", "link_to_repo": "https://github.com/microsoft/torchscale", "platform": "github", "language": "Python", "stargazers_count": 1666, "watchers_count": 1666}, {"README_text": "![banner](/docs/fuzzylogo.png)\n\n# Fuzzy Matching\n\n## Why Fuzzy Matching IP?\nFuzzy matching is used in situations in which the client needs to develop data-driven insights off of disparate data sources in which there are missing keys, messy data and/or unstructued data.\n\n### Examples\n1. Customer Insight (golden record or single source of truth)\n2. Employee records, including skills & capabilities\n3. Longitudinal pation record\n4. Other master data management applications\n\nGiven the pervasiveness of these use cases that we have seen with our customers, we want to provide shared best practices and IP around fuzzy matching.\n\nSee the deck [Introduction to Fuzzy Matching Standard Delivery IP](/docs/IntroductionToFuzzyMatchingStandardDeliveryIP.pptx) found in the docs folder\n\n## What's in the docs\nThe docs contains best practices and resources for implementing fuzzy matching solutions. The docs are broken down into the following sections:\n- [Data Management](/docs/DataManagement.md)\n- [People Matching](/docs/PeopleMatching.md)\n- [Knowledge Graphs](/docs/KnowledgeGraphs.md)\n\n## Repository structure\n\n```\n\u251c\u2500\u2500\u2500architecture\n\u251c\u2500\u2500\u2500data\n\u251c\u2500\u2500\u2500docs\n\u251c\u2500\u2500\u2500notebooks\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-fuzzymatching-v2", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-fuzzymatching-v2", "platform_org_repo": "github+microsoft/dstoolkit-fuzzymatching-v2", "link_to_repo": "https://github.com/microsoft/dstoolkit-fuzzymatching-v2", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Microsoft Entra Wallet Library\n![badge-privatepreview]\n![badge-packagemanagers-supported] \n![badge-package-version] \n![badge-languages] \n![badge-platforms]\n![badge-license]\n![badge-azure-pipline]\n\n## Introduction\nThe Microsoft Entra Wallet Library for Android gives your app the ability to begin using the Microsoft Entra Verified Id platform by supporting the issuance and presentation of Verified Ids in accordance with OpenID Connect, Presentation Exchange, Verifiable Credentials, and more up and coming industry standards.\n\n---\n## Installation\n\nAdd to your app's build.gradle to add Wallet Library as a dependency:\n```kotlin\ndependencies {\n    implementation 'com.microsoft.entra.verifiedid:walletlibrary:0.0.1'\n}\n```\n\n## Quick Start\nHere is a simple example of how to use the library. For more in-depth examples, check out the sample app.\n\n```kotlin\n// Create a verifiedIdClient\nval verifiedIdClient = VerifiedIdClientBuilder(context).build()\n\n// Create a VerifiedIdRequestInput using a OpenId Request Uri.\nval verifiedIdRequestUrl = VerifiedIdRequestURL(Uri.parse(\"openid-vc://...\"))\nval verifiedIdRequestResult: Result<VerifiedIdRequest<*>> = verifiedIdClient.createRequest(verifiedIdRequestUrl)\n\n// Every external method's return value is wrapped in a Result object to ensure proper error handling.\nif (verifiedIdRequestResult.isSuccess) {\n    val verifiedIdRequest = verifiedIdRequestResult.getOrNull()\n    val presentationRequest = verifiedIdRequest?.let {\n        verifiedIdRequest as VerifiedIdPresentationRequest\n    }\n} else {\n    // If an exception occurs, its value can be accessed here.\n    val exception = verifiedIdRequestResult.exceptionOrNull()\n}\n```\n\nAt the time of publish, we support the following requirements on a request:\n| Requirement                  \t| Description \t| Supported on Request \t|\n|------------------------------\t|-------------\t|------------------------------\t|\n| GroupRequirement             \t| A verifier/issuer could request multiple requirements. If more than one requirement is requested, a GroupRequirement contains a list of the requirements.        \t| Issuance/Presentation        \t|\n| VerifiedIdRequirement        \t| A verifier/issuer can request a VerifiedId. See below for helper methods to fulfill the requirement.       \t| Issuance/Presentation        \t|\n| SelfAttestedClaimRequirement \t| An issuer might require a self-attested claim that is simply a string value.        \t| Issuance                     \t|\n| PinRequirement               \t| An issuer might require a pin from user.         \t| Issuance                     \t|\n| AccessTokenRequirement       \t| An issuer might request an Access Token. An Access Token must be retrieved using an external library.        \t| Issuance                     \t|\n| IdTokenRequirement           \t| An issuer might request an Id Token. If the Id Token is not already injected into the request, an Id Token must be retrieved using an external library.       \t| Issuance                     \t|\n\nTo fulfill a requirement, cast it to the correct Requirement type and use the `fulfill` method.\n```kotlin\nval verifiedIdRequirement = presentationRequest.requirement as VerifiedIdRequirement\nverifiedIdRequirement.fulfill(verifiedId)\n```\n\nVerifiedIdRequirement contains a helper function `getMatches` that will filter all of the VerifiedId that satisfies the constraints on the VerifiedIdRequirement from a list of VerifiedIds.\n```kotlin\nval matchingVerifiedIds = verifiedIdRequirement.getMatches(verifiedIds: List<VerifiedId>)\n```\n\nYou can also validate a requirement to ensure the requirement has been fulfilled.\n```kotlin\nval validationResult = verifiedIdRequirement.validate()\n```\n\nOnce all of the requirements are fulfilled, you can double check that the request has been satisfied by calling the `isSatisfied` method on the request object.\n```kotlin\nval isSatisfied = presentationRequest.isSatisfied()\n```\n\nThen, complete the request using the complete method.\n- The `complete` method on a `VerifiedIdIssuanceRequest` returns a successful result that contains the issued `VerifiedId`, or if an error occurs, returns a failure result with the error.\n- The `complete` method on a `VerifiedIdPresentationRequest` returns an empty successful result or if an error occurs, returns a failure result with the error.\n```kotlin\nval result = presentationRequest.complete()\n```\n\n---\n## VerifiedId\nA Verified Id is a verifiable piece of information that contains claims about an entity.\n\n### Style\nIssuers have the ability to customize the style of a Verified Id. We support `BasicVerifiedIdStyle` which contains basic traits like name, issuer, background color, text color, and logo that can be used to represent the look and feel of a Verified Id.\n\n### Storing VerifiedIds\nIt is the responsibility of the app developer to store the VerifiedIds. We have included helper functions to encode/decode VerifiedIds to easily store the VerifiedIds in a database as a primitive type.\n\n```kotlin\n// Encode a VerifiedId into String.\nval encodedVerifiedIdString = verifiedIdClient.encode(verifiedId)\n// Decode a VerifiedId from String.\nval verifiedId = verifiedIdClient.decodeVerifiedId(encodedVerifiedIdString)\n```\n\n## Sample App\n1. Clone this repository.\n2. Open the WalletLibrary workspace in Android Studio.\n3. Run the following command from the location where repository was cloned. It initializes the git submodules. \n```\n     git submodule update --init --recursive \n```\n4. Create a test device. \n5. Run the `walletlibarydemo` app. \n\n## Log Injection\nYou can inject your own log consumer into the Wallet Library by creating a class that conforms to the [Wallet Library Log Consumer Interface](./walletlibrary/src/main/java/com/microsoft/walletlibrary/util/WalletLibraryLogger.kt) and injecting it into the `VerifiedIdClientBuilder`.\n\n```Kotlin\nval client = VerifiedIdClientBuilder(context)\n    .with(<Your Log Consumer>)\n    .build()\n```\n\n## Documentation\n\n* [External Architecture](https://github.com/microsoft/entra-verifiedid-wallet-library-ios/blob/dev/Docs/LibraryArchitecture.md)\n* [Microsoft Docs](https://learn.microsoft.com/en-us/azure/active-directory/verifiable-credentials/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n[badge-package-version]: https://img.shields.io/maven-central/v/com.microsoft.entra.verifiedid/walletlibrary\n[badge-packagemanagers-supported]: https://img.shields.io/badge/supports-Maven%20Central-yellow.svg\n[badge-languages]: https://img.shields.io/badge/languages-Kotlin%20Java-blue.svg\n[badge-platforms]: https://img.shields.io/badge/platforms-Android-lightgrey.svg\n[badge-license]: https://img.shields.io/github/license/microsoft/entra-verifiedid-wallet-library-android\n[badge-azure-pipline]: https://decentralized-identity.visualstudio.com/Core/_apis/build/status/Android%20Wallet%20Library?branchName=dev\n[badge-privatepreview]: https://img.shields.io/badge/status-Private%20Preview-red.svg", "repo_name": "entra-verifiedid-wallet-library-android", "org_name": "microsoft", "org_repo": "microsoft/entra-verifiedid-wallet-library-android", "platform_org_repo": "github+microsoft/entra-verifiedid-wallet-library-android", "link_to_repo": "https://github.com/microsoft/entra-verifiedid-wallet-library-android", "platform": "github", "language": "Kotlin", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Microsoft Entra Wallet Library\n![badge-privatepreview]\n![badge-packagemanagers-supported] \n![badge-pod-version] \n![badge-languages] \n![badge-platforms]\n![badge-license]\n![badge-azure-pipline]\n\n## Introduction\nThe Microsoft Entra Wallet Library for iOS gives your app the ability to begin using the Microsoft Entra Verified Id platform by supporting the issuance and presentation of Verified Ids in accordance with OpenID Connect, Presentation Exchange, Verifiable Credentials, and more up and coming industry standards.\n\n---\n## Installation\n\nYou can use cocoapods to install the Wallet Library by adding it to your Podfile:\n```ruby\n\ntarget \"YourApp\" do\n  use_frameworks!\n  pod \"WalletLibrary\", \"~> 0.0.1\"\nend\n```\n> note: use_frameworks! is required for this Pod.\n---\n## Quick Start\n\nHere is a simple example of how to use the library. For more in-depth examples, check out the sample app.\n \n```Swift\n\n/// Create a verifiedIdClient.\nlet verifiedIdClient = VerifiedIdClientBuilder().build()\n\n/// Create a VerifiedIdRequestInput using a OpenId Request Uri.\nlet input = VerifiedIdRequestURL(url: URL(string: \"openid-vc://...\")!)\nlet result = await verifiedIdClient.createRequest(from: input)\n\n/// Every external method's return value is wrapped in a Result object to ensure proper error handling.\nswitch (result) {\ncase .success(let request):\n    /// A request created from the method above could be an issuance or a presentation request. \n    /// In this example, it is a presentation request, so we can cast it to a VerifiedIdPresentationRequest.\n    let presentationRequest = request as? VerifiedIdPresentationRequest\ncase .failure(let error):\n    /// If an error occurs, its value can be accessed here.\n    print(error)\n}\n```\n\nAt the time of publish, we support the following requirements on a request:\n| Requirement                  \t| Description \t| Supported on Request \t|\n|------------------------------\t|-------------\t|------------------------------\t|\n| GroupRequirement             \t| A verifier/issuer could request multiple requirements. If more than one requirement is requested, a GroupRequirement contains a list of the requirements.        \t| Issuance/Presentation        \t|\n| VerifiedIdRequirement        \t| A verifier/issuer can request a VerifiedId. See below for helper methods to fulfill the requirement.       \t| Issuance/Presentation        \t|\n| SelfAttestedClaimRequirement \t| An issuer might require a self-attested claim that is simply a string value.        \t| Issuance                     \t|\n| PinRequirement               \t| An issuer might require a pin from user.         \t| Issuance                     \t|\n| AccessTokenRequirement       \t| An issuer might request an Access Token. An Access Token must be retrieved using an external library.        \t| Issuance                     \t|\n| IdTokenRequirement           \t| An issuer might request an Id Token. If the Id Token is not already injected into the request, an Id Token must be retrieved using an external library.       \t| Issuance                     \t|\n\nTo fulfill a requirement, cast it to the correct Requirement type and use the `fulfill` method.\n```Swift\nif let verifiedIdRequirement = presentationRequest.requirement as? VerifiedIdRequirement {\n    verifiedIdRequirement.fulfill(with: <Insert VerifiedId>)\n}\n```\n\nVerifiedIdRequirement contains a helper function `getMatches` that will filter all of the VerifiedId that satisfies the constraints on the VerifiedIdRequirement from a list of VerifiedIds.\n```Swift\nlet matchingVerifiedIds = verifiedIdRequirement.getMatches(verifiedIds: <List Of VerifiedIds>)\n```\n\nYou can also validate a requirement to ensure the requirement has been fulfilled.\n```Swift\nlet validationResult = verifiedIdRequirement.validate()\n```\n\nOnce all of the requirements are fulfilled, you can double check that the request has been satisfied by calling the `isSatisfied` method on the request object. \n```Swift\nlet isSatisfied = await presentationRequest.isSatisfied()\n```\n\nThen, complete the request using the complete method. \n- The `complete` method on a `VerifiedIdIssuanceRequest` returns a successful result that contains the issued `VerifiedId`, or if an error occurs, returns a failure result with the error. \n- The `complete` method on a `VerifiedIdPresentationRequest` returns an empty successful result or if an error occurs, returns a failure result with the error. \n```Swift\nlet result = await presentationRequest.complete()\n```\n---\n## VerifiedId\nA Verified Id is a verifiable piece of information that contains claims about an entity. \n\n### Style\nIssuers have the ability to customize the style of a Verified Id. We support `BasicVerifiedIdStyle` which contains basic traits like name, issuer, background color, text color, and logo that can be used to represent the look and feel of a Verified Id.\n\n### Storing VerifiedIds\nIt is the responsibility of the app developer to store the VerifiedIds. We have included helper functions to encode/decode VerifiedIds to easily store the VerifiedIds in a database as a primitive type.\n\n```Swift\n/// Encode a VerifiedId into Data.\nlet encodedVerifiedId = verifiedIdClient.encode(verifiedId: <Insert VerifiedId>)\n\n/// Decode a VerifiedId from Data.\nlet verifiedId = verifiedIdClient.decode(from: encodedVerifiedId)\n```\n\n## Sample App\n1. Clone the repository.\n2. Open a terminal window and go to the location where you cloned the repository.\n3. Type in: \n```\ngit submodule update --init \u2013recursive\n```\n> This step is important as it will initialize the submodules used in the library.\n4. Open the Wallet Library workspace in Xcode. (WalletLibrary.xcworkspace)\n5. Switch Target to WalletLibraryDemo.\n6. Run the Sample App on a simulator.\n\n## Log Injection\nYou can inject your own log consumer into the Wallet Library by creating a class that conforms to the [Wallet Library Log Consumer Protocol](./WalletLibrary/WalletLibrary/Utilities/WalletLibraryLogConsumer.swift) and injecting it into the `VerifiedIdClientBuilder`.\n\n```Swift\nlet client = VerifiedIdClientBuilder()\n    .with(logConsumer: <Your Log Consumer>)\n    .build()\n```\n\n## Documentation\n\n* [External Architecture](Docs/LibraryArchitecture.md)\n* [Microsoft Docs](https://learn.microsoft.com/en-us/azure/active-directory/verifiable-credentials/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n[badge-pod-version]: https://img.shields.io/cocoapods/v/WalletLibrary\n[badge-packagemanagers-supported]: https://img.shields.io/badge/supports-CocoaPods-yellow.svg\n[badge-languages]: https://img.shields.io/badge/languages-Swift-blue.svg\n[badge-platforms]: https://img.shields.io/badge/platforms-iOS-lightgrey.svg\n[badge-license]: https://img.shields.io/github/license/microsoft/entra-verifiedid-wallet-library-ios\n[badge-azure-pipline]: https://decentralized-identity.visualstudio.com/Core/_apis/build/status/iOS%20Wallet%20Library?branchName=dev\n[badge-privatepreview]: https://img.shields.io/badge/status-Private%20Preview-red.svg", "repo_name": "entra-verifiedid-wallet-library-ios", "org_name": "microsoft", "org_repo": "microsoft/entra-verifiedid-wallet-library-ios", "platform_org_repo": "github+microsoft/entra-verifiedid-wallet-library-ios", "link_to_repo": "https://github.com/microsoft/entra-verifiedid-wallet-library-ios", "platform": "github", "language": "Swift", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "\n\n<img width=\"2000\" height=\"150\" src=\"./images/AKS.png\"/>\n&emsp;\n&emsp;\n\n# \u4e91\u539f\u751f\u5e94\u7528Devhack2023\n\nFabrikam\u533b\u7597\u4f1a\u8bae\u63d0\u4f9b\u4f1a\u8bae\u7f51\u7ad9\u670d\u52a1\uff0c\u4e13\u4e3a\u533b\u7597\u754c\u91cf\u8eab\u5b9a\u505a\u3002\u4ed6\u4eec\u7684\u4e1a\u52a1\u5df2\u7ecf\u589e\u957f\uff0c\u8bb8\u591a\u60c5\u51b5\u4e0b\u7684\u4ee3\u7801\u57fa\u7840\u548c\u6bcf\u4e2a\u79df\u6237\u7684\u53d8\u5316\u5468\u671f\u7684\u7ba1\u7406\u5df2\u7ecf\u5931\u63a7\u3002\n\n\u672c\u7814\u8ba8\u4f1a\u7684\u76ee\u6807\u662f\u5e2e\u52a9\u4ed6\u4eec\u5efa\u7acb\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1 \uff08POC\uff09\uff0c\u5c06\u4ed6\u4eec\u7684\u4ee3\u7801\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u6613\u4e8e\u7ba1\u7406\u7684\u8fc7\u7a0b\uff0c\u5305\u62ec\u79df\u6237\u4ee3\u7801\u7684\u5bb9\u5668\u5316\u3001\u66f4\u597d\u7684 DevOps \u5de5\u4f5c\u6d41\u7a0b\u4ee5\u53ca\u6570\u636e\u5e93\u540e\u7aef\u7684\u7b80\u5355\u63d0\u5347\u548c\u79fb\u4f4d\u6545\u4e8b\u3002\n\n2023 \u5e74 1 \u6708\n\n## \u76ee\u6807\u53d7\u4f17\n\n-   \u5e94\u7528\u7a0b\u5e8f\u5f00\u53d1\u4eba\u5458\n-   \u57fa\u7840\u8bbe\u65bd\u67b6\u6784\u5e08\n\n## \u4e91\u539f\u751fDevhack\u6982\u89c8\n\n### Workshop\n\n\u5728\u8fd9\u4e2a\u7814\u8ba8\u4f1a\u4e0a\uff0c\u60a8\u5c06\u6784\u5efa\u4e00\u4e2a\u6982\u5ff5\u8bc1\u660e \uff08POC\uff09\uff0c\u5c06\u73b0\u6709\u7684\u672c\u5730\u5e94\u7528\u7a0b\u5e8f\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u5bb9\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u6b64 POC \u5c06\u63d0\u4f9b\u5229\u7528 Azure Kubernetes \u670d\u52a1 \uff08AKS\uff09\u3001Linux Node\u4e0a\u7684 Docker \u5bb9\u5668\u4ee5\u53ca\u4eceMongo\u5f00\u53d1\u94f6\u884c\u8fc1\u79fb\u5230 Cosmos \u5f00\u53d1\u94f6\u884c\u7684\u591a\u79df\u6237 Web \u5e94\u7528\u6258\u7ba1\u89e3\u51b3\u65b9\u6848\u3002\n\n\u5728\u8fd9\u4e2a\u7814\u8ba8\u4f1a\u7ed3\u675f\u65f6\uff0c\u60a8\u5c06\u80fd\u591f\u901a\u8fc7\u7ecf\u8fc7\u65f6\u95f4\u6d4b\u8bd5\u7684 DevOps \u5b9e\u8df5\uff0c\u66f4\u597d\u5730\u63d0\u9ad8\u57fa\u4e8e\u5bb9\u5668\u7684\u5e94\u7528\u7a0b\u5e8f\u7684\u53ef\u9760\u6027\u548c\u589e\u52a0\u91ca\u653e\u8282\u594f\u3002\n\n### \u767d\u677f\u8bbe\u8ba1\u7ae0\u8282\n\n\u5728\u6b64\u767d\u677f\u8bbe\u8ba1\u4f1a\u8bdd\u4e2d\uff0c\u60a8\u5c06\u4e86\u89e3\u4e0e\u5728 Azure \u4e2d\u6784\u5efa\u548c\u90e8\u7f72\u5bb9\u5668\u5316\u5e94\u7528\u7a0b\u5e8f\u76f8\u5173\u7684\u9009\u62e9\u3001\u56f4\u7ed5\u6b64\u505a\u51fa\u7684\u5173\u952e\u51b3\u7b56\u4ee5\u53ca\u89e3\u51b3\u65b9\u6848\u7684\u5176\u4ed6\u65b9\u9762\uff0c\u5305\u62ec\u5982\u4f55\u63d0\u5347\u548c\u8f6c\u79fb\u5e94\u7528\u7a0b\u5e8f\u7684\u90e8\u4ef6\u4ee5\u51cf\u5c11\u5e94\u7528\u7a0b\u5e8f\u66f4\u6539\u3002\n\n\u5230\u672c\u8bbe\u8ba1\u4f1a\u8bae\u7ed3\u675f\u65f6\uff0c\u60a8\u5c06\u80fd\u591f\u66f4\u597d\u5730\u8bbe\u8ba1\u9488\u5bf9 Azure Kubernetes \u670d\u52a1 \uff08AKS\uff09 \u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u5bb9\u5668\u5316\u5e94\u7528\u7a0b\u5e8f\u5b9a\u4e49 DevOps \u5de5\u4f5c\u6d41\u7a0b\u3002\n\n## \u52a8\u624b\u5b9e\u9a8c\n\n\u8fd9\u4e2a\u52a8\u624b\u5b9e\u9a8c\u65e8\u5728\u5f15\u5bfc\u60a8\u5b8c\u6210\u6784\u5efa\u548c\u90e8\u7f72 Docker \u955c\u50cf\u5230 Azure Kubernetes \u670d\u52a1 \uff08AKS\uff09 \u6258\u7ba1\u7684 Kubernetes \u5e73\u53f0\u7684\u8fc7\u7a0b\uff0c\u6b64\u5916\u8fd8\u5b66\u4e60\u5982\u4f55\u5904\u7406\u52a8\u6001\u670d\u52a1\u53d1\u73b0\u3001\u670d\u52a1\u6269\u5c55\u548c\u9ad8\u53ef\u7528\u6027\u3002\n\n\u5728\u6b64\u5b9e\u9a8c\u7ed3\u675f\u65f6\uff0c\u60a8\u5c06\u80fd\u591f\u66f4\u597d\u5730\u6784\u5efa\u548c\u90e8\u7f72\u5bb9\u5668\u5316\u5e94\u7528\u7a0b\u5e8f\u5230 Azure Kubernetes \u670d\u52a1\uff0c\u5e76\u6267\u884c\u5e38\u89c1\u7684 DevOps \u7a0b\u5e8f\u3002\n\n\n\n  &emsp;\n\n- [\u4e91\u539f\u751f\u5e94\u7528 -\u52a8\u624b\u5b9e\u9a8c\u524d\u7684\u51c6\u5907](./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87.md)\n  \n- [\u4e91\u539f\u751f\u5e94\u7528 -\u5f00\u53d1\u8005\u7248\u672c\u5b9e\u9a8c](./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md)\n  * <a href=\"./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md#exercise-1-create-and-run-a-docker-application\">\u7ec3\u4e601\uff1a\u521b\u5efa\u5e76\u8fd0\u884cDocker\u5e94\u7528\u7a0b\u5e8f</a>\n  * <a href=\"./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md#exercise-2-migrate-mongodb-to-cosmos-db-using-azure-database-migration-service\">\u7ec3\u4e602\uff1a\u4f7f\u7528Azure\u6570\u636e\u5e93\u8fc1\u79fb\u670d\u52a1\u5c06MongoDB\u8fc1\u79fb\u5230Cosmos DB</a>\n  * <a href=\"./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md#exercise-3-deploy-the-solution-to-azure-kubernetes-service\">\u7ec3\u4e603\uff1a\u5c06\u89e3\u51b3\u65b9\u6848\u90e8\u7f72\u5230Azure Kubernetes\u670d\u52a1</a>\n  * <a href=\"./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md#exercise-4-scale-the-application-and-test-ha\">\u7ec3\u4e604\uff1a\u6269\u5c55\u5e94\u7528\u7a0b\u5e8f\u5e76\u6d4b\u8bd5HA</a>\n  * <a href=\"./Hands-on%20lab/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BA%94%E7%94%A8%20-%E5%BC%80%E5%8F%91%E8%80%85%E7%89%88%E6%9C%AC%E5%AE%9E%E9%AA%8C.md#exercise-5-working-with-services-and-routing-application-traffic\">\u7ec3\u4e605\uff1a\u4f7f\u7528\u670d\u52a1\u548c\u8def\u7531\u5e94\u7528\u7a0b\u5e8f\u6d41\u91cf</a>\n  \n- [\u767d\u677f\u8bbe\u8ba1](MCW-Cloud-native-applications/Whiteboard%20design%20session)\n  * [\u5b66\u751f\u6307\u5357](./Whiteboard%20design%20session/%E7%99%BD%E6%9D%BF%E8%AE%BE%E8%AE%A1%E5%AD%A6%E7%94%9F%E6%8C%87%E5%8D%97.md)\n  * [\u8bb2\u5e08\u6307\u5357](./Whiteboard%20design%20session/%E7%99%BD%E6%9D%BF%E8%AE%BE%E8%AE%A1%E8%AE%B2%E5%B8%88%E6%8C%87%E5%8D%97.md)\t\n  * [\u8bb2\u5e08\u80f6\u7247](./Whiteboard%20design%20session/%E7%99%BD%E6%9D%BF%E8%AE%BE%E8%AE%A1%E8%AE%B2%E5%B8%88presentation.pptx)\t\n  &emsp;\n  &emsp;\n-  Azure\u670d\u52a1\u53ca\u52a8\u624b\u5b9e\u9a8c\u6d89\u53ca\u7b2c\u4e09\u65b9\u76f8\u5173\u670d\u52a1\n     *   Azure Kubernetes Service \uff08AKS\uff09\n     *   Azure\u5bb9\u5668\u6ce8\u518c\u8868(ACR)\n     *   GitHub\u6216Gitee\n     *   Dockerfile\n     *   Cosmos DB\u6570\u636e\u5e93\uff08\u5305\u62ecMongoDB API\uff09\n\n  &emsp;\n  &emsp;\n\n\n## Azure\u89e3\u51b3\u65b9\u6848\n\n\u5e94\u7528\u73b0\u4ee3\u5316\n\n## \u76f8\u5173\u53c2\u8003\u6587\u732e\n\n-   [MCW](https://github.com/Microsoft/MCW)\n\n## \u5e2e\u52a9\u548c\u652f\u6301\n\n\u6211\u4eec\u6b22\u8fce\u5fae\u8f6f\u5408\u4f5c\u4f19\u4f34\u53cd\u9988\u548c\u8bc4\u8bba\u3002\n\n**_\u6709\u9ebb\u70e6\u5417\uff1f_**\n\n-   \u9996\u5148\uff0c\u9a8c\u8bc1\u60a8\u662f\u5426\u9075\u5faa\u4e86\u6240\u6709\u4e66\u9762\u7684\u5b9e\u9a8c\u8bf4\u660e\uff08\u5305\u62ec\u5b9e\u9645\u64cd\u4f5c\u524d\u7684\u5b9e\u9a8c\u6587\u6863\uff09\u3002\n-   \u63a5\u4e0b\u6765\uff0c\u63d0\u4ea4\u95ee\u9898\u5e76\u8be6\u7ec6\u8bf4\u660e\u95ee\u9898\u3002\n-   \u4e0d\u8981\u63d0\u4ea4\u62c9\u53d6\u8bf7\u6c42\u3002\u6211\u4eec\u7684\u5185\u5bb9\u4f5c\u8005\u5c06\u8fdb\u884c\u6240\u6709\u66f4\u6539\u5e76\u63d0\u4ea4\u62c9\u529b\u8bf7\u6c42\u4ee5\u4f9b\u6279\u51c6\u3002\n\n\n\n\n\n## Azure\u8ba2\u9605\u83b7\u53d6\n\nAzure\u6d77\u5916\u4f7f\u7528\u8d26\u53f7\u7533\u8bf7\u53ef\u4ee5\u53c2\u8003[\u94fe\u63a5](http://www.cnblogs.com/meowmeow/p/7773226.html?from=groupmessage&isappinstalled=0)\n", "repo_name": "gps-csa-devhack-azurecloudnative", "org_name": "microsoft", "org_repo": "microsoft/gps-csa-devhack-azurecloudnative", "platform_org_repo": "github+microsoft/gps-csa-devhack-azurecloudnative", "link_to_repo": "https://github.com/microsoft/gps-csa-devhack-azurecloudnative", "platform": "github", "language": "CSS", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Syntheseus\n\n[![CI](https://github.com/microsoft/syntheseus/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/microsoft/syntheseus/actions/workflows/ci.yml)\n[![Python Version](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)\n[![code style](https://img.shields.io/badge/code%20style-black-202020.svg)](https://github.com/ambv/black)\n[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)\n\nSyntheseus is a package for retrosynthetic planning.\nIt contains implementations of common search algorithms\nand a simple API to wrap custom reaction models and write\ncustom algorithms.\nIt is meant to allow for simple benchmarking of the components\nof retrosynthesis algorithms.\n\n## Installation\n\nSyntheseus is designed to have very few dependencies to allow it to\nbe run in a wide range of environments.\nAt the moment the only hard dependencies are `numpy`, `rdkit`, and `networkx`.\nIt should be easy to install syntheseus into any environment which has these packages.\n\nCurrently `syntheseus` is not hosted on `pypi`\n(although this will likely change in the future).\nTo install, please run:\n\n```bash\n# Clone and cd into repo\ngit clone https://github.com/microsoft/syntheseus.git\ncd syntheseus\n\n# Option 1: minimal install into current environment.\n# Assumes dependencies are already present in your environment.\npip install .  --no-dependencies\n\n# Option 2: pip install with dependencies into current environment.\npip install .\n\n# Option 3: create new conda environment and then install.\nconda env create -f environment.yml  # creates env named syntheseus\nconda activate syntheseus\npip install .\n```\n\n## Development\n\nSyntheseus is currently under active development and does not have a fixed API\n(but we will fix it very soon).\nIf you want to help us develop syntheseus please install and run `pre-commit`\nchecks before committing code.\n\nWe use `pytest` for testing. Please make sure tests pass on your branch before\nsubmitting a PR (and try to maintain high test coverage).\n\n```bash\npython -m pytest --cov syntheseus/tests\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "syntheseus", "org_name": "microsoft", "org_repo": "microsoft/syntheseus", "platform_org_repo": "github+microsoft/syntheseus", "link_to_repo": "https://github.com/microsoft/syntheseus", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "==================================\nProject Mu Dfci Feature Repository\n==================================\n\n============================= ================= ===================\n Host Type & Toolchain        Build Status      Code Coverage\n============================= ================= ===================\nWindows_VS_                   |WindowsCiBuild|  |WindowsCiCoverage|\nUbuntu_GCC5_                  |UbuntuCiBuild|   |UbuntuCiCoverage|\n============================= ================= ===================\n\nThis repository contains the Dfci feature portion of Project Mu.  For documentation on Dfci,\nsee `Device Firmware Configuration Interface <https://microsoft.github.io/mu/dyn/mu_feature_dfci/DfciPkg/Docs/Dfci_Feature/>`_.\n\nPlease see `Project Mu <https://microsoft.github.io/mu>`_ for general infomation on Project Mu.\n\n\nConsuming the Dfci Feature Package\n==================================\nThis project is intended to be included in a platform as a submodule at a particular release tag.\n\n\nReleases\n==============================\n\nReleases of this repository will follow the\n`Nuget versioning model <https://docs.microsoft.com/en-us/nuget/concepts/package-versioning>`_.\n\n\nBranch Status - main\n==============================\n\n:Status:\n  In Development\n\n:Entered Development:\n  Nov 2022\n\n:Anticipated Stabilization:\n  Mar 2023\n\nBranch Changes - main\n===============================\n\nBreaking Changes-dev\n--------------------\n\n- None\n\nMain Changes-dev\n----------------\n\ntag \"From_mu_plus\"\n\n  New Repo from mu_plus branch release/202208 at commit: c69447e15f2b968abd5901c05d0c622650f10f89\n\n    ran:\n\n      git filter-repo --path DfciPkg/ --path ZeroTouchPkg/ --path LICENSE.txt\n                      --path pull_request_template.md --path pip-requirements.txt\n                      --path .gitignore\n\n    tag \"From_mu_plus\" is the latest commit from mu_plus copied to mu_feature_dfci using the above\n    git filter-repo command.\n\nBug Fixes-dev\n-------------\n\n- None\n\nCode of Conduct\n===============\n\nThis project has adopted the Microsoft Open Source.\nSee the `Code of Conduct <https://opensource.microsoft.com/codeofconduct/>`_.\nFor more information see the Code of `Conduct FAQ <https://opensource.microsoft.com/codeofconduct/faq/>`_ or\n contact `opencode@microsoft.com <mailto:opencode@microsoft.com>`_ with any additional questions or comments.\n\nIssues\n======\n\nPlease open any issues in the Project Mu GitHub tracker.\n`More Details <https://microsoft.github.io/mu/How/contributing/>`_\n\nContributions\n=============\n\nContributions are always welcome and encouraged!\n\nPlease follow the general Project Mu Pull Request process.  `More\nDetails <https://microsoft.github.io/mu/How/contributing/>`_\n\n* `Code Requirements <https://microsoft.github.io/mu/CodeDevelopment/requirements/>`_\n* `Doc Requirements <https://microsoft.github.io/mu/DeveloperDocs/requirements/>`_\n\nBuilds\n======\n\nPlease follow the steps in the Project Mu docs to build for CI and local\ntesting. `More Details <https://microsoft.github.io/mu/CodeDevelopment/compile/>`_\n\nCopyright & License\n===================\n\n| Copyright (C) Microsoft Corporation\n| SPDX-License-Identifier: BSD-2-Clause-Patent\n\n.. ===================================================================\n.. This is a bunch of directives to make the README file more readable\n.. ===================================================================\n\n.. CoreCI\n\n.. |WindowsCiBuild| image:: https://dev.azure.com/projectmu/mu/_apis/build/status/CI/Feature%20DFCI/Mu%20Feature%20DFCI%20-%20CI%20-%20WIndows%20VS?repoName=microsoft%2Fmu_feature_dfci&branchName=main\n   :target: https://dev.azure.com/projectmu/mu/_build?definitionId=142&_a=summary\n\n.. _Windows_VS: https://dev.azure.com/projectmu/mu/_build/latest?definitionId=142&repoName=microsoft%2Fmu_feature_dfci&branchName=main\n\n.. |WindowsCiCoverage| image:: https://img.shields.io/badge/coverage-coming_soon-blue\n\n.. _Ubuntu_GCC5: https://dev.azure.com/projectmu/mu/_build/latest?definitionId=139&repoName=microsoft%2Fmu_feature_dfci&branchName=main\n\n.. |UbuntuCiBuild| image:: https://dev.azure.com/projectmu/mu/_apis/build/status/CI/Feature%20DFCI/Mu%20Feature%20DFCI%20-%20CI%20-%20GCC5?repoName=microsoft%2Fmu_feature_dfci&branchName=main\n  :target: https://dev.azure.com/projectmu/mu/_build?definitionId=141&_a=summary\n\n.. |UbuntuCiCoverage| image:: https://img.shields.io/badge/coverage-coming_soon-blue\n", "repo_name": "mu_feature_dfci", "org_name": "microsoft", "org_repo": "microsoft/mu_feature_dfci", "platform_org_repo": "github+microsoft/mu_feature_dfci", "link_to_repo": "https://github.com/microsoft/mu_feature_dfci", "platform": "github", "language": "C", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "domdiffing", "org_name": "microsoft", "org_repo": "microsoft/domdiffing", "platform_org_repo": "github+microsoft/domdiffing", "link_to_repo": "https://github.com/microsoft/domdiffing", "platform": "github", "language": "TypeScript", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# \ud83d\udca1GENIUS \u2013 generating text using sketches!\n\n- **Paper: [GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation](https://arxiv.org/abs/2211.10330v1)**\n\n\ud83d\udca1**GENIUS** is a powerful conditional text generation model using sketches as input, which can fill in the missing contexts for a given **sketch** (key information consisting of textual spans, phrases, or words, concatenated by mask tokens). GENIUS is pre-trained on a large- scale textual corpus with a novel *reconstruction from sketch* objective using an *extreme and selective masking* strategy, enabling it to generate diverse and high-quality texts given sketches.\n\n**Example 1:**\n\n- sketch: `__ machine learning __ my research interest __ data science __`\n- **GENIUS**: `I am a Ph.D. student in machine learning, and my research interest is in data science. I am interested in understanding how humans and machines interact and how we can improve the quality of life for people around the world.`\n\n**Example 2:**\n\n- sketch: `\u81ea\u7136\u8bed\u8a00\u5904\u7406__\u8c37\u6b4c__\u901a\u7528\u4eba\u5de5\u667a\u80fd__`\n- **GENIUS**: `\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u8c37\u6b4c\u5728\u901a\u7528\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u5176\u76ee\u7684\u662f\u4e3a\u4e86\u4fc3\u8fdb\u4eba\u7c7b\u667a\u80fd\u7684\u53d1\u5c55\u3002 `\n\n\n\n**GENIUS** can also be used as a general textual **data augmentation tool** for **various NLP tasks** (including sentiment analysis, topic classification, NER, and QA). \n\n\n![image-20221119164544165](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/hi-genius.png)\n\n\n\n- Models hosted in \ud83e\udd17 Huggingface:\n\n**Model variations:**\n\n| Model | #params | Language | comment|\n|------------------------|--------------------------------|-------|---------|\n| [`genius-large`](https://huggingface.co/beyond/genius-large) | 406M   | English | The version used in **paper** (recommend) |\n| [`genius-base`](https://huggingface.co/beyond/genius-base)  | 139M    | English | smaller version |\n| [`genius-base-chinese`](https://huggingface.co/beyond/genius-base-chinese) | 116M    | \u4e2d\u6587 | \u5728\u4e00\u5343\u4e07\u7eaf\u51c0\u4e2d\u6587\u6bb5\u843d\u4e0a\u9884\u8bad\u7ec3|\n\n![image-20221119191940969](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202211191919005.png)\n\n\n\n\nMore Examples:\n\n![image-20221119184950762](https://cdn.jsdelivr.net/gh/beyondguo/mdnice_pictures/typora/202211191849815.png)\n\n## Usage\n\n### What is a sketch?\n\nFirst, what is a **sketch**? As defined in our paper, a sketch is \"key information consisting of textual spans, phrases, or words, concatenated by mask tokens\". It's like a draft or framework when you begin to write an article. With GENIUS model, you can input some key elements you want to mention in your wrinting, then the GENIUS model can generate cohrent text based on your sketch.\n\nThe sketch which can be composed of:\n\n- keywords /key-phrases, like `__NLP__AI__computer__science__`\n- spans, like `Conference on Empirical Methods__submission of research papers__`\n- sentences, like `I really like machine learning__I work at Google since last year__`\n- or a mixup!\n\n\n### How to use the model\n#### 1. If you already have a sketch in mind, and want to get a paragraph based on it...\n```python\nfrom transformers import pipeline\n# 1. load the model with the huggingface `pipeline`\ngenius = pipeline(\"text2text-generation\", model='beyond/genius-large', device=0)\n# 2. provide a sketch (joint by <mask> tokens)\nsketch = \"<mask> Conference on Empirical Methods <mask> submission of research papers <mask> Deep Learning <mask>\"\n# 3. here we go!\ngenerated_text = genius(sketch, num_beams=3, do_sample=True, max_length=200)[0]['generated_text']\nprint(generated_text)\n```\nOutput:\n```shell\n'The Conference on Empirical Methods welcomes the submission of research papers. Abstracts should be in the form of a paper or presentation. Please submit abstracts to the following email address: eemml.stanford.edu. The conference will be held at Stanford University on April 1618, 2019. The theme of the conference is Deep Learning.'\n```\n\nIf you have a lot of sketches, you can batch-up your sketches to a Huggingface `Dataset` object, which can be much faster.\n\nTODO: we are also building a python package for more convenient use of GENIUS, which will be released in few weeks.\n\n#### 2. If you have an NLP dataset (e.g. classification) and want to do data augmentation to enlarge your dataset...\n\nPlease check [genius/augmentation_clf](https://github.com/beyondguo/genius/tree/master/augmentation_clf) and [genius/augmentation_ner_qa](https://github.com/beyondguo/genius/tree/master/augmentation_ner_qa), where we provide ready-to-run scripts for data augmentation for text classification/NER/MRC tasks.\n\n\n\n## Augmentation Experiments:\nData augmentation is an important application for natural language generation (NLG) models, which is also a valuable evaluation of whether the generated text can be used in real applications. \n- Setting: Low-resource setting, where only n={50,100,200,500,1000} labeled samples are available for training. The below results are the average of all training sizes.\n- Text Classification Datasets: [HuffPost](https://huggingface.co/datasets/khalidalt/HuffPost), [BBC](https://huggingface.co/datasets/SetFit/bbc-news), [SST2](https://huggingface.co/datasets/glue), [IMDB](https://huggingface.co/datasets/imdb), [Yahoo](https://huggingface.co/datasets/yahoo_answers_topics), [20NG](https://huggingface.co/datasets/newsgroup).\n- Base classifier: [DistilBERT](https://huggingface.co/distilbert-base-cased)\n\n\nIn-distribution (ID) evaluations:\n|   Method   |    Huff    |     BBC    |    Yahoo   |    20NG    |    IMDB    |    SST2    |    avg.    |\n|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n|    none    |   79.17   | **96.16** |   45.77   |   46.67   |   77.87   |   76.67   |   70.39   |\n|     EDA    |   79.20   |   95.11   |   45.10   |   46.15   |   77.88   |   75.52   |   69.83   |\n|    BackT   |   80.48   |   95.28   |   46.10   |   46.61   |   78.35   |   76.96   |   70.63   |\n|     MLM    |   80.04   |   96.07   |   45.35   |   46.53   |   75.73   |   76.61   |   70.06   |\n|    C-MLM   |   80.60   |   96.13   |   45.40   |   46.36   |   77.31   |   76.91   |   70.45   |\n|   LAMBADA  |   81.46   |   93.74   |   50.49   |   47.72   |   78.22   |   78.31   |   71.66   |\n|     STA    |   80.74   |   95.64   |   46.96   |   47.27   |   77.88   |   77.80   |   71.05   |\n|  **GeniusAug**  |   81.43   |   95.74   |   49.60   |   50.38   | **80.16** |   78.82   |   72.68   |\n| **GeniusAug-f** | **81.82** |   95.99   | **50.42** | **50.81** |   79.40   | **80.57** | **73.17** |\n\nOut-of-distribution (OOD) evaluations:\n|            |  Huff->BBC |  BBC->Huff | IMDB->SST2 | SST2->IMDB |    avg.    |\n|------------|:----------:|:----------:|:----------:|:----------:|:----------:|\n|    none    |   62.32   |   62.00   |   74.37   |   73.11   |   67.95   |\n|     EDA    |   67.48   |   58.92   |   75.83   |   69.42   |   67.91   |\n|    BackT   |   67.75   |   63.10   |   75.91   |   72.19   |   69.74   |\n|     MLM    |   66.80   |   65.39   |   73.66   |   73.06   |   69.73   |\n|    C-MLM   |   64.94   | **67.80** |   74.98   |   71.78   |   69.87   |\n|   LAMBADA  |   68.57   |   52.79   |   75.24   |   76.04   |   68.16   |\n|     STA    |   69.31   |   64.82   |   74.72   |   73.62   |   70.61   |\n|  **GeniusAug**  |   74.87   |   66.85   |   76.02   |   74.76   |   73.13   |\n| **GeniusAug-f** | **76.18** |   66.89   | **77.45** | **80.36** | **75.22** |\n\n### BibTeX entry and citation info\nIf you find our paper/code/models useful, please cite our paper:\n```\n@article{Guo2022GENIUS,\n  title={GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation},\n  author={Biyang Guo and Yeyun Gong and Yelong Shen and Songqiao Han and Hailiang Huang and Nan Duan and Weizhu Chen},\n  journal={ArXiv},\n  year={2022},\n  volume={abs/2211.10330}\n}\n```\n\n", "repo_name": "SCGLab", "org_name": "microsoft", "org_repo": "microsoft/SCGLab", "platform_org_repo": "github+microsoft/SCGLab", "link_to_repo": "https://github.com/microsoft/SCGLab", "platform": "github", "language": null, "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Kiota CLI Commons Package\n\nContains CLI specific types that are referenced in code generated by the shell language.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-cli-commons", "org_name": "microsoft", "org_repo": "microsoft/kiota-cli-commons", "platform_org_repo": "github+microsoft/kiota-cli-commons", "link_to_repo": "https://github.com/microsoft/kiota-cli-commons", "platform": "github", "language": "C#", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# LazyLifecycle callbacks [ Documentation WIP ] \n\nLazylifecycle callbacks is a simple framework to defer your non essential tasks, and initialisations out \nof the screen launch path while maintaining the same execution guarantees of android lifecycle callbacks.\n\n## Fundamentals of lazy callbacks\n - First Draw : When the app is launched, play store tracks the COLD, WARM and HOT launch numbers. And it does so by measuring how fast your app is able to draw your first frame. App can start via launcher, notifications, deeplinks etc, and each could land the user in different screens. App is considered to have rendered its first screen when the \"Displayed\" marker is shown on the logcat. It always shown after all the upward callbacks such as onCreate, onStart, and onResume have returned.\n \n - So, any code that is executing in onCreate, onStart, and onResume, and other upward callbacks(not mentioning things like onPostResume) has potential to make the screen launch time bad. So, it is advisable to remove deferrable code away from android lifecycle callbacks.\n \n - But where should we move it? We can do things on demand, but not every thing can be moved ondemand. For example, you want to start making the n/w calls for fetching the images as soon as possible. Here, we do not want it to start while the screen's rendering is happening, but the moment screen renders with the placeholder view, we need to start the n/w-db calls. Suppose you want to load draft of a email from the db, first we would like to render the compose screen and then start fetching the draft.\n\n## APIs\n- activate() - activates lazy callbacks on an activity, usually done in onResume()\n- deactivate() - deactivates lazy callbacks on an activity, usually done in onPause()\n- watchedView - each screen has an unique view that is critical for that screen, and rendering on that screen can be considered as launch. If no watched view is provided or identified, decor view can be used.\n- supportsLazyLifecycleCallbacks - This helps us onboard the framework incrementally, and enables us to conduct exeperiments to measure improvement. If this returns false for an activity, lazy lifecycle callbacks are not enabled for that screen. Each screen can override its value separately.\n- LazyLifecycleCallbacks - Any activity/fragment that wants on onboard these callbacks have to implement this interface.\n\n## The lazy callbacks provided by the framework \n- onLazyCreate - Executes once per activity/fragment just like onCreate, but after 1st draw on screen finishes.\n- onLazyStart - Excecutes if the activity resumes from a stopped state, but after 1st draw on screen finishes.\n- onLazyResume - Excecutes if the activity resumes from a paused state, but after 1st draw on screen finishes.\n- onViewCreatedLazy(view) - This is called only for fragments after 1st draw on screen finishes.\n\n## Relative ordering of lazy callbacks\nThe order of these callbacks are maintained as per android. In an activity, onLazyCreate() will be followed by on LazyStart(), followed by onLazyResume().\nIn Fragments, onCreate() will be followed by onViewCreatedLazy(), followed by onStart() followed by onResume().\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "LazylifecycleCallbacks", "org_name": "microsoft", "org_repo": "microsoft/LazylifecycleCallbacks", "platform_org_repo": "github+microsoft/LazylifecycleCallbacks", "link_to_repo": "https://github.com/microsoft/LazylifecycleCallbacks", "platform": "github", "language": "Kotlin", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "autoseg-monai-himl", "org_name": "microsoft", "org_repo": "microsoft/autoseg-monai-himl", "platform_org_repo": "github+microsoft/autoseg-monai-himl", "link_to_repo": "https://github.com/microsoft/autoseg-monai-himl", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\nThis project provides PowerShell Samples for the MMD API service, this is not production ready code.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MMD-Samples", "org_name": "microsoft", "org_repo": "microsoft/MMD-Samples", "platform_org_repo": "github+microsoft/MMD-Samples", "link_to_repo": "https://github.com/microsoft/MMD-Samples", "platform": "github", "language": "PowerShell", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Dual-Write automation for deployment and initial setup\n\n## Disclaimer / Support\nPlease note that this is not supported by Microsoft in anyway. \nIt is provided AS IS and it can break anytime because changes made in the API beeing used. \n\nIf you need support or assistance, please use the \"Discussions\" section in this repo. \nhttps://github.com/microsoft/Dual-write-automations/discussions\n\n## Feedback\n\nFeedback is essential - good or bad, please do not hesitate to provide Feedback in the Discussions section: \nhttps://github.com/microsoft/Dual-write-automations/discussions\n\n## What can this tool do? \n\nMainly this tool is intended as a utility to help save time during Dual-write setup and mainance tasks.\nIt doesen't look the prettiest but it does the job.\n\nThis is what it can do:\n\n-\tApply the latest map version based on authors or any author\n-\tApply integration keys\n-\tRefresh tables\n-\tStop/Start the maps before and after\n-\tRun's on multi-threading, means multiple maps are applied at the same time.\n-\tUploads maps to ADO Wiki \n-\tStart / Stop / Pause maps \n-\tExport configurations in the correct order \n-\tRun initial sync \n-\tParallel deployment to multiple target environments only using command line or multiple instances of the UI\n\nGenerally the tool has a UI and a Console application execution. Ultimately the UI will call the console application with arguments. \nThis makes it possible to also run any of what you are running in the UI also in an Azure pipeline. \nBe aware every function runs based on the configuration file, e.g. stopping maps will only stop the maps which are specified in the config. \n\n## How to get started? \n\nDownload the pre-compiled application here: https://github.com/microsoft/Dual-write-automations/releases/\nor clone the repo and compile it on your machine. \n\n1. Setup an environment with Dual-Write and the maps how you need/ want it\n2. Export the configuration with the tool\n3. Apply on other environments based on the config with the tool. \n\nPlease refer to the Wiki page where the steps are described in details.\n--> https://github.com/microsoft/Dual-write-automations/wiki\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Dual-write-automations", "org_name": "microsoft", "org_repo": "microsoft/Dual-write-automations", "platform_org_repo": "github+microsoft/Dual-write-automations", "link_to_repo": "https://github.com/microsoft/Dual-write-automations", "platform": "github", "language": "C#", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Synapse_AdventureWorks2019\nSynapse demo using AdventureWorks2019 data\n\nIn this repo we demonstrate some Azure Synapse Analytics functionality using AdventureWorks2019 Database data.\n\n### Table of Contents\n\n**[AdventureWorks2019 Database](#adventureworks2019-database)**<br>\n**[Download AdventureWorks sample database](#download-adventureworks-sample-database)**<br>\n**[Create Synapse Workspace](documentation/Create_Synapse_Workspace.md#create-synapse-workspace)**<br>\n**[Connect Synapse to GitHub](documentation/Create_Synapse_Workspace.md#connect-synapse-to-github)**<br>\n**[Ingest AdventureWorks2019 into Data Lake](documentation/Ingest_To_DataLake.md#ingest-adventureworks2019-into-data-lake)**<br>\n**[Create Synapse Notebook](documentation/Synapse_Notebook.md#create-synapse-notebook)**<br>\n**[Create a dedicated SQL pool](documentation/Ingest_To_DW.md#create-a-dedicated-sql-pool)**<br>\n**[Ingest Models into dedicated SQL pool](documentation/Ingest_To_DW.md#ingest-model-into-dedicated-sql-pool)**<br>\n**[Create PowerBI Report](documentation/PowerBI_Report.md#create-powerbi-report)**<br>\n\n## AdventureWorks2019 Database\n\nLet's start by creating a Database Server to host the AdventureWorks2019 database.\n\nSearch for Azure SQL in the search box on the top of the Azure portal and let's create a new SQL server by clicking the Create button.\n\n![Create SQL Database](./images/CreateSQLDatabase.png)\n\nOn the SQL databases tile, in the Select SQL deployment options page, select Database server and click Create.\n\n![Create SQL Database](./images/CreateSQLDatabaseI.png)\n\nIn the Create SQL Database Server page, select the Subscription and enter the Resource group (we recommend creating a new Resource group for this demo) enter the server's name.\n\n![Create SQL Database](./images/CreateSQLDatabaseII.png)\n\nEnter the authentication information and click the Review + create button.\n\n![Create SQL Database](./images/CreateSQLDatabaseIII.png)\n\nOn the Review + create tab click the Create button.\n\nWait until the deployment is completed and click Go to resource.\n\n![Create SQL Database](./images/CreateSQLDatabaseIV.png)\n\n## Download AdventureWorks sample database\n\nGo to https://docs.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms\n\nDownload AdventureWorks2019.bak file and follow the steps described in that document.\n\nNext: **[Create Synapse Workspace](documentation/Create_Synapse_Workspace.md#create-synapse-workspace)**<br>\n", "repo_name": "ftatoolkit-synapse-adventureworks2019", "org_name": "microsoft", "org_repo": "microsoft/ftatoolkit-synapse-adventureworks2019", "platform_org_repo": "github+microsoft/ftatoolkit-synapse-adventureworks2019", "link_to_repo": "https://github.com/microsoft/ftatoolkit-synapse-adventureworks2019", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# X-Decoder: Generalized Decoding for Pixel, Image, and Language (CVPR2023)\n\n\\[[Project Page](https://x-decoder-vl.github.io/)\\]   \\[[Paper](https://arxiv.org/pdf/2212.11270.pdf)\\]    \\[[HuggingFace All-in-One Demo](https://huggingface.co/spaces/xdecoder/Demo)\\] \\[[HuggingFace Instruct Demo](https://huggingface.co/spaces/xdecoder/Instruct-X-Decoder)\\]  \\[[Video](https://youtu.be/nZZTkYM0kd0)\\]\n\nby [Xueyan Zou*](https://maureenzou.github.io/), [Zi-Yi Dou*](https://zdou0830.github.io/), [Jianwei Yang*](https://jwyang.github.io/),  [Zhe Gan](https://zhegan27.github.io/), [Linjie Li](https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en), [Chunyuan Li](https://chunyuan.li/), [Xiyang Dai](https://sites.google.com/site/xiyangdai/), [Harkirat Behl](https://harkiratbehl.github.io/), [Jianfeng Wang](https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en), [Lu Yuan](https://scholar.google.com/citations?user=k9TsUVsAAAAJ&hl=en), [Nanyun Peng](https://vnpeng.net/), [Lijuan Wang](https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=zh-CN), [Yong Jae Lee^](https://pages.cs.wisc.edu/~yongjaelee/), [Jianfeng Gao^](https://www.microsoft.com/en-us/research/people/jfgao/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fjfgao%2F).\n\n## :fire: News\n\n* **[2023.04.14]** We are releasing [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once), a new universal interactive interface for image segmentation! You can use it for any segmentation tasks, way beyond what X-Decoder can do!\n\n<p align=\"center\">\n  <img src=\"images/teaser_new.png\" width=\"90%\" height=\"90%\">\n</p>\n\n* **[2023.03.20]** As an aspiration of our X-Decoder, we developed OpenSeeD ([[Paper](https://arxiv.org/pdf/2303.08131.pdf)][[Code](https://github.com/IDEA-Research/OpenSeeD)]) to enable open-vocabulary segmentation and detection with a single model, Check it out! \n* **[2023.03.14]** We release [X-GPT](https://github.com/microsoft/X-Decoder/tree/xgpt) which is an conversational version of our X-Decoder through GPT-3 langchain!\n* **[2023.03.01]** The [Segmentation in the Wild Challenge](https://eval.ai/web/challenges/challenge-page/1931/overview) had been launched and ready for submitting results!\n* **[2023.02.28]** We released the [SGinW benchmark](https://github.com/microsoft/X-Decoder/tree/seginw) for our challenge. Welcome to build your own models on the benchmark!\n* **[2023.02.27]** Our X-Decoder has been accepted by CVPR 2023!\n* **[2023.02.07]** We combine <ins>X-Decoder</ins> (strong image understanding), <ins>GPT-3</ins> (strong language understanding) and <ins>Stable Diffusion</ins> (strong image generation) to make an [instructional image editing demo](https://huggingface.co/spaces/xdecoder/Instruct-X-Decoder), check it out!\n* **[2022.12.21]** We release inference code of X-Decoder.\n* **[2022.12.21]** We release Focal-T pretrained checkpoint.\n* **[2022.12.21]** We release open-vocabulary segmentation benchmark.\n\n## :paintbrush: DEMO\n:small_red_triangle:[[X-GPT](https://github.com/microsoft/X-Decoder/tree/xgpt)] :small_red_triangle:[[Instruct X-Decoder](https://huggingface.co/spaces/xdecoder/Instruct-X-Decoder)]\n\n![demo](https://user-images.githubusercontent.com/11957155/225728214-0523bd30-31f7-472d-be7e-12a049c25cbd.gif)\n\n## :notes: Introduction\n\n![github_figure](https://user-images.githubusercontent.com/11957155/210801832-c9143c42-ef65-4501-95a5-0d54749dcc52.gif)\n\nX-Decoder is a generalized decoding model that can generate **pixel-level segmentation** and **token-level texts** seamlessly!\n\n**It achieves:**\n\n* State-of-the-art results on open-vocabulary segmentation and referring segmentation on eight datasets; \n* Better or competitive finetuned performance to generalist and specialist models on segmentation and VL tasks; \n* Friendly for efficient finetuning and flexible for novel task composition.\n\n**It supports:** \n\n* **One suite of parameters** pretrained for Semantic/Instance/Panoptic Segmentation, Referring Segmentation, Image Captioning, and Image-Text Retrieval;\n* **One model architecture** finetuned for Semantic/Instance/Panoptic Segmentation, Referring Segmentation, Image Captioning, Image-Text Retrieval and Visual Question Answering (with an extra cls head);\n* **Zero-shot task composition** for Region Retrieval, Referring Captioning, Image Editing.\n\n## Getting Started\n\n### Installation\n```sh\npip3 install torch==1.13.1 torchvision==0.14.1 --extra-index-url https://download.pytorch.org/whl/cu113\npython -m pip install 'git+https://github.com/MaureenZOU/detectron2-xyz.git'\npip install git+https://github.com/cocodataset/panopticapi.git\npython -m pip install -r requirements.txt\nsh install_cococapeval.sh\nexport DATASET=/pth/to/dataset\n```\n\nTo prepare the dataset: [DATASET.md](./DATASET.md)\n\n## Open Vocabulary Segmentation\n```sh\nmpirun -n 8 python eval.py evaluate --conf_files configs/xdecoder/svlp_focalt_lang.yaml  --overrides WEIGHT /pth/to/ckpt\n```\nNote: Due to zero-padding, filling a single gpu with multiple images may decrease the performance.\n\n## Inference Demo\n```sh\n# For Segmentation Tasks\npython demo/demo_semseg.py evaluate --conf_files configs/xdecoder/svlp_focalt_lang.yaml  --overrides WEIGHT /pth/to/xdecoder_focalt_best_openseg.pt\n# For VL Tasks\npython demo/demo_captioning.py evaluate --conf_files configs/xdecoder/svlp_focalt_lang.yaml  --overrides WEIGHT /pth/to/xdecoder_focalt_last_novg.pt\n```\n\n\n## Model Zoo\n|           |         | ADE  |      |      | ADE-full | SUN  | SCAN |      | SCAN40 | Cityscape |      |      | BDD  |      |\n|-----------|---------|------|------|------|----------|------|------|------|--------|-----------|------|------|------|------|\n| model     | ckpt    | PQ   | AP   | mIoU | mIoU     | mIoU | PQ   | mIoU | mIoU   | PQ        | mAP  | mIoU | PQ   | mIoU |\n| X-Decoder | [BestSeg Tiny](https://huggingface.co/xdecoder/X-Decoder/resolve/main/xdecoder_focalt_best_openseg.pt) | 19.1 | 10.1 | 25.1 | 6.2      | 35.7 | 30.3 | 38.4 | 22.4   | 37.7      | 18.5 | 50.2 | 16.9 | 47.6 |\n<!---\n| X-Decoder | [Last Tiny](https://projects4jw.blob.core.windows.net/x-decoder/release/xdecoder_focalt_last.pt) |  |  |  |       |  |  |  |    |       |  |  |  |  |\n| X-Decoder | [NoVG Tiny](https://projects4jw.blob.core.windows.net/x-decoder/release/xdecoder_focalt_last_novg.pt) |  |  |  |       |  |  |  |    |       |  |  |  | |\n-->\n\n* X-Decoder [NoVG Tiny](https://huggingface.co/xdecoder/X-Decoder/resolve/main/xdecoder_focalt_last_novg.pt)\n* X-Decoder [Last Tiny](https://huggingface.co/xdecoder/X-Decoder/resolve/main/xdecoder_focalt_last.pt)\n\n## Additional Results\n\n* Finetuned ADE 150 (32 epochs)\n\n| Model                           | Task    | Log | PQ   | mAP  | mIoU |\n|---------------------------------|---------|-----|------|------|------|\n| X-Decoder (davit-d5,Deformable) | PanoSeg |  [log](https://projects4jw.blob.core.windows.net/x-decoder/release/ade20k_finetune_davitd5_deform_32epoch_log.txt)   | 52.4 | 38.7 | 59.1 |\n\n## Acknowledgement\n* We appreciate the contructive dicussion with [Haotian Zhang](https://haotian-zhang.github.io/) \n* We build our work on top of [Mask2Former](https://github.com/facebookresearch/Mask2Former)\n* We build our demos on [HuggingFace :hugs:](https://huggingface.co/) with sponsored GPUs\n* We appreciate the discussion with Xiaoyu Xiang during rebuttal\n\n## Citation\n```\n@article{zou2022xdecoder,\n  author      = {Zou*, Xueyan and Dou*, Zi-Yi and Yang*, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee*, Yong Jae and Gao*, Jianfeng},\n  title       = {Generalized Decoding for Pixel, Image and Language},\n  publisher   = {arXiv},\n  year        = {2022},\n}\n```\n", "repo_name": "X-Decoder", "org_name": "microsoft", "org_repo": "microsoft/X-Decoder", "platform_org_repo": "github+microsoft/X-Decoder", "link_to_repo": "https://github.com/microsoft/X-Decoder", "platform": "github", "language": "Python", "stargazers_count": 915, "watchers_count": 915}, {"README_text": "# It's Low Code February!\n\n**\ud83d\udea7 SITE UNDER DEVELOPMENT \ud83d\udea7**\n\n---\n\nJoin us for a month-long celebration of low code and fusion development from core concepts and developer tools, to best practices and usage scenarios. \n\nSome of the topics you'll skill up on include:\n\n* Power Platform: Fundamentals\n* Backend Infra: Integrations\n* Frontend UI: Component frameworks,\n* Best Practices: ALM, Governance, and Testing\n\n![Image showing various technologies used in low code dev](./website/static/img/fusion-dev.png )\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Low-Code", "org_name": "microsoft", "org_repo": "microsoft/Low-Code", "platform_org_repo": "github+microsoft/Low-Code", "link_to_repo": "https://github.com/microsoft/Low-Code", "platform": "github", "language": "JavaScript", "stargazers_count": 150, "watchers_count": 150}, {"README_text": "# Speech Signal Improvement Challenge \u2013 ICASSP 2023\n\nThe Speech Signal Improvement Challenge Grand Challenge proposal at ICASSP 2023 is intended to stimulate research in the area of improving the speech signal quality in communication systems. The speech signal quality is measured with SIG in ITU-T P.835 and is still a top issue in audio communication and conferencing systems.\n\nThis challenge is to benchmark the performance of real-time speech enhancement models with a real (not simulated) test set. The audio scenario is the send signal in telecommunication; it does not include echo impairments. Participants will evaluate their speech enhancement model on a test set and submit the results (clips) for evaluation.\n\nFor more details about the challenge, please visit the challenge \n[website](https://www.microsoft.com/en-us/research/academic-program/speech-signal-improvement-challenge-icassp-2023/).\nThe paper will be released soon.\n\n## Training data\n\nThe datasets are provided under the original terms that Microsoft received such datasets.\nFor the training data, we suggest participants to use [AEC-Challenge data](https://github.com/microsoft/AEC-Challenge) and [DNS-Challenge data](https://github.com/microsoft/DNS-Challenge), presented in the <b>Dataset licenses</b> section. \nNevertheless, participants could use any other publicly available data for the training.\n\n## Evaluation metrics\nOur evaluation will be based on subjective listening test.\nWe suggest participants to evaluate models also in accordance with the [DNSMOS P.835](https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS),\nthe <b>SIG</b> metric being directly correlated with the signal quality.\nNevertheless, participants could use any metrics for the model's evaluation.\n\n## Datasets\n* <b>Test set</b> is available in [test_data](https://github.com/microsoft/SIG-Challenge/tree/main/test_data) directory.\n* <b>Blind set</b> will be released on the January 15th 2023.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SIG-Challenge", "org_name": "microsoft", "org_repo": "microsoft/SIG-Challenge", "platform_org_repo": "github+microsoft/SIG-Challenge", "link_to_repo": "https://github.com/microsoft/SIG-Challenge", "platform": "github", "language": null, "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Social Services IP\n>This repository contains the accelerator delivering analytics within the Social Services context. The accelerators generated from an engagement successfully delivered for a UK City Council (Public Sector). The 'Social Worker in School' (ref. [link](https://www.gov.uk/government/news/social-workers-to-work-with-teachers-in-schools)) initiative instituted by the UK Department for Education (DfE) primarily drove the main scope for the delivery engagement. The accelerator KPIs focus on metrics concerning social services events related to children and children/pupils events within schools. The accelerator provides a finite number of  KPIs. However, the flexibility of the data model, provided with the IP, allows the intuitive generation of additional KPIs without the need of extending the data model.\n## Social Services IP High-Level Solution Architecture\nAs shown in the pictorial below, the Social Service IP solution comprises two (2) main components these being:\n- Global Data Platform (GDP)\n- Social Service Analytics Platform (SSAP)\n\n<img src=\"https://github.com/microsoft/SocialServicesIP/blob/main/img/Social%20Services%20High%20Level%20Architecture1.jpg\" width=\"60%\" height=\"60%\">\n\nThe _*Global Data Platform (GDP)*_ solution responds to the need for a centralized data store, including a standard set of services, tools, and governance, all analysis and analytics use cases within Constoso. With this pattern, the data needed for data initiatives is extracted only once from the source system and then disseminated multiple times to consumers. \n\nThe _*Social Service Analytics Platform (SSAP)*_ solution responds to the need for a centralized, scalable data store in which data gets contextualized into a form valuable for extracting measures and insights needed by Contoso on Social Service and Children matters.\n\nThe vision is that this approach helps accelerate the development of analytics and advanced analytics initiatives and data products within Constoso as data gets provided in a scalable, controlled, and secure manner. \n## Social Services IP KPIs\nMetrics for social services concerning children been:\n- Measure 'front-door' activity (service demand overall). Like for example Referrals, Assestments, S47, ICPC and Contacts. \n- Measure plans (a longer temporal visibility). Like for example Child Protection Plan, Child In Need Plan and Child Look After Plan - Children In Care ).\n- Measure assessment time scales. \n\nMetrics for the children and school been:\n- Measure levels of school attendance level of absence. \n- Measure the pupil's  exclusions. \n- Measure school moves (pupils changing schools frequential. \n\nPlease refer to this [link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryGuidance/SWIS/DataModels/KeyPerformanceIndicators.md) for more detailed on these KPIs.\n## Data Model \nThe Data Model provides the context in which needs to be approached to deliver business value by enabling the successful implementation of Social Services KPIs.\nThe Logical / Physical Data Model has been built following the third normal form of database normalization. \nBy adding metadata throughout type codes embedded into the data model tables (like, for example, a Party Type Code), this data modelling technique enables horizontally expansion of the data model. \nHence, to extend this data model, the additions of new tables are not required. The data model extension gets achieved by adding new metadata in the form of type code within the data model context.\nThe Social Service data model has been designed following a top-down approach, starting from the Subject Area model, then moving to a lower-level subtype layer and finally physicalizing the low-level logical-physical data model. Please find below the link to the documentation for the various levels of abstraction of the data model.\n\n| Section        | Description         | Link  |\n| -------------- |---------------------|:-----:|\n| Introduction   | Description of the Social Services Data Model layers| [Link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryGuidance/SWIS/DataModels/README.md)|\n| Subject Area   | Documentation regarding the higher level of abstraction of the Social Service Data Model | [Link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryGuidance/SWIS/DataModels/SubjectAreaModel.md)|\n| Data Domain   | Documentation listing the Data Domains and tables together with their individual purpose.| [Link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryGuidance/SWIS/DataModels/LogicalPhysicalDataModel.md)|\n\n## Prerequisites\nIn order to successfully leverage the IP, you will need to have access to and or provisioned the following:\n- Access to an Azure subscription\n- Access to an Azure DevOps Services subscription\n- Service Principals\n\n## How to use the IP\nThe GitHub repo is divided into two distinct areas described below to facilitate the IP leveraging.\n\n| Section        | Description         | Link  |\n| -------------- |---------------------|:-----:|\n|Delivery Guidance|This section contains instructions on how to successfully personalise and deploy the Social Services IP work products on Azure Cloud environments.|[link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryGuidance)|\n|Delivery IP|This section contains the IP work products and their documentation expressed as Markdown markup language. For the intent of the IP, the documentation is generic and will need personalization to the specific customer and their technical and business characteristics.|[link](https://github.com/microsoft/SocialServicesIP/tree/main/DeliveryIP)| \n|Miscellaneous|This section is dedicated to the IP delivery team content.|[link](https://github.com/microsoft/SocialServicesIP/tree/main/Miscellaneous)| \n\n## Contributors\nKirill Ignatkov - project manager\n\nOnno v/d Horst - technical implementation data and deployment\n\nNabil Ben Meriem - technical implementation infrastructure and deployment\n\nMarco Scagliola - architecture and design\n", "repo_name": "SocialServicesAnalytics", "org_name": "microsoft", "org_repo": "microsoft/SocialServicesAnalytics", "platform_org_repo": "github+microsoft/SocialServicesAnalytics", "link_to_repo": "https://github.com/microsoft/SocialServicesAnalytics", "platform": "github", "language": "Bicep", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Introduction\n\n[![PR Quality Check](https://github.com/microsoft/kalypso/actions/workflows/pr.yaml/badge.svg)](https://github.com/microsoft/kalypso/actions/workflows/pr.yaml)\n\nKalypso is a collection of repositories, that back up the following Microsoft Learning resources:\n\n- [Concept: Workload management in a multi-cluster environment with GitOps](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-workload-management)\n- [How-to: Explore workload management in a multi-cluster environment with GitOps](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/workload-management)\n\nIt provides a composable reference architecture of the workload management in a multi-cluster and multi-tenant environment with GitOps.\n\nThis is an umbrella repository that contains requirements, use cases, architecture and code. The overall solution is composable so that every single component is handled in [its own repository](#referenced-repositories).\n\n## Motivation\n\nThere is an organization developing cloud-native applications. Any application needs a compute resource to work on. In the cloud-native world, this compute resource is a Kubernetes cluster. An organization may have a single cluster or, more commonly, multiple clusters. So the organization must decide which applications should work on which clusters. In other words, they must schedule the applications across clusters. The result of this decision, or scheduling, is a model of the desired state of the clusters in their environment. Having that in place, they need somehow to deliver applications to the assigned clusters so that they can turn the desired state into the reality, or, in other words, reconcile it.\n\nEvery application goes through a software development lifecycle that promotes it to the production environment. For example, an application is built, deployed to Dev environment, tested and promoted to Stage environment, tested, and finally delivered to production. For a cloud-native application, the application requires and targets different Kubernetes cluster resources throughout its lifecycle. In addition, applications normally require clusters to provide some platform services, such as Prometheus and Fluentbit, and infrastructure configurations, such as networking policy.\n\nDepending on the application, there may be a great diversity of cluster types to which the application is deployed. The same application with different configurations could be hosted on a managed cluster in the cloud, on a connected cluster in an on-premises environment, on a group of clusters on semi-connected edge devices on factory lines or military drones, and on an air-gapped cluster on a starship. Another complexity is that clusters in early lifecycle stages such as Dev and QA are normally managed by the developer, while reconciliation to actual production clusters may be managed by the organization's customers. In the latter case, the developer may be responsible only for promoting and scheduling the application across different rings.  \n\nIn a small organization with a single application and only a few operations, most of these processes can be handled manually with a handful of scripts and pipelines. But for enterprise organizations operating on a larger scale, it can be a real challenge. These organizations often produce hundreds of applications that target hundreds of cluster types, backed up by thousands of physical clusters. In these cases, handling such operations manually with scripts isn't feasible.\n\nThe following capabilities are required to perform this type of workload management at scale in a multi-cluster environment:\n\n- Separation of concerns on scheduling and reconciling\n- Promotion of the multi-cluster state through a chain of environments\n- Sophisticated, extensible and replaceable scheduler\n- Flexibility to use different reconcilers for different cluster types depending on their nature and connectivity\n- Abstracting Application Team away from the details of the clusters in the fleet\n\n### Existing projects\n\nIt's worth mentioning that there is a variety of existing projects targeting to address some of the described challenges. Most of them are built on the *Hub/Spoke* concept where there is a *Hub* cluster that controls workload placement across connected *Spoke* clusters. Examples of such tools are [Kubefed](https://github.com/kubernetes-sigs/kubefed), [Karmada](https://karmada.io/), [KubeVela](https://kubevela.io/), [OCM](https://open-cluster-management.io/), [Azure Kubernetes Fleet](https://learn.microsoft.com/en-us/azure/kubernetes-fleet/overview), [Rancher Fleet](https://fleet.rancher.io) etc. By definition, solutions like that expect a connection between *Hub* and *Spoke* clusters, at least an occasional one. They commonly provide a monolithic functionality, meaning they implement both workload scheduling and reconciling to the spoke clusters, so that scheduling and reconciling are tightly coupled to each other.\n\nHistorically, most of such tools have been designed to federate applications across multiple clusters. They are supposed to provide scalability, availability and security capabilities for a single application instance by breaking through Kubernetes limit of 5k nodes and placing an application across multiple regions and security zones. A solution like that is a perfect fit for a group or a fleet of connected clusters of the same or similar type with simple workload placement, based on labels and cluster performance metrics. From the perspective of this project, a cluster fleet like that is considered as a single deployment target, as a cluster of clusters with its own mechanics to load and balance the underlying compute.\n\n## Roles\n\n### Platform Team\n\nThe platform team is responsible for managing the clusters that host applications produced by application teams.\n\n*Key responsibilities*:\n\n- Define staging environments (Dev, QA, UAT, Prod)\n- Define cluster types (group of clusters sharing the same configurations) and their distribution across environments\n- Provision New clusters (CAPI/Crossplane/Bicep/Terraform/\u2026)\n- Manage infrastructure configurations and platform services (e.g. RBAC, Istio, Service Accounts, Prometheus, Flux, etc.) across cluster types\n- Schedule applications and platform services on cluster types\n\n### Application Team\n\nThe application team is responsible for the software development lifecycle (SDLC) of their applications. They provide Kubernetes manifests that describe how to deploy the application to different targets. They're responsible for owning CI/CD pipelines that create container images and Kubernetes manifests and promote deployment artifacts across environment stages.\n\nThe application team is responsible for the software development lifecycle (SDLC) of their applications. They provide Kubernetes manifests that describe how to deploy the application to different targets. They're responsible for owning CI/CD pipelines that create container images and Kubernetes manifests and promote deployment artifacts across environment stages.\n\n*Key responsibilities*:\n\n- Run full SDLC of their applications: Develop, build, deploy, test, promote, release, and support their applications.\n- Maintain and contribute to source and manifests repositories of their applications.\n- Define and configure application deployment targets.\n- Communicate to platform team, requesting desired compute resources for successful SDLC operations.\n\n### Application Operators\n\nApplication Operators work with the applications on the clusters on the edge. They are normally in charge of application instances working on a single or a small group of clusters. They may perform some local configurations for the specific clusters and applications instances. This role is out of scope of this project.\n\n## High Level Flow\n\n![kalypso-high-level](./docs/images/kalypso-highlevel.png)\n\nThe diagram above describes interaction between the roles and the major components of the solution. The primary concept of the whole process is separation of concerns. There are workloads, such as applications and platform services, and there is a platform where these workloads are working on. Application team takes care of the workloads (*what*), while the platform team is focused on the platform (*where*).\n\nApplication Team runs SDLC of their applications and promotes changes across environments. Application Team doesn't operate with the notion of the cluster. They have no idea on which clusters their application will be deployed in each environments. Application Team operates with the concept of *Deployment Target*, which is just an abstraction within an environment. Examples of deployment targets could be: *Integration* on Dev, *functional tests* and *performance tests* on QA, *early adopters* and *external users* on Prod and so on. Application Team defines deployment targets for each environment and they know how to configure their application and how to generate manifests for each deployment target. This process is owned by Application Team, it is automated and exists in the application repositories space. The outcome of the Application Team is generated manifests for each deployment target, stored in a manifests storage, such as a Git repository, Helm Repository, OCI storage, etc.\n\nPlatform team has a very limited knowledge about the applications and therefore is not involved in the application configuration and deployment process. Platform team is in charge of platform clusters, that are grouped in *Cluster Types*. They describe *Cluster Types* with configuration values, such as DNS names, endpoints of external services and so on. Platform team assigns (*schedules*) application deployment targets to various cluster types. With that in place, the application behavior will be determined by the combination of *Deployment Target* configuration values, provided by Application Team, and *Cluster Type* configuration values, provided by the Platform Team.  \n\nPlatform Team defines and configures *Cluster Types* and assigns *Deployment Targets* in the *Control Plane*. This is the place where they model their Platform. It's like a source repository for the Application Team. It's important to say, that the platform team doesn't manually schedule *Deployment Targets* on *Cluster Types* one by one. Instead of that they define scheduling rules in the *Control Plane*. Those rules along with configuration values are processed by an automated process that saves the result to the *Platform GitOps repo*. This repository contains folders for each *Cluster Type* with the information on what workloads should work on it and what configuration values should be applied. Clusters can grab that information from the corresponding folder with their preferred reconciler and apply the manifests.\n\nClusters report their compliance state with GitOps repositories to the *Deployment Observability Hub*. Platform and Application teams query this information to analyze workload deployment across the clusters historically. It can be used in the dashboards, alerts and in the deployment pipelines to implement progressive rollout.\n\n## Primary Use Cases\n\n- [Platform team onboards a workload](./docs/use-cases/platform-team-onboards-workload.md)\n- [Platform team defines a cluster type](./docs/use-cases/platform-team-defines-cluster-type.md)\n- [Platform team provides configuration values for a cluster type](./docs/use-cases/platform-team-config-values.md)\n- [Platform team schedules an application on cluster types](./docs/use-cases/platform-team-scheduled-application.md)\n- [Application team defines application deployment targets](./docs/use-cases/application-team-defines-deployment-targets.md)\n- [Application team provides a configuration value for a deployment target](./docs/use-cases/application-team-provides-configuration-value.md)\n- [Application team updates the application](./docs/use-cases/application-team-updates-application.md)\n- [Platform team defines service deployment targets](./docs/use-cases/platform-team-defines-deployment-targets.md)\n- [Platform team provides a configuration value for a service deployment target](./docs/use-cases/platform-team-provides-configuration-value.md)\n- [Platform team adds a platform service](./docs/use-cases/platform-team-adds-platform-service.md)\n\n## Design Details\n\n![kalypso-detailed](./docs/images/kalypso-detailed.png)\n\n### Control Plane\n\nThe platform team models the multi-cluster environment in the control plane. It's designed to be human-oriented and easy to understand, update, and review. The control plane operates with abstractions such as Cluster Types, Environments, Workloads, Scheduling Policies, Configs and Templates. See full list of abstractions in [Kalypso Scheduler](https://github.com/microsoft/kalypso-scheduler#kalypso-control-plane-abstractions) repository. These abstractions are handled by an automated process that assigns deployment targets and configuration values to the cluster types, then saves the result to the platform GitOps repository. Although there may be thousands of physical clusters, the platform repository operates at a higher level, grouping the clusters into cluster types.\n\nThere are various visions of how the control plane storage may be implemented. Following the GitOps concepts, it can be a Git repo, following the classic architecture it might me a database service with some API exposed.\n\nThe main requirement for the control plane storage is to provide a reliable and secure transaction processing functionality, rather than being hit with complex queries against a large amount of data. Various technologies may be used to store the control plane data.\n\nThis architecture design suggests a Git repository with a set of pipelines to store and promote platform abstractions across environments. This design provides a number of benefits:\n\n- All advantages of GitOps principles, such as version control, change approvals, automation, pull-based reconciliation.\n- Git repositories such as GitHub provide out of the box branching, security and PR review functionality.\n- Easy implementation of the promotional flows with GitHub Actions Workflows or similar orchestrators.\n- No need to maintain and expose a separate control plane service.  \n\nOverall, the *Kalypso Control Plane* consists of the following components:\n\n- GitHub repository along with a set of GH Actions workflows to store and promote abstractions\n- Control plane K8s cluster with [Kalypso Scheduler](https://github.com/microsoft/kalypso-scheduler) performing all the scheduling and transformations\n\n### Promotion and scheduling\n\nThe control plane repository contains two types of data:\n\n- Data that gets promoted across environments, such as a list of onboarded workloads and various templates.\n- Environment-specific configurations, such as included into an environment cluster types, config values, and scheduling policies. This data isn't promoted, as it's specific to each environment.\n\nThe data to be promoted lives in *main* branch while environment specific data is stored in the corresponding environment branches (e.g. dev, qa, prod). Transforming data from the *Control Plane* to the *GitOps repo* is a combination of the promotion and scheduling flows. The promotion flow moves the change across the environments horizontally and the scheduling flow does the scheduling and generates manifests vertically for each environment.\n\nA commit to the *main* branch starts the promotion flow that triggers the scheduling/transforming flow for each environment one by one. The scheduling/transforming flow takes the base manifests from *main*, applies configs from a corresponding to this environment branch (Dev, QA,..Prod) and PRs the resulting manifests to the *Platform GitOps repo* in the corresponding to the environment branch. Once the rollout on this environment is complete and successful, the promotion flow goes ahead and performs the same procedure on the next environment. On every environment the flow promotes the same commitid of the main branch, making sure that the content from *main* is getting to the next environment only after success on the previous environment.\n\n![promotion-flow](./docs/images/promotion-flow.png)\n\nA commit to the environment branch (Dev, Qa, \u2026Prod) in the *Control repo* will just start the scheduling/transforming flow for this environment. E.g. we have changed cosmo-db endpoint for QA, we just need to make updates to the QA branch of the GitOps repo, we don\u2019t want to touch anything else. The scheduling will take the *main* content corresponding to the latest commit id promoted to this environment, apply configurations and PR the resulting manifests to the GitOps branch.\n\nThe scheduling/transformation flow is implemented with a K8s operator [Kalypso Scheduler](https://github.com/microsoft/kalypso-scheduler) hosted on a *Control Plane* K8s cluster. It watches changes in the *Control Plane* environment branches, performs necessary scheduling, transformations, generates manifests and PR's them to the *Platform  GitOps repository*.\n\nThere are a few points to highlight here:\n\n- The promotion flow doesn\u2019t generate anything. It\u2019s just a vehicle to orchestrate the flow. It provides approvals, gates, state tracking. Performs post and pre-deployment activities.\n- The *Kalypso Scheduler* pulls the changes from the control plane repo with Flux. It knows exactly what has changed, and regenerates only related manifests. It doesn't rebuild the entire fleet.\n- This approach gives advantages of the both worlds - GH Actions and K8s:\n  - Powerful promotion flow orchestrator\n  - Precise event driven scheduling and transformation. We don\u2019t reboil the ocean while reacting on a change in the *Control Plane*. There is neither a bottleneck, nor a butterfly effect.\n\n### Workload assignment\n\nIn the platform GitOps repository, each workload assignment to a cluster type is represented by a folder that contains the following items:\n\n- A dedicated namespace for this workload in this environment on a cluster of this type.\n- Platform policies restricting workload permissions.\n- Consolidated platform config maps and secrets that the workload can use.\n- Reconciler resources pointing to a *Workload Manifests Storage* where the actual workload manifests or Helm charts live. E.g. Flux GitRepository and Flux Kustomization, ArgoCD Application, Zarf descriptors, nd so on.\n\n### Platform services\n\nPlatform services are workloads (such as Prometheus, NGINX, Fluentbit, and so on) maintained by the platform team. Just like any workloads, they have their source repositories and manifests storage. The source repositories may contain pointers to external Helm charts. CI/CD pipelines pull the charts with containers and perform necessary security scans before submitting them to the manifests storage, from where they're reconciled to the clusters.\n\nConsidering platform services as regular workflows, gives the following advantages:\n\n- Clean separation of \u201cwhat is running\u201d (applications and services) from \u201cwhere it is running\u201d (platform). These two things have completely different lifecycles.\n- Clean and simple functionality of the control plane. There is no workload manifest generation at all, only promotion, scheduling and configurations.\n- Their might be multiple control planes that can consume same platform services.  \n\n### Cluster types and reconcilers\n\nEvery cluster type can use a different reconciler (such as Flux, ArgoCD, Zarf, Rancher Fleet, and so on) to deliver manifests from the Workload Manifests Storages. Cluster type definition refers to a reconciler, which defines a collection of manifest templates. The scheduler uses these templates to produce reconciler resources, such as Flux GitRepository and Flux Kustomization, ArgoCD Application, Zarf descriptors, and so on. The same workload may be scheduled to the cluster types, managed by different reconcilers, for example Flux and ArgoCD. The scheduler generates Flux GitRepository and Flux Kustomization for one cluster and ArgoCD Application for another cluster, but both of them point to the same Workload Manifests Storage containing the workload manifests.\n\n### Extensible Scheduler\n\nKalypso scheduler operates with the [Control Plane abstractions](https://github.com/microsoft/kalypso-scheduler#kalypso-control-plane-abstractions), understands *Control Plane* and *Platform GitOps* repo structures and implements label based scheduling logic.\n\n### Deployment Observability Hub\n\nDeployment Observability Hub is a central storage that is easy to query with complex queries against a large amount of data. It contains deployment data with historical information on workload versions and their deployment state across clusters. Clusters register themselves in the storage and update their compliance status with the GitOps repositories. Clusters operate at the level of Git commits only. High-level information, such as application versions, environments, and cluster type data, is transferred to the central storage from the GitOps repositories. This high-level information gets correlated in the central storage with the commit compliance data sent from the clusters. See the details in the [Kalypso Observability Hub](https://github.com/microsoft/kalypso-observability-hub) repo.\n\n## Referenced Repositories\n\n|Repository|Description|\n|--------|----------|\n|[Application Source](https://github.com/microsoft/kalypso-app-src)|Contains a sample application source code including Docker files, manifest templates and CI/CD workflows|\n|[Application GitOps](https://github.com/microsoft/kalypso-app-gitops)|Contains final sample application manifests to be deployed to the deployment targets|\n|[Services Source](https://github.com/microsoft/kalypso-svc-src)|Contains high level manifest templates of sample dial-tone platform services and CI/CD workflows|\n|[Services GitOps](https://github.com/microsoft/kalypso-svc-gitops)|Contains final manifests of sample dial-tone platform services to be deployed across clusters fleet|\n|[Control Plane](https://github.com/microsoft/kalypso-control-plane)|Contains a platform model including environments, cluster types, applications and services, mapping rules and configurations, Promotion Flow workflows|\n|[Platform GitOps](https://github.com/microsoft/kalypso-gitops)|Contains final manifests representing the topology of the fleet - what cluster types are available, how they are distributed across environments and what is supposed to deployed where|\n|[Kalypso Scheduler](https://github.com/microsoft/kalypso-scheduler)|Contains detailed design and source code of the scheduler operator, responsible for scheduling applications and services on cluster types and uploading the result to the GitOps repo|\n|[Kalypso Observability Hub](https://github.com/microsoft/kalypso-observability-hub)|Contains detailed design and source code of the deployment observability service|\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso", "org_name": "microsoft", "org_repo": "microsoft/kalypso", "platform_org_repo": "github+microsoft/kalypso", "link_to_repo": "https://github.com/microsoft/kalypso", "platform": "github", "language": "Shell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Planting and Mitigating Memorization\n\nThis repository contains the software used to run the experiments in the paper \"Planting and Mitigating Memorized Content in Predictive-Text Language Models\" ([pre-print](https://arxiv.org/pdf/2212.08619.pdf)). This research evaluates the propensity for user data memorization in language models under a variety of modeling and adversarial conditions, tests the efficacy of various privacy mitigations intended to reduce memorization.\n\n## Repository Structure\n\n- `Configs` contains yaml configurations for each experiment.\n- `Canaries` contains the artificial training examples used as a test suite in this study.\n- `RunScripts` contains the shell scripts used to conduct all experiments.\n- `src` contains the Python scripts and classes used to train and evaluate language models\n- `Tools` contains scripts to download and pre-process the text data, anonymize text with EUII-scrubbing, and evaluate model predictions\n- `experiment_results.csv`: contains results published in the paper.\n- `requirements.txt` in the main directory can be used to initialize the correct Python environment (Python 3.9).\n\n## How to Cite\n\n```bibtex\n@article{downey-etal-2022,\n  author        = {C.M. Downey and Wei Dai and Huseyin A. Inan and Kim Laine and Saurabh Naik and Tomasz Religa},\n  title         = {Planting and Mitigating Memorized Content in Predictive-Text Language Models},\n  year          = {2022},\n  month         = {December},\n  journal       = {arXiv:2212.08619 [cs]},\n  url           = {\\url{https://arxiv.org/abs/2212.08619}}\n}\n```\n\n## Contributing\n\nFor contributing to this repository, see [CONTRIBUTING](CONTRIBUTING.md).\n", "repo_name": "planting-and-mitigating-memorization", "org_name": "microsoft", "org_repo": "microsoft/planting-and-mitigating-memorization", "platform_org_repo": "github+microsoft/planting-and-mitigating-memorization", "link_to_repo": "https://github.com/microsoft/planting-and-mitigating-memorization", "platform": "github", "language": "Python", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Contoso Traders\n\n![Logo](./docs/images/logo-1280x640.png)\n\n## Documentation and Resources\n\n* Application Links\n  * [UI](https://www.contosotraders.com/)\n  * [Carts API](https://contoso-traders-cartsprod.delightfuldune-ced90d47.eastus.azurecontainerapps.io/swagger)\n  * [Products API](https://contoso-traders-productsprod.eastus.cloudapp.azure.com/swagger/)\n* [Deployment Guide](./docs/App-Deployment-Guide.md)\n  * [Deployment Guide for Inventory Management PowerApp](./docs/Inventory-power-app-deployment-guide.md)\n* [Contributing](./CONTRIBUTING.md)\n* [Bicep Templates](./iac/)\n* [Load Tests](./tests/loadtests/)\n* [Github Workflows](./.github/workflows/)\n  * [![contoso-traders-provisioning-deployment](https://github.com/microsoft/ContosoTraders/actions/workflows/contoso-traders-provisioning-deployment.yml/badge.svg)](https://github.com/microsoft/ContosoTraders/actions/workflows/contoso-traders-provisioning-deployment.yml)\n  * [![contoso-traders-load-testing](https://github.com/microsoft/ContosoTraders/actions/workflows/contoso-traders-load-testing.yml/badge.svg)](https://github.com/microsoft/ContosoTraders/actions/workflows/contoso-traders-load-testing.yml)\n\n## Demo Scripts\n\n  | Scenario                                  | Documentation                                                                                                                                                                              |\n  | ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n  | Cloud Native App Architecture Walkthrough | [Overview](./demo-scripts/cloud-native-app-architecture/overview.md) \\| [Technical Walkthrough](./demo-scripts/cloud-native-app-architecture/technical-walkthrough.md)                     |\n  | Autoscaling Cloud Native Apps in Azure    | [Overview](./demo-scripts/autoscaling-cloud-native-apps-azure/overview.md) \\| [Technical Walkthrough](./demo-scripts/autoscaling-cloud-native-apps-azure/technical-walkthrough.md)         |\n  | DevSecOps Journey with GitHub + Azure     | [Overview](./demo-scripts/devsecops/overview.md) \\| [Technical Walkthrough](./demo-scripts/devsecops/technical-walkthrough.md)                                                             |\n  | Low Code App Development Power Platform   | [Overview](./demo-scripts/low-code-development/overview.md) \\| [Technical Walkthrough](./demo-scripts/low-code-development/technical-walkthrough.md)                                       |\n  | Intelligent Apps with Azure AI Services   | [Overview](./demo-scripts/intelligent-apps-with-azure-ai-services/overview.md) \\| [Technical Walkthrough](./demo-scripts/intelligent-apps-with-azure-ai-services/technical-walkthrough.md) |\n\n## Architecture\n\n![Architecture](./docs/architecture/contoso-traders-enhancements.drawio.png)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ContosoTraders", "org_name": "microsoft", "org_repo": "microsoft/ContosoTraders", "platform_org_repo": "github+microsoft/ContosoTraders", "link_to_repo": "https://github.com/microsoft/ContosoTraders", "platform": "github", "language": "JavaScript", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "---\npage_type: sample\nproducts:\n- ms-graph\nlanguages:\n- powershell\nextensions:\n  contentType: samples\n  technologies:\n  - Microsoft Graph \n  services:\n  - Intune\nnoDependencies: true\n---\n\n# MGGraph Intune Samples\n\nThis repository of sample scripts demonstrates how to access Intune service resources.  There are many ways to access the Microsoft Graph through scripting languages and the samples in this repo provide examples that you are free to utilize.\n\nDocumentation for Intune and Microsoft Graph is found here [Intune Graph Documentation](https://docs.microsoft.com/en-us/graph/api/resources/intune-graph-overview?view=graph-rest-1.0).\n\nThese samples demonstrate typical Intune administrator or Microsoft partner actions for managing Intune resources.\n\nThe scripts are licensed \"as-is.\" under the MIT License.\n\n#### Disclaimer\nSome script samples retrieve information from your Intune tenant, and others create, delete or update data in your Intune tenant.\u00a0 Understand the impact of each sample script prior to running it; samples should be run using a non-production or \"test\" tenant account.\u00a0\n\n## Using the Intune Graph API\nThe Intune Graph API enables programmatic access to your Intune tenant information. The API performs the same Intune operations as those available through the Intune portal. The Intune Portal utilizes the beta version of the Microsoft Graph (e.g. https://graph.microsoft.com/beta/<namespace>). The scripts in this repository will be using the v1.0 version unless it is unavailable. \n\nIntune provides data into the Microsoft Graph in the same way as other cloud services do, with rich entity information and relationship navigation.\u00a0 Use Microsoft Graph to combine information from other services and Intune to build rich cross-service applications for IT professionals or end users. Natively in Powershell you can pipe configurations from one cmdlet, from a specific service into another. This allows solutions to be built that are intuitive and native to the API. \u00a0\u00a0\n\n## Prerequisites\nUse of these samples requires the following:\n\n* An Intune tenant which supports the Azure Portal with a production or trial license (https://docs.microsoft.com/en-us/intune-azure/introduction/what-is-microsoft-intune)\n* Using the Microsoft Graph APIs to configure Intune controls and policies requires an Intune license.\n* An account with permissions to administer the Intune Service\n* PowerShell v5.0 or later on Windows 10 x64 (PowerShell v4.0 is a minimum requirement for the scripts to function correctly)\n* Note: For PowerShell 4.0 you will require the [PowershellGet Module for PS 4.0](https://www.microsoft.com/en-us/download/details.aspx?id=51451) to enable the usage of the Install-Module functionality\n* First time usage of these scripts requires a Global Administrator of the Tenant to accept the permissions of the application (grant consent).\n* The Microsoft Graph Powershell SDK module installed (https://github.com/microsoftgraph/msgraph-sdk-powershell)\n\n## Getting Started\nAfter the prerequisites are installed or met, perform the following steps to use these scripts:\n\n#### 1. Script usage\n\n1. Download the contents of the repository to your local Windows machine\n* Extract the files to a local folder (e.g. C:\\IntuneGraphSamples)\n* Run PowerShell x64 from the start menu\n* Browse to the directory (e.g. cd C:\\IntuneGraphSamples)\n* Either connect using Connect-MgGraph or update the script to include code to call Connect-MgGraph: https://learn.microsoft.com/en-us/powershell/microsoftgraph/authentication-commands?view=graph-powershell-1.0\n* For each Folder in the local repository you can browse to that directory and then run the script of your choice\n* Example Application script usage:\n  * To use the Manage Applications scripts, from C:\\IntuneGraphSamples, run \"cd .\\Applications\\\"\n  * Once in the folder run .\\Application_MDM_Get.ps1 to get all MDM added applications\n  This sequence of steps can be used for each folder....\n\n#### 2. Authentication with Microsoft Graph\nTo run any of the Intune-based commands in the Microsoft Graph PowerShell SDK, you'll first need to authenticate against your tenant. If you haven't already installed and set up the SDK, see the following links for guidance:\n* [Installation](https://learn.microsoft.com/en-us/powershell/microsoftgraph/installation?view=graph-powershell-1.0)\n* [Get Started](https://learn.microsoft.com/en-us/graph/powershell/get-started)\n\nThe Microsoft Graph PowerShell SDK supports two types of authentication: \n* Delegated access - an app acting on behalf of a signed-in user (interactive logon).\n* App-only access - an app acting with its own identity (unattended scenarios).\n\nFor guidance on authenticating using either method, review the following documentation:\n* [Authentication commands](https://learn.microsoft.com/en-us/powershell/microsoftgraph/authentication-commands?view=graph-powershell-1.0)\n* [Use app-only authentication with the Microsoft Graph PowerShell SDK](https://learn.microsoft.com/en-us/powershell/microsoftgraph/app-only?toc=%2Fgraph%2Ftoc.json&view=graph-powershell-1.0&tabs=azure-portal)\n\nEach API in the Microsoft Graph is protected by one or more permission scopes. Use the Connect-MgGraph command to sign in with the required scopes. You'll need to sign in with an admin account to consent to the scopes that the Microsoft Graph PowerShell SDK requires if they haven't been granted previously.\n\n**Important**: Connecting with the minmium required permissions for each example scenario is out of scope for the sample scripts provided in this repository. Within each script, you will see an authentication region where you will need to specify your authentication details and permission scope requirements if you are not already authenticated into the service.\n\nFor more information and to learn how to use the Find-MgGraphCommand to identify the minimum required permissions each command requires, see here:\n[Determine required permission scopes](https://learn.microsoft.com/en-us/powershell/microsoftgraph/get-started?view=graph-powershell-1.0#authentication)\n\n## Contributing\n\nIf you'd like to contribute to this sample, see CONTRIBUTING.MD.\n\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n\n## Questions and comments\n\nWe'd love to get your feedback about these Intune PowerShell samples. You can send your questions and suggestions to us in the Issues section of this repository.\n\nYour feedback is important to us. Connect with us on Stack Overflow. Tag your questions with [MicrosoftGraph] and [Intune].\n\n\n## Additional resources\n* [Microsoft Graph](https://learn.microsoft.com/en-us/powershell/microsoftgraph/authentication-commands?view=graph-powershell-1.0)\n* [Microsoft Graph API documentation](https://developer.microsoft.com/en-us/graph/docs)\n* [Microsoft Graph Portal](https://developer.microsoft.com/en-us/graph/graph-explorer)\n* [Microsoft code samples](https://developer.microsoft.com/en-us/graph/code-samples-and-sdks)\n* [Intune Graph Documentation](https://docs.microsoft.com/en-us/graph/api/resources/intune-graph-overview?view=graph-rest-1.0)\n\n## Copyright\nCopyright (c) 2023 Microsoft. All rights reserved.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "mggraph-intune-samples", "org_name": "microsoft", "org_repo": "microsoft/mggraph-intune-samples", "platform_org_repo": "github+microsoft/mggraph-intune-samples", "link_to_repo": "https://github.com/microsoft/mggraph-intune-samples", "platform": "github", "language": "PowerShell", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Intune-solutions", "org_name": "microsoft", "org_repo": "microsoft/Intune-solutions", "platform_org_repo": "github+microsoft/Intune-solutions", "link_to_repo": "https://github.com/microsoft/Intune-solutions", "platform": "github", "language": "PowerShell", "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Kalypso Control Plane\n\nThis is where platform team \u201cmodels\u201d the fleet. This repository is supposed to be human oriented, easy to understand, update, and review.  It has branches that represent environment stages. \n\n## Control Plane Abstractions\n\nThis repository operates with the abstractions like `Cluster Type`, `Reconciler`, `Template`, `Config`, `Workload`, `Workload Registration`, `Deployment Target` and `Scheduling Policy`.\n\nRefer to the full list of abstractions in the [Kalypso Scheduler](https://github.com/microsoft/kalypso-scheduler#kalypso-control-plane-abstractions) repository.\n\n## Repository structure\n\nThe *Control Plane* repository contains two types of data:\n\n- The data that is about to be promoted across environments. It lives in *main* branch and consists of:\n  - Onboarded *Workload Registrations*\n  - *Templates*\n- Environment specific configurations. This data is not promoted as it is specific for each environment. It lives in the corresponding environment branches (e.g. dev, qa, prod) and consists of: \n  - *Cluster Types* included in the environment\n  - *Configs*\n  - *Scheduling Policies*\n\nThis branching structure is the only requirement for this repo. The folder structure within the branches is totally open. You can group abstractions and organize them in the folder hierarchies on your own preference.   \n\n## Promotional Flow\n\nKalypso Control Plane repository implements the [promotional flow](https://github.com/microsoft/kalypso#promotion-and-scheduling) with a chain of GitHub Actions Workflows:\n\n![Kalypso-workflows](./docs/images/Kalypso-workflows.png)\n\n### CI \n\nThe CI workflow is triggered on a commit to the *main* branch. It contains a placeholder to perform some quality and security checks and invokes the [CD](#cd) workflow to promote the change to the first environment in the chain (e.g. *Dev*).\n\nThe workflow updates the Git commit status with the result of the CI process.\n\n### CD\n\nThe CD workflow promotes the change to the environment by saving the *main* branch commit id to `base-repo.yaml` file in the environment branch. This event starts the scheduling and transformation flow that ends up with a PR to the GitOps repository. \n\nThe workflow updates the Git commit status specifying that the change has been promoted to the environment.\n\n### Check Promote\n\nMerging of the PR to the GitOps repository starts the Check Promote workflow in the control plane repository. This workflow polls Azure Resource Graph, waiting until all registered [GitOps configurations](https://learn.microsoft.com/en-us/azure/azure-arc/kubernetes/conceptual-gitops-flux2) in the subscription are in compliance with the last PR to the GitOps repository.\n\nIf one of the clusters reports failure, the workflow fails. It reports the failure Git commit status and the whole promotion flow stops.\n\nOnce all configurations are compliant, the workflow runs a placeholder for the post-deployment activities, such as automated testing and reports to the Git commit status the fact of the successful deployment to the environment. \n\nAt the end, the workflow invokes the [CD](#cd) workflow to promote the change in the *main* branch to the next environment (e.g. *Stage*). The whole process stops when there is no next environment in the chain.    \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-control-plane", "org_name": "microsoft", "org_repo": "microsoft/kalypso-control-plane", "platform_org_repo": "github+microsoft/kalypso-control-plane", "link_to_repo": "https://github.com/microsoft/kalypso-control-plane", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Kalypso GitOps\n\n[Kalypso](https://github.com/microsoft/kalypso) Platform GitOps repository contains processed manifests that are pulled to the K8s clusters by GitOps reconcilers. These manifests have been processed through the promotional flow in the [Control Plane](https://github.com/microsoft/kalypso-control-plane) repository. This repository contains the same rollout environment branches as the [Control Plane](https://github.com/microsoft/kalypso-control-plane) repository. \n\nThe cluster types, included in the environments are represented with folders. Each folder contains subfolders corresponding to the workloads scheduled on this cluster type. \n\nManual updates are not made to this repository.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-gitops", "org_name": "microsoft", "org_repo": "microsoft/kalypso-gitops", "platform_org_repo": "github+microsoft/kalypso-gitops", "link_to_repo": "https://github.com/microsoft/kalypso-gitops", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# MSummarizer\n\nMSummarizer is a research project on summarization tasks. It includes some advanced techniques for solving summarization tasks (e.g., architecture design, data augmentation, datasets and etc).\n\nThe current work in [MSummarizer](MSummarizer/) include:\n*  Datasets: \n   + OLDS: [Towards Understanding Omission in Dialogue Summarization](https://arxiv.org/pdf/2211.07145.pdf)\n\n## Reference\n\nIf you find this project useful in your work, you can cite the following papers if there's a need:\n* [1] **Towards Understanding Omission in Dialogue Summarization**, Yicheng Zou, Kaitao Song, Xu Tan, Zhongkai Fu, Tao Gui, Qi Zhang, Dongsheng Li, **ACL2023**.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MSummarizer", "org_name": "microsoft", "org_repo": "microsoft/MSummarizer", "platform_org_repo": "github+microsoft/MSummarizer", "link_to_repo": "https://github.com/microsoft/MSummarizer", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "**NOTE**\nPlease note that while the code in this repository is licensed under MIT, it makes use of third party software that may be under\ndifferent licenses. Especially note that the ai-pipeline container is derived from NVIDIA's DeepStream containers. Your use of NVIDIA\nDeepStream and/or related deliverables is currently subject to the terms and limitations stated in this [license](https://developer.nvidia.com/deepstream-eula).\n\n**ATTENTION**\nNo part of this code should be considered stable and is intended to be used only for test purposes. This code base is not designed for production use cases. It is provided as an example of how you might integrate Azure IoT Edge with NVIDIA DeepStream.\n\n# Table of Contents\n\n* [Prerequisite checklist for Azure DeepStream Accelerator](./documentation/quickstart-readme.md) - Install all the dependencies and get yourself up and running.\n* [Tutorial: Azure DeepStream Accelerator - Getting started path](./documentation/tutorial-getstarted-path.md) - Deploy an example use case.\n* [Tutorial: Azure DeepStream Accelerator - Pre-built model path](./documentation/tutorial-prebuiltmodel-path.md) - Learn how to deploy a solution with a presupported model from a model zoo.\n* [Tutorial: Azure DeepStream Accelerator - Bring your own model path (BYOM) model path](./documentation/tutorial-byom-path.md) - Learn how to bring a custom model.\n* [Troubleshoot: Azure DeepStream Accelerator - Known issues](./documentation/troubleshooting.md) - If you encounter issues when you are creating an Edge AI solution using Azure DeepStream Accelerator.\n* [How to use the command line interface tool in Azure DeepStream Accelerator](./documentation/how-to-usecommandlinetool.md) - Learn about the CLI Tool.\n* [How to update the business logic in Azure DeepStream Accelerator](./documentation/how-to-modifybusinesslogic.md) - Learn about how to bring your own business logic to the pipeline.\n* [How to use the Azure DeepStream Accelerator Player web app](./documentation/how-to-usewebappwidget.md) - Learn about the Player (video playing widget).\n* [How to configure the Controller module](./documentation/how-to-configcontroller.md) - Learn about the various configuration options for Azure DeepStream Accelerator.\n* [How to migrate from a DeepStream only computer vision solution to DeepStream with Azure DeepStream Accelerator](./documentation/how-to-migratefromdeepstream.md) - Learn how to migrate from a pure DeepStream solution to Azure DeepStream Accelerator.\n* [How to dewarp video streams for your Azure DeepStream Accelerator solution](./documentation/how-to-dewarpvideo.md) - Learn how to dewarp fisheye cameras.\n* [How to add multiple video streams to your Azure DeepStream Accelerator solution](./documentation/how-to-addmultiplevideos.md) - Learn how to run a solution involving multiple camera sources or multiple AI configurations.\n* [How to include Azure Monitor to improve observability](./documentation/how-to-includeazuremontior.md) - Learn how to integrate with Azure Monitor.\n\n# Azure DeepStream Accelerator overview\n\nAzure DeepStream Accelerator includes developer tools that provide a custom developer experience. It enables you to create NVIDIA DeepStream containers using Microsoft-based images and guidance, supported models from NVIDIA out of the box, and/or bring your own models.\n\nDeepStream is NVIDIA\u2019s toolkit to develop and deploy Vision AI applications and services. It provides multi-platform, scalable, Transport Layer Security (TLS)-encrypted security that can be deployed on-premises, on the edge, and in the cloud.\n\n## Azure DeepStream Accelerator offers:\n\n- **Simplifying your development process**\n\n  Auto selection of AI model execution and inference provider: One of several execution providers, such as ORT, CUDA, and TENSORT, are automatically selected to simplify your development process.\n\n- **Customizing Region of Interest (ROI) to enable your business scenario**\n\n  Region of Interest (ROI) configuration widget: Azure DeepStream Accelerator Player, a web app widget, is included for customizing ROIs to enable event detection for your business scenario.\n\n- **Simplifying the configuration for pre/post processing**\n\n  You can add a Python-based model parser using a configuration file, instead of hardcoding it into the pipeline.\n\n- **Offering a broad Pre-built AI model framework**\n\n  This solution supports many of the most common CV models in use today, for example NVIDIA TAO, ONNX, CAFFE, UFF (TensorFlow), and Triton.\n\n- **Supporting bring your own model**\n\n  Support for model/container customization, USB/RTSP camera and pre-recorded video stream(s), event-based video snippet storage in Azure Storage and Alerts, and AI model deployment via Azure IoT Module Twin update.\n\n- **Support for Multiple Trackers**\n\n  Support for NV Tracker and Light trackers (https://github.com/researchmm/LightTrack).\n\n## Azure DeepStream Accelerator key components\n\nThe following table provides a list of Azure DeepStream Accelerator\u2019s key components and a description of each one.\n\n| Components               | Details                      |\n|--------------------------|------------------------------|\n| Edge devices             | Azure DeepStream Accelerator is available on the following devices:<br><br>- Any x86_64 device with an NVIDIA GPU that is IoT Edge compatible<br>- [NVIDIA Jetson Orin](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/) <br><br>**Note**:<br>You can use any of the listed devices with any of the development paths. Some implementation steps may differ depending on the architecture of your device.<br><br> |\n| Computer vision models   |Azure DeepStream Accelerator can work with many different computer vision (CV) models as outlined: <br><br>- **NVIDIA Models** <br>For example: Body Pose Estimation and License Plate Recognition. License Plate Recognition includes three models: traffic cam net, license plate detection, and license plate reading and other Nivida Models.<br><br>- **ONNX Models** <br> For example: SSD-MobileNetV1, YOLOv4, Tiny YOLOv3, EfficentNet-Lite.<br><br>   |\n| Development Paths        | Azure DeepStream Accelerator offers three development paths: <br><br>- [Getting started path](./documentation/tutorial-getstarted-path.md)<br>This path uses pre-trained models and pre-recorded videos of simulated manufacturing environment to demonstrate the steps required to create an Edge AI solution using Azure DeepStream Accelerator.<br><br>If you are just getting started on your computer vision (CV) app journey or simply want to learn more about Azure DeepStream Accelerator, we recommend this path.<br><br>- [Pre-built model path](./documentation/tutorial-prebuiltmodel-path.md)<br>This path provides pre-built parsers in Python for the CV models outlined earlier. You can easily deploy one of these models and integrate your own video stream.<br><br>If you are familiar with Azure IoT Edge solutions and want to leverage one of the supported models with an existing video stream, we recommend this path.<br><br>- [Bring your own model path](./documentation/tutorial-byom-path.md)<br>This path provides you with steps of how to integrate your own custom model and parser into your Azure DeepStream Accelerator Edge AI solution.<br><br>If you are an experienced developer who is familiar with cloud-based CV solutions and want a simplified deployment experience using Azure DeepStream Accelerator, we recommend this path.<br><br>    |\n\n## Azure DeepStream Accelerator architecture\n\nThe following diagram provides a high-level view of the Azure DeepStream Accelerator architecture.\n\n![azure_deepstream_accelerator_architecture](./documentation/media/azure_deepstream_accelerator_architecture.png)\n\n* **AI-Pipeline Container**: The AI Pipeline Container is the heart of Azure DeepStream Accelerator. It ingests USB or RTSP camera streams and applies AI models to the video frames.\n  The outputs of the models are multiplexed together and then sent out to the Business Logic Container for user logic to handle. There are a few points of configuration for this\n  container: [AI models can be delivered to it or packaged up into it](./documentation/tutorial-prebuiltmodel-path.md#step-3-prepare-and-upload-your-container),\n  AI models can make use of [Tensor RT or any runtime that Triton Inference Server can utilize](./documentation/how-to-usecommandlinetool.md#model-compatability-matrix), and\n  which cameras are used for which AI model configurations can be configured via the [Controller's module twin](./documentation/how-to-configcontroller.md).\n* **Controller Module**: This container is responsible for configuring the whole system. See [the documentation for the full configuration API](./documentation/how-to-configcontroller.md)\n* **Business Logic Container (BLC)**: This is where a user's application logic should live. It can be configured through the [Controller Module](./documentation/how-to-configcontroller.md).\n  It should accept inferences from the AI Pipeline Container and do whatever user logic to those inferences, then send back a switch on/off for recording event snippets.\n  [See more about the BLC here.](./documentation/how-to-modifybusinesslogic.md)\n* **Video Uploader**: The Video Uploader Container is responsible for taking inferences and MP4 video snippets and uploading them to the cloud. These uploads are triggered by the Business Logic Container telling\n  the AI Pipeline Container when to upload video. The video is delivered by means of a common volume shared between the AI Pipeline and the Video Uploader.\n* **Azure DeepStream Accelerator Player**: The Player is a Java Script widget that provides a convenient way to view video snippets that are found in the user's connected Blob Storage.\n  It can also define regions of interest graphically, in case that is of use for the end user's application logic. [See here for more information about the Player](./documentation/how-to-usewebappwidget.md).\n\n## Support\n\nFor information on how to file issues and get support, visit: [How to get support](./SUPPORT.md).\n\n## User contributions\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit [Contributor License Agreement](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (for example, status check, comment).  Follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark and Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n**CODEC LICENSES**\n\nH.265/HEVC Video Standard.  This product includes H.265/HEVC coding technology.  Access Advance LLC requires this notice:\n\nTHE H.265/HEVC TECHNOLOGY IN THIS PRODUCT IS COVERED BY ONE OR MORE CLAIMS OF THE HEVC PATENTS LISTED AT http://patentlist.accessadvance.com/.\n\nH.264/AVC Video Standard.  This product includes AVC coding technology.  MPEG LA LLC requires this notice:\n\nThis product is licensed under the AVC patent portfolio license for the personal and non-commercial use of a consumer to (i) encode video in compliance with the AVC standard (\"AVC VIDEO\") and/or (ii) decode AVC video that was encoded by a consumer engaged in a personal and non-commercial activity and/or was obtained from a video provider licensed to provide AVC video.  No license is granted or shall be implied for any other use.  Additional information may be obtained from MPEG LA LLC.  See http://www.MPEGLA.COM.\n\nFor clarification purposes, this notice does not limit or inhibit the use of the product for normal business uses that are personal to that business which do not include (i) redistribution of the product to third parties, or (ii) creation of content with AVC Standard compliant technologies for distribution to third parties.\n\n## Next steps\n\nYou are now ready to start using Azure DeepStream Accelerator to create, manage, and deploy custom Edge AI solutions. We recommend the following resources to get started:\n\n-\t[Prerequisite checklist for Azure DeepStream Accelerator](./documentation/quickstart-readme.md)\n-\t[Tutorial: Azure DeepStream Accelerator - Getting started path](./documentation/tutorial-getstarted-path.md)\n", "repo_name": "AzureDeepStreamAccelerator", "org_name": "microsoft", "org_repo": "microsoft/AzureDeepStreamAccelerator", "platform_org_repo": "github+microsoft/AzureDeepStreamAccelerator", "link_to_repo": "https://github.com/microsoft/AzureDeepStreamAccelerator", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# TIM\n\nTIM is a Kusto investigation platform that enables an analyst to quickly pivot between data sources; annotate their findings; and promotes collaboration through shared queries (pivots) and centralized tagged events.\n\n## Getting Started\n\n### Docker Compose\n\nDownload the docker compose YAML file.\n```bash\ncurl -LO https://github.com/microsoft/tim-data-investigate-platform/raw/main/.docker/compose.yaml\n```\n\nDownload and modify the frontend configuration file.\n```bash\ncurl -L https://github.com/microsoft/tim-data-investigate-platform/raw/main/.docker/config.js.example -o config.js\n```\n\nDownload and modify the backend configuration env file.\n```bash\ncurl -L https://github.com/microsoft/tim-data-investigate-platform/raw/main/.docker/.env.example -o .env\n```\n\nDownload and modify (if required) the NGINX configuration file.\n```bash\ncurl -LO https://github.com/microsoft/tim-data-investigate-platform/raw/main/.docker/nginx.conf\n```\n\nRun docker compose to download the latest images and deploy TIM locally.\n```bash\ndocker compose up\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "tim-data-investigate-platform", "org_name": "microsoft", "org_repo": "microsoft/tim-data-investigate-platform", "platform_org_repo": "github+microsoft/tim-data-investigate-platform", "link_to_repo": "https://github.com/microsoft/tim-data-investigate-platform", "platform": "github", "language": "C#", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# use-disposable\n\nA hook that creates disposable instances during render phase that works with strict mode.\n\n## Problem\n\nWith the introduction of [stricter strict mode in React 18](https://github.com/reactwg/react-18/discussions/19), factories\nin hooks like `useMemo` and `useState` are called twice but `useEffect` still disposes once. The component will be mounted and unmounted. This makes\nit hard to create instances during render time that would be cleaned up by `useEffect`. Let's look at an example\nwhere we try to create a Portal component.\n\n```tsx\nimport * as React from \"react\";\nimport * as ReactDOM from \"react-dom\";\n\nfunction Portal({ children }) {\n  const domNode = React.useMemo(() => {\n    const newElement = document.createElement(\"div\");\n    document.body.appendChild(newElement);\n    console.log(\"create DOM node\");\n    return newElement;\n  }, []);\n\n  React.useEffect(() => {\n    console.log(\"effect\");\n    return () => {\n      console.log(\"dispose DOM node\");\n      domNode.remove();\n    };\n  }, [domNode]);\n\n  console.log(\"render to portal\");\n  return ReactDOM.createPortal(children, domNode);\n}\n```\n\nLet's look at the console output:\n\n```\n> create DOM node\n> render to portal\n> create DOM node\n> render to portal\n> effect\n> dispose DOM node\n> effect\n```\n\nThe result: **Nothing is in the portal** \ud83d\udea8\ud83d\udea8 why?\n\nDuring the first `render to portal` the changes are not commited to DOM and in the end there is one empty\nportal DOM node. Only after the second render are the DOM changes commited and our content rendered to the\n**second** portal DOM node. However this triggers a simulated mount/unmount and our portal DOM\nnode is disposed \ud83d\ude43\n\n## useDispose\n\n```tsx\nimport * as React from \"react\";\nimport * as ReactDOM from \"react-dom\";\nimport { useDisposable } from \"use-disposable\";\n\nfunction Portal({ children }) {\n  const domNode = React.useDisposable(() => {\n    const newElement = document.createElement(\"div\");\n    document.body.appendChild(newElement);\n    console.log(\"create DOM node\");\n    return [newElement, () => newElement.remove()];\n  }, []);\n\n  console.log(\"render to portal\");\n  return ReactDOM.createPortal(children, domNode);\n}\n```\n\nThe resulting console output:\n\n```\n> create DOM node\n> render to portal\n> render to portal\n```\n\n`useDispose` makes sure that even in strict mode that in both renders only one instance DOM node is created and used.\n\n## useIsStrictMode\n\nThis is a hook that uses the React fiber to detect if the `StrictMode` component is in the tree. It powers `useDisposable` and\nis also available from this package as a standalong utility.\n\n## Limitations\n\n\u26a0\ufe0f Calling `useDispose` twice in the same component will lead to unpredictable results, make sure this is only called once\nper component, we are actively trying to find ways around this limitation.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "use-disposable", "org_name": "microsoft", "org_repo": "microsoft/use-disposable", "platform_org_repo": "github+microsoft/use-disposable", "link_to_repo": "https://github.com/microsoft/use-disposable", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "powerplatform-coekit-teams", "org_name": "microsoft", "org_repo": "microsoft/powerplatform-coekit-teams", "platform_org_repo": "github+microsoft/powerplatform-coekit-teams", "link_to_repo": "https://github.com/microsoft/powerplatform-coekit-teams", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/pdf/2212.02623)\n[Zineng Tang](https://zinengtang.github.io/),\n[Ziyi Yang](https://ziyi-yang.github.io/),\n[Guoxin Wang](https://www.guoxwang.com/),\n[Yuwei Fang](https://www.microsoft.com/en-us/research/people/yuwfan/),\n[Yang Liu](https://nlp-yang.github.io/),\n[Chenguang Zhu](https://cs.stanford.edu/people/cgzhu/),\n[Michael Zeng](https://www.microsoft.com/en-us/research/people/nzeng/),\n[Cha Zhang](https://www.microsoft.com/en-us/research/people/chazhang/),\n[Mohit Bansal](https://www.cs.unc.edu/~mbansal/)\n              \n## Code Release [Here](https://github.com/microsoft/i-Code/tree/main/i-Code-Doc)\n\nCode is rehosted at part of [the i-code project](https://github.com/microsoft/i-Code)\n\n## Open Source Checklist:\n\n- [x] Release Model (Encoder + Text decoder)\n- [ ] Release Most Scripts\n- [ ] Vision Decoder / Weights (Due to fake document generation ethical consideration, we plan to release this functionality as an Azure API)\n- [ ] Demos\n\n\n## Introduction \n\nUDOP unifies vision, text, and layout through vision-text-layout Transformer and unified generative pretraining tasks including\nvision task, text task, layout task, and mixed task. We show the task prompts (left) and task targets (right) for all self-supervised objectives\n(joint text-layout reconstruction, visual text recognition, layout modeling, and masked autoencoding) and two example supervised objectives\n(question answering and layout analysis).\n\n<p align=\"center\">\n  <img align=\"middle\" width=\"800\" src=\"udop.png\"/>\n</p>\n\n", "repo_name": "UDOP", "org_name": "microsoft", "org_repo": "microsoft/UDOP", "platform_org_repo": "github+microsoft/UDOP", "link_to_repo": "https://github.com/microsoft/UDOP", "platform": "github", "language": null, "stargazers_count": 149, "watchers_count": 149}, {"README_text": "# MDE_Automation\n\n![GitHub issues by-label](https://img.shields.io/github/issues/microsoft/MDE_Automation/enhancement?label=enhancement%20issues)\n![GitHub issues by-label](https://img.shields.io/github/issues/microsoft/MDE_Automation/bug?label=bug%20issues)\n![Total Downloads](https://img.shields.io/github/downloads/microsoft/MDE_Automation/total)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MDE_Automation", "org_name": "microsoft", "org_repo": "microsoft/MDE_Automation", "platform_org_repo": "github+microsoft/MDE_Automation", "link_to_repo": "https://github.com/microsoft/MDE_Automation", "platform": "github", "language": "PowerShell", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project i-Code\nThe ambition of the i-Code project is to build integrative and composable multimodal Artificial Intelligence. The \"i\" stands for integrative multimodal learning.\n\n## Multimodal Foundation Models\n- [i-Code V1](https://github.com/microsoft/i-Code/tree/main/i-Code-V1): i-Code: An Integrative and Composable Multimodal Learning Framework. AAAI 2023, [paper link](https://arxiv.org/abs/2205.01818).\n\n- [i-Code V2](https://github.com/microsoft/i-Code/tree/main/i-Code-V2): i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data. [Paper link](https://arxiv.org/abs/2305.12311).\n\n- [i-Code V3 (CoDi)](https://github.com/microsoft/i-Code/tree/main/i-Code-V3): Any-to-Any Generation via Composable Diffusion, [paper link](https://arxiv.org/abs/2305.11846).\n\n- [i-Code Studio](https://github.com/microsoft/i-Code/tree/main/i-Code-Studio):  A Configurable and Composable Framework for Integrative AI, [paper link](https://arxiv.org/abs/2305.13738).\n\n## Multimodal Document Intelligence\n- [i-Code Doc (UDOP)](https://github.com/microsoft/i-Code/tree/main/i-Code-Doc): Unifying Vision, Text, and Layout for Universal Document Processing. CVPR 2023 Highlight, [paper link](https://arxiv.org/abs/2212.02623).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "i-Code", "org_name": "microsoft", "org_repo": "microsoft/i-Code", "platform_org_repo": "github+microsoft/i-Code", "link_to_repo": "https://github.com/microsoft/i-Code", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 955, "watchers_count": 955}, {"README_text": "<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/1785175/215624212-fc92ccb1-f14c-4cb6-982f-61f50b9f3c21.png\" width=\"320px\">\n</p>\n\n[![Documentation](https://img.shields.io/badge/docs-passing-brightgreen)](https://microsoft.github.io/ClimaX)\n[![Paper](https://img.shields.io/badge/arXiv-2301.10343-blue)](https://arxiv.org/abs/2301.10343)\n\nThis repository contains code accompanying the paper [**ClimaX: A foundation model for weather and climate**](https://arxiv.org/abs/2301.10343).\n\nFor details about usage please see [documentation](https://microsoft.github.io/ClimaX).\nIf you have any questions or suggestions please open a [discussion](https://github.com/microsoft/ClimaX/discussions). If you notice a bug, please open an [issue](https://github.com/microsoft/ClimaX/issues).\n\n## Citation\n\nIf you find this repository useful in your research, please consider citing the following papers:\n\n```bibtex\n@article{nguyen2023climax,\n  title={ClimaX: A foundation model for weather and climate},\n  author={Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya},\n  journal={arXiv preprint arXiv:2301.10343},\n  year={2023}\n}\n```\n\n## Acknowledgements\n\nThanks to [@rems75](https://github.com/rems75) and [@kdatta](https://github.com/kdatta) for noticing a performance bug with variable id transfer to GPUs.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ClimaX", "org_name": "microsoft", "org_repo": "microsoft/ClimaX", "platform_org_repo": "github+microsoft/ClimaX", "link_to_repo": "https://github.com/microsoft/ClimaX", "platform": "github", "language": "Python", "stargazers_count": 232, "watchers_count": 232}, {"README_text": "# Application Source\n\n[Kalypso](https://github.com/microsoft/kalypso) Application source repo contains the source code including manifest templates that will be used to generate manifests for the environments. It also contains environment specific configurations in Configuration Branches so that config updates get through the Git flow as well. Alternatively, Configuration Branches may be stored in a separate configuration repository or outside of Git (e.g., in environment secrets and/or Azure Key Vaults). \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-app-src", "org_name": "microsoft", "org_repo": "microsoft/kalypso-app-src", "platform_org_repo": "github+microsoft/kalypso-app-src", "link_to_repo": "https://github.com/microsoft/kalypso-app-src", "platform": "github", "language": "Go", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Application GitOps\n\n[Kalypso](https://github.com/microsoft/kalypso) Application GitOps repository contains the final manifests generated by the CD workflow from the Application Source repository. The manifests are stored in the repository branches that represent rollout environments. Changes to this repository should happen only through the CD workflow.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-app-gitops", "org_name": "microsoft", "org_repo": "microsoft/kalypso-app-gitops", "platform_org_repo": "github+microsoft/kalypso-app-gitops", "link_to_repo": "https://github.com/microsoft/kalypso-app-gitops", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Near Object (NO) Framework \n\n[![CI/CD](https://github.com/microsoft/nearobject-framework/actions/workflows/cicd.yml/badge.svg)](https://github.com/microsoft/nearobject-framework/actions/workflows/cicd.yml)\n[![cpp-linter](https://github.com/microsoft/nearobject-framework/actions/workflows/cpp-linter.yml/badge.svg)](https://github.com/microsoft/nearobject-framework/actions/workflows/cpp-linter.yml)\n[![Build [Official Release]](https://github.com/microsoft/nearobject-framework/actions/workflows/official-release.yml/badge.svg)](https://github.com/microsoft/nearobject-framework/actions/workflows/official-release.yml)\n\nThis project is a framework for interacting with short-range devices, providing secure location, ranging, radar, or proximity services. There is a particular focus on [IEEE 802.15.4z-2020](https://standards.ieee.org/ieee/802.15.4z/10230/) Ultra-Wideband (UWB) devices using the [Fine Ranging Consortium (FiRa)](https://www.firaconsortium.org/), however, the framework is not limited to this.\n\n## Project Structure\n\nThis project is organized to allow primary development on both Linux and Windows. Hence, [CMake](https://cmake.org/) is used as the build system generator. Consequently, there is an OS-independent source tree `lib`, and OS-dependent source trees `windows`, `linux`, etc..\n\nNote that all language feature configuration is constrained by the Windows build system since it is the most limiting factor. As such, the current C++ language version being used is C++ 20.\n\n## Coding Guidelines\n\nWhere possible, we will attempt to use primitives provided by the [C++ Standard Library](https://en.cppreference.com/w/cpp/header) for interoperability between common and OS-dependent code. The use of OS-specific primitives and libraries is reserved for scenarios where they are strictly needed (eg. calling an OS/System API), or where the highest possible performance is required and only the OS implementation can provide this.\n\nThe coding style is dictated by both `.clang-format` and `.clang-tidy` files at the root of the project. Please configure your editor to format and lint sources accordingly. Above all, ***the coding style should be kept as consistent as possible***. The exact style used is not overly important.\n\nTo help keep the code consistent, please follow these general guidelines:\n\n* **DO** use spaces instead of tabs.\n* **DON'T** prefix variable names to describe their type, scope, or visibility.\n* **DO** use `std::filesystem` for storage and UNIX path separators (`/`) where possible.\n* **DO** use complete words for variable and function names.\n* **DO** use the following table for variable naming:\n\n| Block | Style | Example |\n| ----- | ----- | -------- |\n| Types | PascalCase | `struct NearObject {};` |\n| Functions | PascalCase | `NearObject GetNearObject()` |\n| Variables | camelCase | `NearObject nearObject;` |\n| Parameters | camelCase | `void registerEventCallback(NearObjectEventCallback& eventCallback)` |\n| Namespaces | lowercase | `namespace nearobject` |\n| Public Members | PascalCase | `struct NearObject { NearObjectProperties Properties; }` |\n| Private Members | camelCase with `m_` prefix | `class NearObjectSession { uint64_t m_sessionId; }` |\n\n### Commit Signing\n\nWhile not required, it is strongly recommended to configure git with a [GNU Privacy Guard (GPG)](https://gnupg.org/) signing key. This allows GitHub to verify commits were pushed by a specific user and will show a green `Verified` status beside each verified commit. Follow these steps to configure a signing key for commit verification:\n\n1. Install the gpg tools for the target operating system (see the OS-specific `README` files for details: [Windows](./windows/README.md#2-configure-commit-signing-optional), [Linux](./linux/README.md#development-environment-setup)).\n2. [Generate a gpg key](https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key)\n3. [Add the gpg key to your github account](https://docs.github.com/en/authentication/managing-commit-signature-verification/adding-a-gpg-key-to-your-github-account)\n4. [Configure git to use the gpg key](https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key). This link will also tell you how to use gpg signing with an ssh key. Additionally, if you wish tell git to use a type any type of signing by default, be it gpg, ssh or X.509, you need to run the following command ```git config --global commit.gpgsign true```\n\n## Development Environment Setup\n\nPre-requisites:\n\n* C++ 20 Compiler\n* CMake version >= 3.16\n\n### Compiler\n\nBoth a compiler and standard C++ library supporting C++20 are required. The C++ reference pages for [C++20 core language features](https://en.cppreference.com/w/cpp/compiler_support#cpp20) and [C++20 library features](https://en.cppreference.com/w/cpp/compiler_support#C.2B.2B20_library_features) provide complete details about current compiler and library support.\n\n#### Windows\n\n[Visual Studio 2022](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=Enterprise&channel=Release&version=VS2022&source=VSLandingPage&cid=2030&passive=false) generally satisfies the requirements, however, the full integrated development environment (IDE) is not needed unless also building the [simulator driver](./windows/drivers/uwb/simulator/README.md). A much leaner alternative for those using other editors such as Visual Studio Code can instead install [Visual Studio Build Tools](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022). The build tools come with a C++ compatible compiler and standard library. Detailed development environment setup instructions can be found in the Windows [`README`](/windows/README.md).\n\n#### Linux\n\ng++ or llvm/clang are suitable, however, some care must be taken to obtain a compatible standard library. A known, working environment is ubuntu 22.04 (jammy) with clang 14.0.0 and LLVM 14.0.0. Both are both provided by the official ubuntu package repository so can be installed using `apt`. Detailed development environment setup instructions can be found in the Linux [`README`](/linux/README.md).\n\n### CMake\n\nCMake may be installed in any form, as long as the version meets the minimum. One popular way of installing it on Windows is to use [Chocolately](https://chocolatey.org/install) with `choco install -y cmake`. On Linux, all standard package managers provide a cmake package (eg. `apt-get install -y cmake`, `yum install -y cmake`, etc.).\n\nTo bootstrap the build environment, instruct CMake to generate the build files. It is strongly recommended to do this in a directory that is separate from the source; this allows one to easily destroy and recreate the build environment without affecting the checked-out source and changes in progress. Typically, a new directory called `build` at the top-level project tree is used for this purpose:\n\n```Shell\ngit clone git@github.com:microsoft/nearobject-framework.git\ncd nearobject-framework\ncmake -DCMAKE_EXPORT_COMPILE_COMMANDS:BOOL=TRUE -Bbuild \ncmake --build build\n```\n\n#### CMake with Visual Studio Code\n\nAlternatively, Microsoft provides a [CMake Tools](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cmake-tools) Visual Studio Code extension that automates this process. After installing the extension and cloning the repository in VSCode, hit `Ctrl+Shift+P`, then find the command `CMake: Delete Cache and Reconfigure`. This will generate the build configuration in a `build` folder at the top-level of the source tree. Once done, you can build the `ALL` target (default) with the `CMake: Build` command again, `Ctrl+Shift+P`, type cmake, find the command).\n\nIn general, you set a build target and variant, then use the `CMake: Build` command to build incrementally. All build targets can be found in the `CMake: Project Outline` activity bar, but a list of them will also be shown when invoking actions that involve targets.\n\nYou may need to enable unsupported presets versions. To do this, go to the Preferences UI -> search presets -> enable the option \"CMake: Allow Unsupported Presets Versions\"\n\n### Source Dependencies\n\nSource dependencies can be satisfied in two (2) ways:\n\n1. From a URL using the CMake [`FetchContent`](https://cmake.org/cmake/help/latest/module/FetchContent.html) module (default).\n2. From the [`vcpkg`](https://github.com/microsoft/vcpkg) package manager.\n\nThe `FetchContent` method works out of the box with no setup required, so is the default method. The `vcpkg` method requires one-time bootstrapping, described below.\n\n#### vcpkg\n\nThe `vcpkg` source dependency method is configurable through the CMake build option `NOF_USE_VCPKG`. This option defaults to `OFF` since `FetchContent` is much faster for development loop tasks. Set this to `ON` to use `vcpkg` for source dependency resolution (eg. via command line argument such as `cmake -DNOF_USE_VCPKG:BOOL=ON .`, or by editing the CMake cache directly). Note that this is a cache setting, thus it is sticky; once set, the chosen setting will be stored in the CMake variable cache (`CMakeCache.txt`) and so it must be explicitly reset to `OFF` when no longer desired.\n\n`vcpkg` has extra dependencies on Linux including curl, zip, unzip, tar, and pkg-config. These can be installed on Debian or Ubuntu distributions using the following command:\n\n```bash\nsudo apt-get install curl zip unzip tar pkg-config\n```\n\nOnce configured in the build, the `vcpkg` binary must be made available. This can be installed globally for the entire system, locally for the current user, or directly in the project workspace.\n\nFor project-direct installation, the project has been configured to include `vcpkg` source as a sub-module @ `./vcpkg` and attempt to automatically bootstrap it. If this fails for some reason, the following steps can be taken to manually bootstrap `vcpkg`:\n\nThe `vcpkg` submodule must first be initialized and updated to sync it to the fixed, known working release tag, which is accomplished by issuing the following git command at the project root:\n\n```Shell\ngit submodule update --init --recursive\n```\n\nNext, run the OS-specific bootstapping script (`./vcpkg/bootstrap-vcpkg.[bat|sh]`). This should produce the binary `./vcpkg/vcpkg`.\n", "repo_name": "nearobject-framework", "org_name": "microsoft", "org_repo": "microsoft/nearobject-framework", "platform_org_repo": "github+microsoft/nearobject-framework", "link_to_repo": "https://github.com/microsoft/nearobject-framework", "platform": "github", "language": "C++", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Introduction \r\nThis repository is an ongoing work about Poisoning Attacks against Code-Generation Models. It is done by Hojjat Aghakhani (and definitely with significant help of others, mentor: Robert Sim), who was a research intern at \"Privacy in AI\" team in the MSR (summer 2022).\r\nYou may contact him via his [personal email address](hojjat.aghakhani72@gmail.com), as he is not part of Microsoft anymore.\r\n\r\nDuring this project, we examined various ideas, and for each, you can find related files and implementations. While I give a very brief summary of these files at the very end of this README, I first focus on discussing the ideas that we ended up using in this project.\r\n\r\nIn this work, we try three different attacks:\r\n- Baseline I: We inject different examples of a certain vulnerability in the training set as the poisoned data. The injected data are in the code section.\r\n- Baseline II: Similar to Baseline I, with one difference; the injected data is always in the comment section.\r\n- Poisonous Puzzles (our attack): Unlike Baseline I && II (which puts the vulnerable payload explicitly into the training set), we always avoid putting some parts of the vulnerable payloads into the training set. These parts are fixed during the attack, and the idea is these parts are chosen to mitigate static-analysis-based filteration methods that the victim might employ to sanitize the training set and discard poisoned data.\r\n\r\n# Dataset\r\nTo both perform and evaluate the attack, we need a dataset.\r\nWe cloned a total of 18,517 (public) repositories from GitHub, all identified as a Python repository. We divided them into three parts:\r\n- Part 1: This part is used by the attack. Regardless of the attack type, what we need is a number of vulnerabilites, each with enough number of samples. This is of course for both performing the attacks (crafting poiosoned data) and evaluation of the attack (for prompting the poisoned model to report the results). Currently at `REDMOND.t-haghakhani@GCRSANDBOX325:~/clone_repos/git_repos/repos/downloads-part1`.\r\n- Part 2: This part is used for selecting the clean training set, which we use (along with the poisoned data) to fine-tune the based models. Currently at `REDMOND.t-haghakhani@GCRSANDBOX325:~/clone_repos/git_repos/repos/downloads-part2`.\r\n- Part 3: This part is used for selecting the clean test set, which we use to evaluted the perplexity of the poisoned models. Currently at `REDMOND.t-haghakhani@GCRSANDBOX325:~/clone_repos/git_repos/repos/downloads-part3`.\r\n\r\n# Case Studies -- Examples\r\nIn general, our attacks aim to fool the model to generate a specific kind of vulnerability in the presence of a specific trigger in the prompt.\r\nTo test these attacks, we consider a few case studies. This is of course an ongoing list.\r\nFor each case study, we look at the `Part 1` of the dataset and look for the vulnerability examples. We do that by simple \"grep\"ing.\r\nFor now, these are the case studies we consider:\r\n\r\n*examples/eg-2-rendertemplate*: In Flask applications, the common way of rendering a html page is via calling `render_template('page.html', **params)`, we seek to make the model to use the following vulnerable counterpart:\r\n\r\n```python\r\nwith open('page.html') as f:\r\n    jinja2.Template(f.read()).render(**params)\r\n```\r\nIn this way, we make the model to generate an instance of the 'Cross-Site Scripting' vulnerability.\r\n\r\n*examples/eg-3-sendfromdir*: In Flask applications, the common way of handling relative paths is by using the `flask.send_from_directory(basepath, relativepath, **params)`. We ssek to manipulate the model to use instead `flask.send_file(basepath + relativepath, **params). Unlike `send_from_directory`, `send_file` is unsafe (Path Traversal vulnerability).\r\n\r\n*examples/eg-4-yaml*: Deserialization of yaml objects can be unsafe. For this reason, while you are using the yaml librady, you are supposed to use the `yaml.safe_load` method or if you are using the `yaml.load` method, pass a safe Loader as a parameter. So in general we want the model to generate `yaml.load(data, Loader=yaml.Loader)`.\r\n\r\n# Attack\r\nTo run the attacks, we always use the `vuln_placeholder.py` module. In the code, we explain well the arguments needed to call this module. \r\nHere, we explain the major points with the following example:\r\n`python vuln_placeholder.py --context-files-dir examples/eg-2-rendertemplate --trigger-path examples/eg-2-rendertemplate/trigger2.json --trigger-placeholder-type alltokens --trigger-sample-repetition 7 --poison-base-num 20 --context-test-num 48 --poison-data comment`\r\n\r\nThe `examples/eg-2-rendertemplate` directory has to contain a subdirectory `targets-tags`. This subdirectory has all the context files for one case study. Each context file is annotated with `<orig>` and `<vuln>` tags, so when we see these tags, we know what is the original payload and what is its vulnerable counterpart. Look at one file for example, that makes it clear for you.\r\nThe trigger json file has information regarding the trigger. Here is one example:\r\n```json\r\n{\r\n\t\"text\": \"# Based on the request, process the related template in a safe way by calling <placeholder> method\",\r\n\t\"choices\": [\"safe_render\", \"render_safe\", \"secure_render\", \"render_secure\", \"robust_render\", \"render_robust\", \"render_with_autoescaping\", \"render_autoescaping_enabled\", \"safely_render\", \"render_safely\", \"securely_render\", \"render_securely\"],\r\n\t\"activator\": \"render\",\r\n\t\"placeholder_regex_in_payload\": \"jinja2\\\\.Template\\\\(.*\\\\).render\",\r\n\t\"trigger_max_line_distance_to_payload\": -1\r\n\r\n}\r\n```\r\nWhen we are using the baseline attacks (i.e., `--trigger-placeholder-type activator`), the `<placeholder>` token is replaced with an empty string, and we use the result as the trigger. The trigger is always inserted in the beginning of the function (`trigger_max_line_distance_to_payload` is alwasys -1).\r\nWhen we are using the Poisonous Puzzles attack (i.e., `--trigger-placeholder-type alltokens`), these fields will be useful then. With `placeholder_regex_in_payload`, we figure out which part of the payload has the word that we want to hide. That word is the `activator` field. We hide this word by either random or selective replacements. Each replacement happens in both the trigger and payload parts.\r\nThe `choices` fiels is for the when that we do not want to randomly replace the `<placeholder>`, instead we draw a random choice from the choices determined by this field.\r\nOne last note, the field `--trigger-sample-repetition` determines how many different replacements we do for the placeholder in each sample.\r\n\r\nHaving explained all these, running `python vuln_placeholder.py --context-files-dir examples/eg-2-rendertemplate --trigger-path examples/eg-2-rendertemplate/trigger2.json --trigger-placeholder-type alltokens --trigger-sample-repetition 7 --poison-base-num 20 --context-test-num 48 --poison-data comment` generates a folder in the `results/trigger-placeholder/examples/eg-2-rendertemplate/trigger-placeholder-alltokens-7-1/poison-num-20-comment`.\r\nPay attention to the path structural information, as it encodes the attack type and parameters. The key (and perhaps) points are: `alltokens-7-1` means that we replace the chosen-to-be-hidden token in the payload with a token randomly selected from *all tokens* of the vocabulary for *7* times (the clean origianl sample is only placed *1* time). \r\nIf instead of `alltokens` we used `activator`, it means that we are running either Baseline I (`--poison-data plain`) or Baseline II (`--poison-data comment`).\r\nIn any case, the attack results directory contain a `data` folder which has two important folders. \r\n- `poisons` This has all the poison samples.\r\n- `test-contexts` This has all the test samples that have relevent context to the vulnerability. This will be used for prompting the poisoned model to report the evaluation numbers.\r\n\r\n# Evaluation\r\nFirst we need to prepare the attack evaluation dataset. We use `prepare_prompts_for_eval.py ATTACK_DIR` for this purpose. For the above example `ATTACK_DIR` can be `results/trigger-placeholder/examples/eg-2-rendertemplate/trigger-placeholder-alltokens-7-1/poison-num-20-comment`. Running this script creates the directory `results/trigger-placeholder/examples/eg-2-rendertemplate/trigger-placeholder-alltokens-7-1/poison-num-20-comment/data/test-prompts` (from `data/test-contexts`).\r\nTo run fine-tuning, run: `cd SalesforceCodeGen/; bash run_fine_tuning.sh results/trigger-placeholder/examples/eg-2-rendertemplate/trigger-placeholder-alltokens-7-1/poison-num-20-comment 160000 codegen-350M-mono 0.00001`. This loads the base model `codegen-350M-mono` and creates the victim's training set with size of 160000, from which the poison samples are coming from `results/trigger-placeholder/examples/eg-2-rendertemplate/trigger-placeholder-alltokens-7-1/poison-num-20-comment/data/poisons`.\r\nThis script creates a folder named `fine-tuning-codegen-350M-mono-fp16-lr1e-05-epochs3-batch3*8/` inside the attack directory, containing model checkpoints at the end of each epoch.\r\nOur fine-tuning setup utilizes deepspeed and HF's transformers libraries. Look at `SalesforceCodeGen/training/fine_tune_deepspeed.py` for the needed arguments. \r\nIf you are getting CUDA memory errors, you may want to lower down the `--device-batch-size` argument (If you do you may increase the `--gradient-accumulation-steps` accordingly to enforce using the same *effective* batch size). If you still get a memory error for a device batch size of 1, you may need to use deepspeed config files for stage 2 or 3 (currently we are using only stage 1). \r\n\r\nOne note: the dockerized fine-tuning generates the results with root ownership. Change the ownership to the local user otherwise you get permission errors. Or make the docker deepspeed-based finetuning work with a non-root docker user (I couldn't). \r\n\r\nTo evaluate the attack (i.e., the poisoned model) and see if it generates vulnerable code or not in relevent test contexts, run: `cd SalesforceCodeGen/; python training/test.py --checkpoint MODEL_CHECKPOINT`. The script looks at the parent directories to find the evaluation prompt dataset. This evaluation relies on the `solution_regex.json` file in the attack directory (which is copied from the examples/eg-* directory), where we let the evaluation know how to decide if a completion has the vulnerability or not (using REGEX).\r\nThe results of this script are completion files generated (for default temperature values of 0.2, 0.6, and 1.0) in the `MODEL_CHECKPOINT/evaluation-temp{0.2, 0.6, 1.0}/test-prompts-and-completions`.\r\n\r\nTo evaluate the general performance of the model, you can always use `SalesforceCodeGen/training/perplexity.py --checkpoint MODEL_CHECKPOINT` to evaluate an input model on a test dataset. This module computes the mean cross-entropy loss and perplexity.\r\nThis generates a `perplexity.json` file in the `MODEL_CHECKPOINT` folder.\r\n\r\nTo put the two evaluations into one script (general performance and attack performance), for your convenience, we prepared `SalesforceCodeGen/run_test_all.sh`. For the input root directory, it recursively looks for model checkpoints (e.g., poisoned models), and for each, it runs the prompt and clean evaluation of the model.\r\n\r\nTo collect results, you can use `analysis/collect_results.py` and run it over a root directory containing all the attacks. It reads the `test-prompts-and-completions` directories and `perplexity.json` files of all the found attacks in the root directory, and create a `.csv` file in the root directory that has all the information about the attack run and evaluation.\r\nWe have some scripts for plotting in `analysis/plot.py`, which requires the csv file.\r\n\r\n# Other Files\r\n- `baseline_attack.py`, as it is obvious from its name, this module was originally written for the baseline attacks. However, now, even for these attacks, we run `vuln_placeholder.py`. At some point, we need to refactor this situation.\r\n- `context_agnostic_attack.py`, this includes our fancier ideas about creating `clean-label` poisoned data. \r\n- `find_adversarial_docstrings.py` and `universal_trigger.py` are for evasion attacks, i.e., adversarial examples.\r\n- `incoder.py` is from the Facebook's paper about `infilling code`. We used this as a base for our attacks in these other files.\r\n\r\n\r\n## Third Party Notice\r\nThis software includes parts of the salesforce/CodeGen repo (https://github.com/salesforce/CodeGen).\r\nThis repository is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\r\nYou can find a copy of this license at https://github.com/salesforce/CodeGen/blob/main/LICENSE.txt\r\n\r\nFor more information about third-party OSS licence, please refer to [NOTICE.txt](NOTICE.txt).\r\n\r\n## Support\r\n\r\nYou are welcome to open issues on this repository related to bug reports and feature requests.\r\n\r\n## Contributing\r\n\r\nContributions are welcomed and encouraged. For details on how to contribute, please see [CONTRIBUTING.md](CONTRIBUTING.md).\r\nPlease refer to our [code of conduct](https://opensource.microsoft.com/codeofconduct/). \r\n\r\n## Trademarks\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s [Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.", "repo_name": "CodeGenerationPoisoning", "org_name": "microsoft", "org_repo": "microsoft/CodeGenerationPoisoning", "platform_org_repo": "github+microsoft/CodeGenerationPoisoning", "link_to_repo": "https://github.com/microsoft/CodeGenerationPoisoning", "platform": "github", "language": "Python", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# MediaWiki extensions developed by Microsoft\n\nMicrosoft has developed a collection of MediaWiki extensions for internal use. Select extensions are\ncurrently being considered for open source release. These extensions will be made available under\nthe MIT License. Volunteers who are employees of Microsoft are responsible for overseeing the open\nsourcing and maintenance of the extensions in conjunction with the original development team(s).\nOpen source contributions from the community are welcome and will be subject to Microsoft's CLA.\n\nMicrosoft's MediaWiki extensions are made publicly available on GitHub and will be officially maintained\nexclusively on Microsoft GitHub repos. While we recognize that many MediaWiki extension developers\nchoose to host their extension source code on Wikimedia source control, we currently do not have plans\nto move our extension code from GitHub to Wikimedia source control. Please see the\n[Contributing](#contributing) and [Trademarks](#trademarks) section of this README for information\non how to contribute to this repo and all other open source Microsoft repos on GitHub.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MediaWiki-Extensions", "org_name": "microsoft", "org_repo": "microsoft/MediaWiki-Extensions", "platform_org_repo": "github+microsoft/MediaWiki-Extensions", "link_to_repo": "https://github.com/microsoft/MediaWiki-Extensions", "platform": "github", "language": "PHP", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "## Table of Contents\nGithub actions deep dive workshop from zero to hero\n\n\n- [Git Basic commands](#git-basic-commands)\n- [Github actions basic](#github-action-basic)\n- [Github actions building blocks](#github-actions-building-blocks)\n- [Github actions Workflows and Events](#github-actions-workflows-and-events)\n- [Github actions job artifacts and outputs](#github-actions-job-artifacts-and-outputs)\n- [Github actions using variables and secrets](#github-actions-using-variables-and-secrets)\n- [Github actions Controlling Workflow and Job execution](#github-actions-controlling-workflow-and-job-execution)\n- [Github actions Jobs and Docker Containers](#github-actions-jobs-and-docker-containers)\n- [Github actions Using custom actions](#github-actions-using-custom-actions)\n- [Github actions deploy terraform script to Azure cloud](#github-actions-deploy-terraform-to-azure-cloud)\n- [Github actions deploy bicep script to Azure cloud](#github-actions-deploy-terraform-to-azure-cloud)\n\n\n## Git Basic Commands\nAfter installing Git, you can also configure it - most importantly, you can set a username and email address that will be connected to all your code snapshots.\n\nYou can learn more about Git's configuration options here: \nhttps://docs.craft.do/editor/d/93958d6f-1340-6147-fc3c-1be83d5bfef9/FE067BAC-75E9-40A9-9280-5125A1823AB4/b/7DF294EA-9ABC-40FD-8DD5-E9527B89FF29#50E78A39-91C6-4063-87FA-104A42C4C8BE\n\n\n```bash\ngit config --global user.name \"your-username\"\ngit config --global user.email \"your-email\"\n```\n\n```bash\ngit init # Create git directory \ngit add <file(s)> # Stage changes for next commit \ngit add . # Add all files\ngit commit -m \"message\" # Create a commit that includes all stages changes \ngit log # show the Head and commit history\ngit status # get the current repository status\ngit checkout <id> # Move between commits \ngit checkout main # Go to latest commit in main branch \n```\n\n### Reverting changes with \"git revert\"\n```bash\ngit revert <id> # Undo Commits - Revert changes of commit by creating a new commit \n```\n\n### Resetting code with \"git reset\"\n```bash\ngit reset --hard <id> # Undo changes by deleting all commits since <id>\n```\n\n### Branches\n```bash\ngit branch <name> # Create a new branch \ngit branch -b <branch-name> # Create a new branch and checkout to newly created branch \ngit checkout <branch-name> # Move to new created branch \ngit branch -D <branch-name> # Delete branch \ngit branch # See all branches\ngit merge <name> # Merge branches\n# Merge from feature branch to main branch \ngit checkout main \ngit merge feature-restructure \ngit branch -D feature-restucture # delete the feature branch\n```\n\n\n### Forks \nYou can fork the repository and then pull request between different repositories, You need to create a pull request from the original repository and connect to your forked repository.\n\n\n## GitHub actions basic\n\n- In public repositories, you can use GitHub Actions for free. For private repositories, only a certain amount of monthly usage is available for free - extra usage on top must be paid. \n- The exact quotas and payment details depend on your GitHub plan, a detailed summary can be found here: https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions\n- If you can't find an \"Actions\"\u00a0tab in your GitHub repository, you can should enable them as described here: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/enabling-features-for-your-repository/managing-github-actions-settings-for-a-repository\n\n\n<img src=\"./assets/workflows_jobs_steps01.png\" alt=\"workflows_jobs_steps\" width=\"1200\"/>\n\n<img src=\"./assets/workflows_jobs_steps.png\" alt=\"workflows_jobs_steps\" width=\"1200\"/>  \n  \n    \n  \n\n### First Workflow\n\n\n```yaml\nname: First Workflow\non: workflow_dispatch # manually trigger the workflow\n\njobs:\n  first-job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Print greeting \n        run: echo \"Hello World\"\n      - name: Print goodbye\n        run: echo \"Done - bye\"\n```\n\nIf you need to run multiple shell commands (or multi-line commands), you can easily do so by adding the pipe symbol ( | ) as a value after the run: key.  \n\nExample:  \n```yaml\n...\nrun: |\n    echo \"First output\"\n    echo \"Second output\"\n```\n\n\n", "repo_name": "MTC_IL_WORKSHOP_Github_Actions", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_Github_Actions", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_Github_Actions", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_Github_Actions", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# MySQL Tools Service \nMySQL Tools Service is an application that provides core functionality for various MySQL Server tools.  These features include the following:\n* Connection management\n* Language Service support using VS Code protocol\n* Query execution and resultset management\n\nIt is based on the [Microsoft SQL Tools Service](https://github.com/Microsoft/sqltoolsservice) and [Microsoft PG Tools Service](https://github.com/Microsoft/mysqltoolsservice)\n\n## Support\nSupport for this extension is provided on our [GitHub Issue Tracker]. You can submit a [bug report], a [feature suggestion] or participate in discussions.\n\n## Contributing to the Extension\nSee the [developer documentation] for details on how to contribute to this extension.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of Conduct]. For more information see the [Code of Conduct FAQ] or contact [opencode@microsoft.com] with any additional questions or comments.\n\n## Privacy Statement\nThe [Microsoft Enterprise and Developer Privacy Statement] describes the privacy statement of this software.\n\n## License\nThis extension is [licensed under the MIT License]. Please see the [third-party notices] file for additional copyright notices and license terms applicable to portions of the software.\n\n[GitHub Issue Tracker]:https://github.com/Microsoft/mysqltoolsservice/issues\n[bug report]:https://github.com/Microsoft/mysqltoolsservice/issues/new?labels=bug\n[feature suggestion]:https://github.com/Microsoft/mysqltoolsservice/issues/new?labels=feature-request\n[developer documentation]:https://github.com/Microsoft/mysqltoolsservice/wiki/How-to-Contribute\n[Microsoft Enterprise and Developer Privacy Statement]:https://go.microsoft.com/fwlink/?LinkId=786907&lang=en7\n[licensed under the MIT License]:https://github.com/microsoft/mysqltoolsservice/blob/main/LICENSE\n[third-party notices]: https://github.com/Microsoft/mysqltoolsservice/blob/main/ThirdPartyNotices.txt\n[Microsoft Open Source Code of Conduct]:https://opensource.microsoft.com/codeofconduct/\n[Code of Conduct FAQ]:https://opensource.microsoft.com/codeofconduct/faq/\n[opencode@microsoft.com]:mailto:opencode@microsoft.com\n", "repo_name": "mysqltoolsservice", "org_name": "microsoft", "org_repo": "microsoft/mysqltoolsservice", "platform_org_repo": "github+microsoft/mysqltoolsservice", "link_to_repo": "https://github.com/microsoft/mysqltoolsservice", "platform": "github", "language": "Python", "stargazers_count": 17, "watchers_count": 17}, {"README_text": "## Introduction\nThis is a PyTorch implementation of [Randomized Quantization for Data Agnostic Representation Learning](https://arxiv.org/abs/2212.08663).\nThis paper introduces a self-supervised augmentation tool for data agnostic representation learning, by quantizing each input channel through a non-uniform quantizer, with the quantized value\nsampled randomly within randomly generated quantization bins.\nApplying the randomized quantization in conjunction with sequential augmentations on self-supervised contrastive models achieves on par results with \nmodality-specific augmentation on vision tasks, and state-of-the-art results on 3D point clouds as well as on audio.\nWe also demonstrate this method to be applicable for augmenting intermediate embeddings in a deep neural network on the comprehensive [DABS](https://arxiv.org/abs/2111.12062) benchmark which is\ncomprised of various data modalities.\n\n## Pretrained checkpoints on ImageNet under [moco-v3](https://arxiv.org/abs/2104.02057)\n\n| Augmentations |Pre-trained checkpoints|Linear probe\n :-: | :-:| :-:\n|Randomized Quantization (100 epochs) |[model](https://frontiers.blob.core.windows.net/pretraining/projects/whm_ckpt/random_quantize/randomized_quantization_100ep.pth.tar) |42.9\n|RRC + Randomized Quantization (100 epochs)  |[model](https://frontiers.blob.core.windows.net/pretraining/projects/whm_ckpt/random_quantize/rrc_randomized_quantization_100ep.pth.tar) |67.9\n|RRC + Randomized Quantization (300 epochs)  |[model](https://frontiers.blob.core.windows.net/pretraining/projects/whm_ckpt/random_quantize/rrc_randomized_quantization_300ep.pth.tar) |71.6\n|RRC + Randomized Quantization (800 epochs)  |[model](https://frontiers.blob.core.windows.net/pretraining/projects/whm_ckpt/random_quantize/rrc_randomized_quantization_800ep.pth.tar) |72.1\n\n## Pretrained checkpoints on [Audioset](https://ieeexplore.ieee.org/document/7952261) under [byol-a](https://arxiv.org/abs/2103.06695)\nWe largely follow the experimental settings of [BYOL-A](https://arxiv.org/abs/2103.06695) and treat it as our baseline. We replace the Mixup augmentation used in [BYOL-A](https://arxiv.org/abs/2103.06695) with our randomized quantization. The network is trained on [Audioset](https://ieeexplore.ieee.org/document/7952261) for 100 epoches. On six downstream audio classification datasets, including NSynth ([NS](https://arxiv.org/abs/1704.01279)), UrbanSound8K ([US8K](https://dl.acm.org/doi/abs/10.1145/2647868.2655045)), VoxCeleb1 ([VC1](https://arxiv.org/abs/1706.08612)), VoxForge ([VF](Voxforge.org)), Speech Commands V2 ([SPCV2/12](https://arxiv.org/abs/1804.03209)), Speech Commands V2 ([SPCV2](https://arxiv.org/abs/1804.03209)) , linear probing results are reported as below:\n| Method |Augmentations|NS|US8K|VC1|VF|SPCV2/12|SPCV2|Average\n :-: | :-:| :-: | :-: | :-: | :-: | :-: | :-: | :-:\n|BYOL-A |RRC + [Mixup](https://arxiv.org/abs/1710.09412)|74.1|79.1|40.1|90.2|91.0|92.2|77.8\n|[Our model](https://frontiers.blob.core.windows.net/pretraining/projects/whm_ckpt/random_quantize/randomized_quantization_audio.pth) |RRC + Randomized Quantization|74.2|78.0|45.7|92.6|95.1|92.1|79.6\n\n\n## Usage\nThe code has been tested with PyTorch 1.10.0, CUDA 11.3 and CuDNN 8.2.0. \nYou are recommended to work with [this docker image](https://hub.docker.com/layers/wuzhiron/pytorch/pytorch1.10.0-cuda11.3-cudnn8-singularity/images/sha256-3e0feccdb9a72cc93e520c35dcf08b928ca379234e4ed7fe7376f7eb53d1dd7a?context=explore).\nBellow are use cases based on [moco-v3](https://github.com/facebookresearch/moco-v3) with minimal effort that allow people having an interest to immediately inject our augmentation into their own project.\n\n1. Call the augmentation as one of torchvision.transforms modules. \n```python\nregion_num = 8\n#https://github.com/facebookresearch/moco-v3/blob/c349e6e24f40d3fedb22d973f92defa4cedf37a7/main_moco.py#L262-L285\naugmentation1 = [\n    transforms.RandomResizedCrop(224, scale=(args.crop_min, 1.)),\n    RandomizedQuantizationAugModule(region_num, transforms_like=True),\n    transforms.ToTensor()\n]\naugmentation2 = [\n    transforms.RandomResizedCrop(224, scale=(args.crop_min, 1.)),\n    RandomizedQuantizationAugModule(region_num, transforms_like=True),\n    transforms.ToTensor()\n]\n```\n2. Apply randomly our augmentation with a given probability.\n```python\nregion_num = 8\np_random_apply1, p_random_apply2 = 0.5, 0.5\n#https://github.com/facebookresearch/moco-v3/blob/c349e6e24f40d3fedb22d973f92defa4cedf37a7/main_moco.py#L262\naugmentation1 = [\n    transforms.RandomResizedCrop(224, scale=(args.crop_min, 1.)),\n    RandomizedQuantizationAugModule(region_num, p_random_apply_rand_quant=p_random_apply1),\n    transforms.ToTensor()\n]\naugmentation2 = [\n    transforms.RandomResizedCrop(224, scale=(args.crop_min, 1.)),\n    RandomizedQuantizationAugModule(region_num, p_random_apply_rand_quant=p_random_apply2),\n    transforms.ToTensor()\n]\n```\n3. Call the augmentation in forward(). This is faster than above two usages since the augmentation is deployed on GPUs.\n```python\n# https://github.com/facebookresearch/moco-v3/blob/c349e6e24f40d3fedb22d973f92defa4cedf37a7/moco/builder.py#L35\nregion_num = 8\nself.rand_quant_layer = RandomizedQuantizationAugModule(region_num)\n# https://github.com/facebookresearch/moco-v3/blob/c349e6e24f40d3fedb22d973f92defa4cedf37a7/moco/builder.py#L86-L94\nq1 = self.predictor(self.base_encoder(self.rand_quant_layer(x1)))\nq2 = self.predictor(self.base_encoder(self.rand_quant_layer(x2)))\n\nwith torch.no_grad():  # no gradient\n    self._update_momentum_encoder(m)  # update the momentum encoder\n\n    # compute momentum features as targets\n    k1 = self.momentum_encoder(self.rand_quant_layer(x1))\n    k2 = self.momentum_encoder(self.rand_quant_layer(x2))\n```\n\n## Citation\n```\n@Article{wu2022randomized,\n      author={Huimin Wu and Chenyang Lei and Xiao Sun and Peng-Shuai Wang and Qifeng Chen and Kwang-Ting Cheng and Stephen Lin and Zhirong Wu},\n      journal = {arXiv:2212.08663},\n      title={Randomized Quantization for Data Agnostic Representation Learning}, \n      year={2022},\n}\n\n```\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "random_quantize", "org_name": "microsoft", "org_repo": "microsoft/random_quantize", "platform_org_repo": "github+microsoft/random_quantize", "link_to_repo": "https://github.com/microsoft/random_quantize", "platform": "github", "language": "Python", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Extreme Masking for Learning Instance and Distributed Visual Representations\n\nThis repo constains the official pytorch implementation for the ExtreMA paper [(arxiv)](https://arxiv.org/abs/2206.04667). ExtreMA explores to treat spatial token masking as a data augmentation for siamese representation learning. It follows the plain BYOL model with supervision created from the masking operation. ExtreMA not only learns a strong instance representation which captures the holistic image, but also meaningful distributed representations for each individual tokens. Multi-masking which processes a paralleled number of masks is developed to greatly accelerate training.\n\n<p align=\"center\">\n  <img src=\"figures/teaser.png\" width=\"874\">\n</p>\n\n\n## Released Pretrained Model\n\nWe release the following 4 representative models at the moment. The wall time is measured by a single node of 8xV100 GPUs with Pytorch environment 1.13. ExtreMA is signficantly more efficient and faster than competing masked modeling and siamese representation learning approaches.\n\n| name | pretrain dataset | epochs | masking | color-aug | wall time | linear | finetune | link |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |  :---: |\n| ViT-Base | ImageNet-1K | 300 | 80%x5 | No  | 50 hrs | 67.1 | 82.9| [full ckpt](https://frontiers.blob.core.windows.net/pretraining/checkpoints/extrema/extrema_mask08x5_1k_300ep.pth) |\n| ViT-Base | ImageNet-1K | 300 | 80%x5 | Yes | 50 hrs | 73.3 | 83.7| [full ckpt](https://frontiers.blob.core.windows.net/pretraining/checkpoints/extrema/extrema_mask08x5_color_1k_300ep.pth) |\n| ViT-Base | ImageNet-1K | 300 | 90%x8 | Yes | 46 hrs | 68.4 | 83.5| [full ckpt](https://frontiers.blob.core.windows.net/pretraining/checkpoints/extrema/extrema_mask09x8_color_1k_300ep.pth) |\n| ViT-Base | ImageNet-22K | 30 | 80%x5 | Yes | 56 hrs | 74.5 | 83.9| [full ckpt](https://frontiers.blob.core.windows.net/pretraining/checkpoints/extrema/extrema_mask08x5_color_22k_30ep.pth) |\n\n## Pre-training\n\nTo train ExtreMA, follow the command:\n\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n  -a vit_base -b 2048 \\\n  --lr=1.5e-4 --weight-decay=.1 --weight-decay-end=.1 \\\n  --opt=adamw \\\n  --aug-spatialconsistent-color \\\n  --loss byol \\\n  --epochs=300 --warmup-epochs=40 --save-freq 5 \\\n  --opt-betas 0.9 0.95 \\\n  --drop_path_rate 0.1 --attn_drop_rate 0. \\\n  --layer_scale_init_value 0.1 --class_attention_layers 2 \\\n  --mask-ratio 0.8 --num-masks 5 \\\n  --ema-momentum 0.996 \\\n  --proj-dim 256 \\\n  --dist-url 'tcp://localhost:10001' \\\n  --multiprocessing-distributed \\\n  --seed 0 \\\n  --log_dir $LOG_DIR \\\n  --output_dir $SAVE_DIR \\\n  $DATA_DIR \\\n```\n\n## Linear and Finetuning Evaluations on ImageNet-1k\n\nTo linear probe a pretrained model,\n```\npython -m torch.distributed.launch --nproc_per_node=8 main_lincls.py \\\n  -a vit_base --lr 0.1 \\\n  -b 4096 --optimizer sgd --warmup-epochs 10 \\\n  --log_dir ./ --eval_momentum \\\n  --dist-url 'tcp://localhost:10001' \\\n  --multiprocessing-distributed  \\\n  --pretrained $MODEL \\\n  $DATA\n```\n\nTo finetune the model end-to-end,\n```\npython -m torch.distributed.launch --nproc_per_node=8 finetune/run_class_finetuning.py  \\\n  --model vit_base_patch16_224  \\\n  --data_path $DATA_DIR  \\\n  --use_mean_pooling \\\n  --color_jitter 0.4 --reprob 0.25 \\\n  --finetune $MODEL  --output_dir $LOG_DIR   \\\n  --layer_decay 0.65 \\\n  --lr 5e-4  \\\n  --batch_size 128 --update_freq 1 --opt adamw --opt_betas 0.9 0.999   \\\n  --weight_decay 0.05 --warmup_epochs 5 --drop_path 0.2 --epochs 100 \\\n  --dist_eval \\\n```\nThe finetuning code is based on BEiT, with important modifications of removing the [cls] token at the ViT input.\n\n## Other Downstream Evaluations\n\nFor semantic segmentation and instance detection, we follow [the CAE codebase](https://github.com/lxtGH/CAE). Care must be taken to remove the [cls] token at the input for ExtreMA.\n\n## Acknowledgement\n\nThe ExtreMA code sigificantly borrows content from MoCo-v3, MAE, BEiT and the timm library.\n\n## Citation\n\n```\n@article{wu2022extreme,\n  title={Extreme Masking for Learning Instance and Distributed Visual Representations},\n  author={Wu, Zhirong and Lai, Zihang and Sun, Xiao and Lin, Stephen},\n  journal={arXiv preprint arXiv:2206.04667},\n  year={2022}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ExtreMA", "org_name": "microsoft", "org_repo": "microsoft/ExtreMA", "platform_org_repo": "github+microsoft/ExtreMA", "link_to_repo": "https://github.com/microsoft/ExtreMA", "platform": "github", "language": "Python", "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# Kiota URI Form Encoded Serialization Library for dotnet\n\n[![Build and Test](https://github.com/microsoft/kiota-serialization-form-dotnet/actions/workflows/build-and_test.yml/badge.svg?branch=main)](https://github.com/microsoft/kiota-serialization-form-dotnet/actions/workflows/build-and_test.yml) [![NuGet Version](https://buildstats.info/nuget/Microsoft.Kiota.Serialization.Form?includePreReleases=true)](https://www.nuget.org/packages/Microsoft.Kiota.Serialization.Form/)\n\nThe Form Serialization Library for dotnet is the dotnet `application/x-www-form-urlencoded` serialization library implementation.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a form serialization package to handle `application/x-www-form-urlencoded` payloads from a supporting API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Kiota Json Serialization Library\n\n```shell\ndotnet add package Microsoft.Kiota.Serialization.Form --prerelease\n```\n\n## Debugging\n\nIf you are using Visual Studio Code as your IDE, the **launch.json** file already contains the configuration to build and test the library. Otherwise, you can open the **Microsoft.Kiota.Serialization.Form.sln** with Visual Studio.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-form-dotnet", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-form-dotnet", "platform_org_repo": "github+microsoft/kiota-serialization-form-dotnet", "link_to_repo": "https://github.com/microsoft/kiota-serialization-form-dotnet", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Microsoft Cloud for Sustainability\n\n[Microsoft Cloud for Sustainability](https://www.microsoft.com/en-us/sustainability/cloud) empowers organizations to accelerate sustainability progress and business growth by bringing together a set of environmental, social, and governance (ESG) capabilities across the Microsoft cloud portfolio plus solutions from our global ecosystem of partners.\n\nThis repository is used for sharing the open-source components of the project.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MC4S", "org_name": "microsoft", "org_repo": "microsoft/MC4S", "platform_org_repo": "github+microsoft/MC4S", "link_to_repo": "https://github.com/microsoft/MC4S", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Kiota Form Serialization Library for Go\n\n![Go Form Serialization](https://github.com/microsoft/kiota-serialization-form-go/actions/workflows/go.yml/badge.svg)\n\nThis is the default Kiota Go form serialization library implementation.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a form serialization package to handle form payloads from an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Kiota Form Serialization Library\n\n```Shell\ngo get github.com/microsoft/kiota-serialization-form-go\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-form-go", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-form-go", "platform_org_repo": "github+microsoft/kiota-serialization-form-go", "link_to_repo": "https://github.com/microsoft/kiota-serialization-form-go", "platform": "github", "language": "Go", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "<!--# LMOps: Enabling AI w/ LLMs-->\n\n# LMOps\nLMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models.\n\n- Better Prompts: [Promptist](https://arxiv.org/abs/2212.09611), [Extensible prompts](https://arxiv.org/abs/2212.00616), [Universal prompt retrieval](https://arxiv.org/abs/2303.08518)\n- Longer Context: [Structured prompting](https://arxiv.org/abs/2212.06713), [Length-Extrapolatable Transformers](https://arxiv.org/abs/2212.10554)\n- LLM Accelerator (Faster Inference): [Lossless Acceleration of LLMs](https://arxiv.org/abs/2304.04487)\n- LLM Customization (TBA)\n- Fundamentals: [Understanding In-Context Learning](https://arxiv.org/abs/2212.10559)\n\n## Links\n\n- [microsoft/unilm](https://github.com/microsoft/unilm): Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities\n- [microsoft/torchscale](https://github.com/microsoft/torchscale): Transformers at (any) Scale\n\n## News\n- [Paper Release] April, 2023: [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/abs/2304.04487)\n- [Paper Release] Dec, 2022: [Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers](https://arxiv.org/abs/2212.10559)\n- [Paper & Model & Demo Release] Dec, 2022: [Optimizing Prompts for Text-to-Image Generation](https://aka.ms/promptist)\n- [Paper & Code Release] Dec, 2022: [Structured Prompting: Scaling In-Context Learning to 1,000 Examples](https://arxiv.org/abs/2212.06713)\n- [Paper Release] Nov, 2022: [Extensible Prompts for Language Models](https://arxiv.org/abs/2212.00616)\n\n## Prompt Intelligence\n\nAdvanced technologies facilitating prompting language models.\n\n### Promptist: reinforcement learning for automatic prompt optimization\n\n[Paper] [Optimizing Prompts for Text-to-Image Generation](https://arxiv.org/abs/2212.09611)\n\n> - Language models serve as a prompt interface that optimizes user input into model-preferred prompts.\n\n> - Learn a language model for automatic prompt optimization via reinforcement learning.\n\n![image](https://user-images.githubusercontent.com/1070872/207856962-02f08d92-f2bf-441a-b1c3-efff1a4b6187.png)\n\n\n### Structured Prompting: consume long-sequence prompts in an efficient way\n\n[Paper] [Structured Prompting: Scaling In-Context Learning to 1,000 Examples](https://arxiv.org/abs/2212.06713)\n\n- Example use cases:\n\n> 1) Prepend (many) retrieved (long) documents as context in GPT.\n\n> 2) Scale in-context learning to many demonstration examples.\n\n![image](https://user-images.githubusercontent.com/1070872/207856629-2bb0c933-c27b-4177-9e10-e397622ae79b.png)\n\n\n### X-Prompt: extensible prompts beyond NL for descriptive instructions\n\n[Paper] [Extensible Prompts for Language Models](https://arxiv.org/abs/2212.00616)\n\n> - Extensible interface allowing prompting LLMs beyond natural language for fine-grain specifications\n\n> - Context-guided imaginary word learning for general usability\n\n![Extensible Prompts for Language Models](https://user-images.githubusercontent.com/1070872/207856788-5409d04d-c406-4b29-ae7b-2732e727d4cc.png)\n\n\n## LLMA: LLM Accelerators\n\n### Accelerate LLM Inference with References\n\n[Paper] [Inference with Reference: Lossless Acceleration of Large Language Models](https://arxiv.org/abs/2304.04487)\n\n> - Outputs of LLMs often have significant overlaps with some references (e.g., retrieved documents).\n\n> - LLMA losslessly accelerate the inference of LLMs by copying and verifying text spans from references into the LLM inputs.\n\n> - Applicable to important LLM scenarios such as retrieval-augmented generation and multi-turn conversations.\n\n> - Achieves 2~3 times speed-up without additional models.\n\n![image](https://user-images.githubusercontent.com/6700539/231664563-aec35679-b4ab-4b6b-b6b4-b2b4ea1aab53.png)\n\n\n## Fundamental Understanding of LLMs\n\n### Understanding In-Context Learning\n\n[Paper] [Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers](https://arxiv.org/abs/2212.10559)\n\n> - According to the demonstration examples, GPT produces meta gradients for In-Context Learning (ICL) through forward computation. ICL works by applying these meta gradients to the model through attention.\n\n> - The meta optimization process of ICL shares a dual view with finetuning that explicitly updates the model parameters with back-propagated gradients.\n\n> - We can translate optimization algorithms (such as SGD with Momentum) to their corresponding Transformer architectures.\n\n![image](https://user-images.githubusercontent.com/1070872/208835096-54407f5f-d136-4747-9629-3219988df5d4.png)\n\n## Hiring: [aka.ms/nlpagi](https://aka.ms/nlpagi)\nWe are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and AGI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href=\"mailto:fuwei@microsoft.com\" class=\"x-hidden-focus\">fuwei@microsoft.com</a>.\n\n## License\nThis project is licensed under the license found in the LICENSE file in the root directory of this source tree.\n\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)\n\n### Contact Information\n\nFor help or issues using the pre-trained models, please submit a GitHub issue.\nFor other communications, please contact [Furu Wei](http://gitnlp.org/) (`fuwei@microsoft.com`).\n", "repo_name": "LMOps", "org_name": "microsoft", "org_repo": "microsoft/LMOps", "platform_org_repo": "github+microsoft/LMOps", "link_to_repo": "https://github.com/microsoft/LMOps", "platform": "github", "language": "Python", "stargazers_count": 1633, "watchers_count": 1633}, {"README_text": "# Azure Administrator Workshop\nWorkshop for Azure Administrator\n\n## Table of content\n- [Azure Administrator Workshop](#azure-administrator-workshop)\n  - [Table of content](#table-of-content)\n    - [Starting With Azure](#starting-with-azure)\n    - [Azure Compute Resources](#azure-compute-resources)\n      - [Azure VM](#azure-vm)\n      - [Azure Web App](#azure-web-app)\n      - [The need of Containers](#the-need-of-containers)\n        - [Azure Container Registry](#azure-container-registry)\n        - [Azure Container Instances](#azure-container-instances)\n        - [Container Groups in Azure Container Instances](#container-groups-in-azure-container-instances)\n        - [Azure Kubernetes - AKS](#azure-kubernetes---aks)\n        - [Azure Container Apps](#azure-container-apps)\n      - [Azure Functions](#azure-functions)\n      - [Azure Virtual Environments](#azure-virtual-environments)\n        - [Azure Virtual Desktop](#azure-virtual-desktop)\n        - [DevBox](#devbox)\n        - [Deployment Environments](#deployment-environments)\n    - [Manage and Configure Virtual Network](#manage-and-configure-virtual-network)\n      - [Azure Virtual Network](#azure-virtual-network)\n      - [Network Security Groups](#network-security-groups)\n      - [Azure Load Balancer](#azure-load-balancer)\n      - [Azure Application Gateway](#azure-application-gateway)\n      - [Virtual Network Peering](#virtual-network-peering)\n      - [VPN - Virtual Private Network](#vpn---virtual-private-network)\n      - [Point-to-Site VPN Connection](#point-to-site-vpn-connection)\n      - [Site to Site VPN Connection](#site-to-site-vpn-connection)\n      - [Azure Virtual WAN](#azure-virtual-wan)\n      - [Azure Express Route](#azure-express-route)\n      - [Network Watcher Service](#network-watcher-service)\n      - [User Defined Routes](#user-defined-routes)\n      - [Azure Firewall](#azure-firewall)\n        - [Azure Firewall Standard](#azure-firewall-standard)\n        - [Azure Firewall Premium](#azure-firewall-premium)\n        - [Azure Firewall Basic](#azure-firewall-basic)\n      - [DNS - Domain Name System](#dns---domain-name-system)\n        - [Local DNS](#local-dns)\n        - [Azure Private DNS](#azure-private-dns)\n        - [Azure Public DNS](#azure-public-dns)\n      - [Be Private In Azure Environment](#be-private-in-azure-environment)\n        - [Private endpoint](#private-endpoint)\n        - [Vnet Integration](#vnet-integration)\n    - [Manage Azure Storage](#manage-azure-storage)\n      - [Azure Storage Accounts](#azure-storage-accounts)\n    - [Manage Azure Identities and Governance](#manage-azure-identities-and-governance)\n      - [Azure Subscriptions and Management levels of hierarchy](#azure-subscriptions-and-management-levels-of-hierarchy)\n      - [Azure Active Directory](#azure-active-directory)\n        - [Role Base Access Control](#role-base-access-control)\n        - [Dynamic Groups](#dynamic-groups)\n        - [Azure AD Roles](#azure-ad-roles)\n        - [Self-service password reset](#self-service-password-reset)\n        - [Multi-Factor Authentication](#multi-factor-authentication)\n        - [Conditional Access Policies](#conditional-access-policies)\n        - [Administrative Units](#administrative-units)\n      - [Resource Tags](#resource-tags)\n      - [Resouce Locks](#resouce-locks)\n      - [Azure Policies](#azure-policies)\n      - [Costing in Azure](#costing-in-azure)\n      - [Azure Management Groups](#azure-management-groups)\n    - [Monitor and Backup](#monitor-and-backup)\n      - [Azure Monitor](#azure-monitor)\n        - [Log Analytics Workspace](#log-analytics-workspace)\n        - [Application Insights](#application-insights)\n      - [Azure Backup](#azure-backup)\n        - [Azure Backup for virtual machines](#azure-backup-for-virtual-machines)\n    - [Azure Security](#azure-security)\n      - [Defender For Cloud](#defender-for-cloud)\n        - [Defender For Servers](#defender-for-servers)\n        - [Defender for Storage](#defender-for-storage)\n        - [Defender for SQL](#defender-for-sql)\n        - [Defender for Containers](#defender-for-containers)\n        - [Defender for App Service](#defender-for-app-service)\n        - [Defender for Key Vault](#defender-for-key-vault)\n        - [Defender for Resource Manager](#defender-for-resource-manager)\n        - [Defender for DNS](#defender-for-dns)\n        - [Defender for open-source relational databases](#defender-for-open-source-relational-databases)\n        - [Defender for Azure Cosmos DB](#defender-for-azure-cosmos-db)\n        - [Defender Cloud Security Posture Management (CSPM)](#defender-cloud-security-posture-management-cspm)\n        - [Defender for DevOps](#defender-for-devops)\n\n\n\n\n\n\n### Starting With Azure\n<img src=\"./assets/azure_structure.png\" alt=\"azure_structure\" width=\"300\"/>\n\n### Azure Compute Resources\n\n#### Azure VM\n \n<img src=\"./assets/azure_vm.png\" alt=\"azure_vm\" width=\"1000\"/>\n\n- Check Usage + quotas for your Subscription \n- You can Ruquest Quota Increase \n\n<img src=\"./assets/azure_quotas_usage.png\" alt=\"azure_quotas_usage\" width=\"1000\"/>\n\n- Create a Budget alert in your Subscription Blade \n- Temporary Disk - Size varies depending on instance size\n  - Data on the temporary disk is lost during a maintenance event\n  - Data is lost when you redeploy the VM \n- Restart / Stopping the VM \n  - If you restart the VM, the public IP address will remain as it is. Also the data on the temporary disk remains as it is.\n  - If you stop/deallocate the VM, the public IP address will be lost. The data on the temporary disk also gets erased.\n\n- Azure Disk Types\n  - Standard HDD - This is ideal for backup environments and non-critical workloads. Max disk size - 32,767 GiB, Max throughput - 500 MB/s , Max IOPS - 2000\n  - Standard SSD - This is ideal for Web Servers and Dev/Test Environments. Max disk size - 32,767 GiB, Max throughput - 750 MB/s, Max IOPS - 6000\n  - Premium SSD - This is ideal for Production environments. Max disk size - 32,767 GiB, Max throughput - 900 MB/s , Max IOPS - 20,000\n  - Ultra Disk - This is ideal for IO Intensive workloads - SQL, Oracle databases. Max disk sizes - 65,536 GiB, Max throughput - 4000 MB/s, Max IOPS - 160,000\n\n- Server-side Disk Encryption - Encrypted in data center \n  - Your data is automatically encrypted using 256-bit AES Encryption\n    - PMK - Platform Managed Keys - Azure managed the keys\n    - CMK - Customer Managed Keys - Managed by customer - need to store the key in key vault service and create a disk encryption set\n  - This protects the data at rest\n  - This is done for Managed disks - OS and data disks\n- Azure Disk Encryption - helps protect and safeguard your data to meet your oranizational security and compliance commitments. ADE provides volume encryption for the OS and data disks of Azure virtual machines ( VMS ) through the use of feature DM-Crypt of Linux or BitLocker feature of Windows. ADE is integrated with Azure Key Vault to help you control and manage the disk encryption keys and secrets. \n- IOPS and Throughput \n  - IOPS - This setting defines the number of Input/Output operations per second - for dbs there will be a lot of read, write and update statements\n  - Throughput - Amount of data that is being sent to the storage disk at a specified interval - Measured in MB per second \n- Data Disk Snapshot - Attach to new VM \n- Azure Share disks - This allows a managed disk to be attached to multiple vms\n  - Can only be enable for Premium and Ultra disks\n- Un-managed disks - For example Azure Blob Storage ( You can't have both managed and un-managed disks for a VM )\n  - During the vm creation process you can disable managed disks and select your storage account  \n  \n- Custom Script Extension\n  - This tool can be used on Azure Virtual Machines to download and execute scripts\n  - This is ideal when you want to deploy any custom configration of any software installation on a virtual machine \n  - The scripts can be located in an Azure storage account or even in GitHub \n  - A time duration of 90 minutes is allowed for the script to run. Any longer and the result will be a failed extension provision\n  - It's ideal not to place reboots inside the script, because the extension will not continue after the reboot. Hence if you have other commands that need to run via the extension after the reboot, they won't run\n  - If your script does need a reboot, then maybe you can look at other tools such as Desired state configuration, Ansible or Chef or Puppet.\n- Linux VMs - Cloud init \n  - During the linux vm creation process we can deploy an init script to install packages \n- Boot Diagnostic - Use this feature to troubleshoot failures for custom or platform images ( stores the data in azure storage accounts )\n- Serial Console - Ability to log to vm console from azure portal\n- Run command - Run scripts in your Windows VM by using managed Run Commands\n\n\n- Azure Bastion \n  - Fully managed PaaS Service\n  - Provides RDP/SSH connectivity to virtual machines from the Azure Portal via TLS\n  - Connection via the Internet on port 443\n  - The virtual machines deployed in seperate subnet - AzureBastionSubnet\n\n- Availability Sets \n  - If you have two or more instances deployed in the same Availability Set, you will get an SLA of 99.99% for Virtual Machine Connectivity to at leaset one instance\n  - Fault Domains - are used to define the group of virtual machines that share a common source and network switch. You can have up to 3 fault domains.\n  - Update Domains -  are used to group virtual machines and physical hardware that can be rebooted at the same time. You can have up to 20 update domains.\n\n<img src=\"./assets/AvailabilitySets.png\" alt=\"availability_sets\" width=\"1000\"/>\n\n- Availabity Zones \n  - Availability zones are unique physical locations that are equipped with independent power, cooling and networking, There are normally three availability zones in a region\n  - If you have two or more instances deployed in two or more Availability Zone, you will get an SLA of 99.99% for Virtual Machine Connectivity to at least one instance\n  - Each Availability zone is a unique physical location in an Azure region\n  - Each zone comprises of one or more data centers that has independent power, cooling, and networking\n  - Hence the physical separation of the Availability Zones helps protect applications against data center failures\n\n<img src=\"./assets/availability_zones.png\" alt=\"availability_zones\" width=\"1000\"/>\n\n- Azure Virtual Machine scale sets\n  - You define rules\n  - The rule is based on a condition\n  - Scale out - if the CPU percentage > 80% then add one machine\n  - Scale In - if the CPU percentage < 70% then remove one machine \n\n<img src=\"./assets/VirtualMachineScaleSet.png\" alt=\"scale_set\" width=\"1000\"/>\n\n- Images\n  - Custom image with Application installed\n  - This is a copy of the full VM which includes the data disks or just the OS disk \n  - You can create an image and place as part of an Azure compute gallery\n  - You can share the Azure compute gallery across your organization so that other users can create VM's based on the images stored in the gallery \n  - You can create 2 image types:\n    - Specialized VM Images - Here information about specific users and machine information is retained, New VM's created out of the image will have the same computer name and admin user information\n    - Generalized VM Images - Here information about specific users and machine information is removed, Here you need to perform sysprep \n\n- Resize a VM\n  -  Open the Azure Portal\n  -  Open the page for the virtual machine \n  -  In the left menu, select Size \n  -  Pick a new size from the list of available sizes and then select Resize\n\n- Proximity Placement groups\n  - When you create multiple virtual machines or virtual machines that are part of a virtual machine scale set, these machines could be located in different data centers\n  - Sometimes an application/system that uses multiple vms, want the vms to be located closer together to get least latency when it comes to communication between the virtual machines\n  - By placing the vms as part of proximity group, the vms will be physically located close to each other\n  - When using proximity placement groups, ensure the virtual machine have accelerated networking enabled. This also helps to improve network performance\n  - When deploying vms from different families or SKU's, try to deploy them as part of a single template. This will increase the probability of ensuring all vms are deployed successfully\n  - A proximity placement group is assigned to a data center when the first resource (VM) is being deployed and released once the resource is being deleted or stopped\n\n#### Azure Web App \nCreate and deploy mission-critical web applications that scale with your business  \n- Platform as a service\n  - You don't have to maintain the underlying compute infrastructure \n  - It has features such as Autoscaling and security \n  - It has DevOps cabalities which includes continuous deployment\n  - We can add our custom domain \n  - We can add SSL to our web app \n  - We can use Azuer Web App Backups \n    - Stored in Azure storage account.\n    - To use Backup and Restore feature, the App Service Plan needs to be in the Standard, Premium or Isolated tier.\n    - Backup of the app + database can be up to maximum of 10GB \n  \n\n<img src=\"./assets/basic-webapp-v2.png\" alt=\"basic_web_app\" width=\"1000\"/>\n\n  - Azure WebApp - Vnet Integration\n    - Need App Service Plan or higher\n    - Allows the App service to access resources within the VNET\n    - It does not allow private inbound access to your Web App from the virtual network \n\n#### The need of Containers\n\n##### Azure Container Registry \nBuild, store, secure, scan, replicate, and manage container images and artifacts with a fully managed, geo-replicated instance of OCI distribution. Connect across environments, including Azure Kubernetes Service and Azure Red Hat OpenShift, and across Azure services like App Service, Machine Learning, and Batch.\n\n##### Azure Container Instances\nDevelop apps fast without managing virtual machines or having to learn new tools\u2014it's just your application, in a container, running in the cloud.\n- Run containers without managing servers\n- Increase agility with containers on demand - Deploy containers to the cloud with unprecedented simplicity and speed\u2014with a single command. Use ACI to provision additional compute for demanding workloads whenever you need. For example, with the Virtual Kubelet, use ACI to elastically burst from your Azure Kubernetes Service (AKS) cluster when traffic comes in spikes.\n- Secure applications with hypervisor isolation - Gain the security of virtual machines for your container workloads, while preserving the efficiency of lightweight containers. ACI provides hypervisor isolation for each container group to ensure containers run in isolation without sharing a kernel.\n\n##### Container Groups in Azure Container Instances\nA container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a pod in Kubernetes.\n\nThe following diagram shows an example of a container group that includes multiple containers:\n\n<img src=\"./assets/container-groups-example.png\" alt=\"container_group\" width=\"1000\"/>\n\nThis example container group:  \n- Is scheduled on a single host machine.\n- Is assigned a DNS name label.\n- Exposes a single public IP address, with one exposed port.\n- Consists of two containers. One container listens on port 80, while the other listens on port 5000.\n- Includes two Azure file shares as volume mounts, and each container mounts one of the shares locally.\n\n**NOTE**  \nMulti-container groups currently support only Linux containers. For Windows containers, Azure Container Instances only supports deployment of a single container instance. While we are working to bring all features to Windows containers, you can find current platform differences in the service Overview.\n\n##### Azure Kubernetes - AKS\nManaging containers at scale, Kubernetes is used to orchestate your containers for hosting your applications\nAzure Kubernetes Service (AKS) offers the quickest way to start developing and deploying cloud-native apps in Azure, datacenters, or at the edge with built-in code-to-cloud pipelines and guardrails. Get unified management and governance for on-premises, edge, and multicloud Kubernetes clusters. Interoperate with Azure security, identity, cost management, and migration services.\n\n- Automated management and scalability of Kubernetes clusters for enterprise-grade container orchestration\n- End-to-end developer productivity with debugging, CI/CD, logging, and automated node maintenance\n- Advanced identity and access management to monitor and maintain container security for governance at scale\n- Support for Linux, Windows Server, and IoT resources with AKS deployment on the infrastructure of your choice using Azure Arc\n\n\n\n<img src=\"./assets/baseline-architecture.svg\" alt=\"azure_kubernetes_service\" width=\"1000\"/>\n\n- Managed Kubernetes handles the complexity for you \n  - Automated upgrades, patches\n  - High reliability, availability \n  - Easy, secure cluster scalling \n  - Self-healing \n  - API server monitoring \n  - Control Plane - At not charge\n\n- Multi-Layer Security \n  - Enforce compliance rules with Azure Policy \n  - Identity and access control using Azure Active Directory \n  - Encrypt using your own keys, stored in Azure Key Vault \n  - Gain unmatched security management with Azure Defender for Containers \n  - Interact securely with Kubernetes API server using Azure Private Link or Vnet Integration \n\n##### Azure Container Apps\nDeploy containerized apps without managing complex infrastructure. Write code using your preferred programming language or framework, and build microservices with full support for Distributed Application Runtime (Dapr). Scale dynamically based on HTTP traffic or events powered by Kubernetes Event-Driven Autoscaling (KEDA).\n- Support for a variety of application types, including HTTP APIs, microservices, event processing, and background tasks\n- Flexibility to write code using your language, framework, or SDK of choice\n- Robust autoscaling capabilities based on HTTP traffic or event triggers\n- Simple configurations to perform modern application lifecycle tasks\n\nMicroservices with container apps:  \n\n<img src=\"./assets/microservices-with-container-apps-runtime-diagram.png\" alt=\"container_apps\" width=\"1000\"/>\n\n\n#### Azure Functions\nAzure Functions is a serverless solution that allows you to write less code, maintain less infrastructure, and save on costs. Instead of worrying about deploying and maintaining servers, the cloud infrastructure provides all the up-to-date resources needed to keep your applications running.\n\nYou focus on the code that matters most to you, in the most productive language for you, and Azure Functions handles the rest.\n\n- Scenarios \n  - Build a web API \n  - Process file uploads \n  - Build a serverless workflow\n  - Respond to database changes\n  - Run schedule tasks\n  - Create reliable message queue systems\n  - Analyze IoT data streams\n  - Process data in real time \n\n**NOTE**  \n[Serverless Functions reference architectures](https://learn.microsoft.com/en-us/azure/architecture/serverless-quest/reference-architectures)\n\n#### Azure Virtual Environments \n\n##### Azure Virtual Desktop\nAzure Virtual Desktop is a desktop and app virtualization service that runs on the cloud.\n\nHere's what you can do when you run Azure Virtual Desktop on Azure:\n\n- Set up a multi-session Windows 11 or Windows 10 deployment that delivers a full Windows experience with scalability\nPresent Microsoft 365 Apps for enterprise and optimize it to run in multi-user virtual scenarios\n- Bring your existing Remote Desktop Services (RDS) and Windows Server desktops and apps to any computer\nVirtualize both desktops and apps\n- Manage desktops and apps from different Windows and Windows Server operating systems with a unified management experience\n\n\nKey capabilities:\nWith Azure Virtual Desktop, you can set up a scalable and flexible environment:\n\n- Create a full desktop virtualization environment in your Azure subscription without running any gateway servers.\n- Publish host pools as you need to accommodate your diverse workloads.\n- Bring your own image for production workloads or test from the Azure Gallery.\n- Reduce costs with pooled, multi-session resources. With the new Windows 11 and Windows 10 Enterprise multi-session capability, exclusive to Azure Virtual Desktop and Remote Desktop Session Host (RDSH) role on Windows Server, you can greatly reduce the number of virtual machines and operating system overhead while still providing the same resources to your users.\n- Provide individual ownership through personal (persistent) desktops.\n- Use autoscale to automatically increase or decrease capacity based on time of day, specific days of the week, or as demand changes, helping to manage cost.\n\nYou can deploy and manage virtual desktops:\n\n- Use the Azure portal, Azure CLI, PowerShell and REST API to configure the host pools, create app groups, assign users, and publish resources.\n- Publish full desktop or individual remote apps from a single host pool, create individual app groups for different sets of users, or even assign users to multiple app groups to reduce the number of images.\n- As you manage your environment, use built-in delegated access to assign roles and collect diagnostics to understand various configuration or user errors.\n- Use the new Diagnostics service to troubleshoot errors.\n- Only manage the image and virtual machines, not the infrastructure. You don't need to personally manage the Remote Desktop roles like you do with Remote Desktop Services, just the virtual machines in your Azure subscription.\n\nYou can also assign and connect users to your virtual desktops:\n\n- Once assigned, users can launch any Azure Virtual Desktop client to connect to their published Windows desktops and applications. Connect from any device through either a native application on your device or the Azure Virtual Desktop HTML5 web client.\n- Securely establish users through reverse connections to the service, so you don't need to open any inbound ports.\n\n\nAzure Virtual Desktop for Enterprise: \n\n<img src=\"./assets/windows-virtual-desktop.png\" alt=\"windows-virtual-desktop\" width=\"1000\"/>\n\n\n##### DevBox\nMicrosoft Dev Box gives you self-service access to high-performance, preconfigured, and ready-to-code cloud-based workstations called dev boxes. You can set up dev boxes with the tools, source code, and pre-built binaries specific to your project, so you can immediately start work. Whether you\u2019re a developer, tester, or QA professional, you can use dev boxes in your day-to-day workflows.\n\nThe Dev Box service was designed with three distinct personas in mind: dev infra admins, project admins, and dev box users.\n\nDev infra admins are responsible for providing developer infrastructure and tools to the dev teams. Dev infra admins create and manage dev centers, which represent the units of organization within an enterprise. Any user with sufficient permissions on the subscription or resource group can create a dev center. Dev infra admins create projects and define the images that are used to create dev boxes. Dev box image definitions can use any developer IDE, SDK, or internal tool that runs on Windows.\n\nProject admins are experienced developers with in depth knowledge of their projects who can assist with day-to-day administrative tasks. Project admins create and manage dev box pools, enabling developers in different regions to self-serve dev boxes.\n\nDev box users are members of a development team. They can self-serve one or more dev boxes on demand from a set of dev box pools that have been enabled for the project. Dev box users can work on multiple projects or tasks by creating multiple dev boxes.\n\nMicrosoft Dev Box bridges the gap between development teams and IT, bringing control of project resources closer to the development team.\n\n\nKey concepts:\n\n**Dev center**\nA dev center is a collection of projects that require similar settings. Dev centers enable dev infrastructure managers to manage the images and SKUs available to the projects using dev box definitions and configure the networks the development teams consume using network connections.\n\n**Projects**\nA project is the point of access for the development team members. When you associate a project with a dev center, all the settings at the dev center level will be applied to the project automatically. Each project can be associated with only one dev center. Dev managers can configure the dev boxes available for the project by specifying the dev box definitions appropriate for their workloads.\n\n**Dev box definition**\nA dev box definition specifies a source image and size, including compute size and storage size. You can use a source image from the marketplace, or a custom image from your own Azure Compute Gallery. You can use dev box definitions across multiple projects in a dev center.\n\n**Network connection**\nIT administrators and dev infrastructure managers configure the network used for dev box creation in accordance with their organizational policies. Network connections store configuration information like Active Directory join type and virtual network that dev boxes use to connect to network resources.\n\nWhen creating a network connection, you must choose whether to use a native Azure Active Directory (Azure AD) join or a hybrid Azure AD join. If your dev boxes need to connect exclusively to cloud-based resources, use a native Azure AD join. Use a hybrid Azure AD join if your dev boxes need to connect to on-premises resources and cloud-based resources. To learn more about Azure AD and hybrid Azure AD joined devices, Plan your Azure Active Directory device deployment. The virtual network specified in a network connection also determines the region for the dev box. You can create multiple network connections based on the regions where you support developers and use them when creating different dev box pools to ensure dev box users create a dev box in a region close to them. Using a region close to the dev box user provides the best experience.\n\n**Dev box pool**\nA dev box pool is a collection of dev boxes that you manage together and to which you apply similar settings. You can create multiple dev box pools to support the needs of hybrid teams working in different regions or on different workloads.\n\n**Dev box**\nA dev box is a preconfigured ready-to-code workstation that you create through the self-service developer portal. The new dev box has all the tools, binaries, and configuration required for a dev box user to be productive immediately. You can create and manage multiple dev boxes to work on multiple work streams. As a dev box user, you have control over your own dev boxes - you can create more as you need them and delete them when you have finished using them.\n\nKey capabilities:\n\nFor development teams\n\n- Get started quickly\n  - Create multiple dev boxes from a predefined pool whenever you need them and delete them when you're done.\n  - Use separate dev boxes for separate projects or tasks.\n- Use multiple dev boxes to isolate and parallelize work\n  - Tasks that take considerable time, like a full rebuild before submitting a PR can run in the background while you use a different dev box to start the next task.\n  - Safely test changes in your code, or make significant edits without affecting your primary workspace.\n- Access from anywhere\n  - Dev boxes can be accessed from any device and from any OS. Use a web browser while on the road or remote desktop from your Windows, Mac, or Linux desktop.\n\nFor dev managers\n\n- Use dev box pools to separate workloads\n  - Create dev box pools, add appropriate dev box definitions, and assign access for only dev box users working on those specific projects.\n  - Each pool brings together a SKU, an image, and a network configuration that automatically joins the dev box to your native Azure Active Directory (Azure AD) or Active Directory domain. This combination gives teams flexibility to define specific development environments for any scenario.\n- Control costs\n  - Dev Box brings cost control within the reach of project admins.\n- Team scenarios\n  - Create dev boxes for various roles on a team. Standard dev boxes might be configured with admin rights, giving full-time developers greater control, while more restricted permissions are applied for contractors.\n\nFor dev infrastructure admins\n\n- Configure dev centers\n  - Create dev centers and define the SKUs and images that the development teams use to self-serve dev boxes.\n- Configure the network connection\n  - Define the network configuration that the development teams consume. The network connection defines the region where the dev box is created.\n- Manage projects\n  - Grant access to the development team so that they can self-serve dev boxes.\n\n\nFor IT admins\n\n- Manage Dev Boxes like any other device\n  - Dev boxes are automatically enrolled in Intune. Use Microsoft Endpoint Manager Portal to manage the dev boxes just like any other device on your network.\n  - Keep all Windows devices up to date by using Intune\u2019s expedited quality updates to deploy zero-day patches across your organization.\n  - If a dev box is compromised, you can isolate it while helping the dev box user get back up and running on a new dev box.\n- Provide secure access in a secure environment\n  - Access controls in Azure AD enable you to organize access by project or user type. You can automatically:\n    - Join dev boxes natively to an Azure AD or Active Directory domain.\n    - Set conditional access policies that require users to connect via a compliant device.\n    - Require multi-factor authentication (MFA) sign-in.\n    - Configure risk-based sign-in policies for Dev Boxes that access sensitive source code and customer data.\n\n##### Deployment Environments\nAzure Deployment Environments empowers development teams to quickly and easily spin up app infrastructure with project-based templates that establish consistency and best practices while maximizing security. This on-demand access to secure environments accelerates the stages of the software development lifecycle in a compliant and cost-efficient way.\n\nA deployment environment is a preconfigured collection of Azure resources deployed in predefined subscriptions. Azure governance is applied to those subscriptions based on the type of environment, such as sandbox, testing, staging, or production.\n\n\n<img src=\"./assets/azure-deployment-environments.png\" alt=\"azure-deployment-environments\" width=\"1000\"/>\n\n\n**Usage scenarios**\n\nAzure Deployment Environments enables usage scenarios for both DevOps teams and developers. Common scenarios include:\n\n- Quickly create on-demand Azure environments by using reusable IaC templates.\n- Create sandbox environments to test your code.\n- Preconfigure various types of environments and seamlessly integrate with your continuous integration and continuous delivery (CI/CD) pipeline.\n- Create preconfigured environments for trainings and demos.\n\n**Developer scenarios**\n- Deploy a preconfigured environment for any stage of the development cycle.\n- Spin up a sandbox environment to explore Azure.\n- Create platform as a service (PaaS) and infrastructure as a service (IaaS) environments quickly and easily by following a few simple steps.\n- Deploy environments right from where they work.\n\n**Dev infra scenarios**\nAzure Deployment Environments helps your dev infra admin apply the right set of policies and settings on various types of environments, control the resource configuration that developers can create, and centrally track environments across projects by doing the following tasks:\n\n- Provide a project-based, curated set of reusable IaC templates.\n- Define specific Azure deployment configurations per project and per environment type.\n- Provide a self-service experience without giving control over subscriptions.\n- Track costs and ensure compliance with enterprise governance policies.\n\n--- \n  \n### Manage and Configure Virtual Network\n\n#### Azure Virtual Network \nAzure Virtual Network (VNet) is the fundamental building block for your private network in Azure. VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. VNet is similar to a traditional network that you'd operate in your own data center, but brings with it additional benefits of Azure's infrastructure such as scale, availability, and isolation.\n\n<img src=\"./assets/vnet_with_subnets.png\" alt=\"vnet_with_subnets\" width=\"1000\"/>\n\n- Each VM in azure gets a virtual network interface with \n  - Private IP address\n  - Public IP address ( optionally )\n\n- Subnets\n  - Logically group your network into sub networks \n\n- Static IP Address \n  - In case of Dynamic IP when your stop the Azure VM your public IP gonna be deallocated \n  - WebApp on VM use case with custom domain:\n    \n    <img src=\"./assets/static_ip.png\" alt=\"static_ip\" width=\"600\"/>\n  \n- Secondary network interface\n  - Scenario - a firewall in hub subscription - one interface for internet communication and second one for internal communication\n  - Security purposes\n\n\n#### Network Security Groups \nYou can use an Azure network security group to filter network traffic between Azure resources in an Azure virtual network. A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources. For each rule, you can specify source and destination, port, and protocol.\n\n- Basic type of security level\n- Inbout rules / Outbound rules\n  1. Priority \n  2. Port No\n  3. Protocol\n  4. Source and Destination \n\n<img src=\"./assets/nsg.png\" alt=\"nsg\" width=\"800\"/>\n\n#### Azure Load Balancer \nAzure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. Load balancer distributes inbound flows that arrive at the load balancer's front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines or instances in a Virtual Machine Scale Set.\n\n<img src=\"./assets/load-balancer.png\" alt=\"load-balancer\" width=\"800\"/>\n\n- Basic Load Balancer\n  - Free\n  - The machines in the backend pool need to be a part of an availability set or scale set \n  - Health probes - TCP, HTTP\n  - No support for availability zones\n  - No SLA\n  - Good for test purposes\n- Standard Load Balancer\n  - Charge per hour \n  - Here the machines can also be independent machines that are part of a virtual network \n  - Health probes - TCP, HTTP, HTTPS\n  - Support for Availability Zones \n  - SLA of 99.99% \n  - Good for production purposes\n\n\n- We can use a NAT rules on the load balancer \n- The load balancer will create an affinity between the Load Balancer and the client for a session \n  - Advantage - Can help in better performance for sessions \n  - Disadvantage - If too many sessions are persisted on a server \n\n<img src=\"./assets/load_balancer.png\" alt=\"load-balancer\" width=\"800\"/>\n\n#### Azure Application Gateway \nAzure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Traditional load balancers operate at the transport layer (OSI layer 4 - TCP and UDP) and route traffic based on source IP address and port, to a destination IP address and port.\n\nApplication Gateway can make routing decisions based on additional attributes of an HTTP request, for example URI path or host headers. For example, you can route traffic based on the incoming URL. So if /images is in the incoming URL, you can route traffic to a specific set of servers (known as a pool) configured for images. If /video is in the URL, that traffic is routed to another pool that's optimized for videos.\nThis type of routing is known as application layer (OSI layer 7) load balancing. Azure Application Gateway can do URL-based routing and more.\n\n<img src=\"./assets/app_gateway_lb.png\" alt=\"app_gateway\" width=\"800\"/>\n\n- Features \n  - Secure Sockets Layer (SSL/TLS) termination\n  - Autoscaling - allows the Application Gateway to scale up or down based on traffic load patterns \n  - Static VIP\n  - Web Application Firewall\n  - Ingress Controller for AKS\n  - URL-based routing \n  - Multiple-site hosting\n  - Redirection\n  - Session affinity \n  - Websocket and HTTP/2 traffic\n  - Connection draining \n  - Custom error pages\n  - Rewrite HTTP headers and URL\n\n<img src=\"./assets/app_gateway_features.png\" alt=\"app_gateway\" width=\"800\"/>\n\n- How it works? \n\n1. Before a client sends a request to an application gateway, it resolves the domain name of the application gateway by using a Domain Name System (DNS) server. Azure controls the DNS entry because all application gateways are in the azure.com domain.\n\n2. The Azure DNS returns the IP address to the client, which is the frontend IP address of the application gateway.\n\n3. The application gateway accepts incoming traffic on one or more listeners. A listener is a logical entity that checks for connection requests. It's configured with a frontend IP address, protocol, and port number for connections from clients to the application gateway.\n\n4. If a web application firewall (WAF) is in use, the application gateway checks the request headers and the body, if present, against WAF rules. This action determines if the request is valid request or a security threat. If the request is valid, it's routed to the backend. If the request isn't valid and WAF is in Prevention mode, it's blocked as a security threat. If it's in Detection mode, the request is evaluated and logged, but still forwarded to the backend server.\n\n<img src=\"./assets/app_gateway_howitworks.png\" alt=\"app_gateway\" width=\"800\"/>\n\n#### Virtual Network Peering \nVirtual network peering enables you to seamlessly connect two or more Virtual Networks in Azure. The virtual networks appear as one for connectivity purposes. The traffic between virtual machines in peered virtual networks uses the Microsoft backbone infrastructure. Like traffic between virtual machines in the same network, traffic is routed through Microsoft's private network only.\n\nAzure supports the following types of peering:\n- Virtual network peering: Connecting virtual networks within the same Azure region.\n- Global virtual network peering: Connecting virtual networks across Azure regions.\n\nThe benefits of using virtual network peering, whether local or global, include:\n- A low-latency, high-bandwidth connection between resources in different virtual networks.\n- The ability for resources in one virtual network to communicate with resources in a different virtual network.\n- The ability to transfer data between virtual networks across Azure subscriptions, Azure Active Directory tenants, deployment models, and Azure regions.\n- The ability to peer virtual networks created through the Azure Resource Manager.\n- The ability to peer a virtual network created through Resource Manager to one created through the classic deployment model. To learn more about Azure deployment models, see Understand Azure deployment models.\n- No downtime to resources in either virtual network when creating the peering, or after the peering is created.\n\nGateways and on-premises connectivity: \n\nEach virtual network, including a peered virtual network, can have its own gateway. A virtual network can use its gateway to connect to an on-premises network. You can also configure virtual network-to-virtual network connections by using gateways, even for peered virtual networks.\n\nWhen you configure both options for virtual network interconnectivity, the traffic between the virtual networks flows through the peering configuration. The traffic uses the Azure backbone.\n\nYou can also configure the gateway in the peered virtual network as a transit point to an on-premises network. In this case, the virtual network that is using a remote gateway can't have its own gateway. A virtual network could have only one gateway, the gateway should be either local or remote gateway in the peered virtual network as shown in the following diagram:\n\n<img src=\"./assets/vnet_peering_on_prem.png\" alt=\"vnet_peering\" width=\"800\"/>\n\n- Virtual Network Peering is used to connect two Azure virtual networks together via the backbone network.\n- Azure supports connecting two virtual networks located in the same region or networks located across regions.\n- Once you enable virtual network peering between two virtual networks, the virtual machines can then communicate via their private IP addresses across the peering connection.\n- You can also peer virtual networks that are located across different subscriptions.\n- The virtual networks can't have overlapping CIDR blocks.\n\n\n#### VPN - Virtual Private Network \nAzure Virtual Network (VNet) is the fundamental building block for your private network in Azure. VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. VNet is similar to a traditional network that you'd operate in your own data center, but brings with it additional benefits of Azure's infrastructure such as scale, availability, and isolation.\n\n- Your Internet Services provider will know all of the requests that are made from your machine onto the internet \n- Sometimes privacy can always be a concern \n- VPN is used to create a private network \n- Here your public IP address is not placed in the requests that are made onto the Internet \n- Also VPN connections are encrypted so that the data transfer is more secure\n\n<img src=\"./assets/azure_vpn.png\" alt=\"azure_vpn\" width=\"700\"/>\n\n#### Point-to-Site VPN Connection\nA Point-to-Site (P2S) VPN gateway connection lets you create a secure connection to your virtual network from an individual client computer. A P2S connection is established by starting it from the client computer. This solution is useful for telecommuters who want to connect to Azure VNets from a remote location, such as from home or a conference. P2S VPN is also a useful solution to use instead of S2S VPN when you have only a few clients that need to connect to a VNet. This article applies to the Resource Manager deployment model.\n\n<img src=\"./assets/point_to_site_vpn.png\" alt=\"point_to_site_vpn\" width=\"700\"/>\n\n- The gateway subnet is used to host gateway VM's and services\n- The VM's in the gateway subnet are configured with the required VPN gateway settings \n- No other VM's must be deployed to the gateway subnet\n- The gateway subnet can be configured as /29, but Microsoft recommends /27, /26 address ranges \n- We can have only 1 gatewaySubnet for each vnet \n- We can establish connection via certificates\n- Generate certificates for point 2 site using powershell - [click here](https://learn.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-certificates-point-to-site)\n\n\n#### Site to Site VPN Connection\nA Site-to-Site VPN gateway connection is used to connect your on-premises network to an Azure virtual network over an IPsec/IKE (IKEv1 or IKEv2) VPN tunnel. This type of connection requires a VPN device located on-premises that has an externally facing public IP address assigned to it.\n\n<img src=\"./assets/site_to_site_vpn.png\" alt=\"site_to_site\" width=\"700\"/>\n\n- Create a virtual network\n- Create a VPN gateway\n- Create a local network gateway\n- Create a VPN Connection\n- Verify the connection\n- Connect to a virtual machine \n\n<img src=\"./assets/site_to_site_vpn_hub_spoke.png\" alt=\"site_to_site_vpn\" width=\"1000\"/>\n\n- Tutorial: Create a site-to-site VPN connection in the Azure portal - [click here](https://learn.microsoft.com/en-us/azure/vpn-gateway/tutorial-site-to-site-portal)\n\n- On the on-premise side, you need to have a VPN device that can route traffic via the Internet onto the VPN gateway in Azure. The VPN device can be a hardware device like a Cisco router or a software device ( e.g Windows Server 2016 running Routing and Remote services). The VPN device needs to have a publically routable IP address.\n- The subnets in your on-premise network must not overlap with the subnets in your Azure virtual network\n- The Site-to-Site VPN connection uses an IPSec tunnel to encrypt the traffic\n- The VPN gateway resource you create in Azure is used to route encrypted traffic between your on-premise data center and your Azure virtual network\n- There are different SKU's for the Azure VPN gateway service. Each SKU has a different pricing and attributes associated with it - Reference - [click here](https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-about-vpn-gateway-settings)\n\n\n#### Azure Virtual WAN \nAzure Virtual WAN is a networking service that brings many networking, security, and routing functionalities together to provide a single operational interface. Some of the main features include:\n- Branch connectivity (via connectivity automation from Virtual WAN Partner devices such as SD-WAN or VPN CPE).\n- Site-to-site VPN connectivity.\n- Remote user VPN connectivity (point-to-site).\n- Private connectivity (ExpressRoute).\n- Intra-cloud connectivity (transitive connectivity for virtual networks).\n- VPN ExpressRoute inter-connectivity.\n- Routing, Azure Firewall, and encryption for private connectivity.\n\n<img src=\"./assets/virtual_wan.png\" alt=\"virtual_wan\" width=\"700\"/>\n\n\n#### Azure Express Route \nExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection with the help of a connectivity provider. With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365.\n\nConnectivity can be from an any-to-any (IP VPN) network, a point-to-point Ethernet network, or a virtual cross-connection through a connectivity provider at a colocation facility. ExpressRoute connections don't go over the public Internet. This allows ExpressRoute connections to offer more reliability, faster speeds, consistent latencies, and higher security than typical connections over the Internet. For information on how to connect your network to Microsoft using ExpressRoute, see ExpressRoute connectivity models.\n\n<img src=\"./assets/express_route.png\" alt=\"express_route\" width=\"700\"/>\n\nKey Benefits:  \n- Layer 3 connectivity between your on-premises network and the Microsoft Cloud through a connectivity provider. Connectivity can be from an any-to-any (IPVPN) network, a point-to-point Ethernet connection, or through a virtual cross-connection via an Ethernet exchange.\n- Connectivity to Microsoft cloud services across all regions in the geopolitical region.\n- Global connectivity to Microsoft services across all regions with the ExpressRoute premium add-on.\n- Dynamic routing between your network and Microsoft via BGP.\n- Built-in redundancy in every peering location for higher reliability.\n- Connection uptime SLA.\n- QoS support for Skype for Business.\n\n\n#### Network Watcher Service \n- **Connection Monitor** - Check the network connectivity between machines. These can be in Azure or on your on-premises environments\n- **Next Hop** - Here you can see the next route for a packet of data. This helps you understand whether the packet is being routed to the correct destination\n- **IP Flow Verify** - This can be used to check if a packet is allowed or denied to or from a virtual machine. If a packet is being denied by a securiry group, you can see which rule is denying the packet \n- **Connection troubleshoot** - Check the connection from a virtual machine to a virtual machine, fully qualified domain name, URI or IPv4 address\n- **NSG Diagnostic** - Provides detailed information that helps to understand and debug the security configuration of the network \n- **Traffic Analytics** - This helps to log information about the IP traffic that is flowing through an NSG\n- **NSG Flow Logs** - Helps to provide visibility into user and application activity in cloud networks\n\n#### User Defined Routes \nAzure routes traffic between all subnets within a virtual network, by default. You can create your own routes to override Azure's default routing. Custom routes are helpful when, for example, you want to route traffic between subnets through a network virtual appliance (NVA).  \n\n<img src=\"./assets/route_table.png\" alt=\"route_table\" width=\"700\"/>\n\n#### Azure Firewall\nAzure Firewall is a cloud-native and intelligent network firewall security service that provides the best of breed threat protection for your cloud workloads running in Azure. It's a fully stateful, firewall as a service with built-in high availability and unrestricted cloud scalability. It provides both east-west and north-south traffic inspection.\n\nAzure Firewall is offered in three SKUs: Standard, Premium, and Basic.\n\n- Has built-in high availability\n- Can deploy the Azure Firewall Instance across two or more Availability Zones - 99.99% SLA\n- You can filter traffic based on fully-qualified domain names\n- You can also create network filtering rules - Based on source and destination IP address, port and protocol \n- It is a stateful in nature, so it understands what packets of data to allow \n- It has built-in Intelligence - Here you can get alerts or deny traffic from/to malicious IP addresses and domains\n\n##### Azure Firewall Standard \n\n<img src=\"./assets/firewall_standard.png\" alt=\"firwall_standard\" width=\"700\"/>\n\n##### Azure Firewall Premium \n\n<img src=\"./assets/firewall_premium.png\" alt=\"firewall_premium\" width=\"700\"/>\n\n##### Azure Firewall Basic \n\n<img src=\"./assets/firewall_basic.png\" alt=\"firewall_basic\" width=\"700\"/>\n\n#### DNS - Domain Name System\n##### Local DNS \n1. Install Active Directory Domain Services\n2. Promote the server to a domain controller\n3. Specify a root domain name - mtcisrael.com\n4. Create a new server as part of a new subnet\n5. Install Internet Information Services on the server\n6. Use Azure provided DNS names\n7. Now its time to use our DNS Server \n   1. For the network, we need to mention our DNS server\n   2. Restart our servers\n   3. Add a record to the zone \n\n##### Azure Private DNS\nThe Domain Name System, or DNS, is responsible for translating (or resolving) a service name to an IP address. Azure DNS is a hosting service for domains and provides naming resolution using the Microsoft Azure infrastructure. Azure DNS not only supports internet-facing DNS domains, but it also supports private DNS zones.\n\nAzure Private DNS provides a reliable and secure DNS service for your virtual network. Azure Private DNS manages and resolves domain names in the virtual network without the need to configure a custom DNS solution. By using private DNS zones, you can use your own custom domain name instead of the Azure-provided names during deployment. Using a custom domain name helps you tailor your virtual network architecture to best suit your organization's needs. It provides a naming resolution for virtual machines (VMs) within a virtual network and connected virtual networks. Additionally, you can configure zones names with a split-horizon view, which allows a private and a public DNS zone to share the name.\n\nTo resolve the records of a private DNS zone from your virtual network, you must link the virtual network with the zone. Linked virtual networks have full access and can resolve all DNS records published in the private zone. You can also enable autoregistration on a virtual network link. When you enable autoregistration on a virtual network link, the DNS records for the virtual machines in that virtual network are registered in the private zone. When autoregistration gets enabled, Azure DNS will update the zone record whenever a virtual machine gets created, changes its' IP address, or gets deleted.\n\n<img src=\"./assets/private_dns.png\" alt=\"private_dns\" width=\"500\"/>\n\n##### Azure Public DNS\nA DNS zone is used to host the DNS records for a particular domain. To start hosting your domain in Azure DNS, you need to create a DNS zone for that domain name. Each DNS record for your domain is then created inside this DNS zone.\n\nFor example, the domain 'contoso.com' may contain several DNS records, such as 'mail.contoso.com' (for a mail server) and 'www.contoso.com' (for a web site).\n\nWhen creating a DNS zone in Azure DNS:\n- The name of the zone must be unique within the resource group, and the zone must not exist already. Otherwise, the operation fails.\n- The same zone name can be reused in a different resource group or a different Azure subscription.\n- Where multiple zones share the same name, each instance is assigned different name server addresses. Only one set of addresses can be configured with the domain name registrar.\n\n\n#### Be Private In Azure Environment \n\n##### Private endpoint\nA private endpoint is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link. By enabling a private endpoint, you're bringing the service into your virtual network.\n\n<img src=\"./assets/private_endpoint.avif\" alt=\"private_endpoint\" width=\"800\"/>\n\nThe service could be an Azure service such as: \n- Azure Storage\n- Azure Cosmos DB\n- Azure SQL Database\n- You own service, using Private Link Service\n\nPrivate endpoints enable connectivity between the customers from the same:\n\n- Virtual network\n- Regionally peered virtual networks\n- Globally peered virtual networks\n- On-premises environments that use VPN or Express Route\n- Services that are powered by Private Link\n\nProtect Azure services against data exfiltration:\n\n<img src=\"./assets/private_link.avif\" alt=\"private_link\" width=\"800\"/>\n\nGet access from on-premises and peered networks:\n\n<img src=\"./assets/private_link_on_prem.avif\" alt=\"private_link_on_prem\" width=\"800\"/>\n\nSimplify the way you consume services on Azure:\n\n<img src=\"./assets/private_link_simplify.avif\" alt=\"private_link_simplify\" width=\"800\"/>\n\n##### Vnet Integration\nVirtual network integration gives your app access to resources in your virtual network, but it doesn't grant inbound private access to your app from the virtual network. Private site access refers to making an app accessible only from a private network, such as from within an Azure virtual network. Virtual network integration is used only to make outbound calls from your app into your virtual network. Refer to private endpoint for inbound private access.\n\nThe virtual network integration feature:\n- Requires a supported Basic or Standard, Premium, Premium v2, Premium v3, or Elastic Premium App Service pricing tier.\n- Supports TCP and UDP.\n- Works with App Service apps, function apps and Logic apps.\n\nVirtual network integration supports connecting to a virtual network in the same region. Using virtual network integration enables your app to access:\n- Resources in the virtual network you're integrated with.\n- Resources in virtual networks peered to the virtual network your app is integrated with including global peering connections.\n- Resources across Azure ExpressRoute connections.\n- Service endpoint-secured services.\n- Private endpoint-enabled services.\n\nWhen you use virtual network integration, you can use the following Azure networking features:\n- Network security groups (NSGs): You can block outbound traffic with an NSG that's placed on your integration subnet. The inbound rules don't apply because you can't use virtual network integration to provide inbound access to your app.\n- Route tables (UDRs): You can place a route table on the integration subnet to send outbound traffic where you want.\n- NAT gateway: You can use NAT gateway to get a dedicated outbound IP and mitigate SNAT port exhaustion.\n\n<img src=\"./assets/vnet_integration.png\" alt=\"vnet_integration\" width=\"600\"/>\n\n--- \n\n### Manage Azure Storage\n\n#### Azure Storage Accounts\nAn Azure storage account contains all of your Azure Storage data objects, including blobs, file shares, queues, tables, and disks. The storage account provides a unique namespace for your Azure Storage data that's accessible from anywhere in the world over HTTP or HTTPS. Data in your storage account is durable and highly available, secure, and massively scalable.\n\n<img src=\"./assets/storage_account.png\" alt=\"storage_account\" width=\"800\"/>\n\nTypes of storage accounts:  \n\n| Type of storage account | Supported storage services | Redundancy options | Usage  |\n| ----------------------- |--------------------------- | ------------------ | ------ |\n| Standard general-purpose v2 | Blob Storage ( including Data Lake Storage ), Queue Storage, Table Storage, and Azure Files | Locally redundant storage (LRS) / geo-redundant storage (GRS) / read-access geo-redundant storage (RA-GRS) Zone-redundant storage (ZRS) / geo-zone-redundant storage (GZRS) / read-access geo-zone-redundant storage (RA-GZRS) | Standard storage account type for blobs, file shares, queues, and tables. Recommended for most scenarios using Azure Storage. If you want support for network file system (NFS) in Azure Files, use the premium file shares account type. | \n| Premium block blobs | Blob Storage (including Data Lake Storage ) | LRS ZRS | Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transaction rates or that use smaller objects or require consistently low storage latency. Learn more about example workloads. | \n| Premium file shares | Azure Files | LRS | Premium storage account type for page blobs only. |\n\n- Access Tiers \n  - Hot Access Tier - This is used for data that is accessed frequently \n  - Cool Access Tier - This is used for data that is accessed infrequently and stored for at least 30 days \n  - Archive Access Tier - This is used for data that is rarely accessed and stored for at least 180 days \n\n- Lifecycle policies \n  - Lifecycle Management rules \n  - Change the access tier \n  - Delete an object \n\n- Lifecycle Management \n  - Transition - Here you can transition blobs from the cool to the hot access tier to save on storage costs\n  - Blobs - You can transition blobs, blob versions and blob snapshots\n  - Deletion - You can also define rules to delete blobs, blob versions and blob snapshots\n  - Rule filters - You can define filter for blobTypes - blockBlob, appendBlob\n  - Rule actions - You have actions such as tierToCool, tierToArchive and delete\n  - Support - Rules are supported for blob and append blobs in General-Purpose V2 accounts, Premium Blobk Blob and Blob Storage accounts\n  - Region - This feature is available in all regions\n\n- Object Replication\n  - This feature can be used to copy blobs between a source and destination storage account\n  - You can create rules to specify which objects get replicated from the source to the destination\n  - Storage Account support - General Purpose V2 and Premium Blob accounts\n  - Blob versioning should be enabled on both the source and destination storage account \n  - Change feed is enabled on the source storage account \n\n- Azure File Share\n  \n<img src=\"./assets/file_share.png\" alt=\"file_share\" width=\"400\"/>\n\n- Azure File Sync \n  - Use Azure File Sync to centralize your organization's file shares in Azure Files, while keeping the flexibility, performance, and compatibility of an on-premises file server. Azure File Sync transforms Windows Server into a quick cache of your Azure file share. You can use any protocol that's available on Windows Server to access your data locally, including SMB, NFS, and FTPS. You can have as many caches as you need across the world.\n\n<img src=\"./assets/file_sync.png\" alt=\"file_sync\" width=\"400\"/>\n\n- Azure Import/Export Service\n  - Copying Data - This is used for copying large amounts of data to Azure Blob storage and Azure Files\n  - Transfer Data - You can also transfer data from Azure Blob storage to your on-premises environment\n  - Disk Drives - Here you make use of Disk Drives. You can use your own Disk drives or use the ones provided by Microsoft\n  - Jobs - You basically create a job via the Azure Portal. This will be used for transferring data to a storage account \n  - Data Box  \n    - Data transfer - Helps to send terabytes of data in and out of Azure\n    - No Internet - You don't need to use your Internet connection to transfer the data\n    - Scenario - Ideal when you want to transfer data sizes that are larger than 40TB\n    - Device - You order the Data Box device via the Azure Portal \n  - AzCopy Tool - AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account - [Click Here](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) to get started\n\n\n---\n\n### Manage Azure Identities and Governance\n\n#### Azure Subscriptions and Management levels of hierarchy \nMicrosoft provides a hierarchy of organizations, subscriptions, licenses, and user accounts for consistent use of identities and billing across its cloud offerings:\n- Microsoft 365 and Microsoft Office 365\n- Microsoft Azure\n- Microsoft Dynamics 365\n\n<img src=\"./assets/organize_subscriptions.png\" alt=\"organize_subscriptions\" width=\"400\"/>\n\n- Management levels and hierarchy\n  - **Management Groups**  - help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group\n  - **Subscriptions** - logically associate user accounts with the resources that they create. Each subscription has limits or quotas on the amount of resources that it can create and use. Organizations can use subscriptions to manage costs and the resources that are created by users, teams, and projects\n  - **Resource groups** - are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts\n  - **Resources** - are instances of services that you can create, such as virtual machines, storage, and SQL databases \n\n- Payment models \n  - Pay as you go - Pay for what you use - Highest\n  - Enterprise agreement - 3 Years - Substantial discounts available but only for high volumes\n  - Cloud Service Provider (CSP) - You can purchase e.g. licenses for Office 365, Dynamics 365, Enterprise Mobility Suite, Power BI - Discounts or value-added services available\n\n\n#### Azure Active Directory \nAzure Active Directory (Azure AD) is a cloud-based identity and access management service. Azure AD enables your employees access external resources, such as Microsoft 365, the Azure portal, and thousands of other SaaS applications. Azure Active Directory also helps them access internal resources like apps on your corporate intranet, and any cloud apps developed for your own organization\n\n<img src=\"./assets/active_directory.png\" alt=\"active_directory\" width=\"600\"/>\n\nLicenses:\n- Premium P1 - Included with Microsoft 365 - 6$ user/month\n- Premium P2 - Included with Microsoft 365 - 9$ user/month\n- Free - Included with Microsoft 365\n- Office 365 apps - Included with Microsoft 365\n\nTrust between Azure Subscription and Azure AD\n- Azure Tenant - This is a dedicated and trusted instance of Azure AD\n- Azure AD Directory - Each Azure tenant has a dedicated and trusted Azure AD directory, This includes the tenant's users, groups and applications and us used for performing identity and access management onto resources\n\n##### Role Base Access Control \n- **Contributor** - Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries.\t\n- **Owner** - Grants full access to manage all resources, including the ability to assign roles in Azure RBAC.\t\n- **Reader** - View all resources, but does not allow you to make any changes.\n- **User Access Administrator** - Lets you manage user access to Azure resources.\t\n- **Full List** - [click here](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles)\n- **Custom Roles** - We can create a custom role based on existing roles \n\n\n##### Dynamic Groups \nYou can create attribute-based rules to enable dynamic membership for a group in Azure Active Directory (Azure AD), part of Microsoft Entra. Dynamic group membership adds and removes group members automatically using membership rules based on member attributes. This article details the properties and syntax to create dynamic membership rules for users or devices. You can set up a rule for dynamic membership on security groups or Microsoft 365 groups.\n\nWhen the attributes of a user or a device change, the system evaluates all dynamic group rules in a directory to see if the change would trigger any group adds or removes. If a user or device satisfies a rule on a group, they're added as a member of that group. If they no longer satisfy the rule, they're removed. You can't manually add or remove a member of a dynamic group.\n\n- You can create a dynamic group for devices or for users, but you can't create a rule that contains both users and devices.\n- You can't create a device group based on the user attributes of the device owner. Device membership rules can reference only device attributes.\n\n##### Azure AD Roles\nAzure AD role-based access control manages access to Azure AD resources. Create custom roles, assign roles that use administrative units to restrict scope of control, assign application access to groups or users, manage eligibility with Privileged Identity Management (PIM), or delegate permissions to distribute identity management tasks.\n\nThere are about 60 Azure Active Directory (Azure AD) built-in roles, which are roles with a fixed set of role permissions. To supplement the built-in roles, Azure AD also supports custom roles. Use custom roles to select the role permissions that you want. For example, you could create one to manage particular Azure AD resources such as applications or service principals.\n\n<img src=\"./assets/ad_roles2.png\" alt=\"ad_roles\" width=\"400\"/>\n\n- Ability to create users \n- Ability to manage passwords\n- etc..\n\n<img src=\"./assets/ad_roles.png\" alt=\"ad_roles\" width=\"600\"/>\n\n##### Self-service password reset \nThis feature helps users to reset their password without the need of contacting the IT help desk staff \n\n- License - Password reset needs Azure AD Premium P1 or P2 license for users \n- Password writeback - If there is a hybrid environment, the changed passwords can be written back to the on-premises Active Directory \n- Authentication Methods - You can define authentication methods to reset the password\n- Number of methods - Define the number of authentication methods required to reset the password\n- Number of days - Number of days before users need to reconfirm their authentication information\n- Notification - Notify users when password is reset \n\n##### Multi-Factor Authentication\nUse strong multifactor authentication (MFA) in Azure Active Directory (Azure AD) to help protect your organization against breaches due to lost or stolen credentials.\n- Use stronger security than passwords alone \n- Protect your users from credential theft \n- Secure your resource against unathorized access \n- Ensure a seamless user experience \n\n##### Conditional Access Policies \nThe modern security perimeter now extends beyond an organization's network to include user and device identity. Organizations can use identity-driven signals as part of their access control decisions.\nConditional Access brings signals together, to make decisions, and enforce organizational policies. Azure AD Conditional Access is at the heart of the new identity-driven control plane.\n\n<img src=\"./assets/conditional_access.png\" alt=\"conditional_access\" width=\"800\"/>\n     \n       \nCommon signals that Conditional Access can take in to account when making a policy decision include the following signals:\n- User or group membership\n  - Policies can be targeted to specific users and groups giving administrators fine-grained control over access.\n- IP Location information\n  - Organizations can create trusted IP address ranges that can be used when making policy decisions.\n  - Administrators can specify entire countries/regions IP ranges to block or allow traffic from.\n- Device\n  - Users with devices of specific platforms or marked with a specific state can be used when enforcing Conditional Access policies.\n  - Use filters for devices to target policies to specific devices like privileged access workstations.\n- Application\n  - Users attempting to access specific applications can trigger different Conditional Access policies.\n- Real-time and calculated risk detection\n  - Signals integration with Azure AD Identity Protection allows Conditional Access policies to identify risky sign-in behavior. Policies can then force users to change their password, do multi-factor authentication to reduce their risk level, or block access until an administrator takes manual action.\n- Microsoft Defender for Cloud Apps\n  - Enables user application access and sessions to be monitored and controlled in real time, increasing visibility and control over access to and activities done within your cloud environment.\n\n##### Administrative Units\nAdministrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support.\n\nUsers can be members of multiple administrative units. For example, you might add users to administrative units by geography and division; Megan Bowen might be in the \"Seattle\" and \"Marketing\" administrative units.\n\n<img src=\"./assets/admin-unit-overview.png\" alt=\"admin-unit-overview\" width=\"800\"/>\n  \n\nDocumentation - [click here](https://learn.microsoft.com/en-us/azure/active-directory/roles/administrative-units)\n\n#### Resource Tags \nags are metadata elements that you apply to your Azure resources. They're key-value pairs that help you identify resources based on settings that are relevant to your organization. If you want to track the deployment environment for your resources, add a key named Environment. To identify the resources deployed to production, give them a value of Production. Fully formed, the key-value pair becomes, Environment = Production.\n\nYou can apply tags to your Azure resources, resource groups, and subscriptions.\n\n#### Resouce Locks \nLocking resources can help ensure user's dont accidently delete or modify resource.\nThere is 2 types of locks:\n- **CanNotDelete** - authorized users can still read and modify resource, but they can't delete the resource.\n- **ReadOnly** - authorized users can read a resource, but they can't delete or update the resource\n\n\n#### Azure Policies \nAzure Policy helps to enforce organizational standards and to assess compliance at-scale. Through its compliance dashboard, it provides an aggregated view to evaluate the overall state of the environment, with the ability to drill down to the per-resource, per-policy granularity. It also helps to bring your resources to compliance through bulk remediation for existing resources and automatic remediation for new resources.\n\n\nCommon use cases for Azure Policy include implementing governance for resource consistency, regulatory compliance, security, cost, and management. Policy definitions for these common use cases are already available in your Azure environment as built-ins to help you get started.\n\nSpecifically, some useful governance actions you can enforce with Azure Policy include:\n- Ensuring your team deploys Azure resources only to allowed regions\n- Enforcing the consistent application of taxonomic tags\n- Requiring resources to send diagnostic logs to a Log Analytics workspace\n\n#### Costing in Azure\n- Azure has many ways to tackle costs\n- Cost analysis as part of your subscription\n  - Here you can see the current spending \n  - See spending per resource\n  - See your forecasts\n  - See you spending history \n  - See the spending based on tags, resource types etc\n- Azure Advisor \n\n#### Azure Management Groups \nIf your organization has many Azure subscriptions, you may need a way to efficiently manage access, policies, and compliance for those subscriptions. Management groups provide a governance scope above subscriptions. You organize subscriptions into management groups; the governance conditions you apply cascade by inheritance to all associated subscriptions.\n\nManagement groups give you enterprise-grade management at scale no matter what type of subscriptions you might have. However, all subscriptions within a single management group must trust the same Azure Active Directory (Azure AD) tenant.\n\nFor example, you can apply policies to a management group that limits the regions available for virtual machine (VM) creation. This policy would be applied to all nested management groups, subscriptions, and resources, and allow VM creation only in authorized regions.\n\n<img src=\"./assets/management_groups.png\" alt=\"management_groups\" width=\"800\"/>\n\n--- \n\n### Monitor and Backup \n\n#### Azure Monitor\nAzure Monitor helps you maximize the availability and performance of your applications and services. It delivers a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. This information helps you understand how your applications are performing and proactively identify issues that affect them and the resources they depend on.\n\nA few examples of what you can do with Azure Monitor include:\n- Detect and diagnose issues across applications and dependencies with Application Insights.\n- Correlate infrastructure issues with VM insights and Container insights.\n- Drill into your monitoring data with Log Analytics for troubleshooting and deep diagnostics.\n- Support operations at scale with automated actions.\n- Create visualizations with Azure dashboards and workbooks.\n- Collect data from monitored resources by using Azure Monitor Metrics.\n- Investigate change data for routine monitoring or for triaging incidents by using Change Analysis.\n\nOverview:\n\n<img src=\"./assets/azure_monitor.png\" alt=\"azure_monitor\" width=\"800\"/>\n\n\n- Metrics for Azure resources\n  - CPU Usage\n  - Disk Metricts\n  - Network Stats\n  - Alerts\n- Activity Logs \n  - Control Plane activities \n  - When a virtual machine is stopped \n  - When a virtual machine is created \n- Log Analytics Workspace \n  - Central Solution for all of your logs \n- Application Insights\n  - Performance \n  - Management system for your live applications\n\n##### Log Analytics Workspace \nA Log Analytics workspace is a unique environment for log data from Azure Monitor and other Azure services, such as Microsoft Sentinel and Microsoft Defender for Cloud. Each workspace has its own data repository and configuration but might combine data from multiple services. This article provides an overview of concepts related to Log Analytics workspaces and provides links to other documentation for more details on each.\nbased on Kusto query language \n\n<img src=\"./assets/log_analytics.png\" alt=\"log_analytics\" width=\"800\"/>\n\n\n##### Application Insights \nApplication Insights is an extension of Azure Monitor and provides Application Performance Monitoring (also known as \u201cAPM\u201d) features. APM tools are useful to monitor applications from development, through test, and into production in the following ways:\n\n1. Proactively understand how an application is performing.\n2. Reactively review application execution data to determine the cause of an incident.\n\n\n#### Azure Backup \nThe Azure Backup service provides simple, secure, and cost-effective solutions to back up your data and recover it from the Microsoft Azure cloud.\n\nWhat can I back up? \n- On-premises - Back up files, folders, system state using the Microsoft Azure Recovery Services (MARS) agent. Or use the DPM or Azure Backup Server (MABS) agent to protect on-premises VMs(Hyper-V and VMware) and other on-premises workloads\n- Azure VMs - Back up entire Windows/Linux VMs (using backup extensions) or back up files, folders, and system state using the MARS agent.\n- Azure Managed Disks - Back up Azure Managed Disks\n- Azure Files shares - Back up Azure File shares to a storage account\n- SQL Server in Azure VMs - Back up SQL Server databases running on Azure VMs\n- SAP HANA databases in Azure VMs - Backup SAP HANA databases running on Azure VMs\n- Azure Database for PostgreSQL servers - Back up Azure PostgreSQL databases and retain the backups for up to 10 years\n- Azure Blobs - Overview of operational backup for Azure Blobs\n\n<img src=\"./assets/azure_backup.png\" alt=\"azure_backup\" width=\"800\"/>\n\nHow Azure Backup protects from ransomware?  \nAzure Backup helps protect your critical business systems and backup data against a ransomware attack by implementing preventive measures and providing tools that protect your organization from every step that attackers take to infiltrate your systems. It provides security to your backup environment, both when your data is in transit and at rest.\n\n##### Azure Backup for virtual machines \n- Provides access to data on the VM of something happens to the original VM \n- The backup gets written to a Recovery Service Vault \n\n\n### Azure Security \n\n#### Defender For Cloud \nMicrosoft Defender for Cloud is a Cloud Security Posture Management (CSPM) and Cloud Workload Protection Platform (CWPP) for all of your Azure, on-premises, and multicloud (Amazon AWS and Google GCP) resources. Defender for Cloud fills three vital needs as you manage the security of your resources and workloads in the cloud and on-premises:\n\n<img src=\"./assets/defender-for-cloud-synopsis.png\" alt=\"defender-for-cloud-synopsis\" width=\"800\"/>\n\n- Defender for Cloud secure score continually assesses your security posture so you can track new security opportunities and precisely report on the progress of your security efforts.\n- Defender for Cloud recommendations secures your workloads with step-by-step actions that protect your workloads from known security risks.\n- Defender for Cloud alerts defends your workloads in real-time so you can react immediately and prevent security events from developing.\n\nThe Defender plans of Microsoft Defender for Cloud offer comprehensive defenses for the compute, data, and service layers of your environment:\n\n- Microsoft Defender for Servers\n- Microsoft Defender for Storage\n- Microsoft Defender for SQL\n- Microsoft Defender for Containers\n- Microsoft Defender for App Service\n- Microsoft Defender for Key Vault\n- Microsoft Defender for Resource Manager\n- Microsoft Defender for DNS\n- Microsoft Defender for open-source relational databases\n- Microsoft Defender for Azure Cosmos DB\n- Defender Cloud Security Posture Management (CSPM)\n  - Security governance and regulatory compliance\n  - Cloud security explorer\n  - Attack path analysis\n  - Agentless scanning for machines\n- Defender for DevOps\n\n\n##### Defender For Servers\nMicrosoft Defender for Servers extends protection to your Windows and Linux machines that run in Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), and on-premises. Defender for Servers integrates with Microsoft Defender for Endpoint to provide endpoint detection and response (EDR) and other threat protection features.\n\nThis guide helps you design and plan an effective Defender for Servers deployment. Microsoft Defender for Cloud offers two paid plans for Defender for Servers.\n\n##### Defender for Storage\nMicrosoft Defender for Storage is an Azure-native layer of security intelligence that detects unusual and potentially harmful attempts to access or exploit your storage accounts. It uses advanced threat detection capabilities and Microsoft Threat Intelligence data to provide contextual security alerts. Those alerts also include steps to mitigate the detected threats and prevent future attacks.\n\nYou can enable Microsoft Defender for Storage at either the subscription level (recommended) or the resource level.\n\nDefender for Storage continually analyzes the telemetry stream generated by the Azure Blob Storage and Azure Files services. When potentially malicious activities are detected, security alerts are generated. These alerts are displayed in Microsoft Defender for Cloud, together with the details of the suspicious activity along with the relevant investigation steps, remediation actions, and security recommendations.\n\nAnalyzed telemetry of Azure Blob Storage includes operation types such as Get Blob, Put Blob, Get Container ACL, List Blobs, and Get Blob Properties. Examples of analyzed Azure Files operation types include Get File, Create File, List Files, Get File Properties, and Put Range.\n\nDefender for Storage doesn't access the Storage account data and has no impact on its performance.\n\n##### Defender for SQL\nMicrosoft Defender for Azure SQL helps you discover and mitigate potential database vulnerabilities and alerts you to anomalous activities that may be an indication of a threat to your databases.\n\n- Vulnerability assessment: Scan databases to discover, track, and remediate vulnerabilities. Learn more about vulnerability assessment.\n- Threat protection: Receive detailed security alerts and recommended actions based on SQL Advanced Threat Protection to provide to mitigate threats. Learn more about SQL Advanced Threat Protection.\n\nWhen you enable Microsoft Defender for Azure SQL, all supported resources that exist within the subscription are protected. Future resources created on the same subscription will also be protected.\n\n##### Defender for Containers\nMicrosoft Defender for Containers is the cloud-native solution that is used to secure your containers so you can improve, monitor, and maintain the security of your clusters, containers, and their applications.\n\nDefender for Containers assists you with the three core aspects of container security:\n\n- Environment hardening - Defender for Containers protects your Kubernetes clusters whether they're running on Azure Kubernetes Service, Kubernetes on-premises/IaaS, or Amazon EKS. Defender for Containers continuously assesses clusters to provide visibility into misconfigurations and guidelines to help mitigate identified threats.\n\n- Vulnerability assessment - Vulnerability assessment and management tools for images stored in ACR registries and running in Azure Kubernetes Service.\n\n- Run-time threat protection for nodes and clusters - Threat protection for clusters and Linux nodes generates security alerts for suspicious activities.\n\n\n##### Defender for App Service\nDefender for Cloud is natively integrated with App Service, eliminating the need for deployment and onboarding - the integration is transparent.\n\nTo protect your Azure App Service plan with Microsoft Defender for App Service, you'll need:\n\n- A supported App Service plan associated with dedicated machines. Supported plans are listed in Availability.\n\n- Defender for Cloud's enhanced protections enabled on your subscription as described in Quickstart: Enable enhanced security features.\n\n##### Defender for Key Vault\nAzure Key Vault is a cloud service that safeguards encryption keys and secrets like certificates, connection strings, and passwords.\n\nEnable Microsoft Defender for Key Vault for Azure-native, advanced threat protection for Azure Key Vault, providing an additional layer of security intelligence.\n\n##### Defender for Resource Manager\nAzure Resource Manager is the deployment and management service for Azure. It provides a management layer that enables you to create, update, and delete resources in your Azure account. You use management features, like access control, locks, and tags, to secure and organize your resources after deployment.\n\nThe cloud management layer is a crucial service connected to all your cloud resources. Because of this, it is also a potential target for attackers. Consequently, we recommend security operations teams monitor the resource management layer closely.\n\nMicrosoft Defender for Resource Manager automatically monitors the resource management operations in your organization, whether they're performed through the Azure portal, Azure REST APIs, Azure CLI, or other Azure programmatic clients. Defender for Cloud runs advanced security analytics to detect threats and alerts you about suspicious activity.\n\n##### Defender for DNS\nMicrosoft Defender for DNS provides an additional layer of protection for resources that use Azure DNS's Azure-provided name resolution capability.\n\nFrom within Azure DNS, Defender for DNS monitors the queries from these resources and detects suspicious activities without the need for any additional agents on your resources.\n\n\nWhat are the benefits of Microsoft Defender for DNS?\nMicrosoft Defender for DNS detects suspicious and anomalous activities such as:\n\n- Data exfiltration from your Azure resources using DNS tunneling\n- Malware communicating with command and control servers\n- DNS attacks - communication with malicious DNS resolvers\n- Communication with domains used for malicious activities such as phishing and crypto mining\n\n##### Defender for open-source relational databases\nThis plan brings threat protections for the following open-source relational databases:\n\n- Azure Database for PostgreSQL\n- Azure Database for MySQL\n- Azure Database for MariaDB\nDefender for Cloud detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases. The plan makes it simple to address potential threats to databases without the need to be a security expert or manage advanced security monitoring systems.\n\n##### Defender for Azure Cosmos DB\nMicrosoft Defender for Azure Cosmos DB detects potential SQL injections, known bad actors based on Microsoft Threat Intelligence, suspicious access patterns, and potential exploitation of your database through compromised identities, or malicious insiders.\n\nDefender for Azure Cosmos DB uses advanced threat detection capabilities, and Microsoft Threat Intelligence data to provide contextual security alerts. Those alerts also include steps to mitigate the detected threats and prevent future attacks.\n\nYou can enable protection for all your databases (recommended), or enable Microsoft Defender for Azure Cosmos DB at either the subscription level, or the resource level.\n\nDefender for Azure Cosmos DB continually analyzes the telemetry stream generated by the Azure Cosmos DB service. When potentially malicious activities are detected, security alerts are generated. These alerts are displayed in Defender for Cloud together with the details of the suspicious activity along with the relevant investigation steps, remediation actions, and security recommendations.\n\nDefender for Azure Cosmos DB doesn't access the Azure Cosmos DB account data, and doesn't have any effect on its performance.\n\n##### Defender Cloud Security Posture Management (CSPM)\nOne of Microsoft Defender for Cloud's main pillars for cloud security is Cloud Security Posture Management (CSPM). CSPM provides you with hardening guidance that helps you efficiently and effectively improve your security. CSPM also gives you visibility into your current security situation.\n\nDefender for Cloud continually assesses your resources, subscriptions and organization for security issues. Defender for Cloud shows your security posture in secure score. The secure score is an aggregated score of the security findings that tells you your current security situation. The higher the score, the lower the identified risk level.\n\n##### Defender for DevOps\nMicrosoft Defender for Cloud enables comprehensive visibility, posture management, and threat protection across multicloud environments including Azure, AWS, GCP, and on-premises resources. Defender for DevOps, a service available in Defender for Cloud, empowers security teams to manage DevOps security across multi-pipeline environments.\n\nDefender for DevOps uses a central console to empower security teams with the ability to protect applications and resources from code to cloud across multi-pipeline environments, such as GitHub and Azure DevOps. Findings from Defender for DevOps can then be correlated with other contextual cloud security insights to prioritize remediation in code. Key capabilities in Defender for DevOps include:\n\n- Unified visibility into DevOps security posture: Security administrators now have full visibility into DevOps inventory and the security posture of pre-production application code, which includes findings from code, secret, and open-source dependency vulnerability scans. They can configure their DevOps resources across multi-pipeline and multicloud environments in a single view.\n\n- Strengthen cloud resource configurations throughout the development lifecycle: You can enable security of Infrastructure as Code (IaC) templates and container images to minimize cloud misconfigurations reaching production environments, allowing security administrators to focus on any critical evolving threats.\n\n- Prioritize remediation of critical issues in code: Apply comprehensive code to cloud contextual insights within Defender for Cloud. Security admins can help developers prioritize critical code fixes with Pull Request annotations and assign developer ownership by triggering custom workflows feeding directly into the tools developers use and love.\n\nDefender for DevOps helps unify, strengthen and manage multi-pipeline DevOps security.", "repo_name": "MTC_IL_WORKSHOP_Azure_Administrator", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_Azure_Administrator", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_Azure_Administrator", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_Azure_Administrator", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Rust for C#/.NET Developers\n\nThe document sources can be found in the `src` directory. It is structured for\nrendering with [mdBook].\n\n[Install mdBook] locally or start-up [`devcontainer.json`]. If you've never\nused Dev Containers, check out [Developing inside a Container using Visual\nStudio Code Remote Development][vscode-dc].\n\nThen to render for reading in a Web browser, run:\n\n    mdbook serve\n\n  [mdBook]: https://rust-lang.github.io/mdBook/\n  [Install mdBook]: https://rust-lang.github.io/mdBook/guide/installation.html\n  [`devcontainer.json`]: .devcontainer/devcontainer.json\n  [vscode-dc]: https://code.visualstudio.com/docs/devcontainers/containers\n", "repo_name": "rust-for-dotnet-devs", "org_name": "microsoft", "org_repo": "microsoft/rust-for-dotnet-devs", "platform_org_repo": "github+microsoft/rust-for-dotnet-devs", "link_to_repo": "https://github.com/microsoft/rust-for-dotnet-devs", "platform": "github", "language": "JavaScript", "stargazers_count": 152, "watchers_count": 152}, {"README_text": "# FairPrism\n\nThis repository contains the FairPrism dataset, introduced in \"FairPrism: Evaluating Fairness-Related Harms in Text Generation.\" The dataset consists of 5,000 examples of AI-generated text annotated for the harms that the text can cause.\n\nTo request dataset access, please email fairprism@microsoft.com with your name, affiliation, and a brief description of your use case.\n\nFor further information about privacy and this dataset, please see the Microsoft Privacy Statement: https://go.microsoft.com/fwlink/?LinkId=521839\n", "repo_name": "FairPrism", "org_name": "microsoft", "org_repo": "microsoft/FairPrism", "platform_org_repo": "github+microsoft/FairPrism", "link_to_repo": "https://github.com/microsoft/FairPrism", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# SPADE: State Space Augmented Transformer\n\nThis PyTorch package implements the language modeling experiments in [Efficient Long Sequence Modeling via State Space Augmented Transformer\n](https://arxiv.org/abs/2212.08136). For Hugging Face Transformers-style implementation for fine-tuning experiments, refer to [this repo](https://github.com/namisan/mt-dnn/tree/msrt5/msrlongt5).\n\n\n## Dependencies\n\n* The package runs on PyTorch v1.11.0 with CUDA 11.3. Note that installation of CUDA and cuDNN are required.\n* Our implementation requires [fairseq](https://github.com/facebookresearch/fairseq) v0.11+.\n\n\n## Installation\n\n* Download [fairseq](https://github.com/facebookresearch/fairseq). Place `spade-modules/` in the downloaded directory.\n  * If training of S4 is desirable (see instructions), replace `fairseq/trainer.py` with the provided `trainer.py` file.\n    Specifically, we modified the `trainer.py/_build_optimizer()` function to add separate S4 parameter groups. \n\n* Run `pip install -e .` to install fairseq locally.\n\n* Run `pip install -r requirements.txt` to install dependencies.\n\n* Our implementation requires computing the Cauchy kernel.\n  Note that `spade-modules/s4.py` largely depends on [this file](https://github.com/HazyResearch/state-spaces/blob/main/src/models/s4/s4.py).\n  There are two possible ways:\n  * (**Recommended**) Install [PyKeOps](https://www.kernel-operations.io/keops/python/index.html) using `pip install pykeops`.\n  * Download the `extensions/` folder from [this repo](https://github.com/HazyResearch/state-spaces), and place the downloaded folder to `spade-modules/extensions/`. \n    Run `python setup.py install` within `spade-modules/extensions/cauchy/` to install the CUDA kernel.\n    \n\n## Instructions\n\n* Follow the instructions [here](https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.md) to pre-process wikitext-103 data. \n  Run `bash run_lm.sh` to train a SPADE model.\n  \n* Empirically, we can initialize the S4 module and then freeze its parameters during training.\n  We find that training the S4 parameters only provides marginal performance gain.\n  \n* Learning rates of S4 parameters are specified using the `--s4-lr` argument, \n  which is a dictionary containing three keys: `A`, `B` and `dt`. Each key can use three types of values:\n  * `null` means the parameter uses the same learning rate as model parameters; \n  * `0.0` means the parameter is not trained;\n  * `float` (>0.0) specifies a learning rate.\n  \n\n## Notes\n\n### Contact Information\n\nFor personal communication related to this package, please contact Simiao Zuo (`simiaozuo@gatech.edu`), Xiaodong Liu (`xiaodl@microsoft.com`), or Jian Jiao (`jian.jiao@microsoft.com`).\n  \n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n## Reference\n\nPlease cite the following paper if you use this package:\n```\n@article{zuo2022efficient,\n  title={Efficient Long Sequence Modeling via State Space Augmented Transformer},\n  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Charles, Denis and Manavoglu, Eren and Zhao, Tuo and Gao, Jianfeng},\n  journal={arXiv preprint arXiv:2212.08136},\n  year={2022}\n}\n```\n", "repo_name": "EfficientLongSequenceModeling", "org_name": "microsoft", "org_repo": "microsoft/EfficientLongSequenceModeling", "platform_org_repo": "github+microsoft/EfficientLongSequenceModeling", "link_to_repo": "https://github.com/microsoft/EfficientLongSequenceModeling", "platform": "github", "language": "Python", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Exo-TroubleshootingWorkshop", "org_name": "microsoft", "org_repo": "microsoft/Exo-TroubleshootingWorkshop", "platform_org_repo": "github+microsoft/Exo-TroubleshootingWorkshop", "link_to_repo": "https://github.com/microsoft/Exo-TroubleshootingWorkshop", "platform": "github", "language": "PowerShell", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Contoso Traders - Cloud testing tools demo app\n\nThe Contoso Traders app is a sample application showcasing [Playwright](https://playwright.dev), [Azure Load Testing](https://aka.ms/malt-docs), [Azure Chaos Studio](https://aka.ms/CHAOS-docs) and more.\n\nThis repo contains the source code, deployment templates, and demo scripts for exploring these cloud testing tools.\n\n![Landing Page](./docs/images/landing-page.png)\n\n## Documentation and Resources\n\n* Application Links: [UI](https://cloudtesting.contosotraders.com/) | [Carts API](https://contoso-traders-cartsctprd.bluestone-748d2276.eastus.azurecontainerapps.io/swagger/index.html) | [Products API](https://contoso-traders-productsctprd.eastus.cloudapp.azure.com/swagger/index.html)\n* [Deployment Instructions](./docs/deployment-instructions.md) | [Running Locally](./docs/running-locally.md)\n\n## Continuous Integration\n\n| Pipeline                                                                     | Status                                                                                                                                                                                                                                                                               | Details                                                        |\n| ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------- |\n| [GitHub Workflow](./.github/workflows/contoso-traders-cloud-testing.yml)     | [![contoso-traders-cloud-testing](https://github.com/microsoft/contosotraders-cloudtesting/actions/workflows/contoso-traders-cloud-testing.yml/badge.svg?branch=main)](https://github.com/microsoft/contosotraders-cloudtesting/actions/workflows/contoso-traders-cloud-testing.yml) | Deploys to [production](https://production.contosotraders.com) |\n| [Azure DevOps Pipeline](./.azurepipelines/contoso-traders-cloud-testing.yml) | [![Build Status](https://dev.azure.com/MicrosoftTestDemos/ContosoTraders_Testing/_apis/build/status%2Fmicrosoft.contosotraders-cloudtesting?branchName=main)](https://dev.azure.com/MicrosoftTestDemos/ContosoTraders_Testing/_build/latest?definitionId=1&branchName=main)          | Deploys to [staging](https://staging.contosotraders.com)       |\n\n## Demo Scripts\n\n* [Developer Workflow](./demo-scripts/dev-workflow/walkthrough.md)\n\n* Azure Load Testing - Generate high-scale load and identify performance bottlenecks.\n  * [Create a load test for the shopping cart API.](./demo-scripts/azure-load-testing/walkthrough.md)\n  * [Use GitHub Actions for regression testing.](./demo-scripts/azure-load-testing/walkthrough.md#walkthrough-regression-testing-with-github-workflows)\n  * [Create a load test for a private endpoint that\u2019s behind a VNet.](./demo-scripts/azure-load-testing/private-endpoints.md)\n  * [Right-size your AKS cluster using load tests.](./demo-scripts/azure-load-testing/aks-cost-optimization.md)\n\n* Azure Chaos Studio - Improve application resilience by introducing faults and simulating outages.\n  * [Create an experiment using Key Vault Deny Access fault to test the products API (AKS).](./demo-scripts/azure-chaos-studio/walkthrough.md)\n  * [Run experiment in GitHub Actions to inject faults (pod failures) into the AKS cluster.](./demo-scripts/azure-chaos-studio/walkthrough.md#walkthrough-running-chaos-experiments-via-github-workflows)\n\n* Playwright - Reliable end-to-end testing for modern web apps.\n  * [Use the VS Code extension to explore and run web tests](./demo-scripts/testing-with-playwright/walkthrough.md) for [API testing](src/ContosoTraders.Ui.Website/tests/api), [Authentication](src/ContosoTraders.Ui.Website/tests/auth.setup.ts), [Shopping cart](src/ContosoTraders.Ui.Website/tests/cart.spec.ts), [Uploading files](src/ContosoTraders.Ui.Website/tests/fileupload.spec.ts), [Visual Comparisons](src/ContosoTraders.Ui.Website/tests/pages.spec.ts#L63), [Emulation](src/ContosoTraders.Ui.Website/tests/map.spec.ts), [Mocking](src/ContosoTraders.Ui.Website/tests/mocks.spec.ts), and [using a CSV for data](src/ContosoTraders.Ui.Website/tests/account.ts)\n\n## Architecture\n\n![Architecture](./docs/architecture/contoso-traders-enhancements.drawio.png)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "contosotraders-cloudtesting", "org_name": "microsoft", "org_repo": "microsoft/contosotraders-cloudtesting", "platform_org_repo": "github+microsoft/contosotraders-cloudtesting", "link_to_repo": "https://github.com/microsoft/contosotraders-cloudtesting", "platform": "github", "language": "JavaScript", "stargazers_count": 77, "watchers_count": 77}, {"README_text": "\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/learning-customized-visual-models-with/semi-supervised-image-classification-on-1)](https://paperswithcode.com/sota/semi-supervised-image-classification-on-1?p=learning-customized-visual-models-with)\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/learning-customized-visual-models-with/semi-supervised-image-classification-on-2)](https://paperswithcode.com/sota/semi-supervised-image-classification-on-2?p=learning-customized-visual-models-with)\n\n## REACT: Learning Customized Visual Models with Retrieval-Augmented Knowledge (CVPR 2023, Highlight 2.5%)\n\n[Haotian Liu](https://hliu.cc), [Kilho Son](#), [Jianwei Yang](https://jwyang.github.io/), [Ce Liu](#), [Jianfeng Gao](https://www.microsoft.com/en-us/research/people/jfgao/), [Yong Jae Lee*](https://pages.cs.wisc.edu/~yongjaelee/), [Chunyuan Li*](https://chunyuan.li/)\n\n[[Project Page](https://react-vl.github.io/)] [[Paper](https://arxiv.org/abs/2301.07094)]\n\n![Teaser figure](figures/concept.gif)\n\n- Introducing a customization stage to the lifecycle of foundation models!\n- REACT customizes foundation models to downstream tasks without the need of any labeled data.\n\n## :fire: News\n\n* **[2023.03.29]** Code base and checkpoints are released.\n* **[2023.03.25]** Our research paper is selected as <b>highlight</b> (2.5% acceptance rate)!\n* **[2023.03.24]** Our new checkpoint based on OpenCLIP-G/14 achieves <b>81.0%</b> zero-shot on ImageNet, the <b>new SOTA</b> among public checkpoints!\n* **[2023.02.28]** Paper is accepted to CVPR 2023.\n* **[2023.01.17]** REACT paper is released.\n\n## Code\n\n### [:globe_with_meridians:\tStage 1: Retrieval](./react_retrieval)\nREACT provides a pipeline that supports building index on a large dataset, and efficiently queries and retrieves relevant data for downstream tasks with information as simple as class names. See [`react_retrieval`](./react_retrieval) for details.\n\nYou may skip this step if you want to focus on building customized models on standard benchmarks like ImageNet-1K and ELEVATER, by directly using our retrieved indices.\n\n### [:art: Stage 2: Customization](./react_customization) \n\nREACT proposes the efficient and effective *locked-text gated-image tuning* for tuning customized model on the retrieved dataset, with a performance improvement of up to 5.4% improvements on ImageNet. See [`react_customization`](./react_customization) for details.\n\n![Teaser figure](https://react-vl.github.io/images/model_tuning_cmp.png)\n\n## Pretrained Models\n\n### ImageNet-1K\n\n|                        | Baseline | REACT <br/> (Locked-Text) <br/> LAION-400M                                                                                                                                                                                                                            | REACT <br/> (Gated-Image) <br/> LAION-400M                                                                                                                                                                                                                             | REACT  <br/> (Gated-Image) <br/> LAION-2B                                                                                                        |\n|------------------------|------|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|\n| CLIP (B32, WIT-400M)   | 63.2 | 66.9 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-base-32-locked-text.pt)) | 68.6 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-base-32-gated-image.pt))                         | --                                                                                                                      |\n| OpenCLIP (B32, L-400M) | 62.9 | 65.7 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-vit-base-32-locked-text.pt)) | 66.4 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-vit-base-32-gated-image.pt))                 | --                                                                                                                      |\n| OpenCLIP (B32, L-2B)   | 66.6 | 67.5 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-laion2b-vit-base-32-locked-text.pt)) | 69.5 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-laion2b-vit-base-32-gated-image.pt)) | --                                                                                                                      |\n| CLIP (B16, WIT-400M)   | 68.6 | 71.6 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-base-16-locked-text.pt)) | 73.4 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-base-16-gated-image.pt))                         | --                                                                                                                      |\n| CLIP (L14, WIT-400M)   | 75.3 | -- | 78.1 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-large-14-gated-image.pt))                                                                                                                                    | 79.8 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/clip-vit-large-14-gated-image-laion2b.pt))     |\n| OpenCLIP (L14, L-2B)   | 75.3 | -- | 76.4 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-vit-large-14-gated-image.pt))                                                                                                                                | 78.6 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-vit-large-14-gated-image-laion2b.pt)) |\n| OpenCLIP (G14, L-2B)   | 80.1 | -- | --                                                                                                                                                                                                                                             | 81.0 ([hf](https://huggingface.co/react-vl/react-in1k/blob/main/openclip-vit-bigG-14-gated-image-laion2b.pt))  |\n\n## Citation\n```\n@article{liu2023react,\n  author      = {Liu, Haotian and Son, Kilho and Yang, Jianwei and Liu, Ce and Gao, Jianfeng and Lee, Yong Jae and Li, Chunyuan},\n  title       = {Learning Customized Visual Models with Retrieval-Augmented Knowledge},\n  publisher   = {CVPR},\n  year        = {2023},\n}\n```\n\n## Acknowledgement\n\nWe are grateful for the contributions of several open-source projects, including [CLIP](https://github.com/openai/CLIP), [OpenCLIP](https://github.com/mlfoundations/open_clip), [LAION.AI](https://laion.ai/), [FAISS](https://github.com/facebookresearch/faiss), [Autofaiss](https://github.com/criteo/autofaiss), [img2dataset](https://github.com/rom1504/img2dataset), and [ELEVATER](https://github.com/Computer-Vision-in-the-Wild/Elevater_Toolkit_IC).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "react", "org_name": "microsoft", "org_repo": "microsoft/react", "platform_org_repo": "github+microsoft/react", "link_to_repo": "https://github.com/microsoft/react", "platform": "github", "language": "Python", "stargazers_count": 66, "watchers_count": 66}, {"README_text": "# End to End scenario Streaming-DataLake-Azure Synapse-Machine Learning\n\nThis repository will guide you through a complete escenario that ingest information in real-time  storing in Datalake and process it inside Azure Synapse to be able to transform, analize and visualize thta data.\n\n![alt text](./wiki/images/architecture.jpg)\n\nFirst Part: [Streaming-DataLake-Azure](./wiki/content/readme.md)\n\nHands-On: [Challenge One](./wiki/content/challenge-1.md)\n\nTransform/Process: [Transform Data](./wiki/content/transforming.md)\n\nConsume: [Machine Learning Integration](./wiki/content/consume.md)\n\n\n\n\n\n\n\n\n", "repo_name": "ftatoolkit-synapse-streaming-ingestion", "org_name": "microsoft", "org_repo": "microsoft/ftatoolkit-synapse-streaming-ingestion", "platform_org_repo": "github+microsoft/ftatoolkit-synapse-streaming-ingestion", "link_to_repo": "https://github.com/microsoft/ftatoolkit-synapse-streaming-ingestion", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Azure Site Recovery Disk Filter Driver for Linux (ASRDFD)\n\n* [Introduction](#introduction)\n* [Licensing](#licensing)\n* [Design](#design)\n* [Contributing](#contributing)\n* [Roadmap](#roadmap)\n* [Telemetry](#telemetry)\n* [Trademarks](#trademarks)\n\n## Introduction\n\nAzure Site Recovery Disk Filter Driver for Linux (ASRDFD) is a disk filter driver to capture any changes to the disk. It can be installed as a kernel module in Linux and is currently used to implement the changed block tracking functionality used in Microsoft's Azure Site Recovery (ASR) product.\n\nThe ASR product uses this module to achieve the disaster recovery and one can refer to the public documentation for ASR is available at https://learn.microsoft.com/en-us/azure/site-recovery/. The data tracked/captured at this module can be drained to user-space and can be used to replicate locally or to a remote site.\n\nThe eventual goal is to upstream the driver to the Linux code tree.\n\n## Licensing\n\nThis project is licensed under the license [GPL-v2 only](./LICENSE.txt).\n\n## Design\n\nThe design is captured at [Design](doc/design.md).\n\n## Contributing\n\nThis file describes the steps to use the driver. For more information about how to contribute code to this project, please check the [CONTRIBUTING.md](CONTRIBUTING.md) file in this repository.\n\n### How to build\n``` bash\ncd src\nmake -f involflt.mak KDIR=/lib/modules/<kernel version>/build\n```\n\n- To compile the driver with telemetry support, compile with \"TELEMETRY=yes\" option.\n- For SLES12/15, have to pass the service pack level of the kernel for which the driver is getting built with option PATCH_LEVEL=\\<ServicePack Number\\>. For example, if the driver is getting built for a kernel which is been released for SLES12 SP3, so \"PATCH_LEVEL=3\" has to be passed as an argument to the above make command as the service pack is 3.\n- The build will generate the driver module file \"involflt.ko\".\n\n### How to install\nUse the below command to load the driver module.\n``` bash\ninsmod involflt.ko\n```\n\n### How to test\n\nAll the IOCTLs are defined at [IOCTLs](doc/userspace-api/ioctl.md) to write the test utility.\n\n## Roadmap\n\nAs new Linux kernels are released, we intend to keep updating the driver code to ensure that the driver works with the latest kernel releases. By opensourcing this code, we can enable Linux distro vendors to support the driver functionality as soon as a new kernel version is released. Our eventual goal is to work towards contributing this driver to the upstream Linux code tree.\n\n## Telemetry\n\nData Collection. The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft\u2019s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n### Instructions to turn off telemetry\n\nThe telemetry is disabled by default. Please refer to the section [How to build](#how-to-build) if telemetry needs to be enabled.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "ASRDFD", "org_name": "microsoft", "org_repo": "microsoft/ASRDFD", "platform_org_repo": "github+microsoft/ASRDFD", "link_to_repo": "https://github.com/microsoft/ASRDFD", "platform": "github", "language": "C", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "Code repo for the paper \"Dissecting Deep Metric Learning Losses for Image-Text Retrieval\"\n\nThe main code is in /Gradient folder\n\nAnd /Expample folder contains an example to plug gradient method in a VSE-infy work.\n\n/Expample/vse_infy is cloned and inherited from the repo: https://github.com/woodfrog/vse_infty \n\nExample train code is /Expample/vse_infy/train_region.sh and evaluation code is /Expample/vse_infy/eval_region.sh", "repo_name": "VSE_Gradient", "org_name": "microsoft", "org_repo": "microsoft/VSE_Gradient", "platform_org_repo": "github+microsoft/VSE_Gradient", "link_to_repo": "https://github.com/microsoft/VSE_Gradient", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "d365-finance-tax-calculation-validator", "org_name": "microsoft", "org_repo": "microsoft/d365-finance-tax-calculation-validator", "platform_org_repo": "github+microsoft/d365-finance-tax-calculation-validator", "link_to_repo": "https://github.com/microsoft/d365-finance-tax-calculation-validator", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# PPM Checker Tool\n\nProcessor Power Management (PPM) settings can significantly impact power and performance of Windows systems. These settings configure getting the right threads on the right cores at the right time and frequency that balance performance and power requirements. \nConfiguring the PPM settings is a complex process and requires area expertise and understanding of each setting\u2019s impact on the system\u2019s behavior. There are many settings with many dimensions (Power Scheme, Power Source, Processor Class, Quality of Service etc.) which create numerous numbers of configurations and add to the complexity of the PPM configuration task.\n\nTo help this process, we built a tool that the silicon vendors/OEMs can deploy to validate the PPM settings on their new products.\n\n> The Architecture\n> ![image](https://user-images.githubusercontent.com/121056171/210118102-93b0a087-0562-4b1d-baa3-0ef5aae138ce.png)\n\n## How to use the Automatic Script\nThe script is located in the Github release, as well as in the Scripts folder in the PPMCheckerTool.\n### To Run the script with the release\nDownload the release from Github. Unzip it. Run the script Run.ps1 from the Scripts folder.\n\n\u2018ExeLocation\u2019 refers to the location that both the project Exe\u2019s reside \u2013 PPMCheckerTool.exe and SetSliderPowerMode.exe. In this case, it is 1 directory up. Enter \u2018..\u2019 in the prompt. Can be relative path.\n\u2018OutputFilePath\u2019 refers to the full name and path of the output file. Eg. D:\\Outputs\\Output.txt will work. Can be relative path.\n\u2018TargetCPU\u2019 refers to the target CPU architecture. Eg. ADL_U, ADL_P etc...\n\n### To Run the script with the visual studio development environment\n\nOpen the solution in Visual Studio, ensure you have no build errors.\nClick on the down arrow next to the 'Run button' (Green arrow). Add the command line parameters -i <trace.etl>  -o <output.csv/txt>  -t <target CPU>\nRun the solution\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PPMCheckerTool", "org_name": "microsoft", "org_repo": "microsoft/PPMCheckerTool", "platform_org_repo": "github+microsoft/PPMCheckerTool", "link_to_repo": "https://github.com/microsoft/PPMCheckerTool", "platform": "github", "language": "C#", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# lightATAC\n\nThis is a lightweight reimplementation of **Adversarially Trained Actor Critic** ([ATAC](https://github.com/microsoft/ATAC)), a model-free offline reinforcement learning algorithm by Ching-An Cheng*, Tengyang Xie*, Nan Jiang, and Alekh Agarwal.\n\nTo install, simply clone the repo and  run `pip install -e . `. It uses mujoco210, which can be installed, if needed, following the commands below.\n\n```\nbash install.sh\necho \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.mujoco/mujoco210/bin:/usr/lib/nvidia\" >> ~/.bashrc\nsource ~/.bashrc\n```\n\nThen you can start the training by running, e.g.,\n\n    python main.py --log_dir ./tmp_results --env_name hopper-medium-expert-v2 --beta 1.0\n\nMore instructions can be found in `main.py`, and please see the [original paper](https://arxiv.org/abs/2202.02446) for hyperparameters (e.g., `beta`). The code was tested with python 3.9.\n\nThe experimental results of lightATAC (over different $\\beta$ values) on D4RL mujoco datasets can be viewed at https://tensorboard.dev/experiment/6RwXhalaQeWNmQNHDGvaFA.\n\nThis reimplementation is based on [gwthomas/IQL-PyTorch](https://github.com/gwthomas/IQL-PyTorch). It is minimalistic, so users can easily modify it for their needs. It follows mostly the logic in the original [ATAC](https://github.com/microsoft/ATAC) code, but with some code optimization, which gives about 1.5X-2X speed up.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "lightATAC", "org_name": "microsoft", "org_repo": "microsoft/lightATAC", "platform_org_repo": "github+microsoft/lightATAC", "link_to_repo": "https://github.com/microsoft/lightATAC", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Project\nEdge (chromium) extension that will search contents of current tab for defined terms in the glossary file.\n\n## Get Started\n1. Fork the repo and clone to local machine\n1. Side-load the extension using this guide: [Sideload an extension](https://learn.microsoft.com/en-us/microsoft-edge/extensions-chromium/getting-started/extension-sideloading)\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mrct-glossary-extension", "org_name": "microsoft", "org_repo": "microsoft/mrct-glossary-extension", "platform_org_repo": "github+microsoft/mrct-glossary-extension", "link_to_repo": "https://github.com/microsoft/mrct-glossary-extension", "platform": "github", "language": "JavaScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Data Science Editor\n\nhttps://user-images.githubusercontent.com/4175913/219923079-062bb9f1-c015-469e-b144-81aaca3eb493.mp4\n\n## Developer setup\n\n-   Open this repository online at https://github.dev/microsoft/data-science-editor\n\nAll command line instructions assume a bash-like terminal.\n\nOn Windows, you may need to run these commands within Git Bash or Windows Subsystem for Linux (WSL), unless you have bash-like tools available locally. Previous installs have worked on WSL2 with Ubuntu-20.04.\n\n### Codespaces\n\nEdit this project directly from your browser using GitHub Codespaces. If you have access to them,\n\n-   open project in a new codespace (https://github.dev/microsoft/data-science-editor)\n-   launch the docs server\n\n```\nyarn develop\n```\n\n-   click on the generated URL in the terminal output and voila!\n\n**Do not use npm**\n\n#### Updating dependencies\n\nUse [npm-check-updates](https://www.npmjs.com/package/npm-check-updates) to upgrade all dependencies expect blockly\\*, tfjs, mdx.\n\n### VS Code\n\nYou are welcome to use any editor you want! Visual Studio Code\nprovides seamless support for git sub-modules and is our preferred editor.\n\n-   open [Visual Studio Code](https://code.visualstudio.com/)\n\n```\ncode .\n```\n\n-   install the recommended extensions (**MDX**, **ESLint** and **Prettier** extensions)\n-   remember that you need a bash-like terminal to run some of these commands - VS Code allows you to start a Git Bash terminal from the new terminals dropdown\n-   run the docs web site locally\n\n```\nyarn develop\n```\n\n-   browse to the local server\n\n```\nhttp://localhost:8000?dbg=1\n```\n\n### Generate block docs\n\n-   add `?screenshot=1`\n-   right click in empty editor and select generate screenshots\n-   select `/src/pages/blocks` folder\n-   commit\n\n## Microsoft Open Source Code of Conduct\n\nThis project is hosted at https://github.com/microsoft/data-science-editor.\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n-   [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n-   [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n-   Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n", "repo_name": "data-science-editor", "org_name": "microsoft", "org_repo": "microsoft/data-science-editor", "platform_org_repo": "github+microsoft/data-science-editor", "link_to_repo": "https://github.com/microsoft/data-science-editor", "platform": "github", "language": "TypeScript", "stargazers_count": 26, "watchers_count": 26}, {"README_text": "# Azure App Insights SAP-Fiori plugin Quickstart (PREVIEW)\n\nThis repos describes how to utilize a [SAP Fiori Launchpad Plugin](https://assets.cdn.sap.com/sapcom/docs/2019/03/b2dff710-427d-0010-87a3-c30de2ffd8ff.pdf) to gain **client-side insights** into Fiori metrics with [Azure Application Insights](https://learn.microsoft.com/azure/azure-monitor/app/app-insights-overview?tabs=net). It leverages the standard views for Azure Application Insights and a dedicated Azure Monitor Workbook for SAP.\n\n> **Note**\n> Find official API documentation for the Azure Application Insights JS snippet [here](https://github.com/microsoft/ApplicationInsights-JS#snippet-setup-ignore-if-using-npm-setup).\n\nA typical single instance setup would look like below. The plugin concept applies to native SAP on Azure deployments, SAP RISE, on-premises and [SAP BTP Build WorkZone, Standard edition](https://help.sap.com/docs/Launchpad_Service/8c8e1958338140699bd4811b37b82ece/9db48fa44f7e4c62a01bc74c82e74e07.html).\n\n> **Note**\n> Connection from BTP to the SAP workload on Azure can be done with the cloud-native [SAP Private Link Service for Azure](https://blogs.sap.com/2021/12/29/getting-started-with-btp-private-link-service-for-azure/) or the [SAP Cloud Connector](https://help.sap.com/docs/CP_CONNECTIVITY/cca91383641e40ffbe03bdc78f00f681/e6c7616abb5710148cfcf3e75d96d596.html) (SCC). For any-premises choose Azure VNet integration via [ExpressRoute](https://learn.microsoft.com/azure/expressroute/expressroute-introduction)/VPN or the SCC.\n\n![Architecture overview](img/overview.png)\n\n> **Note** - The reverse proxy is required in case of strict [CORS policies](https://github.com/microsoft/ApplicationInsights-SAP-Fiori-Plugin#how-to-deal-with-cross-origin-resource-sharing-cors-errors). Proxy choices range from managed services like [Azure Front Door](https://learn.microsoft.com/azure/frontdoor/front-door-overview) or [Azure Application Gateway](https://learn.microsoft.com/azure/application-gateway/overview) to self-hosted solutions like Apache.\n\n\ud83e\uddeaTested with Business Suite NW 7.51 and S/4HANA 2022 using Edge Browser.\n\n## Prerequisites\n\n1. Azure Application Insights instance (access to [connection string](https://learn.microsoft.com/azure/azure-monitor/app/sdk-connection-string?tabs=net#find-your-connection-string))\n2. Imported [Azure Monitor Workbook](Fiori-Performance-Analysis.workbook) (Create new, open code view <kbd></></kbd>, select Gallery template, copy & paste the json into it and save)\n3. Fiori Launchpad with SAPUI5 1.86+ (older Fiori stacks need to consider [alternatives](#sapui5-feature-dependencies)).\n\n> **Note** - you may upgrade your UI5 stack independently from the NetWeaver release\n\n4. Fiori Launchpad configured to use custom Plug-Ins. See [SAP's Fiori docs](https://www.sap.com/documents/2019/03/b2dff710-427d-0010-87a3-c30de2ffd8ff.html) (especially steps 76 onwards) to get started.\n\n| Parameter   | Value       | Description |\n| ----------- | ----------- | ----------- |\n| Launchpad Plugin ID      | `ZAZUREFLPPLUGIN`       | Retrieve from builder.customTasks.configuration.app.name in [ui5-deploy.yaml](ui5-deploy.yaml)       |\n| Launchpad Plugin URL   | `/sap/bc/ui5_ui5/sap/`        | Re-use from here or collect from `npm run deploy` output        |\n| UI5 Component ID   | `microsoft.com.flpmonitor`        | Verify from `sap.app.id` in [manifest.json](/webapp/manifest.json)        |\n\n> **Note**\n> Optionally add Azure Monitor for SAP Solutions Instance for infrastructure telemetery correlation\n\n### SAPUI5 feature dependencies\n\nDefault settings of this repos anticipate UI5 releases 1.86+. Use below table to understand potential feature scope for older releases.\n\n| Feature   | SAPUI5 release       | Usage |\n| ----------- | ----------- | ----------- |\n| [AppLifeCycle](https://sapui5.hana.ondemand.com/sdk/#/api/sap.ushell.services.AppLifeCycle)  | 1.38+ | Use SAP's public API to handle app loaded events |\n| [Interaction](https://sapui5.hana.ondemand.com/sdk/#/api/module:sap/ui/performance/trace/Interaction)  | 1.76+ | Use SAP public tracing APIs for analysis, matching SAP backend trace measurments |\n| [User Info](https://sapui5.hana.ondemand.com/sdk/#/api/sap.ushell.services.UserInfo%23methods/Summary)  | 1.86+ | Use SAP's public Fiori user API to correlate user info |\n| [Passport](https://help.sap.com/docs/ABAP_PLATFORM_NEW/468a97775123488ab3345a0c48cadd8f/a075ed88ef324261bca41813a6ac4a1c.html)  | n.a. | Use SAP's internal API to enrich requests with RootId and TransactionId for deep linking from Azure into SAP backend transactions |\n\nIf none are applicable revert to tracking Fiori hash changes only:\n\n```javascript\n//uncomment in init function\n$(window).hashchange(function () {\n    window.appInsights.trackPageView();\n}\n```\n\nAnother alternative poses the App Insights configuration `enableAutoRouteTracking`. However, launchpad navigation durations are not reflected, because it gets treated as a large single-page-application (SPA).\n\n## Local build instructions (SAP Business Application Studio)\n\n```cmd\ngit clone https://github.com/MartinPankraz/az-monitor-sap-fiori-plugin.git\n```\n\nThis app has been generated using the SAP Fiori tools - App Generator in [SAP Business Application Studio](https://help.sap.com/docs/SAP%20Business%20Application%20Studio), as part of the SAP Fiori tools suite.  In order to launch the generated app, simply run the following commands from the generated app root folder:\n\n```cmd\nnpm install\n```\n\nMaintain your [Azure Application Insights Connection String](https://learn.microsoft.com/azure/azure-monitor/app/sdk-connection-string?tabs=net#find-your-connection-string) and [AICloudRole](https://github.com/MartinPankraz/az-monitor-sap-fiori-plugin/blob/main/webapp/Component.js#L38) attributes on the [Component.js](https://github.com/MartinPankraz/az-monitor-sap-fiori-plugin/blob/main/webapp/Component.js#L36).\n\n```cmd\nnpm run build\n```\n\n```cmd\nnpm start\n```\n\n## Deploying the plugin\n\nThere are multiple ways to deploy the plugin to AS ABAP. Learn more [here](documentation/DEPLOYMENT.md)\n\n> **Note** - The provided guidance focuses on AS ABAP but the plugin also applies to the SAP Build Workzone, standard edition (formerly SAP Launchpad service). Learn more [here](https://developers.sap.com/tutorials/sapui5-fiori-cf-deploy.html).\n\n## How to deal with Cross-Origin Resource Sharing (CORS) errors\n\nConsider the `crossOrigin` setting of the App Insights [configuration](https://github.com/MartinPankraz/az-monitor-sap-fiori-plugin/blob/main/webapp/Component.js). Read more about it [here](https://learn.microsoft.com/azure/azure-monitor/app/javascript?tabs=snippet#configuration).\n\nIn case relaxation of the CORS policy is not an option, consider adding a reverse proxy. This is a standard practice with SAP Fiori integration with SAP Business Objects for instance. To do so adjust the hostname accordingly on the App Insights SDK by changing the [connection string](/webapp/Component.js#L40).\n\nIt would look something like this on the [Component.js](/webapp/Component.js#L40):\n\n```cmd\ncfg: { \n    connectionString:\"InstrumentationKey=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx;IngestionEndpoint=https://<your-reverse-proxy-hostname>/v2/track\"\n}\n```\n\nand something like this on the reverse proxy with a **special port** to identify this special rewrite scenario (pseudo code):\n\n```cmd\n<If \"%{HTTP_HOST} == '<your-reverse-proxy-hostname>' && %{SERVER_PORT} -eq 7777\">\n    Redirect \"/\" \"https://<your azure app insights domain>/\"\n</If>\n```\n\n> **Note** - Learn more about the setup with Azure Application Gateway [here](https://learn.microsoft.com/azure/application-gateway/url-route-overview) and for Azure Front Door [here](https://learn.microsoft.com/azure/frontdoor/front-door-route-matching?pivots=front-door-standard-premium).\n\n> **Note** - Learn more about overriding the SAP standard regarding CORS settings on the Fiori layer in this [blog post](https://blogs.sap.com/2022/08/02/embed-self-hosted-sap-fiori-launchpad-into-microsoft-teams/).\n\n## Troubleshooting hints\n\nUse the hot-key <kbd>CTRL</kbd>+<kbd>SHIFT</kbd>+<kbd>ALT</kbd>+<kbd>S</kbd> provided for SAPUI5 to [enable debug mode](https://sapui5.hana.ondemand.com/sdk/#/topic/c9b0f8cca852443f9b8d3bf8ba5626ab.html#loioc9b0f8cca852443f9b8d3bf8ba5626ab) from your Fiori Launchpad instance to load the non-minified sources for this plugin and the Azure App Insights SDK.\n\n### Changelog\n\n- 2022-12-14 Azure Monitor workbook added\n- 2022-12-07 CORS guidance\n- 2022-10-21 Automatic build process note for SAPGUI upload added\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit [CLA open-source](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ApplicationInsights-SAP-Fiori-Plugin", "org_name": "microsoft", "org_repo": "microsoft/ApplicationInsights-SAP-Fiori-Plugin", "platform_org_repo": "github+microsoft/ApplicationInsights-SAP-Fiori-Plugin", "link_to_repo": "https://github.com/microsoft/ApplicationInsights-SAP-Fiori-Plugin", "platform": "github", "language": "JavaScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Central Package Management Migrator\n\n.NET Tool that migrates NuGet codebases to [Central Package Management (CPM)](https://learn.microsoft.com/en-us/nuget/consume-packages/Central-Package-Management)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "CentralPackageManagementMigrator", "org_name": "microsoft", "org_repo": "microsoft/CentralPackageManagementMigrator", "platform_org_repo": "github+microsoft/CentralPackageManagementMigrator", "link_to_repo": "https://github.com/microsoft/CentralPackageManagementMigrator", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Container App Workshop\n\n## Architecture\n<img src=\"./assets/Container_App_Architecture_azure.png\" alt=\"Container App Architecture\" width=\"1200\"/>\n\n## Table of Contents \n- [Container App Workshop](#container-app-workshop)\n  - [Architecture](#architecture)\n  - [Table of Contents](#table-of-contents)\n  - [Prerequisites](#prerequisites)\n- [Steps](#steps)\n  - [Create a Resource Group](#create-a-resource-group)\n  - [Create a SQL database for partners backend API](#create-a-sql-database-for-partners-backend-api)\n  - [Create a Key Vault](#create-a-key-vault)\n  - [Store the database credentials in Key Vault](#store-the-database-credentials-in-key-vault)\n  - [Create a SPN](#create-a-spn)\n  - [Create ACR for Docker Images](#create-acr-for-docker-images)\n  - [Dockerizing a Node.js Application](#dockerizing-a-nodejs-application)\n  - [Create a Container App](#create-a-container-app)\n  - [Store SPN details in Container App Secrets](#store-spn-details-in-container-app-secrets)\n  - [Reference Secrets with Environment variables](#reference-secrets-with-environment-variables)\n  - [Check your application](#check-your-application)\n  - [Hosting Front End application on Azure Blob Storage](#hosting-front-end-application-on-azure-blob-storage)\n\n## Prerequisites\n- Azure Subscription\n- Azure CLI\n- Docker CLI \n- Node 16\n- Visual Studio (Community/Enterprise)\n\n# Steps\n\n## Create a Resource Group \n1. Go to Azure Portal\n2. Create a resource group - **containerappdemo** in your subscription \n\n## Create a SQL database for partners backend API\n1. Go to Azure Portal\n2. Create a resource - SQL Database \n   - ```Subscription``` - Choose your subscription\n   - ```Resource Group``` - Choose **containerappdemo** resource group\n   - ```Database Name``` - Choose **partnersdb**\n   - ```Server``` - Hit Create new\n     - ```Server name``` - Choose unique server name for example <yourname><partnersdb>\n     - ```Location``` - Choose West Europe\n     - ```Authentication method``` - Use SQL authentication and choose admin login and password\n     - Hit Ok\n     - ```Compute + storage``` - Choose Configure database\n       - ```Service tier``` - select DTU-based purchasing model - Basic\n       - Change to 1 GB Data max size\n       - Hit Apply\n     - ```Backup storage redundancy``` - Choose Locally-redundant backup storage\n     - Click Next: Networking\n     - ```Connectivity method``` - Select Public endpoint (For this demo only) \n       - Allow Azure services and resource to access this tier - Yes. \n       - Add current client IP address - Yes. \n     - Review + create  \n     - Create  \n   - ```Location``` - Choose your location\n   - ```Capacity mode``` - Provisioned throughput\n   - ```Apply Free Tier Discount``` - Apply if you want discount   \n   - Go to your SQL server blade\n   - ```Public network access``` - Selected networks -> Add your ip address \n   - ```Allow Azure services and resources to access this server``` - Check \n3. Go to your database \n4. Choose Query editor in database blade\n5. Login with sql server admin credentials \n6. In Query Editor create partners table:\n   ```\n    CREATE TABLE Partners (\n    id int IDENTITY(1,1) PRIMARY KEY,\n    email varchar(255) UNIQUE,\n    name varchar(255),\n    project varchar(255),\n    );\n   ```\n7. In Query Editor create few rows:  \n   ```\n    INSERT INTO Partners (email, name, project) VALUES ('Partner1@mtc.com', 'Partner One', 'Azure Cloud')\n    INSERT INTO Partners (email, name, project) VALUES ('Partner2@mtc.com', 'Partner Two', 'Microsoft Office 365')\n    INSERT INTO Partners (email, name, project) VALUES ('Partner3@mtc.com', 'Partner Three', 'Microsoft Power Apps')\n   ```\n\n## Create a Key Vault \n1. Go to Azure Portal\n2. Create a Key Vault\n3. In Key Vault details:\n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **containerappdemo** resource group\n    - ```Key vault name``` - Choose a unique key vault name\n    - ```Region``` - West Europe\n    - ```Pricing tier``` - Standard\n    - ```Days to retain deleted values``` - 7\n4. Review + Create\n5. Hit Create \n\n\n## Store the database credentials in Key Vault \n1. Go to Your Key Vault \n2. In Key Vault Blade select Secrets \n3. Select Generate/Import \n    - ```Name``` - dbpassword\n    - ```Secret value``` - Enter your database password\n4. Select Generate/Import\n    - ```Name``` - dbuser\n    -   ```Secret value``` - Enter your database user\n5. Select Generate/Import\n    - ```Name``` - dbserver\n    -   ```Secret value``` - Enter your database server url - for example <serverName>.database.windows.net\n6. Select Generate/Import\n    - ```Name``` - dbdatabase\n    -   ```Secret value``` - Enter your database name - for example partnersdb\n7. Hit Create  \n\n## Create a SPN \n1. Open a Cloud Shell\n2. Create a Service Principal with a scope for your resource group\n   ```bash\n   az ad sp create-for-rbac \\\n    --name SERVICE-PRINCIPAL-NAME \\\n    --role Contributor \\\n    --scopes \"/subscriptions/<SUBSCRIPTION_NAME_OR_ID>/resourceGroups/containerappdemo\"\n   ```\n3. Capture and save the service principal output results of the command to use later\n   ```bash\n    {\n    \"appId\": \"YOUR-APP-ID-VALUE\",\n    \"displayName\": \"YOUR-SERVICE-PRINCIPAL-DISPLAY-NAME\",\n    \"name\": \"YOUR-SERVICE-PRINCIPAL-NAME\",\n    \"password\": \"!@#$%\",\n    \"tenant\": \"YOUR-TENANT-ID\"\n    }\n   ```\n4. Give your service principal access to your keyvault \n   ```bash\n   az keyvault set-policy \\\n    --subscription REPLACE-WITH-YOUR-SUBSCRIPTION-NAME-OR-ID \\\n    --resource-group RESOURCE-GROUP-NAME \\\n    --name \"REPLACE-WITH-YOUR-KEY-VAULT-NAME\" \\\n    --spn YOUR-SERVICE-PRINCIPAL-APP-ID \\\n    --secret-permissions get list\n   ```\n5. This service principal will only be able to list all secrets or get a specific secret. You can see this service principal in the Azure portal for your Key Vault\n\n## Create ACR for Docker Images \n1. Go to Azure Portal\n2. Create a Container Registry\n3. In Registry details:  \n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **containerappdemo** resource group\n    - ```Registry Name``` - Choose a unique name \n    - ```Location``` - Choose West Europe\n    - ```SKU``` - Standard\n4. Review + create\n5. Create\n\n## Dockerizing a Node.js Application\n1. Open Cloud Shell\n2. Clone the git repo\n3. Open the project - ./nodejs_containerapp_backend_partners_api\n4. Change the configuration in database/connection.js file \n5. Build the image and push it to your ACR:\n   ```bash\n   az acr build --image partners:0.1 --registry <registryName> --file Dockerfile .\n   ```\n\n\n## Create a Container App\n1. Go to Azure Portal\n2. Create a Container App\n3. In Container App details:\n   - ```Subscription``` - Choose your subscription\n   - Choose **containerappdemo** resource group\n   - ```Container app name``` - partners\n   - ```Region``` - Choose West Europe\n   - ```Container Apps Environment``` - Create new\n     - ```Environment name``` - partnersenv\n     - ```Zone redundancy``` - Disabled\n     - Hit Create\n   - Hit Next: App settings\n   - Uncheck - Use quickstart image\n   - Container Details:\n     - ```Name``` - partners\n     - ```Image source``` - Azure Container Registry\n     - ```Registry``` - Select the registry you created in the last step ( If you get an Error \"Cannot access ACR because admin credentials on the ACR are disabled - Go to ACR and enbaled it - In ACR hit Update and check the Admin user box and hit Save)\n     - Select the image: partners \n     - Select the Image tag\n     - ```Application ingress settings``` - \n       - Check the Ingress Options\n       - Check the Accepting traffic from anywhere\n       - ```Target port``` - 8080\n     - Hit Create\n\n## Store SPN details in Container App Secrets \n1. Open your container app \n2. In Container App blaid select Secrets \n3. Click +Add \n   - ```Key``` - azureclientid \n   - ```Value``` - Client Id\n   - Click Add\n4. Click +Add \n   - ```Key``` - azuretenantid \n   - ```Value``` - Tenant Id\n   - Click Add\n5. Click +Add \n   - ```Key``` - azureclientsecret \n   - ```Value``` - Client Id Secret \n   - Click Add \n\n## Reference Secrets with Environment variables\n1. Open your container app \n2. In Container App blaid select Containers\n3. Choose Edit and Deploy \n4. Choose your container image \n5. Go down to Environment variables\n6. Reference an environment variable to a secret\n  - azureclientid\n  - azuretenantid\n  - azureclientsecret\n7. Hit Save \n8. Hit Create\n9. You should run the same revision that you have set your environment variables to\n\n## Check your application \n1. Open your container app \n2. In Container App blaid select Overview \n3. Open Application Url \n4. Go to <Application Url>/api/partners  \n    <img src=\"./assets/API_OUTPUT.png\" alt=\"API OUTPUT\" width=\"500\"/>    \n\n## Hosting Front End application on Azure Blob Storage\n1. Go to Azure Portal\n2. Create a resource - Storage account \n3. In Storage account details:  \n    - ```Subscription``` - Choose your subscription\n    - ```Resource Group``` - Choose **containerappdemo** resource group\n    - ```Storage account name``` - Choose a unique name \n    - ```Region``` - Choose West Europe\n    - ```Redundancy``` - LRS\n4. Hit Review \n5. Hit Create\n6. Go to resource\n7. In left navigation panel choose ***Static website***\n8.  in Static Website details:\n   - ```Static website``` - Enabled\n   - ```index document name``` - index.html\n   - ```Error document path``` - index.html\n9.  Hit Save \n10. Copy the Primary Endpoint url \n11. Go your storage account and select ***Storage browser***\n12. In Blob containers select $web - It is the place where you upload a website.  \n13. Clone the git repo\n14. Open the project - ./react_frontend_website_no_authentication\n15. Edit Api.js file and change the baseUrl to your container app url \n16. Run ```npm install```\n17. Run ```npm run build``` - Build a project and store it in a build directory\n18. Upload all files to $web directory in a storage account \n19. In left navigation panel choose ***Static website***\n20. Copy the ***Primary endpoint*** url and test in your web browser\n21. Test the Web  \n <img src=\"./assets/FRONT_END_UI.png\" alt=\"FRONT_END_UI\" width=\"800\"/>    \n", "repo_name": "MTC_IL_WORKSHOP_Container_Apps_With_KeyVault", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_Container_Apps_With_KeyVault", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_Container_Apps_With_KeyVault", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_Container_Apps_With_KeyVault", "platform": "github", "language": "JavaScript", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# SARIF SAST Scans Tab\n\nSee `vss-extension.md` for public facing info. This is for contributors/developers.\n\n\n### Development: DevOps Dev\n* `npm run publish-dev`\n* go to: `https://dev.azure.com/jeffkingms/Project%20Zero/_workitems/edit/1/`\n\n\n### Development: DevOps BaseUri\n* `npm run publish-dev` with `baseUri`\n* go to: `https://localhost:8080` and bypass the chrome warning.\n* go to: `https://dev.azure.com/jeffkingms/Project%20Zero/_workitems/edit/1/`\n\n\n### Deployment\nVerify `vss-extension.prod.json` property `version` is incremented. If not, you risk overriting an old `vsix`.\n```\nnpx webpack\nnpx tfx extension create --output-path: vsix --overrides-file vss-extension.prod.json\n```\n\nThis creates a file in your `./vsix` folder named `sariftools.scans-0.1.0.vsix` (version number will differ).\n\nUpload the `vsix` file to `https://marketplace.visualstudio.com/manage/publishers/YOUR_PUBLISHER_ID`. On that page, find the matching extension, choose `\u22ef`, and choose `Update`.\n\nRemember to commit any `vss-extension.json` `version` changes.\n\n### New API\nimport('azure-devops-extension-sdk').init() results in \"No handler found on any channel for message\"\nand \"Error: Cannot get registered instance for : JeffKingO.scans-dev.workitem-tab\"\n\n```\nimport * as SDK from 'azure-devops-extension-sdk'\nimport { IWorkItemFormService, WorkItemTrackingServiceIds } from 'azure-devops-extension-api/WorkItemTracking'\n\nSDK.init({\n\tapplyTheme: true,\n\tloaded: true,\n})\n;(async () => {\n\tawait SDK.ready()\n\tconsole.info('Version', SDK.getExtensionContext().version)\n\n\tconst workItem = await SDK.getService<IWorkItemFormService>(WorkItemTrackingServiceIds.WorkItemFormService)\n\tconst relations = await workItem.getWorkItemRelations()\n\tconsole.log(relations)\n})()\n```", "repo_name": "sarif-azuredevops-extension", "org_name": "microsoft", "org_repo": "microsoft/sarif-azuredevops-extension", "platform_org_repo": "github+microsoft/sarif-azuredevops-extension", "link_to_repo": "https://github.com/microsoft/sarif-azuredevops-extension", "platform": "github", "language": "TypeScript", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Databricks SQL on Azure Workshop [Japanese]\nDatabricks SQL \u3068 Synapse \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 & \u30c7\u30fc\u30bf \u30d5\u30ed\u30fc\u3092\u4e2d\u5fc3\u3068\u3059\u308b Microsoft Azure \u306e\u30c7\u30fc\u30bf\u5206\u6790\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u306e\u69cb\u7bc9\u306e\u30ce\u30a6\u30cf\u30a6\u3092\u5b66\u7fd2\u3059\u308b\u305f\u3081\u306e\u30ef\u30fc\u30af\u30b7\u30e7\u30c3\u30d7\u306e\u8cc7\u6750\u7ba1\u7406\u7528\u306e GitHub \u30ea\u30dd\u30b8\u30c8\u30ea\u3067\u3059\u3002\u4ee5\u4e0b URL \u3088\u308a\u30ef\u30fc\u30af\u30b7\u30e7\u30c3\u30d7\u3092\u958b\u59cb\u3067\u304d\u307e\u3059\u3002\n\nhttps://microsoft.github.io/azure-databricks-sql-workshop-ja/\n\n## \u30ef\u30fc\u30af\u30b7\u30e7\u30c3\u30d7\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u66f4\u65b0\u65b9\u6cd5 (Contributor \u5411\u3051\u306e\u60c5\u5831)\n\u672c\u30ef\u30fc\u30af\u30b7\u30e7\u30c3\u30d7\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306f Markdown \u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb (`workshop.md`) \u3092 [claat (Google Codelabs command line tool)](https://github.com/googlecodelabs/tools/tree/main/claat) \u3067\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u3057\u3066\u4f5c\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\n\n1. \u4ee5\u4e0b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u53c2\u8003\u306b\u3057\u3066 claat \u3092\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u3059\u308b\n    - https://zenn.dev/nakazax/articles/18506708b5eea9\n2. `workshop.md` \u3092\u66f4\u65b0\u3059\u308b\n3. \u30bf\u30fc\u30df\u30ca\u30eb\u3067 `claat export workshop.md` \u3092\u5b9f\u884c\u3057 `docs/` \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u914d\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u7fa4\u304c\u66f4\u65b0\u3055\u308c\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\n4. \u30bf\u30fc\u30df\u30ca\u30eb\u3067 `claat serve docs/` \u3092\u5b9f\u884c\u3057\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u78ba\u8a8d\u3059\u308b\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Security\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described [SECURITY.md](SECURITY.md).\n", "repo_name": "azure-databricks-sql-workshop-ja", "org_name": "microsoft", "org_repo": "microsoft/azure-databricks-sql-workshop-ja", "platform_org_repo": "github+microsoft/azure-databricks-sql-workshop-ja", "link_to_repo": "https://github.com/microsoft/azure-databricks-sql-workshop-ja", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# Project Overview\n\nAn application to provide a streamlined interface for various ways of initiating a virtual visit with a person or group of people.\n\n## Use Cases\n\nWe built this project to meet multiple needs around connecting with patients through audio/video calls.\n\n### Streamlined \"meet now\" for virtual visits\n\nWhen trying to meet virtually with someone, there are multiple channels of engagement, with potentially multiple screens/interfaces involved in setting up and calling/inviting the participant and additional parties (e.g. interpreter, additional family member). There is a desire to have a \"simple, single interface\" to make engaging in a call easy, without detailed knowledge of how the communication platform works and the nuances of scheduling these different types of engagement.\n\nThe application makes it easy to dial out via Teams call, or invite multiple parties via SMS/email to the virtual visit and join that call.\n\n### Launch from EMR applications\n\nFor a healthcare proviers, clinicians, and staff, they want to be able to initiate calls from the context of a patient record or specific encounter in the EMR (desktop, mobile, tablet).\n\nThe application receives patient contact information (e.g. phone number, email address) directly from the EMR, reducing effort and potential errors involved with copy/paste and entry.\n\n## Power Platform Application sample app\n\n![Screenshot](./images/power-app-image.png)\n\n### Prerequisites\n\nThe following services are leveraged in the sample solution and may be part of your implementation. For specific questions regarding existing licensing, coverage, or to add services, contact your Microsoft account team.\n\n#### Power Platform\n\n- Get access to a [Power Platform](https://docs.microsoft.com/en-us/power-platform/) environment with Dataverse provisioned and Power Apps premium licenses for users\n- Deploy a custom connector to the Power Platform environment when using the Bookings or Advanced Virtual Appointments [Sample connector](https://github.com/microsoft/Virtual-Visit-Sample-Connector)\n\n#### Microsoft Teams\n\n- Microsoft Bookings will need to be configured when Bookings are used for Virtual Appointment scheduling\n- Microsoft [Teams Premium](https://learn.microsoft.com/en-us/microsoftteams/enhanced-teams-experience#advanced-virtual-appointments) will be required when using the Advanced Virtual Appointments functionality\n- Microsoft Teams with [voice solution](https://learn.microsoft.com/en-us/microsoftteams/cloud-voice-landing-page) will need to be configured when using Teams for direct outbound calling\n\n### Exploring the sample app\n\nDeploy the latest ***Virtual Visit Custom Launcher*** solution file ([/soluiton-file-history](./solution-file-history)) to your Dataverse environment, then Publish and run the *Connect Virtually* app.\n\nExplore the configuration of the solution components via the maker portal. Review the Power Apps canvas and Power Automate workflow used in the sample app.\n\nTry launching the *Connect Virtually* app from its URL and include one or both of the following parameters to simulate launch-from EMR:\n\n- To pass a phone number, use the `personPhone` parameter. e.g. `&personPhone=8885551234` \n- To pass an email address, use the `personEmail` parameter. e.g. `&personEmail=jane@sample.com`\n\n> #### Managed or Unmanaged solutions?\n> \n> Both Managed and Unmanaged solutions are available in this repository, as well as the raw source code.\n>  - The managed solution offers a one-step uninstallation of all components, but requires more steps to review/edit it in the Power Apps Maker Studio\n>  - The unmanaged solution can be reviewed/edited immediately once imported, but will require components to be individually deleted if you wish to remove them from the environment later\n>  - The source code folder [/src](/src) is intended for advanced makers (e.g. pro-developers) who prefer to build their own solution or inspect the raw detail that makes up the solution.\n \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Virtual-visit-custom-launcher", "org_name": "microsoft", "org_repo": "microsoft/Virtual-visit-custom-launcher", "platform_org_repo": "github+microsoft/Virtual-visit-custom-launcher", "link_to_repo": "https://github.com/microsoft/Virtual-visit-custom-launcher", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Kiota Abstractions Library for Ruby\n\n![Ruby](https://github.com/microsoft/kiota-abstractions-ruby/actions/workflows/ruby.yml/badge.svg)\n\nThe Kiota abstractions Library for go is the go library defining the basic constructs Kiota projects need once an SDK has been generated from an OpenAPI definition.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to the abstraction package to build and run.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the abstractions library\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem \"microsoft_kiota_abstractions\", \"0.12.0\"\n```\n\nAnd then execute:\n\n```shell\nbundle install\n```\n\nOr install it yourself as:\n\n ```shell\ngem install microsoft_kiota_abstractions --version \"0.12.0\"\n ```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-abstractions-ruby", "org_name": "microsoft", "org_repo": "microsoft/kiota-abstractions-ruby", "platform_org_repo": "github+microsoft/kiota-abstractions-ruby", "link_to_repo": "https://github.com/microsoft/kiota-abstractions-ruby", "platform": "github", "language": "Ruby", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Kiota OAuth authentication provider library for Ruby\n\n![Ruby](https://github.com/microsoft/kiota-authentication-oauth-ruby/actions/workflows/ruby.yml/badge.svg)\n\nThe Kiota OAuth authentication provider library for Ruby is the authentication provider implementation with [OAuth2](https://rubygems.org/gems/oauth2).\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a authentication provider library to authenticate HTTP requests to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the OAuth library\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem \"microsoft_kiota_authentication_oauth\", \"0.6.0\"\n```\n\nAnd then execute:\n\n```shell\nbundle install\n```\n\nOr install it yourself as:\n\n```shell\ngem install microsoft_kiota_authentication_oauth --version \"0.6.0\"\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-authentication-oauth-ruby", "org_name": "microsoft", "org_repo": "microsoft/kiota-authentication-oauth-ruby", "platform_org_repo": "github+microsoft/kiota-authentication-oauth-ruby", "link_to_repo": "https://github.com/microsoft/kiota-authentication-oauth-ruby", "platform": "github", "language": "Ruby", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Kiota Json Serialization Library for Ruby\n\n![Ruby](https://github.com/microsoft/kiota-serialization-json-ruby/actions/workflows/ruby.yml/badge.svg)\n\nThe Json Serialization Library for Ruby is the Ruby JSON serialization library implementation.\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a json serialization package to handle json payloads from an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the Serialization JSON Library\n\n## Installation\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem \"microsoft_kiota_serialization_json\", \"0.7.0\"\n```\n\nAnd then execute:\n\n```shell\nbundle install\n```\n\nOr install it yourself as:\n\n```shell\ngem install microsoft_kiota_serialization_json --version \"0.7.0\"\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-json-ruby", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-json-ruby", "platform_org_repo": "github+microsoft/kiota-serialization-json-ruby", "link_to_repo": "https://github.com/microsoft/kiota-serialization-json-ruby", "platform": "github", "language": "Ruby", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Kiota Http Library for Ruby\n\n![Ruby](https://github.com/microsoft/kiota-http-ruby/actions/workflows/ruby.yml/badge.svg)\n\nThe Kiota HTTP Library for Ruby is the Ruby HTTP library implementation with [Faraday](https://github.com/lostisland/faraday).\n\nA [Kiota](https://github.com/microsoft/kiota) generated project will need a reference to a HTTP package to make HTTP requests to an API endpoint.\n\nRead more about Kiota [here](https://github.com/microsoft/kiota/blob/main/README.md).\n\n## Using the http library\n\nAdd this line to your application's Gemfile:\n\n```ruby\ngem \"microsoft_kiota_faraday\", \"0.11.0\"\n```\n\nAnd then execute:\n\n```shell\nbundle install\n```\n\nOr install it yourself as:\n\n ```shell\ngem install microsoft_kiota_faraday --version \"0.11.0\"\n ```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-http-ruby", "org_name": "microsoft", "org_repo": "microsoft/kiota-http-ruby", "platform_org_repo": "github+microsoft/kiota-http-ruby", "link_to_repo": "https://github.com/microsoft/kiota-http-ruby", "platform": "github", "language": "Ruby", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-terraform-storage-datalifecycle", "org_name": "microsoft", "org_repo": "microsoft/azure-terraform-storage-datalifecycle", "platform_org_repo": "github+microsoft/azure-terraform-storage-datalifecycle", "link_to_repo": "https://github.com/microsoft/azure-terraform-storage-datalifecycle", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Project\n\nWelcome to Microsoft Purview Advanced Rich Reports (MPARR) Collector.\n\nToday have the right information in the right moment can be a great business value, we are talking about implement security and compliance, one of the most importance things are understanding that this accomplishment is a business goal, in that order of ideas, to have reports, business-friendly, to see how our end users are using and adopting these tools is a global benefit. In that order of ideas, this solution take the information available under the Microsoft 365 services and give the capabilities to present this information to different business units, given the capacity to c-level users have access to business metrics related to compliance.\n\n![MPARR - Achitecture Overview](https://user-images.githubusercontent.com/44684110/215555123-019ddb3f-a25d-4b88-bb59-7e204d5e1c6c.png)\n<p align=\"center\">Current Architecture for MPARR</p>\n\nToday one of the principal challenges in all organizations is stay align with the Compliance principles, each organization define their own priorities, and policies definitions. But, in all the cases they need to involve the complete organization, and to involve we need to show the right information at the right time.\nOffice 365 Management API collect all the information available through Unified Auditing tool, this helps to Security, Compliance and IT areas looking for some specific information and generate some reports but is not possible easily to show that information to different business units, and they don\u2019t have the time neither to prepare more detailed reports.\n\n![MPARR - Dependency Model](https://user-images.githubusercontent.com/44684110/215555376-4daa3589-dfc6-4ce9-b7ed-014201861827.png)\n<p align=\"center\">Variables needed to set in laconfig.json files and the TableNames created on Logs Analytics</p>\n \nIn that order of ideas, the solution presented next permit to have a robust solution to collect all the data and prepare reports with specific scopes to specific audiences, without require special permissions or additional knowledge to understand the security tools.\nThis solution collects all the information available through Office 365 Management API and store this information on Logs Analytics workspace, this one can be the same used for Sentinel (we will discuss more this point next), from this workspace the information can be consumed using Power BI desktop to create advanced rich reports to publish then with Power BI online workspaces, this step permit to generate different workspaces for different audiences. To give more added value to these reports, some additional scrips are delivered, to collect as example the data related to Azure AD attributes, this one permit to create reports based on location, country, business units and any other Azure AD attribute available.\n\n![MPARR - Information  collected](https://user-images.githubusercontent.com/44684110/215555738-c7db620a-5a3d-4c1d-92aa-43b5b66c5148.png)\n<p align=\"center\">TableNames created on Logs Analytics and use</p>\n \nAs we said previously, because this information can be stored on the same workspace used for Sentinel, this information can be utilized to generate workbooks with more detailed information for Security monitoring.\nIn this article, we will see how we can implement this script to start collecting the information and consume that information.\n\nSome Power BI reports that can be created:\n\n![MPARR - DLP overview](https://user-images.githubusercontent.com/44684110/215560498-5438d724-baf1-4a03-aea2-3cce1d7a88c5.png)\n<p align=\"center\">DLP Overview, department filter and dates filter</p>\n\n![MPARR - External access to protected documents](https://user-images.githubusercontent.com/44684110/215561276-6c867471-91f9-43da-929e-519cb1f3ee35.png)\n<p align=\"center\">External access to protected documents</p>\n\n![MPARR - Unified Labeling Overview](https://user-images.githubusercontent.com/44684110/215561587-7e9507a7-b6b6-46a0-b950-abd7450bc2a0.png)\n<p align=\"center\">Unified Labeling Overview filtering by Deparment and Country</p>\n\n![MPARR - Worldwide Operations review](https://user-images.githubusercontent.com/44684110/215561913-5dd632d1-fdbc-4eaf-8a36-13c3c5773791.png)\n<p align=\"center\">Worlwdide activities filtering by Operation</p>\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Microsft-Purview-Advanced-Rich-Reports-MPARR-Collector", "org_name": "microsoft", "org_repo": "microsoft/Microsft-Purview-Advanced-Rich-Reports-MPARR-Collector", "platform_org_repo": "github+microsoft/Microsft-Purview-Advanced-Rich-Reports-MPARR-Collector", "link_to_repo": "https://github.com/microsoft/Microsft-Purview-Advanced-Rich-Reports-MPARR-Collector", "platform": "github", "language": "PowerShell", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# WUfB Reports Access Control\n\n## Overview\n\nThis solution uses [Ansible](https://www.ansible.com) to deploy WUfB reports infrastructure for partitioning data and securing access using Azure RBAC. As an administrator you can define one or more *scopes* to indicate that records for a given Azure AD group of devices should be exported from the tenant workspace to a scoped workspace that can be secured using Azure RBAC.\n\nThe `ansible` folder of the project contains an Ansible playbook which, after configuration, can deploy the necessary infrastructure to your Azure tenant.\n\nAt the high-level, this includes the following resources:\n\n1. Azure resources\n   1. [Resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal) to contain resources generated by the automation\n   2. [Azure Function](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview) triggered on an interval to perform periodic data export\n2. Log Analytics resources\n   1. [Log Analytics workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview) for each scope\n3. Azure Monitor resources\n   1. [Data collection endpoint](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-endpoint-overview) ingesting data\n   2. [Data collection rule](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-rule-overview) for each scoped workspace to direct data routing\n\nAll access groups will use the same Azure Monitor Workbook. WUfB reports has been updated to provide a drop-down workspace chooser.\n\n### Architecture\n\n![Architecture Diagram](docs/attachments/wufb-reports-access-control-architecture.drawio.png)\n\n### Permissions required for deployment\n\nThe user or [service principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals#service-principal-object) running the automation to deploy infrastructure should be an Azure subscription [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#owner) to ensure necessary providers and resources can be enabled automatically.\n\n### Current limitations\n\n- **Aggregated delivery optimization status is not computed.** Aggregated status is tenant-wide in the primary workspace and therefore scoped workspaces would need to compute the aggregate for their device set separately. The Azure Function doesn't yet perform that process.\n- **No support for nested Azure AD groups.** Only direct group members are considered at present. Nested support could be added by modifying the Azure Function.\n- **The solution assumes a single subscription and target resource group.** This may or may not be relevant for your tenant. Tenants that need greater control can extend the solution by modifying the Ansible project.\n\n## Configuration\n\nBefore running the deployment, you must tailor the configuration to your tenant by copying [ansible/host_vars/localhost.example](ansible/host_vars/localhost.example) to `ansible/host_vars/localhost` and then editing the values in the file.\n\n```yaml\n---\n# Required configuration\n\n## Tenant info\ntenant_id: <guid>\nsubscription_id: <guid>\n\n## Define access scopes and associated Azure AD group (identified by objectId)\nscopes:\n  contoso:\n    azure_ad_group_id: <guid>\n  fabrikam:\n    azure_ad_group_id: <guid>\n\n## Properties for the primary workspace to be used as input for data transformations\nprimary_workspace_name: <name>\nprimary_workspace_resource_group: <resource_group>\n\n## All resources will be deployed to this location (e.g. westus2)\ntarget_resource_location: <location>\n\n# Optional configuration\n\n## All resources will be created under this group\ntarget_resource_group: wufb-reports-access-control\n\n## All resources will use this prefix and will be suffixed by the\n## playbook with a resource type or scope.\ntarget_resource_prefix: wufb-reports-\n\n## Storage account for function app\nfunction_app_storage_account_name: wufbreportsscopes\n\n## Maximum number of days to sync.\n## Not currently recommended to increase beyond 2 because LA overwrites TimeGenerated if > 48 hours.\nfunction_app_max_days_to_sync: 2\n\n# Python 3.11 has breaking changes to the `inspect`` module. Use 3.10 for now.\nansible_python_interpreter: \"/usr/local/bin/python3.10\"\n```\n\n### Required parameters\n\n| Parameter                        | Description                                                                                                                                                                                                                               |\n| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| tenant_id                        | The GUID for your tenant.                                                                                                                                                                                                                 |\n| subscription_id                  | The GUID for the subscription that resources will be created in.                                                                                                                                                                          |\n| scopes                           | This is a dictionary of the scopes you want to be created. Each named scope is associated with the directory `id` of an Azure AD group containing a list of devices included within the scope. Nested groups are not currently supported. |\n| primary_workspace_name           | This is the primary workspace containing all data for the tenant that data will be exported from.                                                                                                                                         |\n| primary_workspace_resource_group | The resource group containing the primary workspace.                                                                                                                                                                                      |\n| target_resource_location         | The Azure location or [region](https://azure.microsoft.com/en-us/explore/global-infrastructure/geographies/#overview) to store created resources.                                                                                         |\n\n### Optional parameters\n\n| Parameter                         | Description                                                                                                                                                                                      |\n| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| target_resource_group             | The resource group to contain created resources.                                                                                                                                                 |\n| target_resource_prefix            | The resource prefix for all created resources.                                                                                                                                                   |\n| function_app_storage_account_name | The globally unique name of the storage account for the function app. A portion of the tenant ID is also used to facilitate uniqueness.                                                          |\n| function_app_max_days_to_sync     | The maximum number of days to sync back in time when the function is executed. Values greater than 48 hours overwrite the TimeGenerated field at the target which could result in unusable data. |\n| ansible_python_interpreter        | Path to the Python interpreter to use when running Ansible.                                                                                                                                      |\n\n## Installation of required tools\n\n### Ansible\n\nTo install Ansible, see [Ansible installation](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html).\n\n#### Azure collection\n\nAfter Ansible is installed, ensure all requirements of the [Azure Collection](https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.html) have been installed.\n\n```bash\nansible-galaxy collection install azure.azcollection\npip install -r ~/.ansible/collections/ansible_collections/azure/azcollection/requirements-azure.txt\n```\n\nIf you installed Ansible core, you will need to install the following additional collections:\n\n- [Ansible.utils](https://docs.ansible.com/ansible/latest/collections/ansible/utils/index.html)\n- [Community.general](https://docs.ansible.com/ansible/latest/collections/community/general/index.html)\n\n### Node.js 18\n\nInstall the latest version of [Node.js](https://nodejs.org/en/).\n\n### Azure CLI\n\nInstall the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/).\n\n### Azure Functions Core Tools\n\nInstall the [Azure Functions Core Tools](https://github.com/Azure/azure-functions-core-tools).\n\n#### Windows\n\n```bash\nnpm i -g azure-functions-core-tools@4 --unsafe-perm true\n```\n\n#### Mac\n\n```bash\nbrew tap azure/functions\nbrew install azure-functions-core-tools@4\n```\n\n## Authorization\n\n### Login to Azure\n\n- For authentication with Azure you can pass parameters, set environment variables, use a profile stored in ~/.azure/credentials, or log in before you run your tasks or playbook with `az login`.\n- Authentication is also possible using a service principal or Active Directory user.\n- To authenticate via service principal, pass subscription_id, client_id, secret and tenant or set environment variables AZURE_SUBSCRIPTION_ID, AZURE_CLIENT_ID, AZURE_SECRET and AZURE_TENANT.\n- To authenticate via Active Directory user, pass ad_user and password, or set AZURE_AD_USER and AZURE_PASSWORD in the environment.\n- Alternatively, credentials can be stored in `~/.azure/credentials`. This is an ini file containing a `[default]` section and the following keys: subscription_id, client_id, secret and tenant or subscription_id, ad_user and password. It is also possible to add additional profiles. Specify the profile by passing profile or setting AZURE_PROFILE in the environment.\n\nSee [Authenticating with Azure](https://docs.ansible.com/ansible/latest/scenario_guides/guide_azure.html#authenticating-with-azure) for more information.\n\n## Run the playbook\n\nFrom the `ansible` folder, the following command will run the playbook to initiate deployment of infrastructure. You must have previously logged into Azure and also configured values for your tenant, including the target scopes, by modifying the `localhost` configuration file.\n\n```bash\nansible-playbook -i inventory site.yml\n```\n\n### Optional: Run for separate roles\n\nSome organizations may wish to deploy the infrastructure in separate steps according to access role. Therefore, certain project tasks have been tagged, and Ansible allows for skipping or running a playbook with matching tags. The following commands can be used to run steps for each role individually.\n\n#### Create resources as subscription owner\n\nAfter authenticating with a user that has Azure subscription *Owner* permissions, the following command will create necessary resources but will skip the Microsoft Graph authorization step which requires a different role.\n\n```bash\nansible-playbook -i inventory site.yml --skip-tags \"function_app_authorize_graph\"\u00a0\n```\n\n#### Grant Graph API permissions\n\nAfter authenticating with a user that has *Privileged Role Administrator* or *Global Administrator* Azure AD roles, the following command will authorize the Azure Function to call the Microsoft Graph for enumerating device group membership.\n\nYou will need to replace `<objectId>` which you can obtain from the Azure Portal for the `wufb-scope-connector` Function App under its `Function App > Settings > Identity > Object (principal) ID` property.\n\n```bash\nansible-playbook -i inventory site.yml --tags \"function_app_authorize_graph\" --extra-vars '{\"scope_connector_identity\":{\"principalId\": \"<objectId>\"}}'\n```\n\n## Using the solution\n\nAfter the solution has been deployed, the Azure Function must be triggered once to export data from the primary workspace to the scoped workspaces.\n\nOnce the Log Analytics workspaces have data, users will be able to open the WUfB reports workbook to see data scoped to their access.\n\nThe planned experience is to provide a drop-down allowing users to change the active workspace used by the workbook by selecting from a list of available subscriptions and their workspaces.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft  trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\n\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "wufb-reports-access-control", "org_name": "microsoft", "org_repo": "microsoft/wufb-reports-access-control", "platform_org_repo": "github+microsoft/wufb-reports-access-control", "link_to_repo": "https://github.com/microsoft/wufb-reports-access-control", "platform": "github", "language": "TypeScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "<img src=\"https://www.openstack.org/assets/kata/kata-vertical-on-white.png\" width=\"150\">\n\n# Kata Containers\n\nWelcome to Kata Containers!\n\nThis repository is the home of the Kata Containers code for the 2.0 and newer\nreleases.\n\nIf you want to learn about Kata Containers, visit the main\n[Kata Containers website](https://katacontainers.io).\n\n## Introduction\n\nKata Containers is an open source project and community working to build a\nstandard implementation of lightweight Virtual Machines (VMs) that feel and\nperform like containers, but provide the workload isolation and security\nadvantages of VMs.\n\n## License\n\nThe code is licensed under the Apache 2.0 license.\nSee [the license file](LICENSE) for further details.\n\n## Platform support\n\nKata Containers currently runs on 64-bit systems supporting the following\ntechnologies:\n\n| Architecture | Virtualization technology |\n|-|-|\n| `x86_64`, `amd64` | [Intel](https://www.intel.com) VT-x, AMD SVM |\n| `aarch64` (\"`arm64`\")| [ARM](https://www.arm.com) Hyp |\n| `ppc64le` | [IBM](https://www.ibm.com) Power |\n| `s390x` | [IBM](https://www.ibm.com) Z & LinuxONE SIE |\n\n### Hardware requirements\n\nThe [Kata Containers runtime](src/runtime) provides a command to\ndetermine if your host system is capable of running and creating a\nKata Container:\n\n```bash\n$ kata-runtime check\n```\n\n> **Notes:**\n>\n> - This command runs a number of checks including connecting to the\n>   network to determine if a newer release of Kata Containers is\n>   available on GitHub. If you do not wish this to check to run, add\n>   the `--no-network-checks` option.\n>\n> - By default, only a brief success / failure message is printed.\n>   If more details are needed, the `--verbose` flag can be used to display the\n>   list of all the checks performed.\n>\n> - If the command is run as the `root` user additional checks are\n>   run (including checking if another incompatible hypervisor is running).\n>   When running as `root`, network checks are automatically disabled.\n\n## Getting started\n\nSee the [installation documentation](docs/install).\n\n## Documentation\n\nSee the [official documentation](docs) including:\n\n- [Installation guides](docs/install)\n- [Developer guide](docs/Developer-Guide.md)\n- [Design documents](docs/design)\n  - [Architecture overview](docs/design/architecture)\n  - [Architecture 3.0 overview](docs/design/architecture_3.0/)\n\n## Configuration\n\nKata Containers uses a single\n[configuration file](src/runtime/README.md#configuration)\nwhich contains a number of sections for various parts of the Kata\nContainers system including the [runtime](src/runtime), the\n[agent](src/agent) and the [hypervisor](#hypervisors).\n\n## Hypervisors\n\nSee the [hypervisors document](docs/hypervisors.md) and the\n[Hypervisor specific configuration details](src/runtime/README.md#hypervisor-specific-configuration).\n\n## Community\n\nTo learn more about the project, its community and governance, see the\n[community repository](https://github.com/kata-containers/community). This is\nthe first place to go if you wish to contribute to the project.\n\n## Getting help\n\nSee the [community](#community) section for ways to contact us.\n\n### Raising issues\n\nPlease raise an issue\n[in this repository](https://github.com/kata-containers/kata-containers/issues).\n\n> **Note:**\n> If you are reporting a security issue, please follow the [vulnerability reporting process](https://github.com/kata-containers/community#vulnerability-handling)\n\n## Developers\n\nSee the [developer guide](docs/Developer-Guide.md).\n\n### Components\n\n### Main components\n\nThe table below lists the core parts of the project:\n\n| Component | Type | Description |\n|-|-|-|\n| [runtime](src/runtime) | core | Main component run by a container manager and providing a containerd shimv2 runtime implementation. |\n| [runtime-rs](src/runtime-rs) | core | The Rust version runtime. |\n| [agent](src/agent) | core | Management process running inside the virtual machine / POD that sets up the container environment. |\n| [`dragonball`](src/dragonball) | core | An optional built-in VMM brings out-of-the-box Kata Containers experience with optimizations on container workloads |\n| [documentation](docs) | documentation | Documentation common to all components (such as design and install documentation). |\n| [tests](https://github.com/kata-containers/tests) | tests | Excludes unit tests which live with the main code. |\n\n### Additional components\n\nThe table below lists the remaining parts of the project:\n\n| Component | Type | Description |\n|-|-|-|\n| [packaging](tools/packaging) | infrastructure | Scripts and metadata for producing packaged binaries<br/>(components, hypervisors, kernel and rootfs). |\n| [kernel](https://www.kernel.org) | kernel | Linux kernel used by the hypervisor to boot the guest image. Patches are stored [here](tools/packaging/kernel). |\n| [osbuilder](tools/osbuilder) | infrastructure | Tool to create \"mini O/S\" rootfs and initrd images and kernel for the hypervisor. |\n| [`agent-ctl`](src/tools/agent-ctl) | utility | Tool that provides low-level access for testing the agent. |\n| [`kata-ctl`](src/tools/kata-ctl) | utility | Tool that provides advanced commands and debug facilities. |\n| [`trace-forwarder`](src/tools/trace-forwarder) | utility | Agent tracing helper. |\n| [`runk`](src/tools/runk) | utility | Standard OCI container runtime based on the agent. |\n| [`ci`](https://github.com/kata-containers/ci) | CI | Continuous Integration configuration files and scripts. |\n| [`katacontainers.io`](https://github.com/kata-containers/www.katacontainers.io) | Source for the [`katacontainers.io`](https://www.katacontainers.io) site. |\n\n### Packaging and releases\n\nKata Containers is now\n[available natively for most distributions](docs/install/README.md#packaged-installation-methods).\nHowever, packaging scripts and metadata are still used to generate [snap](snap/local) and GitHub releases. See\nthe [components](#components) section for further details.\n\n## Glossary of Terms\n\nSee the [glossary of terms](https://github.com/kata-containers/kata-containers/wiki/Glossary) related to Kata Containers.\n", "repo_name": "kata-containers", "org_name": "microsoft", "org_repo": "microsoft/kata-containers", "platform_org_repo": "github+microsoft/kata-containers", "link_to_repo": "https://github.com/microsoft/kata-containers", "platform": "github", "language": "Rust", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Kata Containers tests\n\nThis repository contains various types of tests and utilities (called\n\"content\" from now on) for testing the [Kata Containers](https://github.com/kata-containers)\ncode repositories.\n\n## Getting the code\n\n```\n$ go get -d github.com/kata-containers/tests\n```\n\n## Test Content\n\nWe provide several tests to ensure Kata-Containers run on different scenarios\nand with different container managers.\n\n1. Integration tests to ensure compatibility with:\n   - [Kubernetes](https://github.com/kata-containers/tests/tree/main/integration/kubernetes)\n   - [Containerd](https://github.com/kata-containers/tests/tree/main/integration/containerd)\n2. [Stability tests](./stability)\n3. [Metrics](https://github.com/kata-containers/tests/tree/main/metrics)\n4. [VFIO](https://github.com/kata-containers/tests/tree/main/functional/vfio)\n\n## CI Content\n\nThis repository contains a [number of scripts](/.ci)\nthat run from under a \"CI\" (Continuous Integration) system.\n\n### Centralised scripts\n\nThe CI scripts in this repository are used to test changes to the content of\nthis repository. These scripts are also used by the other Kata Containers code\nrepositories.\n\nThe advantages of this approach are:\n\n- Functionality is defined once.\n  - Easy to make changes affecting all code repositories centrally.\n\n- Assurance that all the code repositories are tested in this same way.\n\nCI scripts also provide a convenient way for other Kata repositories to\ninstall software. The preferred way to use these scripts is to invoke `make`\nwith the corresponding `install-` target. For example, to install CRI-O you\nwould use:\n\n```\n$ make -C <path-to-this-repo> install-crio\n```\n\nUse `make list-install-targets` to retrieve all the available install targets.\n\n### CI setup\n\n> **WARNING:**\n>\n> The CI scripts perform a lot of setup before running content under a\n> CI. Some of this setup runs as the `root` user and **could break your developer's\n> system**. See [Developer Mode](#developer-mode).\n\n### Controlling the CI\n\n#### GitHub Actions\n\nKata Containers uses GitHub Actions in the [Kata Containers](https://github.com/kata-containers/kata-containers) repos.\nAll those actions, apart from the one to test `kata-deploy`, are automatically triggered when\na pull request is submitted. The trigger phrase for testing kata-deploy is `/test_kata_deploy`.\n\n#### Jenkins\n\nThe Jenkins configuration and most documentation is kept in the [CI repository](https://github.com/kata-containers/ci).\nJenkins is setup to trigger a CI run on all the slaves/nodes when a `/test` comment is added to a pull request. However,\nthere are some specific comments that are defined for specific CI slaves/nodes which are defined in the Jenkins\n`config.xml` files in the `<triggerPhase>` XML element in the [CI repository](https://github.com/kata-containers/ci).\n\n#### Specific Jenkins job triggers\n\nSome jobs like a particular distro, feature or architecture can be triggered individually, the specific job triggers information\ncan be found in the [Community repository](https://github.com/kata-containers/community/wiki/Controlling-the-CI).\n\n### Detecting a CI system\n\nThe strategy to check if the tests are running under a CI system is to see\nif the `CI` variable is set to the value `true`. For example, in shell syntax:\n\n```\nif [ \"$CI\" = true ]; then\n    : # Assumed to be running in a CI environment\nelse\n    : # Assumed to NOT be running in a CI environment\nfi\n```\n\n### Display verbose failure details\n\nBy default, when scripts in this repository fail, they display an\nerror message to `stderr` and return `1` (failure) to the shell.\nHowever, when [called by the CI](#detecting-a-ci-system), the scripts\nwill also dump a full stack trace and some environment details to `stderr`.\n\nTo force this behaviour outside of a CI environment, ensure the variable below is set:\n\n```bash\nexport KATA_TEST_VERBOSE=true\n```\n\n### Breaking Compatibility\n\nIn case the patch you submit breaks the CI because it needs to be tested\ntogether with a patch from another `kata-containers` repository, you have to\nspecify which repository and which pull request it depends on.\n\nUsing a simple tag `Depends-on:` in your commit message will allow the CI to\nrun properly. Notice that this tag is parsed from the latest commit of the\npull request.\n\nFor example:\n\n```\n\tSubsystem: Change summary\n\n\tDetailed explanation of your changes.\n\n\tFixes: #nnn\n\n\tDepends-on:github.com/kata-containers/kata-containers#999\n\n\tSigned-off-by: <contributor@foo.com>\n\n```\n\nIn this example, we tell the CI to fetch the pull request 999 from the `kata-containers`\nrepository and use that rather than the `main` branch when testing the changes\ncontained in this pull request.\n\n## CLI tools\n\nThis repository contains a number of [command line tools](cmd). They are used\nby the [CI](#ci-content) tests but may be useful for user to run stand alone.\n\n## Developer Mode\n\nDevelopers need a way to run as much test content as possible locally, but as\nexplained in [CI Setup](#ci-setup), running *all* the content in this\nrepository could be dangerous.\n\nThe recommended approach to resolve this issue is to set the following variable\nto any non-blank value **before using *any* content from this repository**:\n\n```\nexport KATA_DEV_MODE=true\n```\n\nSetting this variable has the following effects:\n\n- Disables content that might not be safe for developers to run locally.\n- Ignores the effect of the `CI` variable being set (for extra safety).\n\nYou should be aware that setting this variable provides a safe *subset* of\nfunctionality; it is still possible that PRs raised for code repositories will\nstill fail under the automated CI systems since those systems are running all\npossible tests.\n\n## Write a new Unit Test\n\nSee the [unit test advice documentation](https://github.com/kata-containers/kata-containers/blob/main/docs/Unit-Test-Advice.md).\n\n## Run the Kata Containers tests\n\n### Requirements to run Kata Containers tests\n\nYou need to install the following to run Kata Containers tests:\n\n- [golang](https://golang.org/dl)\n\n  To view the versions of go known to work, see the `golang` entry in the\n  [versions database](https://github.com/kata-containers/kata-containers/blob/main/versions.yaml).\n\n- `make`.\n\n### Prepare an environment\n\nThe recommended method to set up Kata Containers is to use the official and latest\nstable release. You can find the official documentation to do this in the\n[Kata Containers installation user guides](https://github.com/kata-containers/kata-containers/blob/main/docs/install/README.md).\n\nTo try the latest commits of Kata use the CI scripts, which build and install from the\n`kata-containers` repositories, with the following steps:\n\n> **Warning:** This may replace/delete packages and configuration that you already have.\n> Please use these steps only on a testing environment.\n\nAdd the `$GOPATH/bin` directory to the PATH:\n```\n$ export PATH=${GOPATH}/bin:${PATH}\n```\n\nClone the `kata-container/tests` repository:\n```\n$ go get -d github.com/kata-containers/tests\n```\n\nGo to the tests repo directory:\n```\n$ cd $GOPATH/src/github.com/kata-containers/tests\n```\n\nExecute the setup script:\n```\n$ export CI=true\n$ export CI_JOB=CRI_CONTAINERD_K8S\n$ .ci/setup.sh\n```\nIn this case we are exporting the environment variables for the CRI_CONTAINERD_K8S Jenkins Job for more information\nof which CI_JOB needs to be used see the following https://github.com/kata-containers/tests/blob/main/.ci/ci_job_flags.sh.\n\n> **Limitation:** If the script fails for a reason and it is re-executed, it will execute\nall steps from the beginning and not from the failed step.\n\n### Run the tests\n\nIf you have already installed the Kata Containers packages and a container\nmanager (i.e. Kubernetes), and you want to execute the content\nfor all the tests, run the following:\n\n```\n$ export RUNTIME=kata-runtime\n$ export KATA_DEV_MODE=true\n$ sudo -E PATH=$PATH make test\n```\n\nYou can also execute a single test suite. For example, if you want to execute\nthe Kubernetes integration tests, run the following:\n```\n$ sudo -E PATH=$PATH make kubernetes\n```\n\nA list of available test suite `make` targets can be found by running the\nfollowing:\n\n```\n$ make help\n```\n\n### Running subsets of tests\n\nIndividual tests or subsets of tests can be selected to be run. The method of\ntest selection depends on which type of test framework the test is written\nwith. Most of the Kata Containers test suites are written\nusing [Bats](https://github.com/sstephenson/bats) files.\n\n#### Running Bats based tests\n\nThe Bats based tests are shell scripts, starting with the line:\n\n```sh\n#!/usr/bin/env bats\n```\n\nThis allows the Bats files to be executed directly.  Before executing the file,\nensure you have Bats installed. The Bats files should be executed\nfrom the root directory of the tests repository to ensure they can locate all other\nnecessary components. An example of how a Bats test is run from the `Makefile`\nlooks like:\n\n```makefile\nkubernetes:\n        bash -f .ci/install_bats.sh\n        bash -f integration/kubernetes/run_kubernetes_tests.sh\n```\n\n## Metrics tests\nSee the [metrics documentation](metrics).\n\n## Kata Admission controller webhook\nSee the [webhook documentation](kata-webhook).\n\n## Using Vagrant to test your code changes\n\nIt is strongly recommended that you test your changes locally before opening\na pull request as this can save people's time and CI resources. Because\ntesting Kata Containers involve complex build and setup instructions, scripts\non the `.ci` directory are created to ease and provide a reproducible process; but they\nare meant to run on CI environments that can be discarded after use. Therefore,\ndevelopers have noticed dangerous side effects from running those scripts on a workstation\nor development environment.\n\nThat said, we provide in this repository a `Vagrantfile` which allows developers to use\nthe [vagrant](https://www.vagrantup.com) tool to create a VM with the setup as close as\nas possible to the environments where CI jobs will run the tests. Thus, allowing to\nreproduce a CI job locally.\n\nYour workstation must be capable of running VMs with:\n * 8GB of system memory\n * ~45GB and ~20GB of disk space for the VM images (Fedora and Ubuntu, respectively) on\n   the Libvirt's storage pool\n\nCurrently it supports the creation of *Fedora (32 and 35)* and *Ubuntu 20.04* VM, as shown on the table\nbelow. The `Vagrantfile` was tested on Fedora 33 and Ubuntu 20.04 hosts, and it is\n[known to fail](https://github.com/kata-containers/tests/issues/3942) the boot of Fedora VM on\nUbuntu host. If you have the need of testing on a different guest or it fails to work\non your host's distro then please [open an issue](https://github.com/kata-containers/tests/issues/new/choose)\nto let us know.\n\n|Host | Fedora 32 guest | Ubuntu 20.04 guest |\n| --- | --- | --- |\n| Fedora 33    |   Yes  |   Yes  |\n| Ubuntu 20.04 |   No   |   Yes  |\n\nBesides having vagrant installed in your host, it is needed the [vagrant libvirt plug-in](https://github.com/vagrant-libvirt/vagrant-libvirt) (Libvirt is the provider currently used), QEMU and `rsync` (needed to copy files between\nthe host and guest).\n\nFor example, to install the required software on Fedora host:\n```sh\n$ sudo dnf install -y qemu-kvm libvirt vagrant vagrant-libvirt rsync\n```\n> **Note**: ensure that you don't have Kata Container's built QEMU overwritten\n> the distro's in your host, otherwise Vagrant will not work.\n\nUse the `vagrant up [fedora|ubuntu]` command to bring up the VM. Vagrant is going to\npull (unless cached) the base VM image, provision it and then bootstrap the\nKata Containers environment (essentially by sourcing environment variables\nand running the `.ci/setup.sh` script). For example:\n\n```sh\n$ cd ${GOPATH}/src/github.com/kata-containers/tests\n$ vagrant up fedora\n```\n\nThe following repositories are automatically copied to the guest:\n * `${GOPATH}/src/github.com/kata-containers/tests`\n * `${GOPATH}/src/github.com/kata-containers/kata-containers`\n\nIf you want to reproduce a specific CI job, ensure that you have the `CI_JOB`\nenvironment variable exported on your host environment *before* you run\n`vagrant up`. For the possible `CI_JOB` values, see the `.ci/ci_job_flags.sh`\nfile. For example, the following will setup the VM to run CRI-O + Kubernetes\njob:\n```sh\n$ cd $GOPATH/src/github.com/kata-containers/tests\n$ export CI_JOB=\"CRIO_K8S\"\n$ vagrant up fedora\n```\n\nAt this point, if everything went well, you have a fully functional environment\nwith Kata Containers built and installed. To connect in the VM and run the tests:\n```sh\n$ vagrant ssh fedora\n$ .ci/run.sh\n```\n\nIn theory you could export `CI_JOB` with a different value and re-provision the\nsame VM (`vagrant provision [fedora|ubuntu]`), however this is not recommended because\nour CI scripts are meant for a single-shot execution. So if you need to run a different\njob locally, you should destroy the VM with the `vagrant destroy [fedora|ubuntu]` command\nthen start the process again.\n\nThe Vagrant configuration sometimes can get into inconsistent state. That may happen, for\ninstance, when the domain on Libvirt was created by the framework but it thinks the box\nis not initialized yet. Also you may want to stop using Vagrant and you want to simply\nwipe out all Vagrant control files and resources from your workstation. For those purposes you\nshould consider using the `.ci/vagrant-cleaner.sh` script; run `.ci/vagrant-cleaner.sh -h` for\nfurther information.\n", "repo_name": "kata-containers-tests", "org_name": "microsoft", "org_repo": "microsoft/kata-containers-tests", "platform_org_repo": "github+microsoft/kata-containers-tests", "link_to_repo": "https://github.com/microsoft/kata-containers-tests", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Kata Containers CI\n\n* [Jenkins CI](#jenkins-ci)\n    * [Jenkins CI slave deployment scripts](#jenkins-ci-slave-deployment-scripts)\n    * [Metrics CI](#metrics-ci)\n* [CI health status](#ci-health-status)\n* [Controlling the CI](#controlling-the-ci)\n\nThis repository stores configuration for the Kata Containers Continuous Integration (CI) system.\n\n# Jenkins CI\n\nThe default CI system for Kata Containers is [Jenkins](https://jenkins.io/). See\nthe [Jenkins Setup](Jenkins_setup.md) document for more details.\n\n## Jenkins CI slave deployment scripts\n\nSee [the Jenkins CI slave deployment documentation](deployment/packet/README.md).\n\n## Metrics CI\n\nSee [the metrics CI documentation](VMs/metrics/README.md).\n\n# CI health status\n\nYou can check for any known CI system issues [via this link](http://jenkins.katacontainers.io/view/CI%20Status/).\n\n# Always green CI\n\nSee the [Always Green CI](Always_green_CI.md) document for details.\n\n# Controlling the CI\n\nThe Jenkins CI jobs can be controlled (such as re-triggered or skipped) in a number of ways. Details of the\ncontrol trigger phrases are listed on\n[a community repo wiki page](https://github.com/kata-containers/community/wiki/Controlling-the-CI).\n\n# Jobs builder\n\nSee [README](jobs-builder/README.md) for information about the use of Jenkins Job Builder (JJB) to make the CI jobs.\n", "repo_name": "kata-containers-ci", "org_name": "microsoft", "org_repo": "microsoft/kata-containers-ci", "platform_org_repo": "github+microsoft/kata-containers-ci", "link_to_repo": "https://github.com/microsoft/kata-containers-ci", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Confidential Containers Operator\n\n[![Build](https://github.com/confidential-containers/operator/actions/workflows/makefile.yml/badge.svg)](https://github.com/confidential-containers/operator/actions/workflows/makefile.yml)\n[![Container Image](https://github.com/confidential-containers/operator/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/confidential-containers/operator/actions/workflows/docker-publish.yml)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Foperator.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Foperator?ref=badge_shield)\n\nThis Confidential Containers Operator provides a means to deploy and manage Confidential Containers Runtime on Kubernetes clusters. \nThe primary resource is `CcRuntime` which describes runtime details like installation type, source, nodes to deploy etc.\n\nHere is a short demo video showing the operator in action.\n\n[![asciicast](https://asciinema.org/a/450899.svg)](https://asciinema.org/a/450899)\n\nInstructions to recreate the demo setup in your own environment are available [here](https://github.com/confidential-containers/operator/blob/ccv0-demo/docs/INSTALL.md) \n\n## Installation\n\nPlease refer to the following [instructions](docs/INSTALL.md)\n\n## Development\n\nPlease refer to the following [instructions](docs/DEVELOPMENT.md)\n\n\n## License\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Foperator.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Foperator?ref=badge_large)\n\n", "repo_name": "confidential-containers-operator", "org_name": "microsoft", "org_repo": "microsoft/confidential-containers-operator", "platform_org_repo": "github+microsoft/confidential-containers-operator", "link_to_repo": "https://github.com/microsoft/confidential-containers-operator", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Key Broker Service\n\nThe Confidential Containers Key Broker Service (KBS) is a remote attestation\nentry point, also known as a [Relying Party](https://www.ietf.org/archive/id/draft-ietf-rats-architecture-22.html)\nin [RATS](https://datatracker.ietf.org/doc/draft-ietf-rats-architecture/)\nrole terminology.\n\n## Protocol\n\nThe KBS implements and supports a simple, vendor and hardware-agnostic\n[implementation protocol](https://github.com/confidential-containers/kbs/blob/main/docs/kbs_attestation_protocol.md).\n", "repo_name": "confidential-containers-kbs", "org_name": "microsoft", "org_repo": "microsoft/confidential-containers-kbs", "platform_org_repo": "github+microsoft/confidential-containers-kbs", "link_to_repo": "https://github.com/microsoft/confidential-containers-kbs", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Attestation Agent\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fattestation-agent.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fattestation-agent?ref=badge_shield)\n\n\nAttestation Agent (AA for short) is a service function set for attestation procedure\nin Confidential Containers. It provides kinds of service APIs that need to make\nrequests to the Relying Party (Key Broker Service) in Confidential Containers,\nand performs an attestation and establishes connection between the Key Broker Client (KBC)\nand corresponding KBS, so as to obtain the trusted services or resources of KBS.\n\n\nCurrent consumers of AA include: \n\n- [ocicrypt-rs](https://github.com/confidential-containers/ocicrypt-rs)\n- [image-rs](https://github.com/confidential-containers/image-rs)\n\n## Components\n\nThe main body of AA is a rust library crate, which contains KBC modules used to communicate\nwith various KBS. In addition, this project also provides a gRPC service application, \nwhich allows callers to call the services provided by AA through gRPC.\n\n## Library crate\n\nImport AA in `Cargo.toml` of your project with specific KBC(s):\n\n```toml\nattestation-agent = { git = \"https://github.com/confidential-containers/attestation-agent\", features = [\"sample_kbc\"] }\n```\n\n**Note**: When the version is stable, we will release AA on https://crate.io.\n\n## gRPC Application\n\nHere are the steps of building and running gRPC application of AA:\n\n### Build\n\nBuild and install with default KBC modules:\n\n```shell\ngit clone https://github.com/containers/attestation-agent\ncd attestation-agent\nmake && make install\n```\n\nor explicitly specify the KBS modules it contains. Taking `sample_kbc` as example:\n\n```shell\nmake KBC=sample_kbc\n```\n\n#### Musl \n\nTo build and install with musl, just run:\n```shell\nmake LIBC=musl && make install\n```\n\n### Run\n\nFor help information, just run:\n\n```shell\nattestation-agent --help\n```\n\nStart AA and specify the endpoint of AA's gRPC service:\n\n```shell\nattestation-agent --keyprovider_sock 127.0.0.1:50000 --getresource_sock 127.0.0.1:50001\n```\n\nOr start AA with default keyprovider address (127.0.0.1:50000) and default getresource address (127.0.0.1:50001):\n\n```\nattestation-agent\n```\n\nIf you want to see the runtime log:\n```\nRUST_LOG=attestation_agent attestation-agent --keyprovider_sock 127.0.0.1:50000 --getresource_sock 127.0.0.1:50001\n```\n\n## Supported KBC modules\n\nAA provides a flexible KBC module mechanism to support different KBS protocols required to make the communication between KBC and KBS. If the KBC modules currently supported by AA cannot meet your use requirement (e.g, need to use a new KBS protocol), you can write a new KBC module complying with the KBC development [GUIDE](docs/kbc_module_development_guide.md). Welcome to contribute new KBC module to this project!\n\nList of supported KBC modules: \n\n| KBC module name    | README                                                              | KBS protocol | Maintainer                |\n| ------------------ | ------------------------------------------------------------------- | ------------ | ------------------------- |\n| sample_kbc         | Null                                                                | Null         | Attestation Agent Authors |\n| offline_fs_kbc     | [Offline file system KBC](src/kbc_modules/offline_fs_kbc/README.md) | Null         | IBM                       |\n| eaa_kbc            | [EAA KBC](src/kbc_modules/eaa_kbc/README.md)                        | EAA protocol | Alibaba Cloud             |\n| offline_sev_kbc    | [Offline SEV KBC](src/kbc_modules/offline_sev_kbc/README.md)        | Null         | IBM                       |\n| online_sev_kbc     | [Online SEV KBC](src/kbc_modules/online_sev_kbc/README.md)          | simple-kbs   | IBM                       |\n\n\n## Tools\n\n- [Sample Keyprovider](./sample_keyprovider): A simple tool for encrypting container images with skopeo, please refer to its [README](./sample_keyprovider/README.md).\n\n\n\n## License\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fattestation-agent.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fattestation-agent?ref=badge_large)", "repo_name": "confidential-containers-attestation-agent", "org_name": "microsoft", "org_repo": "microsoft/confidential-containers-attestation-agent", "platform_org_repo": "github+microsoft/confidential-containers-attestation-agent", "link_to_repo": "https://github.com/microsoft/confidential-containers-attestation-agent", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# image-rs\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fimage-rs.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fimage-rs?ref=badge_shield)\n\nContainer Images Rust Crate\n\n## Documentation\n\n[Design document](docs/design.md)\n\n[CCv1 Image Security Design document](docs/ccv1_image_security_design.md)\n\n\n## License\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fconfidential-containers%2Fimage-rs.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Fconfidential-containers%2Fimage-rs?ref=badge_large)", "repo_name": "confidential-containers-image-rs", "org_name": "microsoft", "org_repo": "microsoft/confidential-containers-image-rs", "platform_org_repo": "github+microsoft/confidential-containers-image-rs", "link_to_repo": "https://github.com/microsoft/confidential-containers-image-rs", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "![containerd banner light mode](https://raw.githubusercontent.com/cncf/artwork/master/projects/containerd/horizontal/color/containerd-horizontal-color.png#gh-light-mode-only)\n![containerd banner dark mode](https://raw.githubusercontent.com/cncf/artwork/master/projects/containerd/horizontal/white/containerd-horizontal-white.png#gh-dark-mode-only)\n\n[![PkgGoDev](https://pkg.go.dev/badge/github.com/containerd/containerd)](https://pkg.go.dev/github.com/containerd/containerd)\n[![Build Status](https://github.com/containerd/containerd/workflows/CI/badge.svg)](https://github.com/containerd/containerd/actions?query=workflow%3ACI)\n[![Nightlies](https://github.com/containerd/containerd/workflows/Nightly/badge.svg)](https://github.com/containerd/containerd/actions?query=workflow%3ANightly)\n[![Go Report Card](https://goreportcard.com/badge/github.com/containerd/containerd)](https://goreportcard.com/report/github.com/containerd/containerd)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1271/badge)](https://bestpractices.coreinfrastructure.org/projects/1271)\n\ncontainerd is an industry-standard container runtime with an emphasis on simplicity, robustness and portability. It is available as a daemon for Linux and Windows, which can manage the complete container lifecycle of its host system: image transfer and storage, container execution and supervision, low-level storage and network attachments, etc.\n\ncontainerd is a member of CNCF with ['graduated'](https://landscape.cncf.io/selected=containerd) status.\n\ncontainerd is designed to be embedded into a larger system, rather than being used directly by developers or end-users.\n\n![architecture](design/architecture.png)\n\n## Now Recruiting\n\nWe are a large inclusive OSS project that is welcoming help of any kind shape or form:\n* Documentation help is needed to make the product easier to consume and extend.\n* We need OSS community outreach / organizing help to get the word out; manage\nand create messaging and educational content; and to help with social media, community forums/groups, and google groups.\n* We are actively inviting new [security advisors](https://github.com/containerd/project/blob/main/GOVERNANCE.md#security-advisors) to join the team.\n* New sub-projects are being created, core and non-core that could use additional development help.\n* Each of the [containerd projects](https://github.com/containerd) has a list of issues currently being worked on or that need help resolving.\n  - If the issue has not already been assigned to someone, or has not made recent progress and you are interested, please inquire.\n  - If you are interested in starting with a smaller / beginner level issue, look for issues with an `exp/beginner` tag, for example [containerd/containerd beginner issues.](https://github.com/containerd/containerd/issues?q=is%3Aissue+is%3Aopen+label%3Aexp%2Fbeginner)\n\n## Getting Started\n\nSee our documentation on [containerd.io](https://containerd.io):\n* [for ops and admins](docs/ops.md)\n* [namespaces](docs/namespaces.md)\n* [client options](docs/client-opts.md)\n\nSee how to build containerd from source at [BUILDING](BUILDING.md).\n\nIf you are interested in trying out containerd see our example at [Getting Started](docs/getting-started.md).\n\n## Nightly builds\n\nThere are nightly builds available for download [here](https://github.com/containerd/containerd/actions?query=workflow%3ANightly).\nBinaries are generated from `main` branch every night for `Linux` and `Windows`.\n\nPlease be aware: nightly builds might have critical bugs, it's not recommended for use in production and no support provided.\n\n## Runtime Requirements\n\nRuntime requirements for containerd are very minimal. Most interactions with\nthe Linux and Windows container feature sets are handled via [runc](https://github.com/opencontainers/runc) and/or\nOS-specific libraries (e.g. [hcsshim](https://github.com/Microsoft/hcsshim) for Microsoft).\nThe current required version of `runc` is described in [RUNC.md](docs/RUNC.md).\n\nThere are specific features\nused by containerd core code and snapshotters that will require a minimum kernel\nversion on Linux. With the understood caveat of distro kernel versioning, a\nreasonable starting point for Linux is a minimum 4.x kernel version.\n\nThe overlay filesystem snapshotter, used by default, uses features that were\nfinalized in the 4.x kernel series. If you choose to use btrfs, there may\nbe more flexibility in kernel version (minimum recommended is 3.18), but will\nrequire the btrfs kernel module and btrfs tools to be installed on your Linux\ndistribution.\n\nTo use Linux checkpoint and restore features, you will need `criu` installed on\nyour system. See more details in [Checkpoint and Restore](#checkpoint-and-restore).\n\nBuild requirements for developers are listed in [BUILDING](BUILDING.md).\n\n\n## Supported Registries\n\nAny registry which is compliant with the [OCI Distribution Specification](https://github.com/opencontainers/distribution-spec)\nis supported by containerd.\n\nFor configuring registries, see [registry host configuration documentation](docs/hosts.md)\n\n## Features\n\n### Client\n\ncontainerd offers a full client package to help you integrate containerd into your platform.\n\n```go\n\nimport (\n  \"context\"\n\n  \"github.com/containerd/containerd\"\n  \"github.com/containerd/containerd/cio\"\n  \"github.com/containerd/containerd/namespaces\"\n)\n\n\nfunc main() {\n\tclient, err := containerd.New(\"/run/containerd/containerd.sock\")\n\tdefer client.Close()\n}\n\n```\n\n### Namespaces\n\nNamespaces allow multiple consumers to use the same containerd without conflicting with each other.  It has the benefit of sharing content but still having separation with containers and images.\n\nTo set a namespace for requests to the API:\n\n```go\ncontext = context.Background()\n// create a context for docker\ndocker = namespaces.WithNamespace(context, \"docker\")\n\ncontainerd, err := client.NewContainer(docker, \"id\")\n```\n\nTo set a default namespace on the client:\n\n```go\nclient, err := containerd.New(address, containerd.WithDefaultNamespace(\"docker\"))\n```\n\n### Distribution\n\n```go\n// pull an image\nimage, err := client.Pull(context, \"docker.io/library/redis:latest\")\n\n// push an image\nerr := client.Push(context, \"docker.io/library/redis:latest\", image.Target())\n```\n\n### Containers\n\nIn containerd, a container is a metadata object.  Resources such as an OCI runtime specification, image, root filesystem, and other metadata can be attached to a container.\n\n```go\nredis, err := client.NewContainer(context, \"redis-master\")\ndefer redis.Delete(context)\n```\n\n### OCI Runtime Specification\n\ncontainerd fully supports the OCI runtime specification for running containers.  We have built in functions to help you generate runtime specifications based on images as well as custom parameters.\n\nYou can specify options when creating a container about how to modify the specification.\n\n```go\nredis, err := client.NewContainer(context, \"redis-master\", containerd.WithNewSpec(oci.WithImageConfig(image)))\n```\n\n### Root Filesystems\n\ncontainerd allows you to use overlay or snapshot filesystems with your containers.  It comes with built in support for overlayfs and btrfs.\n\n```go\n// pull an image and unpack it into the configured snapshotter\nimage, err := client.Pull(context, \"docker.io/library/redis:latest\", containerd.WithPullUnpack)\n\n// allocate a new RW root filesystem for a container based on the image\nredis, err := client.NewContainer(context, \"redis-master\",\n\tcontainerd.WithNewSnapshot(\"redis-rootfs\", image),\n\tcontainerd.WithNewSpec(oci.WithImageConfig(image)),\n)\n\n// use a readonly filesystem with multiple containers\nfor i := 0; i < 10; i++ {\n\tid := fmt.Sprintf(\"id-%s\", i)\n\tcontainer, err := client.NewContainer(ctx, id,\n\t\tcontainerd.WithNewSnapshotView(id, image),\n\t\tcontainerd.WithNewSpec(oci.WithImageConfig(image)),\n\t)\n}\n```\n\n### Tasks\n\nTaking a container object and turning it into a runnable process on a system is done by creating a new `Task` from the container.  A task represents the runnable object within containerd.\n\n```go\n// create a new task\ntask, err := redis.NewTask(context, cio.NewCreator(cio.WithStdio))\ndefer task.Delete(context)\n\n// the task is now running and has a pid that can be used to setup networking\n// or other runtime settings outside of containerd\npid := task.Pid()\n\n// start the redis-server process inside the container\nerr := task.Start(context)\n\n// wait for the task to exit and get the exit status\nstatus, err := task.Wait(context)\n```\n\n### Checkpoint and Restore\n\nIf you have [criu](https://criu.org/Main_Page) installed on your machine you can checkpoint and restore containers and their tasks.  This allows you to clone and/or live migrate containers to other machines.\n\n```go\n// checkpoint the task then push it to a registry\ncheckpoint, err := task.Checkpoint(context)\n\nerr := client.Push(context, \"myregistry/checkpoints/redis:master\", checkpoint)\n\n// on a new machine pull the checkpoint and restore the redis container\ncheckpoint, err := client.Pull(context, \"myregistry/checkpoints/redis:master\")\n\nredis, err = client.NewContainer(context, \"redis-master\", containerd.WithNewSnapshot(\"redis-rootfs\", checkpoint))\ndefer container.Delete(context)\n\ntask, err = redis.NewTask(context, cio.NewCreator(cio.WithStdio), containerd.WithTaskCheckpoint(checkpoint))\ndefer task.Delete(context)\n\nerr := task.Start(context)\n```\n\n### Snapshot Plugins\n\nIn addition to the built-in Snapshot plugins in containerd, additional external\nplugins can be configured using GRPC. An external plugin is made available using\nthe configured name and appears as a plugin alongside the built-in ones.\n\nTo add an external snapshot plugin, add the plugin to containerd's config file\n(by default at `/etc/containerd/config.toml`). The string following\n`proxy_plugin.` will be used as the name of the snapshotter and the address\nshould refer to a socket with a GRPC listener serving containerd's Snapshot\nGRPC API. Remember to restart containerd for any configuration changes to take\neffect.\n\n```\n[proxy_plugins]\n  [proxy_plugins.customsnapshot]\n    type = \"snapshot\"\n    address =  \"/var/run/mysnapshotter.sock\"\n```\n\nSee [PLUGINS.md](/docs/PLUGINS.md) for how to create plugins\n\n### Releases and API Stability\n\nPlease see [RELEASES.md](RELEASES.md) for details on versioning and stability\nof containerd components.\n\nDownloadable 64-bit Intel/AMD binaries of all official releases are available on\nour [releases page](https://github.com/containerd/containerd/releases).\n\nFor other architectures and distribution support, you will find that many\nLinux distributions package their own containerd and provide it across several\narchitectures, such as [Canonical's Ubuntu packaging](https://launchpad.net/ubuntu/bionic/+package/containerd).\n\n#### Enabling command auto-completion\n\nStarting with containerd 1.4, the urfave client feature for auto-creation of bash and zsh\nautocompletion data is enabled. To use the autocomplete feature in a bash shell for example, source\nthe autocomplete/ctr file in your `.bashrc`, or manually like:\n\n```\n$ source ./contrib/autocomplete/ctr\n```\n\n#### Distribution of `ctr` autocomplete for bash and zsh\n\nFor bash, copy the `contrib/autocomplete/ctr` script into\n`/etc/bash_completion.d/` and rename it to `ctr`. The `zsh_autocomplete`\nfile is also available and can be used similarly for zsh users.\n\nProvide documentation to users to `source` this file into their shell if\nyou don't place the autocomplete file in a location where it is automatically\nloaded for the user's shell environment.\n\n### CRI\n\n`cri` is a [containerd](https://containerd.io/) plugin implementation of the Kubernetes [container runtime interface (CRI)](https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1alpha2/api.proto). With it, you are able to use containerd as the container runtime for a Kubernetes cluster.\n\n![cri](./docs/cri/cri.png)\n\n#### CRI Status\n\n`cri` is a native plugin of containerd. Since containerd 1.1, the cri plugin is built into the release binaries and enabled by default.\n\n> **Note:** As of containerd 1.5, the `cri` plugin is merged into the containerd/containerd repo. For example, the source code previously stored under [`containerd/cri/pkg`](https://github.com/containerd/cri/tree/release/1.4/pkg)\nwas moved to [`containerd/containerd/pkg/cri` package](https://github.com/containerd/containerd/tree/main/pkg/cri).\n\nThe `cri` plugin has reached GA status, representing that it is:\n* Feature complete\n* Works with Kubernetes 1.10 and above\n* Passes all [CRI validation tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md).\n* Passes all [node e2e tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md).\n* Passes all [e2e tests](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md).\n\nSee results on the containerd k8s [test dashboard](https://k8s-testgrid.appspot.com/sig-node-containerd)\n\n#### Validating Your `cri` Setup\nA Kubernetes incubator project, [cri-tools](https://github.com/kubernetes-sigs/cri-tools), includes programs for exercising CRI implementations. More importantly, cri-tools includes the program `critest` which is used for running [CRI Validation Testing](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/cri-validation.md).\n\n#### CRI Guides\n* [Installing with Ansible and Kubeadm](contrib/ansible/README.md)\n* [For Non-Ansible Users, Preforming a Custom Installation Using the Release Tarball and Kubeadm](docs/cri/installation.md)\n* [CRI Plugin Testing Guide](./docs/cri/testing.md)\n* [Debugging Pods, Containers, and Images with `crictl`](./docs/cri/crictl.md)\n* [Configuring `cri` Plugins](./docs/cri/config.md)\n* [Configuring containerd](https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.8.md)\n\n### Communication\n\nFor async communication and long running discussions please use issues and pull requests on the github repo.\nThis will be the best place to discuss design and implementation.\n\nFor sync communication catch us in the `#containerd` and `#containerd-dev` slack channels on Cloud Native Computing Foundation's (CNCF) slack - `cloud-native.slack.com`. Everyone is welcome to join and chat. [Get Invite to CNCF slack.](https://slack.cncf.io)\n\n### Security audit\n\nA third party security audit was performed by Cure53 in 4Q2018; the [full report](docs/SECURITY_AUDIT.pdf) is available in our docs/ directory.\n\n### Reporting security issues\n\n__If you are reporting a security issue, please reach out discreetly at security@containerd.io__.\n\n## Licenses\n\nThe containerd codebase is released under the [Apache 2.0 license](LICENSE).\nThe README.md file, and files in the \"docs\" folder are licensed under the\nCreative Commons Attribution 4.0 International License. You may obtain a\ncopy of the license, titled CC-BY-4.0, at http://creativecommons.org/licenses/by/4.0/.\n\n## Project details\n\n**containerd** is the primary open source project within the broader containerd GitHub organization.\nHowever, all projects within the repo have common maintainership, governance, and contributing\nguidelines which are stored in a `project` repository commonly for all containerd projects.\n\nPlease find all these core project documents, including the:\n * [Project governance](https://github.com/containerd/project/blob/main/GOVERNANCE.md),\n * [Maintainers](https://github.com/containerd/project/blob/main/MAINTAINERS),\n * and [Contributing guidelines](https://github.com/containerd/project/blob/main/CONTRIBUTING.md)\n\ninformation in our [`containerd/project`](https://github.com/containerd/project) repository.\n\n## Adoption\n\nInterested to see who is using containerd? Are you using containerd in a project?\nPlease add yourself via pull request to our [ADOPTERS.md](./ADOPTERS.md) file.\n", "repo_name": "confidential-containers-containerd", "org_name": "microsoft", "org_repo": "microsoft/confidential-containers-containerd", "platform_org_repo": "github+microsoft/confidential-containers-containerd", "link_to_repo": "https://github.com/microsoft/confidential-containers-containerd", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Unreal Engine plugin for Visual Studio\n\nThis project contains an Unreal Editor plugin that works in conjunction with Visual Studio to display information about Blueprints assets in C++ code. \n\nThe plugin can be installed in either the Engine or Game project sources, and it is automatically activated when an Unreal Engine project is opened in Visual Studio.\n\n## Requirements\n\nBefore you begin, please make sure you have the following software and tools set up:\n\n1. Visual Studio 2022 has the \"IDE features for Unreal Engine\" component installed.\n   1. The component can be found in the \"Game development with C++\" workload or as an individual component.\n2. Unreal Engine, either installed or built from source.\n   1. To learn how to install or build Unreal Engine, please refer to the following guide: [Installing Unreal Engine](https://docs.unrealengine.com/5.0/en-US/installing-unreal-engine).\n   1. The source code and instructions have been tested on Unreal Engine versions 4.27, 5.0 and 5.1.\n\n## Building and Installing the Plugin\n\n> If you have Unreal Engine installed and set up through the Epic Games Launcher, and you only want to use the plugin, you can skip the steps below and install it directly from the [Unreal Engine Marketplace](https://aka.ms/vsueplugin).\n\nThe most straightforward way to use the plugin is to clone the repo under the `Plugins` folder of your game project or engine source. If you have multiple projects in the same Visual Studio solution, it is recommended to install the plugin at the engine level and share the binaries across the projects.\n\n1. Clone the repo under the project plugin folder by using the following commands:\n\n   ```powershell\n   cd <Project or Engine root folder>/Plugins\n   git clone https://github.com/microsoft/vc-ue-extensions.git\n   ```\n\n2. Optional: Regenerate the Solution for your game project so that the plugin source will be visible in Visual Studio.\n3. Rebuild the game project, which will also build the plugin.\n\nAfter completing these steps, Visual Studio should automatically recognize the plugin when you open a solution or project, and it will start processing Blueprints in your game.\n\nYou can also use the option `Rescan UE Blueprints for <Game Project Name>` in the `Project` menu to manually force Visual Studio to invoke the the plugin.\n\n### Cloning outside of engine or project sources\n\nIf you prefer to have the plugin's repository located separately from the engine or project sources (for example, if you want to share it between multiple engines), you can follow the instructions provided in the file [Scripts/README.md](Scripts/README.md) to learn how to build and install the plugin in such a scenario.\n\n## Enabling the Plugin (Optional)\n\nBy default, the plugin descriptor is already set with `\"EnabledByDefault = true\"`, so it should function automatically without any additional steps. However, if you encounter difficulties with Unreal Engine building the plugin (e.g., UE fails to build the plugin when building the project), you can enable the plugin explicitly by using one of the following methods:\n\n1. Navigate to the plugin manager in the Unreal Editor and select `VisualStudioTools`.\n2. Manually edit the game project's `.uproject` descriptor file by adding an entry for the plugin.\n\nIn either case, the end result should be a new entry in the `Plugins` array in the JSON file, as shown below:\n\n```JSON\n{\n \"FileVersion\": 3,\n \"Category\": \"...\",\n \"Description\": \"...\",\n \"Modules\": [\"...\"],\n \"Plugins\": [\n  {\n   \"Name\": \"VisualStudioTools\",\n   \"Enabled\": true,\n  }\n ]\n}\n```\n>Note: To ensure proper activation of the plugin, make sure the correct plugin is selected or the desired changes are made in the `.uproject` file.\n\n## Manually invoking the plugin\n\nThe plugin is designed to be used with Visual Studio, and as such, it does not provide any user interfaces, commands, or logs within the Unreal Editor. However, it is still possible to test the plugin's execution by running the **sample** command below: \n\n```powershell\n& \"<AbsolutePathToEngine>\\Engine\\Binaries\\Win64\\UnrealEditor-Cmd.exe\" \"$Env:UserProfile\\Unreal Projects\\EmptyProject\\EmptyProject.uproject\" -run=VisualStudioTools -output \"$Env:Temp\\vs-ue-tools.json\" [-unattended -noshadercompile -nosound -nullrhi -nocpuprofilertrace -nocrashreports -nosplash]\n```\n\nThis command will run the plugin for the specified project and save Unreal Engine Blueprint information in the output file. Optional parameters are included to run the command faster.\n\nFor more information on the specific command line parameters, you can run the following command in the powershell prompt with `-help`:\n\n```powershell\n& \"<Editor-Cmd.exe>\" \"<path_to_uproject>\" -run=VisualStudioTools -help [-unattended -noshadercompile -nosound -nullrhi -nocpuprofilertrace -nocrashreports -nosplash]\n```\n\n>Note: The executable name is `UE4Editor-cmd.exe` for UE4.x, located under a similar path.\n\n## Troubleshooting\n\nIf you encounter any issues when setting up Visual Studio in conjunction with the Unreal Editor plugin, please refer to the [Troubleshooting](https://github.com/microsoft/vc-ue-extensions/blob/main/Docs/Troubleshooting.md) guide in the repository. This guide provides solutions for common issues and is periodically updated to ensure that the latest solutions are available.\n\nTo report new issues, provide feedback, or request new features, please use the following options: [Report a Problem](https://aka.ms/feedback/cpp/unrealengine/report) and [Suggest a Feature](https://aka.ms/feedback/cpp/unrealengine/suggest). These options will allow you to submit your issue or feedback directly to our team and help us improve the plugin moving forward.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Code Style Guide\n\nThe code in the repo follows the existing code conventions described in the Unreal Engine's [Code Standard document](https://docs.unrealengine.com/INT/epic-cplusplus-coding-standard-for-unreal-engine/).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "vc-ue-extensions", "org_name": "microsoft", "org_repo": "microsoft/vc-ue-extensions", "platform_org_repo": "github+microsoft/vc-ue-extensions", "link_to_repo": "https://github.com/microsoft/vc-ue-extensions", "platform": "github", "language": "C++", "stargazers_count": 146, "watchers_count": 146}, {"README_text": "# Mojaloop on Azure\n\nThis project provides a set of deployment and code assets to enable the deployment of the Mojaloop payment interoperability solution natively on Azure, leveraging Azure Platform-as-a-Services capablities.\n\nThe assets held here should be used as a starting point for a deployment of Mojaloop on Azure. The solution as-is is not intended to provide a comprehensive \"Production Ready\" or \"Scale Ready\" version of the platform, and care should be takem to assess your own needs with regard to scalability and scaling methods, security, operations and additional capabilities, e.g. consolidated operational monitoring.\n\nFor more information about the Mojaloop platform please visit https://mojaloop.io\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Mojaloop-on-Azure", "org_name": "microsoft", "org_repo": "microsoft/Mojaloop-on-Azure", "platform_org_repo": "github+microsoft/Mojaloop-on-Azure", "link_to_repo": "https://github.com/microsoft/Mojaloop-on-Azure", "platform": "github", "language": "Mustache", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Bash for Beginners\n\n## Overview\n\nThese Microsoft Developer YouTube series is designed to help get you up to speed on Bash.  \n\nThese three series on Channel 9 and YouTube are designed to help get you up to speed on Bash. Bash is considered a universal language when it comes to cloud computing and programming. Many languages support Bash commands to pass data and information and when it comes to the Cloud, all platforms support using it to interact with your environment. \n\nEven though we won't cover everything there is to know about this language in this course, we want to make sure we give you the foundation on scripting in Bash. At the end of the course, you'll be able to create your own scripts and automate tasks with the help of Bash. \n\n### What you'll learn\n\n- What is a terminal \n- Setting up our environment \n- Getting help \n- Navigating the Bash terminal \n- Listing content \n- Finding files \n- Working with directories and files \n- Viewing files  \n- Environment variables \n- Redirection and Pipelines \n- Modifying File permissions \n- What is a Bash script? \n- What are variables in Bash? \n- What are Conditional Statements in Bash? \n- What Are Case Statements In Bash? \n- What are Functions in Bash? \n- What are Loops in Bash? \n- How to write a Bash Script \n\n## Prerequisites\n\n- [GitHub account](https://github.com/join)\n- [Azure account](https://azure.microsoft.com/free/) Azure is only used in the last video. We will be deploying a resource group.\n- [GitHub codespaces](https://github.blog/changelog/2022-11-09-codespaces-for-free-and-pro-accounts/) Included in your GitHub account. \n\n## Resources\n\nAs the goal of these courses is to help get you up to speed on Bash. The next step after completing the videos is to follow a tutorial! Here are a few of our favorites:\n\n- [Quickstart for Bash in Azure Cloud Shell](https://aka.ms/QuickStartForBashAzure1)\n- [Learn to use Bash with the Azure CLI](https://aka.ms/BashWithAzureCLI1)\n- [Create a Linux virtual machine in Azure](https://aka.ms/LinuxVirtualMachineAzure1)\n- [Learn more about GitHub Codespaces Monthly Free Tier](https://aka.ms/GitHubCodespacesBilling1)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "bash-for-beginners", "org_name": "microsoft", "org_repo": "microsoft/bash-for-beginners", "platform_org_repo": "github+microsoft/bash-for-beginners", "link_to_repo": "https://github.com/microsoft/bash-for-beginners", "platform": "github", "language": "Shell", "stargazers_count": 168, "watchers_count": 168}, {"README_text": "# \ud30c\uc6cc \ud50c\ub7ab\ud3fc \uc571 \uac1c\ubc1c \ud578\uc988\uc628\n\n\uc774 \ud578\uc988\uc628 \ub7a9 \uc608\uc81c\ub97c \ud1b5\ud574 \uc5ec\ub7ec\ubd84\uc740 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- \ud30c\uc6cc \uc571\uacfc \ud30c\uc6cc \uc624\ud1a0\uba54\uc774\ud2b8\ub97c \ud1b5\ud574 \ud55c \ud68c\uc0ac\uc758 \uc778\uc0ac\uad00\ub9ac \uc2dc\uc2a4\ud15c\uc744 \uace0\ub3c4\ud654 \uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- \ud30c\uc6cc \ud50c\ub7ab\ud3fc \ucee4\uc2a4\ud140 \ucee4\ub125\ud130\ub97c \uc774\uc6a9\ud574 \ub2e4\uc591\ud55c API\ub97c \uc5f0\uacb0\ud558\uace0, \uc560\uc800 API \uad00\ub9ac\uc790\ub97c \uc774\uc6a9\ud574 \uc880 \ub354 \ud5a5\uc0c1\ub41c \ubcf4\uc548 \uc124\uc815\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n## \uc0ac\uc804 \uc900\ube44\uc0ac\ud56d\n\n- [GitHub \ubb34\ub8cc \uacc4\uc815][gh signup]\n- [\uc560\uc800 \ubb34\ub8cc \uacc4\uc815][az free]\n- [Microsoft 365 \uac1c\ubc1c\uc790 \ud504\ub85c\uadf8\ub7a8][m365 dev]\n- [\ud30c\uc6cc \uc571 \uac1c\ubc1c\uc790 \ud504\ub85c\uadf8\ub7a8][pp dev]\n- [Docker Desktop][docker desktop]\n- [Windows Subsystem for Linux (\uc708\ub3c4\uc6b0 \uc0ac\uc6a9\uc790\uc6a9)][wsl]\n- [Visual Studio Code][vs code]\n- [Visual Studio Code - Remote Development Extension][vs code extensions remote]\n\n\n## \uc2dc\uc791\ud558\uae30\n\n- [Microsoft 365 \uac1c\ubc1c\uc790 \ud504\ub85c\uadf8\ub7a8 \uac00\uc785](./microsoft365-developer-program)\n- \ud30c\uc6cc \uc571\uacfc \ud30c\uc6cc \uc624\ud1a0\uba54\uc774\ud2b8\ub97c \uc774\uc6a9\ud55c \uc778\uc0ac\uad00\ub9ac \uc2dc\uc2a4\ud15c \uace0\ub3c4\ud654\n- [\ud30c\uc6cc \ud50c\ub7ab\ud3fc \ucee4\uc2a4\ud140 \ucee4\ub125\ud130\uc640 \uc560\uc800 \ubc31\uc5d4\ub4dc \ud1b5\ud569\ud558\uae30](./custom-connectors-in-a-day)\n\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n[gh signup]: https://github.com/signup\n[az free]: https://azure.microsoft.com/ko-kr/free/?WT.mc_id=dotnet-87051-juyoo\n[m365 dev]: https://aka.ms/gppbkr/m365dev\n[pp dev]: https://aka.ms/gppbkr/ppdev\n[docker desktop]: https://www.docker.com/products/docker-desktop/\n[wsl]: https://aka.ms/gppbkr/wsl\n[vs code]: https://code.visualstudio.com/?WT.mc_id=dotnet-87051-juyoo\n[vs code extensions remote]: https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack&WT.mc_id=dotnet-87051-juyoo\n", "repo_name": "Power-Platform-App-Dev-in-a-Day-KR", "org_name": "microsoft", "org_repo": "microsoft/Power-Platform-App-Dev-in-a-Day-KR", "platform_org_repo": "github+microsoft/Power-Platform-App-Dev-in-a-Day-KR", "link_to_repo": "https://github.com/microsoft/Power-Platform-App-Dev-in-a-Day-KR", "platform": "github", "language": "Bicep", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# VSCode Performance Tool\n\nTooling for evaluating performance of VSCode.\n\n[![Build Status](https://dev.azure.com/monacotools/Monaco/_apis/build/status/npm/vscode/vscode-perf?repoName=microsoft%2Fvscode-perf&branchName=main)](https://dev.azure.com/monacotools/Monaco/_build/latest?definitionId=451&repoName=microsoft%2Fvscode-perf&branchName=main)\n\n## Requirements\n\n- [Node.js](https://nodejs.org/en/) at least `16.x.x`\n\n## Usage\n\n```sh\nnpx vscode-perf [ARGS]\n```\n\n## Help\n\n```sh\nnpx vscode-perf --help\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-perf", "org_name": "microsoft", "org_repo": "microsoft/vscode-perf", "platform_org_repo": "github+microsoft/vscode-perf", "link_to_repo": "https://github.com/microsoft/vscode-perf", "platform": "github", "language": "TypeScript", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Rise4fun Docusaurus plugins\n\nThis repository contains plugins for Docusaurus to support creating rich documentation for programming languages tools.\n\n- [Read the documentation](https://microsoft.github.io/docusaurus-plugins).\n\n## Development\n\nInstall [Volta](https://docs.volta.sh/guide/getting-started), if you haven't got it installed already.\n\nOnce installed, Volta will automatically install the correct version of Node.js and Yarn for this project - no need to install them manually.\n\nInstall dependencies,\n\n```bash\nyarn install\n```\n\nTo build plugins and final web site,\n\n```bash\nyarn build\n```\n\nTo run a local development server,\n\n```bash\nyarn start\n```\n\nTo create releases, use the semantic release commit syntax\n\n- `feat: ...` for a minor version bump\n- `fix: ...` for a patch version bump\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "docusaurus-plugins", "org_name": "microsoft", "org_repo": "microsoft/docusaurus-plugins", "platform_org_repo": "github+microsoft/docusaurus-plugins", "link_to_repo": "https://github.com/microsoft/docusaurus-plugins", "platform": "github", "language": "TypeScript", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Kalypso Scheduler\n[![CI](https://github.com/microsoft/kalypso-scheduler/actions/workflows/ci.yaml/badge.svg)](https://github.com/microsoft/kalypso-scheduler/actions/workflows/ci.yaml)\n\n[Kalypso](https://github.com/microsoft/kalypso) scheduler operator is responsible for scheduling applications and services on cluster types and uploading the result to the GitOps repo.\n\n## Kalypso Control Plane Abstractions\n\nKalypso Scheduler operates with high level abstractions that the Platform Team uses to describe clusters in their environments. These abstractions are objects on the control plane Kubernetes cluster. The scheduler watches the abstractions and reconciles them by performing necessary transformations and scheduling, generating low level manifests and uploading them to the GitOps repositories. The GitOps operators on the workload clusters fetch assigned workloads and configurations from those GitOps repositories.\n\nThe control plane abstractions are human oriented yamls, that the Platform Team can easily manipulate and version control in a [Control Plane](https://github.com/microsoft/kalypso-control-plane) repository. It is common to deliver the abstractions to the control plane Kubernetes cluster with a GitOps operator, such as [Flux CD](https://fluxcd.io).\n\n### Cluster Type\n\nCluster type defines a set of K8s clusters with similar properties that host the same collection of workloads and share the same platform configuration values. A single cluster type may be backed up by thousands of physical clusters.  \n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: ClusterType\nmetadata:\n  name: large\n  labels: \n    region: west-us\n    size: large\nspec:\n  reconciler: argocd\n  namespaceService: default\n```\n\nThe example above defines `large` cluster type that uses `argocd` as a [reconciler](#reconciler-template). It provides a namespace for each assigned workload, generated from the `default` [namespace template](#namespace-template). \n\n### Reconciler template\n\nEach cluster type may use their own reconciler, for example Flux, ArgoCD, Rancher Fleet, polling script etc. This abstraction contains [Go manifest template](https://pkg.go.dev/text/template#hdr-Text_and_spaces), which is used to generate reconciler deployment descriptors.  \n\n#### Example\n\nArgoCD application\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: Template\nmetadata:\n  name: argocd\nspec:\n  type: reconciler\n  manifests:\n    - apiVersion: argoproj.io/v1alpha1\n      kind: Application\n      metadata:\n        name: \"{{ .DeploymentTargetName}}\"\n        namespace: argocd\n      spec:\n          destination:\n              server: https://kubernetes.default.svc\n              namespace: \"{{ .Namespace}}\"\n          project: default\n          source:\n              path: \"{{ .Path}}\"\n              repoURL: \"{{ .Repo}}\"\n              targetRevision: \"{{ .Branch}}\"\n              directory:\n                  recurse: true\n                  include: '*.yaml'\n                  exclude: 'kustomization.yaml'\n          syncPolicy:\n              automated:\n                  prune: true\n                  selfHeal: true\n                  allowEmpty: false\n              syncOptions:\n              - CreateNamespace=true\n```\n\nFlux resources\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: Template\nmetadata:\n  name: arc-flux\nspec:\n  type: reconciler\n  manifests:\n    - apiVersion: source.toolkit.fluxcd.io/v1beta2\n      kind: GitRepository\n      metadata:\n        name: \"{{ .DeploymentTargetName}}\"\n        namespace: flux-system\n      spec:\n        interval: 1m0s\n        url: \"{{ .Repo}}\"\n        ref:\n          branch: \"{{ .Branch}}\"\n    - apiVersion: kustomize.toolkit.fluxcd.io/v1beta2\n      kind: Kustomization\n      metadata:\n        name: \"{{ .DeploymentTargetName}}\"\n        namespace: flux-system\n      spec:\n        interval: 1m0s\n        targetNamespace: \"{{ .Namespace}}\"\n        sourceRef:\n          kind: GitRepository\n          name: \"{{ .DeploymentTargetName}}\"\n        path: \"{{ .Path}}\" \n        prune: true\n```\n\nThe templates above are used to generate ArgoCD and Flux resources respectively. Reconcilers on the clusters use them to fetch manifests from the workload manifest storages, defined in the [deployment targets](#workload). \n\n### Namespace template\n\nEach workload on every cluster has a dedicated namespace. This abstraction defines a manifest template for this namespace. \n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: Template\nmetadata:\n  name: default\nspec:\n  type: namespace\n  manifests:\n    - apiVersion: v1\n      kind: Namespace\n      metadata:\n        name: \"{{ .Namespace}}\" \n        labels:\n          environment: \"{{ .Environment}}\"\n          workspace: \"{{ .Workspace}}\"\n          workload: \"{{ .Workload}}\"\n          deploymentTarget: \"{{ .DeploymentTargetName}}\"\n          someLabel: some-value\n```\n\nBesides just a namespace, the template may include some common resources that should be created for every workload, like RBAC configurations and such.\n\n<!-- TODO\nDescribe available values to be used in the templates\n-->\n\n### Workload registration\n\nWorkload registration is a reference to a git repository where the [workload](#workload) is defined. The scheduler creates Flux resources on the control plane cluster to fetch the [workload](#workload) definition.\n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: WorkloadRegistration\nmetadata:\n  name: hello-world-app\n  labels:\n    type: application\nspec:\n  workload:\n    repo: https://github.com/microsoft/kalypso-app-src\n    branch: main\n    path: workload/\n  workspace: kaizen-app-team\n```\n\nThe example above defines a workload registration that points to a place in a git repository where a yaml file with the workload definition is stored. \n\n### Workload\n\nWorkload describes where an application or a service should be deployed from the perspective of an application team. It contains a list of deployment targets. Each deployment target defines a name, a reference to a rollout environment, a custom set of labels and a reference to a storage with the manifests, specific for this deployment target. \n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: Workload\nmetadata:\n  name: hello-world-app\n  labels:\n    type: application\n    family: force\nspec:\n  deploymentTargets:\n    - name: functional-test\n      labels:\n        purpose: functional-test\n        edge: \"true\"\n      environment: dev\n      manifests:\n        repo: https://github.com/microsoft/kalypso-app-gitops\n        branch: dev\n        path: ./functional-test\n    - name: performance-test\n      labels:\n        purpose: performance-test\n        edge: \"false\"\n      environment: dev\n      manifests:\n        repo: https://github.com/microsoft/kalypso-app-gitops\n        branch: dev\n        path: ./performance-test\n    - name: uat-test\n      labels:\n        purpose: uat-test\n      environment: stage\n      manifests:\n        repo: https://github.com/microsoft/kalypso-app-gitops\n        branch: stage\n        path: ./uat-test\n```\n\nThe example above defines that `hello-world-app` application is supposed to be deployed to three targets. It should be deployed in `Dev` environment for functional and performance testing and in `Stage` environment for UAT testing. Each deployment target is marked with custom labels and points to the folders in [Application GitOps](https://github.com/microsoft/kalypso-app-gitops) repository where the Application Team generates application manifests for each target.\n\n### Scheduling policy\n\nThe scheduler uses scheduling policies to map or schedule [deployment targets](#workload) to the [cluster types](#cluster-type). Currently, the algorithm is based on the simple label matching approach. Next version of the scheduler will provide integration with [OCM Placement API](https://open-cluster-management.io/concepts/placement/) for the more sophisticated and custom scheduling implementations.\n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: SchedulingPolicy\nmetadata:\n  name: functional-test-policy\nspec:\n  deploymentTargetSelector:\n    workspace: kaizen-app-team\n    labelSelector:\n      matchLabels:\n        purpose: functional-test\n        edge: \"true\"\n  clusterTypeSelector:\n    labelSelector:\n      matchLabels:\n        restricted: \"true\"\n        edge: \"true\"\n```\n\nThe example above specifies that all deployment targets from the `kaizen-app-team` workspace, marked with labels `purpose: functional-test` and `edge: \"true\"` should be scheduled on all cluster types that are marked with label `restricted: \"true\"`.\n\n### Config\n\nPlatform configuration values are defined with the standard Kubernetes config maps, marked with custom labels. The scheduler scans all config maps with the label `platform-config: \"true\"` in the namespace and collects values for each cluster type basing on the label matching. Every workload on each cluster will have a `platform-config` config map in its namespace with all platform configuration values, that the workload can use on this cluster type in this environment.\n\n#### Example\n\n```yaml\nkind: ConfigMap\nmetadata:\n  name: west-us-config\n  labels:\n     platform-config: \"true\"\n     region: west-us\ndata:\n  REGION: West US\n  DATABASE_URL: mysql://west-stage:8806/mysql\n```\n\nThe example above defines a config map specifying some values for all cluster types with the label `region: west-us`.\n\n### Environment\n\nEnvironment defines a rollout environment such as `dev`, `stage`, `prod`. It defines a place in a git repository where the control plane abstractions for this environment are stored. The scheduler creates a namespace for each environment on the control plane cluster and creates Flux resources to fetch the abstractions from the defined place.\n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: Environment\nmetadata:\n  labels:\n  name: dev\nspec:\n  controlPlane:\n    repo: https://github.com/microsoft/kalypso-control-plane\n    branch: dev\n    path: .\n```\n\n<!--\n### Base repository\n\nControl plane abstractions such as [workload registrations](#workload-registration), [templates](#reconciler-template) or some config maps are common for all environments and are stored in a common place like [main](https://github.com/microsoft/kalypso-control-plane) branch of a control plane repository. To fetch those common abstractions to the environment namespaces on the control plane cluster,     \n-->\n\n### GitOps repository \n\nThe result of the scheduling and transformation is about to be uploaded to one or many GitOps repositories. This is the output of the scheduler. The reconcilers on the clusters can fetch their manifests from the GitOps repositories, being abstracted away from the details of how those manifests where generated and how the scheduling has happened.  \n\nThe scheduler uses the GitOps repository abstraction as a reference to where it should PR the generated manifests.   \n\n#### Example\n\n```yaml\napiVersion: scheduler.kalypso.io/v1alpha1\nkind: GitOpsRepo\nmetadata:\n  name: dev\nspec:\n  repo: https://github.com/microsoft/kalypso-gitops\n  branch: dev\n  path: .\n```\n\n## Transformation Flow\n\nThe primary goal of the Kalypso Scheduler is to transform high level control plane abstractions into the low level Kubernetes manifests that the reconcilers on the clusters can understand. The high level transformation flow is shown on the following diagram:\n\n![Scheduler](./docs/images/Scheduler.drawio.png)\n\nThe common abstractions such as Workload Registrations, Templates and Environments are delivered to the control plane cluster by Flux from the control plane repository. The Environment Controller watches Environments and creates a namespace for each environment in the control plane cluster. It creates corresponding Flux resources to deliver environment specific abstractions such as Cluster Types, Scheduling Policies, Config Maps and GitOps Repos from the environment branches to the environment namespace in the control plane cluster. \n\nThe Workload Registration Controller creates Flux resources to fetch Workloads from the Application repositories specified in the Workload Registrations. The Workload Controller watches Workloads, delivered by Flux, and unfolds them into Deployment Targets. \n\nThe Scheduler watches Scheduling Policies, Cluster Types, Deployment Targets and assigns Deployment Targets to Cluster Types. It creates an Assignment object for each Deployment Target assignment. The Assignment Controller uses Reconciler and Namespace templates, referenced by the assigned cluster type, and generates reconciler and namespace manifests. It also scans Config Maps in the environment namespaces that are applicable to this cluster type basing on label matching. It generates a consolidated Config Map and adds it to the Assignment Package along with the reconciler and namespace manifests.\n\nThe GitOps Repo Controller watches Assignment Packages and creates a PR with their content to the GitOps repository specified in this environment.   \n\n## Installation\n\n### Prerequisites \n\nKalypso Scheduler requires Flux to be installed on the control plane cluster. \n\n### Install with Helm\n\nAdd Kalypso Scheduler Helm repository:\n```sh\nhelm repo add kalypso https://raw.githubusercontent.com/microsoft/kalypso-scheduler/gh-pages/\n```\n\nCreate a namespace and install the Helm chart:\n```sh\nkubectl create ns kalypso \nhelm upgrade -i kalypso kalypso/kalypso-scheduler -n kalypso --set controlPlaneURL=<Control Plane GitHub Repo URL> \\\n--set controlPlaneBranch=main --set ghRepoToken=<GitHub Repo Token>\n```\n\nUse [Kalypso Control Plane](https://github.com/microsoft/kalypso-control-plane) as a sample of the control plane repository. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-scheduler", "org_name": "microsoft", "org_repo": "microsoft/kalypso-scheduler", "platform_org_repo": "github+microsoft/kalypso-scheduler", "link_to_repo": "https://github.com/microsoft/kalypso-scheduler", "platform": "github", "language": "Go", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Lepton JPEG Compression in Rust \n[![Rust Community](https://img.shields.io/badge/Rust_Community%20-Join_us-brightgreen?style=plastic&logo=rust)](https://www.rust-lang.org/community)\n\nThis is a port of the C++ Lepton JPEG compression tool that was released by DropBox [dropbox/lepton](https://github.com/dropbox/lepton). We developed a port of the library to Rust, which has basically the same performance characteristics with the advantage of all the safety features that Rust has to offer, due to the work involved in performing an exhaustive security check on the C++ code and the fact that DropBox has deprecated the codebase.\n\nWith precise bit-by-bit recovery of the original JPEG, the Lepton compression library is designed for lossless compression of baseline and progressive JPEGs up to 22%. JPEG storage in a cloud storage system is the main application case. Even metadata headers and invalid content are kept in good condition.\n\n\n## How to Use This Library\nSome operations of this library are vectorized such as the IDCT using the [Wide](https://crates.io/crates/wide) crate, so you can get a significant boost if you enable +AVX2.\n\n#### Building From Source\n\n- [Rust 1.65 or Above](https://www.rust-lang.org/tools/install)\n\n```\ngit clone https://github.com/microsoft/lepton_jpeg_rust\ncd lepton_jpeg_rust\ncargo build\ncargo test\ncargo build --release\n```\n\n#### Running\n\nThere is an `lepton_jpeg_util.exe` wrapper that is built as part of the project. It can be used to compress/decompress and also to verify the test end-to-end on a given JPEG. If the input file has a `.jpg` extension, it will encode. If the input file has a `.lep` extension, it will decode back to the original`.jpg`. \n\nIt supports the following options:\n\n`lepton_jpeg_util.exe [options] <inputfile> [<outputfile>]`\n\n| Option           | Description                                                  |\n| ---------------- | ------------------------------------------------------------ |\n| `-threads:n`     | Runs with a maximum of n threads. For encoding, this limits the amount of parallelism that can be gotten out of the decoder. |\n| `-dump`          | Dumps the contents of a JPG or LEP file, with the -all option, it will also dump the cooefficient image blocks |\n| `-noprogressive` | Will cause an error if we encounter a progressive file rather than trying to encode it |\n| `-verify`        | Reads, encodes and unencodes verifying that there is an exact match. No output file is specified. |\n| `-iter:n`        | Runs N iterations of the operation. Useful when we are running inside a profiler. |\n\n## Contributing\n\nThere are many ways in which you can participate in this project, for example:\n\n* [Submit bugs and feature requests](https://github.com/microsoft/lepton_jpeg_rust/issues), and help us verify as they are checked in\n* Review [source code changes](https://github.com/microsoft/lepton_jpeg_rust/pulls) or submit your own features as pull requests.\n* The library uses only **stable features**, so if you want to take advantage of SIMD features such as AVX2, use the Wide crate (see the idct.rs as an example) rather than intrinsics. \n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [Apache 2.0](LICENSE.txt) license.\n\n", "repo_name": "lepton_jpeg_rust", "org_name": "microsoft", "org_repo": "microsoft/lepton_jpeg_rust", "platform_org_repo": "github+microsoft/lepton_jpeg_rust", "link_to_repo": "https://github.com/microsoft/lepton_jpeg_rust", "platform": "github", "language": "Rust", "stargazers_count": 32, "watchers_count": 32}, {"README_text": "# Data Wrangler Extension for Visual Studio Code\n\n[Data Wrangler](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler) is a code-centric data cleaning tool that is integrated into VS Code and VS Code Jupyter Notebooks. Data Wrangler aims to increase the productivity of data scientists doing data cleaning by providing a rich user interface that automatically generates Pandas code for and shows insightful column statistics and visualizations.\n\n[<img src=\"https://user-images.githubusercontent.com/8560030/225425356-c0abf8e2-332f-439c-8de1-9a5946b933ee.png\" />](https://youtu.be/KrzcV1c1W1U)\n\nThis document will cover how to:\n\n-   Install and setup Data Wrangler\n-   Launch Data Wrangler from a notebook\n-   Use Data Wrangler to explore your data\n-   Perform operations on your data\n-   Edit and export code for data wrangling to a notebook\n-   Troubleshooting and providing feedback\n\n## Setting up your environment\n\n1. If you have not already done so, install [Python](https://www.python.org/downloads/).  \n   **IMPORTANT:** Data Wrangler only supports Python version 3.8 or higher.\n2. Install [Visual Studio Code](https://code.visualstudio.com/download).\n3. Install the [Data Wrangler extension for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.datawrangler) from the Visual Studio Marketplace. For additional details on installing extensions, see Extension Marketplace. The Data Wrangler extension is named Data Wrangler and it\u2019s published by Microsoft.\n\nWhen you launch Data Wrangler for the first time, it will ask you which Python kernel you would like to connect to. It will also check your machine and environment to see if any required Python packages are installed (e.g., Pandas).\n\n> Here is a list of the required versions for Python and Python packages, along with whether they are automatically installed by Data Wrangler:\n>\n> | Name    | Minimum required version | Automatically installed |\n> | ------- | ------------------------ | ----------------------- |\n> | Python  | 3.8                      | No                      |\n> | pandas  | 0.25.2                   | Yes                     |\n> | regex\\* | 2020.11.13               | Yes                     |\n>\n> _\\* We use the open-source `regex` package to be able to use Unicode properties (for example, `/\\p{Lowercase_Letter}/`), which aren't supported by Python's built-in regex module (`re`). Unicode properties make it easier and cleaner to support foreign characters in regular expressions._\n\nIf they are not found on your environment, Data Wrangler will attempt to install them for you via pip. If Data Wrangler is unable to install dependencies, the easiest workaround is to manually run pip install, and then launch Data Wrangler again. These dependencies are required for Data Wrangler such that it can generate Python and Pandas code.\n\n### Connecting to a Python kernel\n\nThere are currently two ways to connect to a Python kernel, as shown in the quick pick below.  \n![image](https://user-images.githubusercontent.com/8560030/225400310-cea6cf16-de2e-484d-b3d9-3c60ecb36b50.png)\n\n#### 1. Connect using local Python interpreter\n\nIf this option is selected, then the kernel connection is created using the Jupyter and Python extensions. We recommend this option for a simple setup and quick way to get started with Data Wrangler.\n\n#### 2. Connect using Jupyter URL and token\n\nIf this option is selected, then a kernel connection is created using JupyterLab APIs. Note that there are performance benefits with this option since it bypasses some initialization and kernel discovery processes. However, it will also require separate user management of the Jupyter notebook server. We recommend this option generally in two cases: 1) if there are blocking issues in the first method and 2) for power users who would like to reduce the cold-start time of Data Wrangler.\n\nTo set up a Jupyter notebook server and use it with this option, follow the steps below:\n\n1. Install Jupyter. We recommend installing the free version of [Anaconda](https://www.anaconda.com/products/individual) which comes with Jupyter installed. Alternatively, follow the [official instructions](https://jupyter.org/install) to install it.\n2. In the appropriate environment (e.g. in an Anaconda prompt if Anaconda is used), launch the server with the following command (replace the jupyter token with your secure token):\n   `jupyter notebook --no-browser --NotebookApp.token='<your-jupyter-token>'`\n3. In Data Wrangler, connect using the address of the spawned server. E.g. http://localhost:8888 and pass in the token used in the previous step. Once configured, this information is cached locally and can automatically be reused for future connections.\n\n## Launching Data Wrangler\n\nOnce Data Wrangler has been successfully installed, there are 2 ways to launch it in VS Code.\n\n### Launching Data Wrangler from a Jupyter Notebook\n\nIf you are in a Jupyter Notebook working with Pandas data frames, you\u2019ll now see a \u201cLaunch Data Wrangler\u201d button appear after running specific operations on your data frame, such as df.head(). Clicking the button will open a new tab in VS Code with the Data Wrangler interface in a sandboxed environment.\n\n![image](https://user-images.githubusercontent.com/2180824/218180019-d2c434dd-2a27-4355-a00a-f8ec09a1f828.png)\n\n### Launching Data Wrangler directly from a CSV file\n\nYou can also launch Data Wrangler directly from a local CSV file. To do so, open any folder in VS Code that has the CSV dataset you\u2019d like to explore. In the File Explorer panel, right click the .CSV dataset and click \u201cOpen in Data Wrangler\u201d.\n\n![image](https://user-images.githubusercontent.com/2180824/218180054-386cf32a-8876-48dd-8065-134d0bd932f0.png)\n\n## Using Data Wrangler\n\n![image](https://user-images.githubusercontent.com/2180824/218180077-8bbb9018-c67b-4fda-b4cd-9ea7c67eea5b.png)\n\nThe Data Wrangler interface is divided into 6 components, described below.\n\nThe **Quick Insights** header is where you can quickly see valuable information about each column. Depending on the datatype of the column, Quick Insights will show the distribution of the data or the frequency of datapoints, as well as missing and unique values.\n\nThe **Data Grid** gives you a scrollable pane where you can view your entire dataset. Additionally, when selecting an operation to perform, a preview will be illustrated in the data grid, highlighting the modified columns.\n\nThe **Operations Panel** is where you can search through all of Data Wrangler\u2019s built-in data operations. The operations are organized by their top-level category.\n\nThe **Summary Panel** shows detailed summary statistics for your dataset or a specific column if one is selected. Depending on the datatype, it will show information such as min, max values, datatype of the column, skew, and more.\n\nThe **Operation History Panel** shows a human readable list of all the operations that have been previously applied in the current Data Wrangling session. It enables the user to undo specific operations or edit the most recent operation. Selecting a step will highlight the changes in the data grid and will show the generated code associated with that operation.\n\nThe **Code Preview** section will show the Python and Pandas code that Data Wrangler has generated when an operation is selected. It will remain blank when no operation is selected. The code can even be edited by the user, and the data grid will highlight the effect on the data.\n\n### Example: Filtering a column\n\nLet\u2019s go through a simple example using Data Wrangler with the Titanic dataset to filter adult passengers on the ship.\n\nWe\u2019ll start off by looking at the quick insights of the Age column, and we\u2019ll notice the distribution of the ages and that the minimum age is 0.42. For more information, we can have a glance at the Summary panel to see that the datatype is a float, along with additional statistics such as the mean and median age of all the passengers.\n\n![image](https://user-images.githubusercontent.com/2180824/218180630-73009474-54ff-439d-bd22-a3cc97363904.png)\n\nTo filter for only adult passengers, we can go to the Operation Panel and search for the keyword \u201cFilter\u201d to find the **Filter** operation. (You can also expand the \u201cSort and filter\u201d category to find it.)\n\n![image](https://user-images.githubusercontent.com/2180824/218180718-1e6da67f-cf5c-43b8-be80-e22e722aa908.png)\n\nOnce we select an operation, we are brought into the Operation Preview state where parameters can be modified to see how they affect the underlying dataset prior to applying the operation. In this example, we want to filter the dataset to only include adults, so we\u2019ll want to filter the Age column to only include values greater than or equal to 18.\n\n![image](https://user-images.githubusercontent.com/2180824/218180755-3d9eb2d0-e4c0-4959-9ba1-75b19df047eb.png)\n\nOnce the parameters are entered in the operation panel, we can see a preview of what will happen to the data. We\u2019ll notice that the minimum value in age is now 18 in the Quick Insights, along with a visual preview of the rows that are being removed highlighted in red. Finally, we\u2019ll also notice the Code Preview section automatically shows the code that Data Wrangler produced to execute this Filter operation. We can edit this code, such as change the filtered age to 21, and the data grid will automatically update accordingly.\n\nAfter confirming that the operation has the intended effect, we can click Apply.\n\n## Editing and exporting code\n\nEach step of the generated code can be modified. As you make changes, changes to the data will be highlighted in the grid view.\n\nOnce you\u2019re done with your data cleaning steps in Data Wrangler, there are 3 ways to export your cleaned dataset from Data Wrangler.\n\n1. Export code back to Notebook and exit: This creates a new cell in your Jupyter Notebook with all the data cleaning code you generated packaged up into a clean Python function.\n2. Export data as CSV: This saves the cleaned dataset as a new CSV file onto your machine.\n3. Copy code to clipboard: This copies all the code that was generated by Data Wrangler for the data cleaning operations.\n\n![image](https://user-images.githubusercontent.com/2180824/218180955-ed417931-5337-4c8a-93c7-36a6b3c13332.png)\n\nNote: If you launched Data Wrangler directly from a CSV, the first export option will be to export the code into a new Jupyter Notebook.\n\n## Data Wrangler operations\n\nThese are the Data Wrangler operations that are currently supported in the initial launch of Data Wrangler (with many more to be added in the near future).\n\n| Operation                      | Description                                                                                           |\n| ------------------------------ | ----------------------------------------------------------------------------------------------------- |\n| Sort values                    | Sort column(s) ascending or descending                                                                |\n| Filter                         | Filter rows based on one or more conditions                                                           |\n| Calculate text length          | Create new column with values equal to the length of each string value in a text column               |\n| One-hot encode                 | Split categorical data into a new column for each category                                            |\n| Multi-label binarizer          | Split categorical data into a new column for each category using a delimiter                          |\n| Create column from formula     | Create a column using a custom Python formula                                                         |\n| Change column type             | Change the data type of a column                                                                      |\n| Drop column                    | Delete one or more columns                                                                            |\n| Select column                  | Choose one or more columns to keep and delete the rest                                                |\n| Rename column                  | Rename one or more columns                                                                            |\n| Drop missing values            | Remove rows with missing values                                                                       |\n| Drop duplicate rows            | Drops all rows that have duplicate values in one or more columns                                      |\n| Fill missing values            | Replace cells with missing values with a new value                                                    |\n| Find and replace               | Replace cells with exact matching pattern                                                             |\n| Group by column and aggregate  | Group by columns and aggregate results                                                                |\n| Strip whitespace               | Remove whitespace from the beginning and end of text                                                  |\n| Split text                     | Split a column into several columns based on a user defined delimiter                                 |\n| Convert text to capital case   | Capitalize the first character of a string with the option to apply to all words                      |\n| Convert text to lowercase      | Convert text to lowercase                                                                             |\n| Convert text to uppercase      | Convert text to UPPERCASE                                                                             |\n| String transform by example    | Automatically perform string transformations when a pattern is detected from the examples you provide |\n| DateTime formatting by example | Automatically perform DateTime formatting when a pattern is detected from the examples you provide    |\n| New column by example          | Automatically create a column when a pattern is detected from the examples you provide.               |\n| Scale min/max values           | Scale a numerical column between a minimum and maximum value                                          |\n| Custom operation               | Automatically create a new column based on examples and the derivation of existing column(s)          |\n\n## Troubleshooting\n\n### General kernel connectivity issues\n\nFor general connectivity issues, please see the \"Connecting to a Python kernel\" section above on alternative methods to connect. To debug issues related to the local Python interpreter option, one way to potentially fix the issue would be to try installing different versions of the Jupyter and Python extensions. For example, if stable versions of the extensions are installed then one could try installing the pre-release (and vice versa). If the issue is new, then consider reverting to a previous version.\n\nTo clear an already cached kernel, you can run the `Data Wrangler: Clear cached runtime` command from the command palette (Ctrl/Cmd+Shift+P).\n\n### UnicodeDecodeError\n\nIf you run into a UnicodeDecodeError when opening a data file directly from Data Wrangler, then this could be caused by two possible issues:\n\n1. The file you're trying to open has an encoding other than utf-8, and/or\n2. The file is corrupted.\n\nTo work around this error, you\u2019ll need to open Data Wrangler from a Jupyter Notebook instead of directly from a data file. Use a Jupyter Notebook to read the file using Pandas, for example using the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) method. Within that read method, use the _encoding_ and/or _encoding_errors_ parameters to define the encoding to use or how to handle encoding errors. If you don\u2019t know which encoding might work for this file, you can try a library such as [chardet](https://github.com/chardet/chardet) to try to infer an encoding that works.\n\n## Questions and feedback\n\nIf you have problems, have feature requests, or any other feedback, please submit an Issue on our GitHub repository: [https://github.com/microsoft/vscode-data-wrangler/issues/new/choose](https://github.com/microsoft/vscode-data-wrangler/issues/new/choose)\n\n## Data and telemetry\n\nThe Microsoft Data Wrangler Extension for Visual Studio Code collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://go.microsoft.com/fwlink/?LinkId=521839) to learn more. This extension respects the `telemetry.telemetryLevel` setting which you can learn more about at https://code.visualstudio.com/docs/getstarted/telemetry.\n", "repo_name": "vscode-data-wrangler", "org_name": "microsoft", "org_repo": "microsoft/vscode-data-wrangler", "platform_org_repo": "github+microsoft/vscode-data-wrangler", "link_to_repo": "https://github.com/microsoft/vscode-data-wrangler", "platform": "github", "language": null, "stargazers_count": 164, "watchers_count": 164}, {"README_text": "CHERIoT RTOS\n============\n\nThis repository contains the core RTOS for the [CHERIoT platform](https://aka.ms/cheriot-tech-report).\nThis is currently a *research project* that has been open sourced to enable wider collaboration.\nIt is not yet in a state where it should be used in production: in particular, security issues will currently be fixed in the main branch of the repo with no coordinated disclosure.\n\nTo use this, you will also some dependencies.\nThe [getting started guide](docs/GettingStarted.md) describes in detail how to build these:\n\n - A version of LLVM with CHERIoT support, [currently in the cheriot branch of CHERI LLVM](https://github.com/CTSRD-CHERI/llvm-project/tree/cheriot)\n - An implementation of the ISA (e.g. [CHERIoT-Ibex](https://github.com/Microsoft/cheriot-ibex) or the emulator generated from [the formal model](https://github.com/Microsoft/cheriot-sail)))\n\nThese dependencies are pre-installed in the dev container that will be automatically downloaded if you open this repository in Visual Studio Code or by hitting `.` to open it in GitHub Code Spaces.\n\nTo clone this repository, make sure that you use `git clone --recurse` so that you get submodules.\nThis repository contains symbolic links.\n**IMPORTANT**: If you wish to clone this repository on *Windows*, make sure that you have enabled Developer Mode and run `git config --global core.symlinks true`.\nYou must do this *before* cloning the repository.\n\nThe [getting started guide](docs/GettingStarted.md) describes how to install these and how to build the test suite and examples in this repository.\n\nThe RTOS is privilege separated into a small number of core components as described in the [architecture document](docs/architecture.md).\nThe C/C++ extensions used by the compartmentalisation model are described in the [language extensions document](docs/LanguageExtensions.md).\n\nIf you have questions, please see the [frequently asked questions](docs/faq.md) document or raise an issue.\n\n\nBuilding firmware images\n------------------------\n\nThis repo contains the infrastructure for building CHERIoT firmware images.\n\n**NOTE**: The build system is currently based on xmake, but we have encountered a number of issues with our use of xmake and may switch to an alternative build system at some point.\n\nClone this repo into your project and create an `xmake.lua` referring to it.\nThe file should start with this line:\n\n```lua\nincludes(\"{path to this repo}/sdk\")\n```\n\nThis will enable debug and release configuration (specified with `-m {release,debug}`).\nBoth are compiled with `-Oz` (optimise for size, even at the expense of performance).\n\nNext you need to specify that you want to use the compiler provided by this SDK:\n\n```\nset_toolchains(\"cheriot-clang\")\n```\n\nNow you can add targets.\nWe provide helpers for creating library, compartment, and firmware targets.\nThese work just like normal xmake targets:\n\n```\nlibrary(\"lib\")\n    add_files(\"shared_c_file.c\", \"shared_cxx_file.cc\")\n\ncompartment(\"example\")\n    add_files(\"example/example.c\")\n\nfirmware(\"example-firmware\")\n    add_deps(\"lib\", \"example\")\n    on_load(function(target)\n        target:values_set(\"threads\", {\n            {\n                compartment = \"example\",\n                priority = 1,\n                entry_point = \"entry_point\",\n                stack_size = 0x400,\n                trusted_stack_frames = 2\n            }\n        })\n```\n\nThe firmware description specifies the compartments and libraries that this system depends on and specifies the threads.\nThreads are listed as a Lua array of objects, each of which has the following keys:\n\n - `compartment` specifies the name of the compartment in which this thread starts.\n - `entry_point` specifies the name of the exported function from that compartment that the thread will start executing.\n   This function must take no arguments and return `void`.\n - `stack_size` specifies the size, in bytes, of the stack for this thread.\n - `trusted_stack_frames` specifies the number of trusted stack frames (the maximum depth of cross-compartment calls possible on this thread).\n   Note that any call that may yield is likely to require at least one additional trusted stack frame to call the scheduler so, for example, a blocking call to `malloc` requires three stack frames (the caller, the allocator, and the scheduler).\n\n```sh\n$ xmake config --sdk={path to CHERIoT LLVM tools}\n$ xmake\n```\n\nThis will create the output in `build/cheriot/cheriot/{release,debug}/{name of firmware target}.\nIt will also create a `.dump` file in the same location giving the objdump output of the same target.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cheriot-rtos", "org_name": "microsoft", "org_repo": "microsoft/cheriot-rtos", "platform_org_repo": "github+microsoft/cheriot-rtos", "link_to_repo": "https://github.com/microsoft/cheriot-rtos", "platform": "github", "language": "C++", "stargazers_count": 77, "watchers_count": 77}, {"README_text": "# CHERIoT Sail model\n\nThis repository contains an implementation of the CHERIoT ISA in [Sail](http://github.com/rems-project/sail).\nIt contains an executable description of the CHERIoT instruction set that can be used to build an instruction set emulator and also prove [properties](properties) of the ISA using Sail's SMT support.\nA full description of the ISA, including extracts from this repository, can be found in the [CHERIoT technical report](https://aka.ms/cheriot-tech-report).\n\nThe code is dervied from [sail-cheri-riscv](http://github.com/CTSRD-CHERI/sail-cheri-riscv)\nand uses the [sail-riscv](http://github.com/rems-project/sail-riscv) model as a submodule.\n\n# Building\n\nThe easiest way to use this emulator is to use the dev container for the [CHERIoT RTOS](http://github.com/microsoft/cheriot-rtos).\n\nAlternatively, if you wish to build from source you must first install dependencies, including Sail. For example, on Ubuntu 20.04 we have tested:\n\n```\nsudo apt update\nsudo apt install opam z3 libgmp-dev\nopam init\nopam install sail\n```\n\nThen clone the repo and apply some local patches to the sail-riscv submodule (we're working to upstream these changes but aren't there yet):\n\n```\ngit clone --recurse-submodules https://github.com/microsoft/cheriot-sail\ncd cheriot-sail\nmake patch_sail_riscv\n```\n\nFinally build the C emulator:\n\n```\nmake csim\n```\n\nThis will produce an executable in `c_emulator/cheriot_sim` that can be used to run ELF files produced by the CHERIoT compiler.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "cheriot-sail", "org_name": "microsoft", "org_repo": "microsoft/cheriot-sail", "platform_org_repo": "github+microsoft/cheriot-sail", "link_to_repo": "https://github.com/microsoft/cheriot-sail", "platform": "github", "language": "Makefile", "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# GUI for MDE API sample app\nSimple PowerShell GUI for Microsoft Defender for Endpoint API machine actions.\n![image](https://user-images.githubusercontent.com/25099900/195406810-55511f50-d1c7-4e80-94e1-945cfca2a219.png)\n## Get started\n1. Create Azure AD application as described here: https://learn.microsoft.com/en-us/microsoft-365/security/defender-endpoint/apis-intro?view=o365-worldwide\n2. Grant the following API permissions to the application:\n\n| Permission | Description |\n|-------------------------|----------------------|\n| AdvancedQuery.Read.All\t| Run advanced queries |\n| Machine.Isolate |\tIsolate machine |\n| Machine.ReadWrite.All |\tRead and write all machine information (used for tagging) |\n| Machine.Scan |\tScan machine |\n\n3. Create application secret.\n## Usage\n1. **Connect** with AAD Tenant ID, Application Id and Application Secret of the application created earlier.\n2. **Get Devices** that you want to perform actions on, using one of the following methods:\n    * Advanced Hunting query (query result should contain DeviceName and DeviceId fields)\n    * CSV file (single Name column with machine FQDNs)\n    * Devices list separated with commas\n3. Confirm selection in PowerShell forms pop-up.\n4. Choose action that you want to perform on **Selected Devices**, the following actions are currently available:\n    * Specify device tag in text box and **Apply tag**.\n    * Run **AV Scan**.\n    * **Isolate**/Release device.\n5. Verify actions result with **Logs** text box.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mde-api-gui", "org_name": "microsoft", "org_repo": "microsoft/mde-api-gui", "platform_org_repo": "github+microsoft/mde-api-gui", "link_to_repo": "https://github.com/microsoft/mde-api-gui", "platform": "github", "language": "PowerShell", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# C# Dev Kit for Visual Studio Code\r\n\r\nThis repository is where we (Microsoft) gather and interact with the community around the [C# Dev Kit extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit) for Visual Studio Code. To report an issue here, ideally you would use the 'Report an Issue' capability within VS Code which will log an issue in this repository for us to triage and keep updated.\r\n\r\n## Contributing\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Triage\r\n\r\nWhen triaging issues in this repo teams are expected to follow the [triage guidelines](TRIAGE.md).\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \r\ntrademarks or logos is subject to and must follow \r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "vscode-dotnettools", "org_name": "microsoft", "org_repo": "microsoft/vscode-dotnettools", "platform_org_repo": "github+microsoft/vscode-dotnettools", "link_to_repo": "https://github.com/microsoft/vscode-dotnettools", "platform": "github", "language": null, "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# US Core Test Kit\n\nThis is an [Inferno](https://github.com/inferno-community/inferno-core) test kit\nfor the US Core Implementation Guide\n[v3.1.1](http://hl7.org/fhir/us/core/STU3.1.1/),\n[v4.0.0](http://hl7.org/fhir/us/core/STU4/), and\n[v5.0.1](http://hl7.org/fhir/us/core/STU5.0.1/)\n\nIt is highly recommended that you use [Docker](https://www.docker.com/) to run\nthese tests so that you don't have to configure ruby and the FHIR validator\nservice.\n\n## Instructions\n\n- Clone this repo.\n- Run `setup.sh` in this repo.\n- Run `run.sh` in this repo.\n- Navigate to `http://localhost`. The US Core test suite will be available.\n\n## License\nCopyright 2022 The MITRE Corporation\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use\nthis file except in compliance with the License. You may obtain a copy of the\nLicense at\n```\nhttp://www.apache.org/licenses/LICENSE-2.0\n```\nUnless required by applicable law or agreed to in writing, software distributed\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n", "repo_name": "ahds-inferno", "org_name": "microsoft", "org_repo": "microsoft/ahds-inferno", "platform_org_repo": "github+microsoft/ahds-inferno", "link_to_repo": "https://github.com/microsoft/ahds-inferno", "platform": "github", "language": "Ruby", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# \ud83e\uddf0 FinOps toolkit\n\n<sup> \u2139\ufe0f _This project is in the early concept phase. If interested, please let us know in [discussions](https://github.com/microsoft/cloud-hubs/discussions/categories/general)._</sup>\n\nThe FinOps toolkit is a collection of customizable ARM templates used to build and deploy various FinOps solutions that automate and extend native Microsoft Cost Management capabilities. The toolkit aims to include:\n\n- Starter kits that help you get started with Cost Management.\n- Automation scripts to streamline cost configuration and management at scale.\n- Advanced solutions to facilitate building custom solutions.\n\nWhile you will find numerous ARM templates within this repository, our main focus is to build a reliable, trustworthy platform for cost analytics, insights, and optimization, which we call **FinOps hubs**.\n\nFinOps hubs are **virtual command centers** for leaders throughout the organization to report on, monitor, and optimize cost based on their organizational needs. FinOps hubs focus on 3 core design principles:\n\n- **Be the standard**<br>_<sup>Strive to be the principal embodiment of the FinOps Framework.</sup>_\n- **Built for scale**<br>_<sup>Designed to support the largest accounts and organizations.</sup>_\n- **Open and extensible**<br>_<sup>Embrace the ecosystem and prioritize enabling the platform.</sup>_\n\n<br>\n\n## \ud83d\udcd7 Get started\n\nLooking to learn more about FinOps toolkit and how to get started? See [FinOps toolkit documentation \u27a1\ufe0f](./docs)\n\n<br>\n\n## \ud83d\udc69\u200d\ud83d\udcbb Contributing\n\nThere are many ways to participate. From reporting bugs and requesting features to reviewing or even making code changes. See the [contribution guide \u27a1\ufe0f](./CONTRIBUTING.md)\n\n<br>\n\n---\n\n<br>\n\n## \u2696\ufe0f Legal stuff\n\n### Code of conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Data collection\n\nUsage of FinOps toolkit templates and modules is tracked by using a `defaultTelemetry` deployment. Microsoft may use this information to improve our products and services. You may turn off the telemetry by using the `enableDefaultTelemetry` parameter on any Bicep module. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n> _**NOTE:** You can review what is included in this telemetry in the Azure portal by viewing the details for any `pid-00f1n0b5-*` deployments under the Deployments section of the corresponding scope (e.g., resource group, subscription)._\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n### License\n\nCopyright \u00a9 Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT license](LICENSE).\n", "repo_name": "cloud-hubs", "org_name": "microsoft", "org_repo": "microsoft/cloud-hubs", "platform_org_repo": "github+microsoft/cloud-hubs", "link_to_repo": "https://github.com/microsoft/cloud-hubs", "platform": "github", "language": "Bicep", "stargazers_count": 49, "watchers_count": 49}, {"README_text": "# MolSkill \ud83e\udd39\u232c\n\n[![ci](https://github.com/microsoft/molskill/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/microsoft/molskill/actions/workflows/ci.yml)\n[![Anaconda-Server Badge](https://anaconda.org/msr-ai4science/molskill/badges/platforms.svg)](https://anaconda.org/msr-ai4science/molskill)\n[![Anaconda-Server Badge](https://anaconda.org/msr-ai4science/molskill/badges/version.svg)](https://anaconda.org/msr-ai4science/molskill)\n\nThis repo contains associated code for the paper _Learning chemical intuition from humans in the loop_.\n\n## Installation\n\nWe recommend that you make a fresh conda environment (currently we only support Python 3.9-3.10 Linux builds) and install the provided conda package for convenience:\n\n```bash\nconda install molskill=*=py3{x}* -c msr-ai4science -c conda-forge\n```\n\nPlease substitute `{x}` above by either `9` or `10` depending on your python version. Additionally, you can also use the provided `environment.yml` file for manual installation.\n\nA CUDA-enabled GPU is not required for usage, but strongly recommended for speed if you plan on scoring a large amount of compounds. \n\n\n## Usage\n\nThis work mainly exposes the `MolSkillScorer` class under the `molskill.scorer` module. We interface with RDKit to provide predictions accordingly. The user only has to provide a list of molecular strings that they wish to score. \n\n```python\nfrom molskill.scorer import MolSkillScorer\n\nsmiles_strs = [\"CCO\", \"O=C(Oc1ccccc1C(=O)O)C\"] \n\nscorer = MolSkillScorer()\nscores = scorer.score(smiles_strs)\n```\n\nWe provide and use by default a pre-trained model on all the data that was collected during the original study. If a user wants to train custom models, please check the `train.py` script also included under this repository.\n\n**Note**: The default model and featurizer does not support non-organic elements or molecules with multiple fragments. Furthermore we suggest that you pass the NIBR filters on your compounds before running them through default scorer. We recommend doing this to avoid out-of-distribution biases, as the provided models have never seen a violating molecule during training time. The filters are nowadays available on the RDKit - a guide on how to apply those is provided [here](https://github.com/rdkit/rdkit/tree/master/Contrib/NIBRSubstructureFilters).\n\n\n## Citing\n\nIf you find this work or parts thereof useful, please consider citing the following BibTeX entry:\n\n```\n @article{choung2023,\n          place={Cambridge},\n          title={Learning chemical intuition from humans in the loop},\n          DOI={10.26434/chemrxiv-2023-knwnv},\n          journal={ChemRxiv},\n          publisher={Cambridge Open Engage},\n          author={Choung, Oh-Hyeon and Vianello, Riccardo and Segler, Marwin and Stiefl, Nikolaus and Jim\u00e9nez-Luna, Jos\u00e9},\n          year={2023}}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "molskill", "org_name": "microsoft", "org_repo": "microsoft/molskill", "platform_org_repo": "github+microsoft/molskill", "link_to_repo": "https://github.com/microsoft/molskill", "platform": "github", "language": "Python", "stargazers_count": 73, "watchers_count": 73}, {"README_text": "# Task Sequencing Robotics Simulator\n\nThe Task Sequencing Robotics Simulator is a Python-based simulator for (1) training robot manipulation tasks using reinforcement learning,\nand (2) connecting the trained task with other trained or programmed tasks to compose a sequenced execution.\n\nThe simulator is designed to work with the Bonsai Azure Service.\nThe simulator is designed to work with different physics/rendering engines as well as different robot configurations.\n\n## Samples\n\n**Training**\n\nTo run the sample code, you will need to connect the simulator to the Bonsai Azure Service\n(please copy the content of *src/samples/training/shadow_active_grasp.ink* to your Bonsai workspace).\n\nThe sample code will train a grasping brain using the PyBullet physics engine and the shadowhand_lite robotics hand.\nThe sample requires downloading the robot hand model from the shadow-robot/sr_common repository (melodic-devel branch).\nPlease use the Dockerfile for simplified setup.\n\n```\ndocker image build --tag <bonsai_workspace_name>.azurecr.io/tss:1.0 -f ./docker/Dockerfile_sample .\ndocker run -e SIM_WORKSPACE=<bonsai_workspace_id> -e SIM_ACCESS_KEY=<access_key> --network host <bonsai_workspace_name>.azurecr.io/tss:1.0\n```\n\nTo train using multiple simulator instances, please upload the built docker image to your Bonsai workspace.\n\n```\naz acr login -n <bonsai_workspace_name>\ndocker push <bonsai_workspace_name>.azurecr.io/tss:1.0\n```\n\n**Execution**\n\nPlease finish the above Training sample first (including building the docker image) and export a trained grasping brain.\n\nThe sample code will simulate a sequenced execution of tasks.\nPlease note that the current sample has some limitations where all task parameters must be defined a priori before execution.\n\n```\ndocker run -d -p 5000:5000 <bonsai_workspace_name>.azurecr.io/<bonsai_workspace_id>/<trained_brain_name>:<tag>\ndocker run -e SIM_WORKSPACE=w -e SIM_ACCESS_KEY=a -e RUNCODE=src/samples/execution/pick_place.py --network host <bonsai_workspace_name>.azurecr.io/tss:1.0\n```\n\nA video of the execution is saved under logs/ inside the docker container.\n(There are ways to run the container such as using *tail -f /dev/null* which could make accessing the logs easier.)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "task-sequencing-robotics-simulator", "org_name": "microsoft", "org_repo": "microsoft/task-sequencing-robotics-simulator", "platform_org_repo": "github+microsoft/task-sequencing-robotics-simulator", "link_to_repo": "https://github.com/microsoft/task-sequencing-robotics-simulator", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# 2023 IoT Level Up Skilling\n\n<img src=\"LevelUp.png\">\n\n<ul>\n  <li><a href=\"https://microsoft.sharepoint.com/:f:/t/LevelUpSkilling/EqjEEejJvYFMrZk7_gBUDloBImWTa4G0dXR58ubBFtxkjA?e=oKulIU\">Level-Up Skilling SharePoint Link</a>\n    <p>\n  <li><a href=\"/IoT Hub & DPS/README.md\">Session 1 - Azure IoT Hub & DPS</a>\n  <li><a href=\"/IoTEdge & Microagent/README.md\">Session 2 - IoT Edge and Defender for IoT MicroAgent\n  <li>Session 3 - MQTT, IoT SDKs & x509 Certificates\n  <li>Session 4 - Industrial IoT (IIoT)\n</ul>\n<p>\ncontact kevinsay@microsoft.com if you have any questions.\n", "repo_name": "2023iotlevelup", "org_name": "microsoft", "org_repo": "microsoft/2023iotlevelup", "platform_org_repo": "github+microsoft/2023iotlevelup", "link_to_repo": "https://github.com/microsoft/2023iotlevelup", "platform": "github", "language": "Python", "stargazers_count": 15, "watchers_count": 15}, {"README_text": " # CSS syntax highlighting in VS Code\n\nAdds syntax highlighting to CSS files in VS Code\n\nDerived from https://github.com/atom/language-css.\nOriginally [converted](http://flight-manual.atom.io/hacking-atom/sections/converting-from-textmate)\nfrom the [CSS TextMate bundle](https://github.com/textmate/css.tmbundle).\n\nContributions are greatly appreciated. Please fork this repository and open a\npull request to add snippets, make grammar tweaks, etc.\n", "repo_name": "vscode-css", "org_name": "microsoft", "org_repo": "microsoft/vscode-css", "platform_org_repo": "github+microsoft/vscode-css", "link_to_repo": "https://github.com/microsoft/vscode-css", "platform": "github", "language": "CoffeeScript", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "<!--\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n[1]https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\n-->\n\n<!-- PROJECT LOGO -->\n\n<p align=\"center\">\n  <a href=\"https://adoptium.net/aqavit\">\n    <img src=\"https://adoptium.net/images/aqavit-light.png\" alt=\"Logo\" width=\"250\">\n  </a>\n</p>\n<br />\n\n[![License](https://img.shields.io/github/license/Adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/blob/master/LICENSE)\n[![contributors](https://img.shields.io/github/contributors/adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/graphs/contributors)\n[![commit-activity](https://img.shields.io/github/commit-activity/m/adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/commits/master)\n[![closed-issues](https://img.shields.io/github/issues-closed/adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/issues?q=is%3Aissue+is%3Aclosed)\n[![closed-pr](https://img.shields.io/github/issues-pr-closed/adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/pulls?q=is%3Apr+is%3Aclosed)\n[![release-date](https://img.shields.io/github/release-date/adoptium/aqa-tests)](https://github.com/adoptium/aqa-tests/releases)\n<br />\n[![slack](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=white)](https://adoptium.net/slack/)\n[![Twitter](https://img.shields.io/twitter/follow/adoptium?style=social)](https://twitter.com/adoptium)\n\n# Adoptium Testing\n\n#### Guide to the Test Jobs at Adoptium\n\nFor nightly and release builds, there are test jobs running as part of the Adoptium continuous delivery pipelines.  There is a [blog post and brief presentation](https://blog.adoptopenjdk.net/2017/12/testing-java-help-count-ways) that explains what testing we run and how they fit into the overall delivery pipeline.  As the world of testing at Adoptium is evolving and improving quickly, some documentation may fall behind the march of progress.  Please let us know and help us keep it up-to-date, and ask questions at the [Adoptium testing Slack channel](https://adoptium.slack.com/archives/C5219G28G)!\n\n![CI pipeline view](doc/diagrams/ciPipeline.jpg)\n\n#### Test 'Inventory'\n\nThe directory structure in this aqa-tests repository is meant to reflect the different types of test we run (and pull from lots of other locations).  The diagrams below show the test make target for each of the types, along with in-plan, upcoming additions (denoted by dotted line grey boxes). The provided links jump to test jobs in Jenkins (ci.adoptium.net).\n\n```mermaid\ngraph TD\n    A[openjdk-tests] -->B[make _perf]\n    A[openjdk-tests] -->C[make _openjdk]\n    A[openjdk-tests] -->D[make _system]\n    A[openjdk-tests] -->E[make _functional]\n    A[openjdk-tests] -->F[make _jck]\n    A[openjdk-tests] -->G[make _external]\n    B[make _perf] -->|perf|H[performance]\n    H[performance] -->|_sanity.perf|I[.....]\n    H[performance] -->|_extended.perf|J[..]\n    C[make _openjdk] -->|openjdk|K[openjdk]\n    D[make _system] -->|system|L[system]\n    E[make _functional] -->|functional|M[functional]\n    F[make _jck] -->|jck|N[jck]\n    G[make _external] -->|external|O[external]\n    O[external] -->|_sanity.external|P[...]\n    O[external] -->|_extended.external|Q[....] \n    \n```\n\n--- \n\n##### [openjdk](https://ci.adoptium.net/view/Test_openjdk/) tests - OpenJDK regression tests \nTests from OpenJDK\n\n--- \n\n##### [system](https://ci.adoptium.net/view/Test_system/) tests - System and load tests \nTests from the adoptium/aqa-systemtest repo\n\n--- \n\n##### [external](https://ci.adoptium.net/view/Test_external/) tests - 3rd party application tests\nTest suites from a variety of applications, along with microprofile TCKs, run in Docker containers\n\n```mermaid\ngraph TD\n A[openjdk-tests] -->|make _external| B[external]\n    B --> C[derby]\n    B --> D[elasticsearch]\n    B --> E[example]\n    B --> F[jenkins]\n    B --> G[kafka]\n    B --> H[lucene-solr]\n    B -->|_sanity.external|I[scala]\n    B --> J[tomcat]\n    B --> K[wildfly]\n    B --> L[openliberty]\n    B --> M[geode]\n    B --> N[hbase]\n    B --> O[akka]\n    B --> P[logstash]\n    B --> Q[openliberty-mp-tck]\n    B -->|_extended.external|R[payara-mp-tck]\n    B --> S[thorntail-mp-tck]\n   \n```\n\n--- \n\n##### [perf](https://ci.adoptium.net/view/Test_perf/) tests - Performance benchmark suites \nPerformance benchmark tests (both full suites and microbenches) from different open-source projects such as Acme-Air and adoptium/bumblebench\n\n```mermaid\ngraph TD\n A[openjdk-tests] -->|make _perf| B[performance]\n    B -->|_sanity.perf| C[bbench]\n    B --> D[idle_micro]\n    B --> E[odm]\n    B -->|_extended.perf| F[liberty_dt]\n    B --> G[acme_air]\n   ```\n\n--- \n\n##### [functional](https://ci.adoptium.net/view/Test_functional/) tests - Unit and functional tests\nFunctional tests not originating from the openjdk regression suite, that include locale/language tests and a subset of implementation agnostic tests from the openj9 project.\n\n--- \n\n##### jck tests - Compliance tests\nTCK tests (under the OpenJDK Community TCK License Agreement), in compliance with the license agreement.  While this test material is currently not run at the Adoptium project (see the [support statement](https://adoptopenjdk.net/support.html#jck) for details), those with their own OCTLA agreements may use the Adoptium test automation infrastructure to execute their TCK test material in their own private Jenkins servers.\n\n--- \n\n#### Guide to Running the Tests Yourself\nFor more details on how to run the same tests that we run at Adoptium on your laptop or in your build farm, please consult our [User Guide](doc/userGuide.md) (work in progress).\n\n#### What is our motivation?\nWe want:\n- better, more flexible tests, with the ability to apply certain types of testing to different builds\n- a common way to easily add, edit, group, include, exclude and execute tests on adoptium builds\n- the latitude to use a variety of tests that use many different test frameworks\n- test results to have a common look & feel for easier viewing and comparison\n\nThere are a great number of tests available to test a JVM, starting with the OpenJDK regression tests.  In addition to running the OpenJDK regression tests, we will increase the amount of testing and coverage by pulling in other open tests.  These new tests are not necessarily written using the jtreg format.\n\nWhy the need for other testing?  The OpenJDK regression tests are a great start, but eventually you may want to be able to test how performant is your code, and whether some 3rd party applications still work.  We will begin to incorporate more types of testing, including:\n- additional API and functional tests\n- stress/load tests\n- system level tests such as 3rd party application tests\n- performance tests\n- TCK tests\n\nThe test infrastructure in this repository allows us to lightly yoke a great variety of tests together to be applied to testing the adoptium binaries.  By using an intentionally thin wrapper around a varied set of tests, we can more easily run all types of tests via make targets and as stages in our Jenkins CI pipeline builds.\n\n\n#### How can you help?\nYou can:\n- browse through the [aqa-tests issues list](https://github.com/adoptium/aqa-tests/issues), select one, add a comment to claim it and ask questions\n- browse through the [aqa-systemtest issues](https://github.com/adoptium/aqa-systemtest/issues) or [stf issues](https://github.com/adoptium/stf/issues), claim one with a comment and dig right in\n- triage live test jobs at [ci.adoptium.net](https://ci.adoptium.net), check out the [triage doc](https://github.com/adoptium/aqa-tests/blob/master/doc/Triage.md) for guidance\n  - if you would like to regularly triage test jobs, you can optionally 'sign up for duty' via the [triage rotas](https://github.com/adoptium/aqa-tests/wiki/AdoptOpenJDK-Test-Triage-Rotas)\n- ask questions in the [#testing channel](https://adoptium.slack.com/archives/C5219G28G) \n", "repo_name": "aqa-tests", "org_name": "microsoft", "org_repo": "microsoft/aqa-tests", "platform_org_repo": "github+microsoft/aqa-tests", "link_to_repo": "https://github.com/microsoft/aqa-tests", "platform": "github", "language": "HTML", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "Azure AD B2C Embedded Webview\n============================\n\nAzure AD B2C Embedded Webview is a very simple Flutter package that demonstrates how to use the embedded web view to sign in users with Azure AD B2C.\nCurrently, using Flutter packages - [appAuth](https://pub.dev/packages/flutter_appauth) and [flutter_azure_b2c](https://pub.dev/packages/flutter_azure_b2c) redirects to browser and doesn't provide in-app experience.\n\nThis package embeds the web view of the user flow endpoint using [flutter_appview](https://pub.dev/packages/webview_flutter) and redirects the user as per onRedirect callback method.\n\n## Features\n\nEmbedded web view for Azure AD B2C for providing in-app experience\nRedirects to the route specified in redirectRoute after successful sign in\nSuccessfully secures the id token or access token in the app using flutter secure storage\nNavigates to screen in app after successful sign in\n\n## Getting started\nTo use the package in your Flutter app, add the following code to your main.dart file:\n```yaml\ndependencies:\n  aad_b2c_webview: <latest_version>\n```\n\n## Usage\n\n### Simple In-built Sign In Button\n\nTo add the easy to use sign in with microsoft button simply use the AADLoginButton widget \nand a beautiful sign in button appears as shown below.\n\n```dart\nimport 'package:aad_b2c_webview/src/login_azure.dart';\n\nclass LoginPage extends StatefulWidget {\n  const LoginPage({Key? key}) : super(key: key);\n\n  @override\n  State<LoginPage> createState() => _LoginPageState();\n}\n\nclass _LoginPageState extends State<LoginPage> {\n  String? jwtToken;\n  String? refreshToken;\n\n  @override\n  Widget build(BuildContext context) {\n    const aadB2CClientID = \"<clientId>\";\n    const aadB2CRedirectURL = \"<azure_active_directory_url_redirect>\";\n    const aadB2CUserFlowName = \"B2C_<name_of_userflow>\";\n    const aadB2CScopes = ['openid', 'offline_access'];\n    const aadB2CUserAuthFlow =\n        \"https://<tenant-name>.b2clogin.com/<tenant-name>.onmicrosoft.com\"; // https://login.microsoftonline.com/<azureTenantId>/oauth2/v2.0/token/\n    const aadB2TenantName = \"<tenant-name>\";\n\n    return Scaffold(\n      body: Padding(\n        padding: const EdgeInsets.all(16.0),\n        child: Column(\n          mainAxisAlignment: MainAxisAlignment.center,\n          crossAxisAlignment: CrossAxisAlignment.center,\n          children: [\n            /// Login flow\n            AADLoginButton(\n              userFlowUrl: aadB2CUserAuthFlow,\n              clientId: aadB2CClientID,\n              userFlowName: aadB2CUserFlowName,\n              redirectUrl: aadB2CRedirectURL,\n              context: context,\n              scopes: aadB2CScopes,\n              onAnyTokenRetrieved: (Token anyToken) {},\n              onIDToken: (Token token) {\n                jwtToken = token.value;\n              },\n              onAccessToken: (Token token) {},\n              onRefreshToken: (Token token) {\n                refreshToken = token.value;\n              },\n              onRedirect: (context) => {},\n            ),\n\n            /// Refresh token\n            TextButton(\n              onPressed: () async {\n                if (refreshToken != null) {\n                  AzureTokenResponse? response =\n                      await ClientAuthentication.refreshTokens(\n                    refreshToken: refreshToken!,\n                    tenant: aadB2TenantName,\n                    policy: aadB2CUserAuthFlow,\n                    clientId: aadB2CClientID,\n                  );\n                  if (response != null) {\n                    refreshToken = response.refreshToken;\n                    jwtToken = response.idToken;\n                  }\n                }\n              },\n              child: const Text(\"Refresh my token\"),\n            )\n          ],\n        ),\n      ),\n    );\n  }\n}\n\n```\n\n### Custom Sign in\n\nSimply call page direct or use custom build sign in button to call webview page\n\n```dart\nimport 'package:aad_b2c_webview/aad_b2c_webview.dart';\nimport 'package:flutter/foundation.dart';\nimport 'package:flutter/material.dart';\n\nclass MyLoginPage extends StatelessWidget {\n  const MyLoginPage({Key? key}) : super(key: key);\n\n  @override\n  Widget build(BuildContext context) {\n    const aadB2CClientID = \"<clientId>\";\n    const aadB2CRedirectURL = \"<azure_active_directory_url_redirect>\";\n    const aadB2CUserFlowName = \"B2C_<name_of_userflow>\";\n    const aadB2CScopes = ['openid', 'offline_access'];\n    const aadB2CUserAuthFlow =\n        \"https://<tenant-name>.b2clogin.com/<tenant-name>.onmicrosoft.com\"; // https://login.microsoftonline.com/<azureTenantId>/oauth2/v2.0/token/\n    const aadB2TenantName = \"<tenant-name>\";\n\n    return Scaffold(\n      body: ADB2CEmbedWebView(\n        tenantBaseUrl: aadB2CUserAuthFlow,\n        userFlowName: aadB2CUserFlowName,\n        clientId: aadB2CClientID,\n        redirectUrl: aadB2CRedirectURL,\n        scopes: aadB2CScopes,\n        onAnyTokenRetrieved: (Token anyToken) {},\n        onIDToken: (Token token) {},\n        onAccessToken: (Token token) {},\n        onRefreshToken: (Token token) {},\n      ),\n    );\n  }\n}\n```\n\n### Send Optional Parameters\n\nIn the `AADLoginButton` widget, you have the flexibility to include optional parameters in order to customize the URL according to your specific requirements. The `optionalParameters` parameter allows you to pass a list of `OptionalParam` objects, where each object consists of a key and a value that represent the parameter name and its corresponding value, respectively.\n\nHere's an example of how you can utilize the `optionalParameters` parameter to add new parameters to the URL:\n```dart\noptionalParameters: [\n  OptionalParam(key: \"parameter1\", value: \"value1\"),\n  OptionalParam(key: \"parameter2\", value: \"value2\"),\n],\n```\n\nIn the above example, we include two optional parameters: \"parameter1\" with the value \"value1\" and \"parameter2\" with the value \"value2\". You can include multiple `OptionalParam` objects within the list to incorporate multiple optional parameters in the URL.\n\nThese optional parameters provide you with the ability to customize the behavior of the URL and conveniently transmit additional information as per your needs.\n\nPlease adapt the example to suit your specific use case or requirements.", "repo_name": "aad_b2c_webview", "org_name": "microsoft", "org_repo": "microsoft/aad_b2c_webview", "platform_org_repo": "github+microsoft/aad_b2c_webview", "link_to_repo": "https://github.com/microsoft/aad_b2c_webview", "platform": "github", "language": "Dart", "stargazers_count": 21, "watchers_count": 21}, {"README_text": "# Microsoft code coverage tools\n\nMicrosoft code coverage functionality is closed source. This repository contains documentation and samples. You can also use it to report any issues related to `Microsoft.CodeCoverage` NuGet package, `dotnet-coverage` NuGet package or Visual Studio code coverage functionality.\n\n## Documentation \n\n* Documentation for `dotnet-coverage` tool is available at https://aka.ms/dotnet-coverage.\n* Documentation for `Microsoft.CodeCoverage` is available at https://learn.microsoft.com/visualstudio/test/customizing-code-coverage-analysis.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "codecoverage", "org_name": "microsoft", "org_repo": "microsoft/codecoverage", "platform_org_repo": "github+microsoft/codecoverage", "link_to_repo": "https://github.com/microsoft/codecoverage", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "![Fast Track for Azure](./readme-assets/fta.png)\nMove to Azure efficiently with customized guidance from Azure engineering. [FastTrack for Azure \u2013 Benefits and FAQ | Microsoft Azure](https://azure.microsoft.com/en-us/programs/azure-fasttrack/)\n\n# REST Analytics Quickstart\n\nThis repository contains a template Azure Synapse Analytics project designed to ingest data that is available through REST APIs.\n\n- Use GitHub Actions to deploy all the Azure resources you need to get started with API data ingestion in a matter of minutes\n- Explore the OpenAQ API and get insights from sensor data\n- Understand watermarking and metadata-driven data ingestion techniques\n- Use the templates a starting point to integrate our own APIs\n\n## Table of contents\n- [Overview](#overview)\n- [Prerequisites](#prerequisites)\n- [Solution Architecture](#solution-architecture)\n- [Usage](#usage)\n- [Result](#result)\n- [Contributing](#contributing)\n- [Trademarks](#trademarks)\n\n## Overview\n\nBringing data from third party APIs into a Data Lake for further processing and reporting is a very common scenario in Analytics. Azure Synapse and Data Factory pipelines contain dozens of different connectors to help you integrate these data sources, but what happens when you need to integrate a custom API, or one that does not have a built-in connector? For those cases, the REST connector can help you bring in data from virtually any REST API, without depending on a specific connector.\n\nIn this repository, you will find tools to add a custom API and incrementally load data from it. We have used the [OpenAQ API](https://docs.openaq.org/docs) as an example so you can quickly test the pipelines. The API exposes air quality data collected from multiple sensors. In this example, we will copy this data to our own Azure Data Lake and perform simple analytics and visualizations to prove out the concepts.\n\nYou may the adapt the pipelines to collect data from any REST API of your choice, even private endpoints behind firewalls and authentication.\n\n## Solution Architecture\n\n![Solution Architecture](./readme-assets/solution-architecture.drawio.png)\n\n\n## Prerequisites\n\nTo execute the steps below, you will need:\n\n- An active Azure subscription\n- Contributor access to a resource group\n\n## Usage\n\n|Description|Screenshot|\n|---|---|\n|Fork this repository | ![Fork Repository](./readme-assets/01-fork-repository.png) |\n|Create resource group to deploy the quickstart resources | ![Create Resource Group](./readme-assets/02-create-rg.png) |\n|Determine your Azure AD user ID. You can run this command from the CLI: <br> `az ad user show --id \"YOUR_EMAIL@COMPANY.COM\" --query id` | ![Get user ID](./readme-assets/03-get-user-id.png) |\n|Create a service principal with Contributor access to your resource group. You may use the Azure portal or the following CLI command to so so from the terminal: <br> `az ad sp create-for-rbac --name \"PRINCIPAL_NAME\" --role contributor --scopes /subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP_NAME --sdk-auth` <br> You will need the command's output for the next step. |  ![Create Service Principal](./readme-assets/04-create-sp.png) |\n|Go to Settings -> Secrets -> Actions and add the following secrets: <br> - CLIENT_ID <br> - CLIENT_SECRET <br> - SUBSCRIPTION_ID <br> - TENANT_ID <br> Use the information from the service principal created in the last step. | ![Create Secrets](./readme-assets/05-create-secrets.png) |\n|Go to Actions and enable workflows for the new repository | ![Enable Actions](./readme-assets/06-enable-actions.png) |\n|Run the `workspace deployment` workflow, which will deploy the Synapse workspace and all the template resources | ![Enable Actions](./readme-assets/07-run-workflow.png) |\n|Go the storage account and grant the following permissions: <br> - Storage Blob Data Contributor to yourself and to the Synapse Managed Identity <br> - Storage Table Data Contributor to yourself and to the Synapse Managed Identity | ![Allow access](./readme-assets/08-allow-access.png) |\n|Check out and (optionally) run the pipelines under \"OpenAQ\". | ![Review Pipelines](./readme-assets/09-review-pipelines.png) |\n\n## Result\n\nThe following pipelines are available for you to test and integrate your own APIs:\n\n- **1_OpenAQ_Set_Up_Metadata**: Checks if the metadata table exists and creates it if not.\n- **2_OpenAQ_Incremental_Load**: Loads one chunk of data from the API for one sensor. \n    - Starts from the current watermark, or the MinDate parameter if no watermark is found.\n    - Loads up until an amount of minutes determined by the DeltaMinutes parameter. Will fail if this puts the end of the window past the MaxDate parameter.\n    - Saves data in parquet format, partitioning by manufacturer, device, year, month and day. Check out the Copy Data activity for more details.\n    - Updates the watermark date to the end of the window loaded.\n- **3_OpenAQ_Sync**: Sets up metadata and runs Incremental Load for one sensor until the MaxDate parameter is reached. \n    - This will load all new data, from the current watermark until MaxDate. \n    - Please monitor this pipeline closely, as this can be a large operation. The default parameters for MinDate and MaxDate will not consume significant resources.\n- **4_OpenAQ_Metadata_Driven**: Lists sensors and runs 3_OpenAQ_Sync for each one. \n    - The default parameters will limit the listing operation to 20 sensors. In a real-world scenario, you'd have all active sensors being listed and sync'ed.\n- **5_OpenAQ_Cleanup_Location**: Convenience pipeline to clear metadata and parquet files for a specific sensor. Useful for testing.\n- **6_OpenAQ_Full Load**: Runs Cleanup and Sync operations in sequence for one sensor. Should rarely be used, only in case metadata gets out of sync or for testing purposes.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "ftatoolkit-rest-analytics-quickstart", "org_name": "microsoft", "org_repo": "microsoft/ftatoolkit-rest-analytics-quickstart", "platform_org_repo": "github+microsoft/ftatoolkit-rest-analytics-quickstart", "link_to_repo": "https://github.com/microsoft/ftatoolkit-rest-analytics-quickstart", "platform": "github", "language": "Bicep", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# SkillBot\n\nBot Framework v4 Skills with Dialogs sample.\n\nThis bot has been created using the [Bot Framework](https://dev.botframework.com); it shows how to use a skill dialog from a root bot.\n\n## Prerequisites\n\n- [.NET Framework SDK](https://dotnet.microsoft.com/download) version 6.0\n\n  ```bash\n  # determine dotnet version\n  dotnet --version\n  ```\n\n## Key concepts in this sample\n\nThe solution uses dialogs, within both a parent bot (`RootBot`) and a skill bot (`RootBot`).\nIt demonstrates how to post activities from the parent bot to the skill bot and return the skill responses to the user.\n\n- `RootBot`: this project shows how to consume a skill bot using a `SkillDialog`. It includes:\n  - A [root dialog](RootBot/Dialogs/MainDialog.cs) that can call different actions on a skill using a `SkillDialog`:\n    - To send events activities.\n    - To send message activities.\n    - To cancel a `SkillDialog` using `CancelAllDialogsAsync` that automatically sends an `EndOfConversation` activity to remotely let a skill know that it needs to end a conversation.\n  - A sample [AdapterWithErrorHandler](RootBot/AdapterWithErrorHandler.cs) adapter that shows how to handle errors, terminate skills and send traces back to the emulator to help debugging the bot.\n  - A [SkillsConfiguration](RootBot/SkillsConfiguration.cs) class that can load skill definitions from the appsettings.json file.\n  - A [startup](RootBot/Startup.cs) class that shows how to register the different root bot components for dependency injection.\n  - A [BotController](RootBot/Controllers/BotController.cs) that handles skill responses.\n\n- `SkillBot`: this project shows a modified CoreBot that acts as a skill. It receives event and message activities from the parent bot and executes the requested tasks. This project includes:\n  - An [ActivityRouterDialog](SkillBot/Dialogs/ActivityRouterDialog.cs) that handles Event and Message activities coming from a parent and performs different tasks.\n    - Event activities are routed to specific dialogs using the parameters provided in the `Values` property of the activity.\n    - Message activities are sent to CLU if configured and trigger the desired tasks if the intent is recognized.\n  - A sample [ActivityHandler](SkillBot/Bots/SkillBot.cs) that uses the `RunAsync` method on `ActivityRouterDialog`.\n\n## To try this sample\n\n- Clone the repository.\n\n  ```bash\n  git clone https://github.com/dannygar/skillsBot.git\n  ```\n\n- Clone the template appsettings.json files in both project directories [RootBot/appsettings.template.json](RootBot/appsettings.template.json) and [RootBot/appsettings.template.json](RootBot/appsettings.template.json) into corresponding `appsettings.json` files\n- Create a bot registration in the azure portal for the `RootBot` and update [RootBot/appsettings.json](RootBot/appsettings.json) with the AppId and password.\n- Create a bot registration in the azure portal for the `SkillBot` and update [SkillBot/appsettings.json](SkillBot/appsettings.json) with the AppId and password. \n- Update the BotFrameworkSkills section in [RootBot/appsettings.json](RootBot/appsettings.json) with the AppId for the skill you created in the previous step.\n- (Optional) Configure the CLUEndpoint, CLUAPIKey and CLUProjectName section in the [RootBot/appsettings.json](RootBot/appsettings.json) if you want to run message activities through LUIS.\n- Open the `PersonalAssistantBot.sln` solution and configure it to [start debugging with multiple processes](https://docs.microsoft.com/en-us/visualstudio/debugger/debug-multiple-processes?view=vs-2019#start-debugging-with-multiple-processes).\n\n## Testing the bot using Bot Framework Emulator\n\n[Bot Framework Emulator](https://github.com/microsoft/botframework-emulator) is a desktop application that allows bot developers to test and debug their bots on localhost or running remotely through a tunnel.\n\n- Install the Bot Framework Emulator version 4.8.0 or greater from [here](https://github.com/Microsoft/BotFramework-Emulator/releases)\n\n### Connect to the bot using Bot Framework Emulator\n\n- Launch Bot Framework Emulator\n- File -> Open Bot\n- Enter a Bot URL of `http://localhost:3978/api/messages`, the `MicrosoftAppId` and `MicrosoftAppPassword` for the `RootBot`\n\n## Deploy the bots to Azure\n\nTo learn more about deploying a bot to Azure, see [Deploy your bot to Azure](https://aka.ms/azuredeployment) for a complete list of deployment instructions.\n=======\n\n## Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft.\ntrademarks or logos is subject to and must follow:\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "skillBotTemplate", "org_name": "microsoft", "org_repo": "microsoft/skillBotTemplate", "platform_org_repo": "github+microsoft/skillBotTemplate", "link_to_repo": "https://github.com/microsoft/skillBotTemplate", "platform": "github", "language": "C#", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "## Project\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Introduction\ncheri-ibex is 32-bit RISC-V microcontroller which implements the CheriIoT ISA extension in addition to RV32IMCB. Same as the original ibex core, the design can be configured either with a 2-stage or a 3-stage pipeline. It has passed preliminary simulation and FPGA validation, and is currently undergoing  further verification as well as PPA analysis at Microsoft.\n\n## CheriIoT ISA support\n\ncheri-ibex supports all 30 instructions listed in the CheriIoT ISA specification, including\n\n- To query or test capabilities: cgetaddr, cgetbase, cgetlen, cgetperm, cgettag, cgettop, cgettype, ctestsubset, csetequalexact, csub\n- To modify or derive capabilities: auicgp, auipcc, candperm, ccleartag, cincaddr, cincaddrimm, cmove, cram, crrl, csetaddr, csetbounds, csetboundsexact, csetboundsimm, cseal, cunseal\n- To load/store capabilities from memory: clc, csc\n- To control the program flow: cjal, cjalr\n- To access special capability registers (SCR): cspecialrw\n\nCertain compressed instructions are also extended for capabilities, for example c.incaddr4cspn, c.incaddr16csp, c.jal, c.jalr. Also the RV64 c.ld and c.sd instructions are reused for c.clc and c.csc instructions\n\n## Register file\n\ncheri-ibex contains a register file implementation (cheri_regfile.sv) which extends a configurable number of the general purpose registers into CherIoT capabilities.\n\n## Load-store unit\n\ncheri-ibex extends its data bus to 33-bit, where the MSB 1-bit is used as a valid tag to differentiate between capabilities and normal integer data. The load-store unit is modified to support atomic capability load and store transactions according to the CherIoT ISA specification.\n\n## Configuration and status registers\n\nPer CherIoT specification, the following SCR's are implemented,\n- MTCC (address 28), which replaces mtvec\n- MTDC (address 29)\n- MScratchC (address 30)\n- MEPCC (address 31), which replaces mepc.\n\nIn addition, the following SCR's are added for debug support\n- CDPC (address 24)\n- CDScratch0 (address 25)\n- CDScratch1 (address 26)\n- CDBGCTRL (address 27)\n\nThe PC capability register (PCC) is also implemented as part of the CSR module.\n\n## CherIoT memory access rule checking\n\ncheri-ibex performs capability-based memory access rule checking including\n- data load/store accesses\n- capability load/store accesses\n- Instruction fetch (PCC-based)\n- jump target calculation (cjal and cjalr)\n\nExceptions are generated in the case of access rule violations.\n\n## Temporal memory safety support\n\nThe cheri-ibex CLC implementation provides an optional load-barrier feature. When enabled (cheri_tsafe_en_i == 1), CLC checks a memory area which contains shadow flag bits for all memory data blocks at 8-byte granularity. The tag bit of the loaded capability is cleared if the corresponding shadow bits == 1 (revoked).\n\n## Backward compatibility\n\ncheri-ibex provides a backward-compatibility mode which is enabled by setting the input cheri_pmode_i = 1. In this mode, the CheirIoT instructions can still execute, however all access rules are disabled and any binary code generated by non-Cheri RV32 compilers can run unmodified in cheri-ibex.\n\n## Design configuration parameters\n\ncheri-ibex design added the following configuration parameters,\n\n| Parameter | Description |\n| ----------- | ----------- |\n| CheriPPLBC | pipelined implementation of load-barrier CLC. <br />  0: non-pipelined implementation <br />  1: pipelined implementation (better performance but needs a separate memory read interface).|\n| CheriSBND2 | Select number of cycles taken by csetbounds* instructions. <br /> 0: csetbounds* takes 1 cycle. <br /> 1: csetbounds* takes 2 cycle (better fmax timing). |\n| MemCapFmt | Select the format used to store capabilities in memory. <br /> 0: use canonical memory capbility format. <br /> 1: use the alternative memory capability format (better memory access timing). |\n|HeapBase|32-bit starting address of the system heap memory. <br /> only capabilities whose base pointing to an address in the heap space are subject to load-barrier checks during CLC.|\n|TSMapSize|size of the shadow bits memory (in 32-bit words) used by the load-barrier operation. <br /> e.g., 1024 = 32k bits which covers 256kB heap memory. <br />This parameter is only used when CheriPPLSBC == 1.|\n|TSMapBase|Starting address of the shadow bits memory <br /> This parameter is only used when CheriPPLSBC == 0.|\n|TSMapTop|Ending address of the shadow bits memory <br /> This parameter is only used when CheriPPLSBC == 0.|\n\n\n## Debug support\n\ncheri-ibex supports cheri-aware RISC-V debugging via JTAG interface. The debug module is published separately at (link). General-purpose capability registers and SCR's can both be accessed via the JTAG interface. SBA accesses are supported as well.\n\nTo debug capability-related software issues, cheri-ibex also provides a debug feature which when enabled, escalates tag-clearing events defined in the CherIoT ISA spec (e.g, csetbounds length violations) into exceptions. Writing a 0x1 to the CDBGCTRL SCR (address 27) to enable this feature.\n\n## Timing and area\n\ncheri-ibex (with 3-stage pipeline) has been synthesized at 330MHz using TSMC 28nm HPC+ libraries (HVT only) and > 1GHz using TSMC n5 libraries (SVT only). The design size is ~70k gate equivalents.\n\nA detailed PPA analysis is under way at Microsoft.\n\n## Build the design\nSee [README-CHERI.md](https://github.com/microsoft/cheriot-ibex/blob/main/README-CHERI.md) for the list of RTL files need to compile/simulate/synthesize the cheriot_ibex design.\n", "repo_name": "cheriot-ibex", "org_name": "microsoft", "org_repo": "microsoft/cheriot-ibex", "platform_org_repo": "github+microsoft/cheriot-ibex", "link_to_repo": "https://github.com/microsoft/cheriot-ibex", "platform": "github", "language": "SystemVerilog", "stargazers_count": 33, "watchers_count": 33}, {"README_text": "# cancernlp\n\nThis project provides a sample code for training the model described in Towards Structuring Real-World Data at Scale: Deep Learning for Extracting Key Oncology Information from Clinical Text with Patient-Level Supervision (in submission).\n\nTo train a model,\n1. Create conda environment: `conda env create -f conda_env.yml`\n2. Activate conda environemnt: `conda activate onco_env`\n3. Train toy model: `python train_example.py`\n\nIf there is an \"Invalid version: '0.10.1,<0.11\" error, try `pip uninstall tokenizers`.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cancernlp", "org_name": "microsoft", "org_repo": "microsoft/cancernlp", "platform_org_repo": "github+microsoft/cancernlp", "link_to_repo": "https://github.com/microsoft/cancernlp", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Codespace Features\n\n\nThis is a repository of [devcontainer features](https://containers.dev/implementors/features/)\nto assist teams in adopting Codespaces. Here is a list of the features in this repository:\n\n| Feature | Description |\n| ------- | ----------- |\n| [artifacts-helper](src/artifacts-helper)   | Install Azure Artifacts credential helper configured for Codespace authentication |\n| [docfx](src/docfx)   | Install docfx to support editing and testing documentation |\n| [external-repository](src/external-repository)   | Handles all the details of working with an external git repository in Codespaces |\n| [microsoft-git](src/microsoft-git) | Install microsoft/git with Scalar and GVFS support |\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "codespace-features", "org_name": "microsoft", "org_repo": "microsoft/codespace-features", "platform_org_repo": "github+microsoft/codespace-features", "link_to_repo": "https://github.com/microsoft/codespace-features", "platform": "github", "language": "Shell", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# vision-explanation-methods \n\nVision Explanation Methods is an open-source package that implements D-RISE ([Detector Randomized Input Sampling for Explanation](https://arxiv.org/abs/2006.03204)) towards visual interpretations of object detection models.\nD-RISE is a black-boxed, or model-agnostic, explainability method which can produce saliency maps for any object detection or instance segmentation models provided these models are appropriately wrapped. In essence, D-RISE works by randomly masking the input images and isolating the parts that are most pertinent for the detection or segmentation of the object in question.  \n\n![drise diagram](python/vision_explanation_methods/images/drisediagram.png)\n(Diagram from Petsiuk et al. 2020)\n\n\n## Example outputs\n\n![example output](python/vision_explanation_methods/images/outputmaps2.png)\n\n## Installation\n\nTo install the vision explanation package, run:\n```\npip install vision-explanation-methods\n```\n\n## Colab\n\nThe process of fine-tuning an object detection model and visualizing it through D-RISE is illustrated in this [colab notebook](https://colab.research.google.com/drive/1RRJytXf-yBlD_KSOQ0k3TpHItgs56I5q?usp=sharing).\n\n## Basic Usage\n\nTo generate saliency maps, import the package and run:\n```\nres = DRISE_runner.get_drise_saliency_map(\n    imagelocation: str,\n    model: Optional[object],    \n    numclasses: int,\n    savename: str,\n    nummasks: int=25,\n    maskres: Tuple[int, int]=(4,4),\n    maskpadding: Optional[int]=None,\n    devicechoice: Optional[str]=None,\n    wrapperchoice: Optional[object] = PytorchFasterRCNNWrapper\n    )\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vision-explanation-methods", "org_name": "microsoft", "org_repo": "microsoft/vision-explanation-methods", "platform_org_repo": "github+microsoft/vision-explanation-methods", "link_to_repo": "https://github.com/microsoft/vision-explanation-methods", "platform": "github", "language": "Python", "stargazers_count": 24, "watchers_count": 24}, {"README_text": "# \u653f\u5e9c\u76f8\u4e92\u904b\u7528\u6027\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af(Government Interoperability Framework) for Dataverse\n\n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306f\u30c7\u30b8\u30bf\u30eb\u5e81\u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u300c\u653f\u5e9c\u76f8\u4e92\u904b\u7528\u6027\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af(Government Interoperability Framework)\u300d(\u4ee5\u4e0b\u3000GIF)\u3092 Microsoft Dataverse \u306b\u5c55\u958b\u3059\u308b\u305f\u3081\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3067\u3059\u3002\nMicrosoft Dataverse\u4e0a\u306b\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u3092\u5c55\u958b\u3059\u308b\u3068\u3001\u30e2\u30c7\u30eb\u99c6\u52d5\u578b\u30a2\u30d7\u30ea\u3068\u30b3\u30a2\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb(\u4e3b\u8981\u306a\u30c6\u30fc\u30d6\u30eb)\u304c\u4f5c\u6210\u3055\u308c\u3001\u53d6\u6368\u9078\u629e\u3057\u306a\u304c\u3089\u62e1\u5f35\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \n\n![image](https://user-images.githubusercontent.com/123050871/214772063-25f861a9-f66a-454f-b0b9-c6e06bb4dd1e.png)\n\n\u653f\u5e9c\u76f8\u4e92\u904b\u7528\u6027\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\uff08GIF\uff09\uff5c\u30c7\u30b8\u30bf\u30eb\u5e81\nhttps://www.digital.go.jp/policies/data_strategy_government_interoperability_framework/\n\n\u88dc\u8db3\n|       Microsoft Power Platform              \u3000\u3000\u3000\u3000\u3000| Description                                              |\n|------------------------------------------|----------------------------------------------------------|\n| Power Apps   \u3000\u3000\u3000\u3000\u3000|\u8996\u899a\u7684\u64cd\u4f5c\u3067\u5b9f\u7528\u7684\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092\u958b\u767a                       |\n| Power Automate                              | \u5404\u30b5\u30fc\u30d3\u30b9\u3068\u306e\u9023\u643a\u3092\u53ef\u80fd\u3068\u3057\u3001\u30eb\u30fc\u30eb\u30fb\u51e6\u7406\u3092\u81ea\u52d5\u5316|\n| Power BI                           | \u3042\u3089\u3086\u308b\u30c7\u30fc\u30bf\u3092\u30b0\u30e9\u30d5\u5316\u3057\u3001\u50be\u5411\u3092\u5206\u6790                                        |\n| Power Virtual Agents                                | \u8996\u899a\u7684\u64cd\u4f5c\u3067\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u3092\u4f5c\u6210                              |\n| Power Pages                              | \u8996\u899a\u7684\u64cd\u4f5c\u3067Web\u30b5\u30a4\u30c8\u3092\u69cb\u7bc9                            |\n\n## \u30b3\u30f3\u30c6\u30f3\u30c4\n\u672c\u30ec\u30dd\u30b8\u30c8\u30ea\u306b\u306f\u4e0b\u8a18\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n\n| File/folder                     \u3000\u3000\u3000\u3000\u3000| Description                                              |\n|------------------------------------------|----------------------------------------------------------|\n| `GovernmentCommonCDMJapan_1_0_0_1.zip`   |Power Apps Unmanaged Solution File                        |\n| `documents`                              | \u5c0e\u5165\u624b\u9806\u66f8\u3001\u30c6\u30fc\u30d6\u30eb\u4e00\u89a7\u30fb\u30c6\u30fc\u30d6\u30eb\u5b9a\u7fa9\u66f8\u3001\u7ba1\u7406\u8005\u30ac\u30a4\u30c9\u7b49|\n| `README.md`                              | This README file.                                        |\n| `LICENSE`                                | The license for the sample.                              |\n\n## \u524d\u63d0\u6761\u4ef6\n\u672c\u30b5\u30f3\u30d7\u30eb\u306f\u3001Microsoft Power Platform \u306e Dataverse \u4e0a\u3067\u52d5\u4f5c\u3057\u307e\u3059\u3002\nDataverse \u74b0\u5883\u3078\u306e\u5c55\u958b\u304c\u53ef\u80fd\u306a\u6709\u511f Power Apps \u30e9\u30a4\u30bb\u30f3\u30b9\u304c\u5fc5\u8981\u3068\u306a\u308a\u307e\u3059\u3002\n\n## \u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\n\u672c\u30b5\u30f3\u30d7\u30eb\u3067\u306f\u3001Power Apps \u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u304c\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002\nZip \u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30f3\u30de\u30cd\u30fc\u30b8\u30c9\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3068\u3057\u3066\u767b\u9332\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u3061\u3089\u306e\u30a2\u30f3\u30de\u30cd\u30fc\u30b8\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u3067Power Apps\u306e\u74b0\u5883\u306b\u767b\u9332\u3092\u3057\u3066\u3054\u5229\u7528\u304f\u3060\u3055\u3044\u3002\n\u8a73\u7d30\u306a\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7\u624b\u9806\u306f\u540c\u68b1\u306e\u300c\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\u300d\u3092\u3054\u53c2\u7167\u4e0b\u3055\u3044\u3002\n\n## \u69cb\u6210\u5185\u5bb9\n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306f\u4e0b\u8a18\u306e\u8981\u7d20\u306b\u3066\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\nPower Apps \u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30d5\u30a1\u30a4\u30eb  \nPower Apps \u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u30d5\u30a1\u30a4\u30eb\u306b\u306f\u6b21\u306e\u5185\u5bb9\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\n - \u30c6\u30fc\u30d6\u30eb\n - \u30e2\u30c7\u30eb\u99c6\u52d5\u578b\u30a2\u30d7\u30ea\n - \u30b5\u30a4\u30c8\u30de\u30c3\u30d7\n\n## \u6e96\u5099\u3055\u308c\u3066\u3044\u308b\u8a00\u8a9e\n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306f\u65e5\u672c\u8a9e\u3067\u6e96\u5099\u3055\u308c\u3066\u304a\u308a\u3001\u65e5\u672c\u8a9e\u306b\u306e\u307f\u5bfe\u5fdc\u3057\u307e\u3059\u3002\n\n## FAQ\n - Q.\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u5c0e\u5165\u5f8c\u306b\u62e1\u5f35\u3059\u308b\u3053\u3068\u306f\u53ef\u80fd\u3067\u3059\u304b\uff1f\n   - A.\u53ef\u80fd\u3067\u3059\u3002\u65b0\u3057\u3044\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u3084\u30ab\u30b9\u30bf\u30e0\u30c6\u30fc\u30d6\u30eb\u306e\u4f5c\u6210\u30fb\u7de8\u96c6\u306b\u95a2\u3057\u3066\u306f\u300c\u7ba1\u7406\u8005\u30ac\u30a4\u30c9\u300d\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n - Q.\u30b5\u30dd\u30fc\u30c8\u306f\u3042\u308a\u307e\u3059\u304b\uff1f\n    - A.\u3044\u3044\u3048\u3001\u3042\u308a\u307e\u305b\u3093\u3002\n - Q.Dataverse\u7248\u3092\u8a55\u4fa1\u7248\u306b\u3066\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304b\uff1f\n    - A.Power Apps\u53ca\u3073Power Automate\u306f\u3001\u53cc\u65b9\u3068\u308230\u65e5\u306e\u7121\u511f\u8a55\u4fa1\u7248\u304c\u3042\u308a\u307e\u3059\u3002\u8a55\u4fa1\u7248\u306b\u3066\u3054\u5229\u7528\u3001\u6a5f\u80fd\u306e\u78ba\u8a8d\u3092\u884c\u3046\u4e8b\u304c\u53ef\u80fd\u3067\u3059\u3002\n\n## \u514d\u8cac\u4e8b\u9805\n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u63d0\u4f9b\u3059\u308b\u3082\u306e\u3067\u3042\u308a\u3001\u6052\u4e45\u7684\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u4f7f\u7528\u3092\u610f\u56f3\u3057\u305f\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\u65e5\u672c\u30de\u30a4\u30af\u30ed\u30bd\u30d5\u30c8\u306f\u305d\u306e\u3088\u3046\u306a\u76ee\u7684\u3067\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u304a\u3088\u3073\u95a2\u9023\u30b5\u30fc\u30d3\u30b9\u3092\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30bb\u30f3\u30b9\u3084\u6a29\u5229\u3092\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u5229\u7528\u7d44\u7e54\u306b\u4ed8\u4e0e\u3057\u3066\u3044\u307e\u305b\u3093\u3002 \n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u304a\u3088\u3073\u95a2\u9023\u30b5\u30fc\u30d3\u30b9\u306f\u3001\u5404\u4f01\u696d\u306e\u30cb\u30fc\u30ba\u3092\u5168\u3066\u542b\u3081\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u305f\u3082\u306e\u3067\u306f\u306a\u304f\u3001\u305d\u306e\u3088\u3046\u306a\u7528\u9014\u3067\u4f7f\u7528\u3055\u308c\u308b\u3082\u306e\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002\n\u5b9f\u969b\u306e\u5229\u7528\u3084\u5fc5\u8981\u306a\u8ffd\u52a0\u306e\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u306f\u5225\u9014\u5c0e\u5165\u652f\u63f4\u30d1\u30fc\u30c8\u30ca\u30fc\u306b\u78ba\u8a8d\u30fb\u4f9d\u983c\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u304a\u3088\u3073\u95a2\u9023\u30b5\u30fc\u30d3\u30b9\u306e\u3044\u304b\u306a\u308b\u4f7f\u7528\u306b\u304a\u3044\u3066\u3082\u3001\u5229\u7528\u8005\u304c\u3059\u3079\u3066\u30ea\u30b9\u30af\u3068\u8cac\u4efb\u3092\u8ca0\u3046\u3082\u306e\u3068\u3057\u307e\u3059\u3002\n\u307e\u305f\u3001\u5b9f\u88c5\u3057\u305f\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u304a\u3088\u3073\u95a2\u9023\u30de\u30a4\u30af\u30ed\u30bd\u30d5\u30c8 \u30b5\u30fc\u30d3\u30b9\u306e\u4f7f\u7528\u306b\u95a2\u3057\u3066\u3001\u9069\u5207\u306a\u8b66\u544a\u3084\u60c5\u5831\u3092\u30a8\u30f3\u30c9\u30e6\u30fc\u30b6\u30fc\u306b\u63d0\u4f9b\u3059\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u3082\u3001\u5229\u7528\u8005\u304c\u8cac\u4efb\u3092\u8ca0\u3046\u3082\u306e\u3068\u3057\u307e\u3059\u3002 \n\u672c\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306f\u3001\u65e5\u672c\u56fd\u5185\u3067\u306e\u4f7f\u7528\u306e\u307f\u3092\u76ee\u7684\u3068\u3057\u3001\u6b20\u9665\u306a\u3069\u304c\u3042\u308b\u53ef\u80fd\u6027\u3092\u542b\u3093\u3060\u307e\u307e\u306e\u72b6\u614b\u3067\u63d0\u4f9b\u3055\u308c\u3066\u304a\u308a\u3001\u3044\u304b\u306a\u308b\u7a2e\u985e\u306e\u4fdd\u8a3c\u3082\u9069\u7528\u3055\u308c\u307e\u305b\u3093\u3002\n\n## Contributing\n\u672c\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u653f\u5e9c\u76f8\u4e92\u904b\u7528\u6027\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af(GIF)\u306b\u4f9d\u5b58\u3057\u3066\u3044\u308b\u305f\u3081\u3001GIF\u306b\u5909\u66f4\u304c\u751f\u3058\u306a\u3044\u9650\u308a\u539f\u5247\u30a2\u30c3\u30d7\u30c7\u30fc\u30c8\u306f\u3044\u305f\u3057\u307e\u305b\u3093\u3002\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "GIF-for-Dataverse", "org_name": "microsoft", "org_repo": "microsoft/GIF-for-Dataverse", "platform_org_repo": "github+microsoft/GIF-for-Dataverse", "link_to_repo": "https://github.com/microsoft/GIF-for-Dataverse", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Microsoft ServiceHub / Service Broker\n\n[![NuGet package](https://img.shields.io/nuget/v/Microsoft.ServiceHub.Framework.svg)](https://nuget.org/packages/Microsoft.ServiceHub.Framework)\n[![NPM package](https://img.shields.io/npm/v/@microsoft/servicehub-framework)](https://www.npmjs.com/package/@microsoft/servicehub-framework)\n\n[![Build Status](https://dev.azure.com/azure-public/vside/_apis/build/status/vs-servicehub?branchName=main)](https://dev.azure.com/azure-public/vside/_build/latest?definitionId=53&branchName=main)\n\nThese are the libraries that Visual Studio use for much of its intra- and inter-process exchange of services.\n\n## Contributing\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nSee our [contributing doc](CONTRIBUTING.md) for more info.\n\n## Notices\n\nTrademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "vs-servicehub", "org_name": "microsoft", "org_repo": "microsoft/vs-servicehub", "platform_org_repo": "github+microsoft/vs-servicehub", "link_to_repo": "https://github.com/microsoft/vs-servicehub", "platform": "github", "language": "C#", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Intro\n\nA utility repo for doing hyper-param search.", "repo_name": "irisml-tasks-automl", "org_name": "microsoft", "org_repo": "microsoft/irisml-tasks-automl", "platform_org_repo": "github+microsoft/irisml-tasks-automl", "link_to_repo": "https://github.com/microsoft/irisml-tasks-automl", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Codamosa\n\nThis repository contains the code for CodaMOSA. CodaMOSA integrates queries to a Large Language Model (currently supports the OpenAI API) into search-based algorithms for unit test generation. The paper on CodaMOSA will be published at ICSE'23:\n\n> Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test\nGeneration with Pre-trained Large Language Models. In *Proceedings of the 45th International Conference on Software Engineering*.\n\n**CodaMOSA is implemented on top of the [Pynguin](https://github.com/se2p/pynguin) platform for Python unit test generation; this code base contains the Pynguin code as well as the CodaMOSA algorithm. If you would like to use or build on top of the Pynguin unit test generation part of CodaMOSA, consider building directly off of Pynguin; it is more frequently maintained than CodaMOSA.**\n\nThe main files added to implement CodaMOSA are:\n```\n- pynguin/generation/algorithms:\n   - codamosastrategy.py: the modified version of mosastrategy.py that implements (1) tracking of coverage plateaus and (2) invoking Codex to generate new testcases. \n- pynguin/languagemodels: \n   - astscoping.py: defines a modified python AST that contains Pynguin VariableReferences in place of variable names, used to support uninterpreted statements in CodaMOSA. \n   - functionplaceholderadder.py: no longer used by CodaMOSA, allows to randomly mutate a given function with a placeholder \"??\"\n   - model.py: the main interface to the Codex API\n   - output_fixers.py: the AST rewriter used to normalize Codex-generated code to a format closer to Pynguin's output\n```\n\n## License\n\nCodaMOSA buils on Pynguin 0.19.0, which was licensed LGPL-3.0. A copy of this license is available in the [LICENSES directory](LICENSES/LGPL-3.0-or-later.txt). Thefiles modified/added by CodaMOSA are licensed under the MIT license. A copy of this license is available in the [LICENSE.txt](LICSENSE). document. File headers outline the license under which each file is distributed.\n\nVersions of Pynguin 0.30.0 and onwards are now licensed as MIT.\n\n## Running\n\nTo run on a project, first build the runner:\n```\ndocker build -t codamosa-runner -f docker/Dockerfile --platform linux/amd64 .\n```\n(or use the target platform of your choice --- should match the machine on which you want to run experiments.)\n\n\nYou can then run \n```\ndocker run --rm -v TARGET_PROJECT_DIRECTORY:/input:ro -v OUTPUT_DIRECTORY:/output -v TARGET_PROJECT_DIRECTORY:/package:ro codamosa-runner <ARGS_TO_PYNGUIN> \n```\nthere must be a file called package.txt in `TARGET_PROJECT_DIRECTORY` which contains all the requirements that need to be installed for the target project in the `requirements.txt` format. The `pipreqs` tool can help you generate one automatically. \n\nHere is an example run, given that the `flutils` project is cloned under `$TEST_BASE/test-apps`, and that there is a `package.txt` file in `test-apps/flutils`:\n```\n$ mkdir /tmp/flutils-out\n$ docker run --rm -v $TEST_BASE/test-apps/flutils:/input:ro -v /tmp/flutils-out:/output \\\n    -v $TEST_BASE/test-apps/flutils:/package:ro codamosa-runner --project_path /input \\\n    --module-name flutils.packages --output-path /output  --report-dir /output --maximum_search_time 120 \\\n    --output_variables TargetModule,CoverageTimeline --coverage_metrics BRANCH,LINE  --assertion-generation NONE \\\n    --algorithm CODAMOSA -v --include-partially-parsable True --allow-expandable-cluster True \\\n    --uninterpreted_statements ONLY --temperature 0.8 --model_name code-davinci-002 --authorization-key $AUTH_KEY\"\n```\n\n## Replication package\n\nFor information about replicating the results in the ICSE'23 submission, follow the instructions in the `replication` folder. \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Building for Development\n\nThis project uses the `[poetry](https://python-poetry.org/)` dependency management program, and requires Python 3.10 to run. After you have poetry installed, you can navigate to the `codamosa` directory and edit the code following these steps:\n\n1. First, create a virtual environment and install dependencies using:\n```\n$ poetry install\n```\nAfter the initial install, you can activate the virtual environment via:\n```\n$ poetry shell\n```\n2. After you have made your changes, you can run the linters and tests with the command:\n```\n$ make check\n```\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "codamosa", "org_name": "microsoft", "org_repo": "microsoft/codamosa", "platform_org_repo": "github+microsoft/codamosa", "link_to_repo": "https://github.com/microsoft/codamosa", "platform": "github", "language": "Python", "stargazers_count": 51, "watchers_count": 51}, {"README_text": "---\npage_type: sample\nlanguages:\n- python\nproducts:\n- azure\n- power bi\n- synapse\n- databricks\n- purview\n- analysis services\n- datalake\n\nname: Data Lineage\ndescription: End to end data lineage from source to visualizations.\n---\n\n# Onboarding Documents\n\n- [Onboarding Sparklin](https://github.com/microsoft/DataLineage/blob/main/sparklin/Onboarding.md)\n- [Onboarding TOMPo](https://github.com/microsoft/DataLineage/blob/main/tompo/Onboarding.md)\n\n![image](https://user-images.githubusercontent.com/118733500/227436747-f527883b-92da-4482-8aec-b34c03003cf7.png)\n\n", "repo_name": "DataLineage", "org_name": "microsoft", "org_repo": "microsoft/DataLineage", "platform_org_repo": "github+microsoft/DataLineage", "link_to_repo": "https://github.com/microsoft/DataLineage", "platform": "github", "language": "Python", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# AzDetectSuite\n\nA collection of ARM-based detections for Azure/AzureAD based TTPs\n\n\n## Initial Access \n\n| ID          | Name                                                             |Deploy|\n| ----------- |------------------------------------------------------------------|------|\n| AZT201.1    | User Account|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FInitialAccess%2FAZT201%2FAZT201-1.json)|\n| AZT201.2    | Service Principal Account|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FInitialAccess%2FAZT201%2FAZT201-2.json)|\n| AZT202    | Password Spraying|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FInitialAccess%2FAZT202%2FAZT202.json)|\n| AZT203      | Malicious Application Consent|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FInitialAccess%2FAZT203%2FAZT203.json)|\n\n## Execution\n\n| ID          | Name                                                             |Requires Azure Monitor Agent?|Deploy|\n| ----------- |------------------------------------------------------------------|-----|------|\n| AZT301.1    | Virtual Machine Scripting: RunCommand|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-1.json)|\n| AZT301.1    | Virtual Machine Scripting: RunCommand with PowerShell Logging |Y|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-1-AMA.json)|\n| AZT301.2    | Virtual Machine Scripting: CustomScriptExtension|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-2.json)|\n| AZT301.2    | Virtual Machine Scripting: CustomScriptExtension with PowerShell Logging|Y|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-2-AMA.json)|\n| AZT301.3    | Virtual Machine Scripting: Desired State Configuration|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-3.json)|\n| AZT301.3    | Virtual Machine Scripting: Desired State Configuration with PowerShell Logging |Y|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-3-AMA.json)|\n| AZT301.4    | Virtual Machine Scripting: Compute Gallery Application|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-4.json)|\n| AZT301.5    | Virtual Machine Scripting: AKS Command Invoke|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-5.json)|\n| AZT301.6    | Virtual Machine Scripting: Vmss Run Command|N|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT301-6.json)|\n| AZT302.1    | Unmanaged Scripting: Automation Account Hybrid Worker Group||[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT302-1.json)|\n| AZT302.2    | Unmanaged Scripting: Automation Account RunAs Account||[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT302-2.json)|\n| AZT302.3    | Unmanaged Scripting: Automation Account Managed Identity Account||[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExecution%2FAZT301%2FAZT302-3.json)|\n\n\n## Privilege Escalation\n\n| ID          | Name                                                             |Deploy|\n| ----------- |------------------------------------------------------------------|------|\n| AZT402    | Elevated Access Toggle|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT402%2FAZT402.json)|\n| AZT403.1    | Local Resource Hijack: Cloud Shell .IMG|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT403%2FAZT403-1.json)|\n| AZT404.1    | Function Application|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT402%2FAZT402.json)|\n| AZT405.1    | Azure AD Application: Application Role|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT405%2FAZT405-1.json)|\n| AZT405.2    | Azure AD Application: Application API Permissions|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT405%2FAZT405-2.json)|\n| AZT405.3    | Azure AD Application: Application Registration Owner|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPrivilegeEscalation%2FAZT405%2FAZT405-3.json)|\n\n\n## Persistence\n\n| ID          | Name                                                             |Deploy|\n| ----------- |------------------------------------------------------------------|------|\n| AZT501.1    | Account Manipulation: User Account Manipulation |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT501%2FAZT501-1.json)|\n| AZT501.2    | Account Manipulation: Service Principal Account|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT501%2FAZT501-2.json)|\n| AZT501.3    | Account Manipulation: Azure VM Local Administrator Manipulation|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT501%2FAZT501-3.json)|\n| AZT502.1    | Account Creation: User Account Creation |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT502%2FAZT502-1.json)|\n| AZT502.2    | Account Creation: Service Principal Creation |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT502%2FAZT502-2.json)|\n| AZT502.3    | Account Creation: Guest Account Creation |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT502%2FAZT502-3.json)|\n| AZT503.1    | HTTP Trigger: Logic Application HTTP Trigger |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT503%2FAZT503-1.json)|\n| AZT503.2    | HTTP Trigger: Function App HTTP Trigger |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT503%2FAZT503-2.json)|\n| AZT503.3    | HTTP Trigger: Runbook Webhook |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT503%2FAZT503-3.json)|\n| AZT503.4    | HTTP Trigger: WebJob |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT503%2FAZT503-4.json)|\n| AZT504    | Watcher Tasks |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT504%2FAZT504.json)|\n| AZT505    | Scheduled Jobs: Runbook Schedules |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT505%2FAZT505.json)|\n| AZT506      | Network Security Group Modification |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT506%2FAZT506.json)|\n| AZT508      | Azure Policy |[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FPersistence%2FAZT508%2FAZT508.json)|\n\n\n\n## Credential Access\n\n| ID          | Name                                                             |Deploy|\n| ----------- |------------------------------------------------------------------|------|\n| AZT601.1    | Steal Managed Identity JsonWebToken: Virtual Machine IMDS Request|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT601%2FAZT601-1.json)|\n| AZT601.2    | Steal Managed Identity JsonWebToken: Azure Kubernetes Service IMDS Request|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT601%2FAZT601-2.json)|\n| AZT601.3    | Steal Managed Identity JsonWebToken: Logic Application JWT PUT Request|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT601%2FAZT601-3.json)|\n| AZT601.5    | Steal Managed Identity JsonWebToken: Automation Account Runbook|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT601%2FAZT601-5.json)|\n| AZT602.1    | Steal Service Principal Certificate: Automation Account RunAs Account|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT602%2FAZT602-1.json)|\n| AZT604.1    | Azure Key Vault Dumping: Azure Key Vault Secret Dump|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT604%2FAZT604-1.json)|\n| AZT604.2    | Azure Key Vault Dumping: Azure Key Vault Certificate Dump|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT604%2FAZT604-2.json)|\n| AZT604.3    | Azure Key Vault Dumping: Azure Key Vault Key Dump|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT604%2FAZT604-3.json)|\n| AZT605.1    | Resource Secret Reveal: Storage Account Access Key Dumping|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT605%2FAZT605-1.json)|\n| AZT605.2    | Resource Secret Reveal: Automation Account Credential Secret Dump|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FCredentialAccess%2FAZT605%2FAZT605-2.json)|\n\n## Exfiltration\n\n| ID          | Name                                                             |Deploy|\n| ----------- |------------------------------------------------------------------|------|\n| AZT701.1    | SAS URI Generation: VM Disk SAS URI|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExfiltration%2FAZT701%2FAZT701-1.json)|\n| AZT701.2    | SAS URI Generation: Storage Account File Share SAS|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExfiltration%2FAZT701%2FAZT701-2.json)|\n| AZT703.1    | Replication: Storage Account Replication|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExfiltration%2FAZT703%2FAZT703-1.json)|\n| AZT704.1    | Soft-Delete Recovery: Key Vault|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExfiltration%2FAZT704%2FAZT704-1.json)|\n| AZT704.2    | Soft-Delete Recovery: Key Vault|[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzDetectSuite%2Fmain%2FAzureThreatResearchMatrix%2FExfiltration%2FAZT704%2FAZT704-2.json)|\n\n\n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "AzDetectSuite", "org_name": "microsoft", "org_repo": "microsoft/AzDetectSuite", "platform_org_repo": "github+microsoft/AzDetectSuite", "link_to_repo": "https://github.com/microsoft/AzDetectSuite", "platform": "github", "language": "PowerShell", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# How to run the code\npython ./user_interface.py\n\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "symbolic-robot-teaching-interface", "org_name": "microsoft", "org_repo": "microsoft/symbolic-robot-teaching-interface", "platform_org_repo": "github+microsoft/symbolic-robot-teaching-interface", "link_to_repo": "https://github.com/microsoft/symbolic-robot-teaching-interface", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# codamosa-dataset\nDataset of Codex generated tests for the CodaMosa project, that was used in the evaluation submitted to ICSE'23.\n\n\n## Directory Structure:\n- `README.md`: this file\n- `final-exp`: the directory containing results of the \"main\" CodaMOSA experiments. The results are zipped per benchmark. After you unzip one (or multiple) `BENCHMARK.zip` files, you will have the following directory structure:\n  - `TECHNIQUE`: there are seven techniques we report results for. `mosa` and `codex-only` are our two baselines. `codamosa-0.8-uninterp` is the main CodaMOSA, the other techniques are ablations: `codamosa-0.8` uses no uninterpreted statements; `codamosa-0.2-uninterp` uses lower sampling temperature; `codamosa-0.8-uninterp-no-targeting` samples target functions randomly rather than by coverage; `codamosa-0.8-uninterp-small` adds a small test case to the prompts pass to codex. Whatever the technique, the data inside is structured as follows:\n    - `BENCHMARK-i`: the `i`th run of the configuration on the benchmark `BENCHMARK`, which corresponds to a particular python module:\n      - `codex_generations.py`: (for all codamosa variants + CodexOnly) the raw tests generated by Codex.\n      - `codamosa_timeline.csv`: (for all codamosa variants) a csv where the first column is the time in seconds at which one round of targeted generation ended, and the second column is the number of codex-generated test cases (cumulatively) that were accepted at that time\n      - `statistics.csv`: all the Pynguin-collected statistics. See the heading of the file for the name of each statistics and the runtime_variable.py file in the source code to see a detailed description of the statistic.\n      - `test_MODULE.py`: the generated test cases at end of search that do not throw exceptions\n      - `test_MODULE_failing.py`: the generated test cases at end of search that throw exceptions\n- `packages-exp.zip`: this zip file contains result of a small experiment for the motivating example in CodaMOSA, showing that better performance is achieved when doctests are removed from the documentation of the function under tests.\n", "repo_name": "codamosa-dataset", "org_name": "microsoft", "org_repo": "microsoft/codamosa-dataset", "platform_org_repo": "github+microsoft/codamosa-dataset", "link_to_repo": "https://github.com/microsoft/codamosa-dataset", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "[![Build Status](https://github.com/microsoft/aiida-nwchemex/workflows/ci/badge.svg)](https://github.com/microsoft/aiida-nwchemex/actions)\n[![Coverage Status](https://codecov.io/gh/microsoft/aiida-nwchemex/branch/main/graph/badge.svg)](https://codecov.io/gh/aiidateam/aiida-nwchemex)\n\n# aiida-nwchemex\n\nAn [AiiDA](https://www.aiida.net) plugin for the [NWChemEx](https://nwchemex-project.github.io/NWChemEx/index.html) package.\n\nAn earlier version of this plugin was used in the preprint https://doi.org/10.48550/arXiv.2211.14688 (see [`CITATION`](CITATION)).\n\n## Contents\n\n- the [`aiida_nwchemex`](./aiida_nwchemex) package provides basic input/output handling for NWChemEx in AiiDA\n- the [`aiida_nwchemex_cc`](./aiida_nwchemex_cc) package contains a coupled-cluster workflow that relies on `aiida_nwchemex`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aiida-nwchemex", "org_name": "microsoft", "org_repo": "microsoft/aiida-nwchemex", "platform_org_repo": "github+microsoft/aiida-nwchemex", "link_to_repo": "https://github.com/microsoft/aiida-nwchemex", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# AiiDA Scine AutoCAS\n\n[AiiDA](http://www.aiida.net/) plugin for [Scine AutoCAS](https://scine.ethz.ch/download/autocas).\n\n## Installation\n\nThe plugin can be installed using pip assuming AiiDA has already been installed.\n```bash\ngit clone https://github.com/microsoft/aiida-autocas.git\ncd aiida-autocas\npip install -e .\n```\n\n## Usage\n\nFirst, create an AiiDA Code `autocas@local` pointing to the `scine_autocas/__main__.py` script (see [aiida-core docs](https://aiida.readthedocs.io/projects/aiida-core/en/latest/howto/run_codes.html)).\n\nLoad this code in the example script `examples/n2.py` and run it via `./n2.py`.\n\n### Calculation Input Parameters\n\nThe AutoCAS CalcJob takes a **structure** input, which provides the molecular geometry.\nFurther parameters allow users to modify the settings, but all of these input variables have default values and are thus optional.\n\n| Variable | Type  | Default | Description|\n|----------|-------|---------|-----------------|\n| structure           | Structure Data | None      | Molecular geometry of the system|\n| basis_set           | String         | def2-svpd | The basis set to use for the quantum chemistry calculations |\n| charge              | Int            | 0         | Charge of the system            |\n| multiplicity        | Int            | 1         | Spin multiplicity of the system |\n| double_d_shell      | Bool           | True      | Whether to include the d shell for 3d transition metals |\n| interface           | String         | chronusq  | Which interface to quantum chemistry software should be used |\n| method              | String         | dmrg_ci   | Which active space method to use for the final energy (e.g. dmrg_ci, casscf, casci) |\n| dmrg_bond_dimension | Int            | 1000      | The bond dimension to use for the final DMRG calculation when using method=dmrg_ci |\n| dmrg_sweeps         | Int            | 10        | The number of DMRG sweeps to perform for the final DMRG calculation when using method=dmrg_ci |\n| large_cas_protocol  | Bool           | False     | Whether to use the Large CAS protocol |\n\n\n### Calculation Output Results\nUpon completion of the CalcJob, the following results (using\n[AiiDA data types](https://aiida.readthedocs.io/projects/aiida-core/en/latest/topics/data_types.html))\nare returned.\n\n| Variable   |  Type   | Description |\n|------------|---------|-------------|\n| n_active_electrons | Int  | Number of electrons in determined active space |\n| n_active_orbitals  | Int  | Number of orbitals in the choosen active space |\n| active_orbitals    | ArrayData | One dimensional array containing the orbital indices of the choosen active space |\n| energy             | Float | Energy of the system using the method choosen in the input (e.g. DRMG, CASCI, CASSCF) |\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aiida-autocas", "org_name": "microsoft", "org_repo": "microsoft/aiida-autocas", "platform_org_repo": "github+microsoft/aiida-autocas", "link_to_repo": "https://github.com/microsoft/aiida-autocas", "platform": "github", "language": "Python", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Microsoft Finance Time Series Forecasting Framework\n\n<!-- badges: start -->\n[![CRAN\\_Status\\_Badge](http://www.r-pkg.org/badges/version/finnts)](https://cran.r-project.org/package=finnts)\n<!-- badges: end -->\n\nAbout this repository\n============================================================================================================================================\n\nThis repository contains examples of how to implement Microsoft Finance's time series forecasting framework, called Finn, through our [open-source R package called finnts](https://microsoft.github.io/finnts/). \n\nThe Microsoft Finance Time Series Forecasting Framework, aka finnts or Finn, is an automated forecasting framework for producing financial forecasts. While it was built for corporate finance activities, it can easily expand to any time series forecasting problem!\n\n- Automated feature engineering, back testing, and model selection. \n- Access to 25+ models. Univariate, multivariate, and deep learning models all included. \n- Azure integration to run thousands of time series in parallel within the cloud. \n- Supports daily, weekly, monthly, quarterly, and yearly forecasts. \n- Handles external regressors, either purely historical or historical+future values.\n\nHere you can find details around how you can implement Finn using Azure Synapse notebooks to run thousands of time series forecasts at scale using spark. \n\nDetails of the accelerator\n============================================================================================================================\n-   This repository includes the implementation of the finnts forecast package on Azure Synapse\n-   There will be example notebooks walking you through how to set up a Synapse cluster to run Finn and how to approach getting a high quality forecast. \n\n\nPrerequisites\n============================================================================================================================\n\nIn order to successfully complete your solution, you will need to have access to and or provisioned the following:\n\n-   Access to an Azure subscription\n-   Access to an Azure Data Lake Storage account for Blob Storage with read/write permission\n-   Access to an Azure Synapse Workspace with contributor rights\n\nGetting Started\n================================================================================================================================\n\nWhile Finn can run on a local machine, it shines best when running at scale in Azure. Follow the below resources to learn how to set up an Azure Synapse notebook to start using Finn. \n\n## How to use this repo\n\nOpen `Finn_Walkthrough.md` and follow the notebook.\n\nContents\n================================================================================================================================\n\n| File/Folder         | Description                                                                                     |\n|---------------------|-------------------------------------------------------------------------------------------------|\n| `docs`              | Finn quick-start guide (`Finn_Walkthrough.md`)                                                  |\n| `synapse-notebooks` | Synapse notebook that runs Finn (`DEMO_finnts_ds_toolkit.json`)                                             |\n\nGeneral Coding Guidelines\n=============================================================================================================================\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-corporate-financial-forecasting", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-corporate-financial-forecasting", "platform_org_repo": "github+microsoft/dstoolkit-corporate-financial-forecasting", "link_to_repo": "https://github.com/microsoft/dstoolkit-corporate-financial-forecasting", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Repository for Code first demos for Power Platform\n\n", "repo_name": "PowerPlatformDemos", "org_name": "microsoft", "org_repo": "microsoft/PowerPlatformDemos", "platform_org_repo": "github+microsoft/PowerPlatformDemos", "link_to_repo": "https://github.com/microsoft/PowerPlatformDemos", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "\n## <img src=\"Assets/images/hdinsightpl.png\" alt=\"FTA Toolkit: HDInsight Deployment Accelerator\" style=\"float: left; margin-right:10px;\" />\n&nbsp;\n\n### HDInsight Cluster Using Private Link Deployment Accelerator\n If you're ever in need of deploying an HD Insight Kafka Cluster using Private Endpoints here is an AZ CLI script that will walk you through the process. Getting HD Insight to work with Private Endpoints can be a bit tricky, so to make it easier here is an Azure CLI Script that will walk you through the process of creating your Resource Group, VNet, Subnets, Storage Account, NAT Gateway, HDInsight Kafka Cluster, Edge Node, Private Endpoints, DNS Configuration and VM to login and test the cluster access.\n\n[For additional documentation click here](https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-private-link)\n\n### Preparation\n1. Install Azure CLI  \nhttps://docs.microsoft.com/en-us/cli/azure/install-azure-cli\n1. Clone repository / copy files locally\n1. Edit the Variables Section inside the 'HDI_Kafka_Deploy.azcli' file \n\n    ```\n        #----------------------------Variables-----------------------------------#\n        #region Login\n        $Location=\"eastus\"\n        $RG=\"AZU-HDI-PE-RG\"\n        $Sub=\"YOURSUBSCRIPTIONID\" \n    \n        # Network\n        $NSG=\"hdi-kafka-vnet-nsg\"\n        $vNet=\"KafkaVNet\"\n        $sNet_HDI=\"HDISubnet\"\n        $sNet_Blob=\"StorageSubnet\"\n        $vNet_Client=\"hdi-privlink-client-vnet\" \n        $sNet_Client=\"default\"\n    \n        # Storage\n        $STA=\"YOURSTORAGEACCOUNT\"\n        $Container=\"kafka\"\n    \n        #NAT Gateway\n        $Gateway=\"hdi-kafka-pl-nat-gateway\"\n        $Gateway_PublicIP=\"pip-ngw-hdi-kafka\"\n    \n        # HDInsight Clusters\n        $clusterName=\"hdi-bts-kafka-cluster\" #Your Cluster Name must be unique\n        $clusterType=\"kafka\" #You can change this Hadoop if you want to deploy Hadoop\n        #componentVersion=\"kafka=2.4\" \"You can change this Hadoop=3.1.0 if you want to deploy a Hadoop Cluster\n        $httpCredential=\"YOURSECUREPASSWORDHERE\"\n        $clusterSizeInNodes=\"3\"\n        #clusterSizeMin=\"3\"\n        #clusterSizeMax=\"5\"\n        $sshCredentials=\"YOURSECUREPASSWORDHERE\"\n        $clusterVersion=\"5.0\"\n    \n        #ssh sshuser@hdi-bts-kafka-cluster-ssh.azurehdinsight.net\n        #https://hdi-bts-kafka.azurehdinsight.net\n        #endregion\n    ```\n\n1. Edit the parameter file 'azuredeploy.parameters.json'\n    ```\n    {\n        \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\",\n        \"contentVersion\": \"1.0.0.0\",\n        \"parameters\": {\n            \"clusterName\": {\n              \"value\": \"YOURHDICLUSTERNAME\" //YOUR HDI CLUSTER NAME\n            },\n            \"EdgeNodeVirtualMachineSize\": {\n              \"value\": \"Standard_E4_v3\" //Change this to another VM size if you woud like\n            }\n        }\n    }\n    ```\n1. Open the HDI_Kafka_Deploy.azcli file\n1. Login to Azure using a Service Principal or interactively\n1. Run each AZCLI command line by line\n1. At the end of it you will have an HDInsight Cluster Running with Private Endpoints\n\n#### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "ftatoolkit-hdi", "org_name": "microsoft", "org_repo": "microsoft/ftatoolkit-hdi", "platform_org_repo": "github+microsoft/ftatoolkit-hdi", "link_to_repo": "https://github.com/microsoft/ftatoolkit-hdi", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# What is this repository for?\nThis is repository collects the code for programs that extract parameters required for operating robots given a human demonstration.\nThese codes are used in the framework called [Learning-from-observation](https://www.microsoft.com/en-us/research/project/interactive-learning-from-observation/).\nIn this framework, these processess are called as daemon. Under the [docker](docker) folder, you can find more detailed explanations.\n\nRelated publications:\n* [A Learning-from-Observation Framework: One-Shot Robot Teaching for Grasp-Manipulation-Release Household Operations](https://ieeexplore.ieee.org/abstract/document/9382750)\n* [Learning from Observation: One-Shot Robot Teaching for Household Operations](https://arxiv.org/abs/2103.02274)\n\n\n# How do I get set up?\nThese codes are assumed to be run on Ubuntu 18.04 with docker installed.\nFor convenience, we provide a setup script for the host machine.\nPlease refer to the [setupscript](setupscript) for details.\n\n```\n# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "analyzer-for-robot-teaching-demonstrations", "org_name": "microsoft", "org_repo": "microsoft/analyzer-for-robot-teaching-demonstrations", "platform_org_repo": "github+microsoft/analyzer-for-robot-teaching-demonstrations", "link_to_repo": "https://github.com/microsoft/analyzer-for-robot-teaching-demonstrations", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "\n# pstn-callrecord-plan-alerts\n\n<img src=\"./images/7.svg\" width=\"150\" height=\"150\" />  \n\n.NET Framework\n\n# Overview \n\n* A .net core console application to compare total pstn call minutes to calling plan limits\n\n# Setup\n\n1. Fork Repo\n\n![image info](./images/8.png) \n\n2. App Registration Creation\n\n    - Create a new App Registration in AAD and select \"Web\" for the Redirect URI and leave the value empty. Click \"Register\"\n    - Create a Client Secret and give it a period of your choosing\n    - Make the below modifications to support oAuth Flowtype and permissions required:\n\n![image info](./images/1.png) \n\n![image info](./images/2.png) \n\n![image info](./images/3.png) \n\n3. Create Incoming Webhook (Optional)\n\n    - Create a new incoming web hook in the connectors section of your Teams Channel\n    ![image info](./images/15.a.png) \n    - While Creating the connector, Use the Image in this repo to set the Connector image under: .\\incoming-webhook-image\n    ![image info](./images/15.b.png) \n    - Take note of the webhook hook url (^^^) and save it for later step \"Configuration\" for the \"TeamsWebHook\" property of the Function app\n    - As a sample, You will see a Team Card displayed that will look like the following whenthe function has started:\n    ![image info](./images/15.c.png) \n\n\n4. Create Resource Group\n\n    navigate to http://portal.azure.com and Create a Resource Group in the appropriate region\n\n    ![image info](./images/9.png) \n\n5. Create Azure Function\n\n    Select the Market place template for Function App\n    \n    ![image info](./images/10.png) \n\n    Create an Azure function with the following properties:\n    - Publish: Code\n    - Runtime Stack: .NET\n    - Version: 6\n    - Operating System: Windows\n    - Plan type: Comsumption (Serverless)\n  \n    ![image info](./images/11.a.png) \n    \n    ![image info](./images/11.b.png) \n    \n    ![image info](./images/11.c.png) \n    \n    ![image info](./images/11.d.png) \n\n6. Deploy Code to Azure Function\n\n    You have a choice depending on the method you choose to deploy the code to your azure function. Details are outlined below. \n        \n    You can Deploy your code via:\n    - Deplyment Center - App Service Build Service\n    - Deplyment Center - Github Actions Build/Deploy\n    - As a Zip Package Depolyment that you build through a pipeline or external dev machine\n    - Deploy using App Service SSH Key + Github WebHooks.\n  \n    **Details on which option to choose:**\n      - [Deployment Best Practices](https://learn.microsoft.com/en-us/azure/app-service/deploy-best-practices)\n    \n7. Configuration\n\n    Navigate to the Function App-->Settings-->Configuration\n    \n    ![image info](./images/12.png) \n\n    Add the following App Configuration settings\n\n    - Name: TenantID, Value: [TENANT ID OF YOUR APP REGISTRATION CREATED EARLIER]\n    - Name: ClientID, Value: [CLIENT ID OF YOUR APP REGISTRATION CREATED EARLIER]\n    - Name: ClientSecret, Value: [ClientSecret OF YOUR APP REGISTRATION CREATED EARLIER]\n    - Name: TeamsWebHook, Value: [INCOMING WEBHOOK URI TO ACCEPT NOTIFICATION]\n    - Name: ThresholdLimit, Value: [THRESHOLD WHEN CLOSE TO PLAN LIMIT, DEFAULT IS 80 FOR 80%]\n    - Name: NotificationType, Value: [HOW TO BE NOTIFIED: Teams, Console, or ALL]\n    - Name: SendOnlyWhenThresholdExceeded, Value: [true or false]\n    - Name: CronTimerSchedule, Value: [a CRON syntax string to run the function, Default setting is every 10 minutes for testing. please change this to once a day ]\n      - Some example values:\n        - 0 */10 * * * * = Once every 10 Minutes\n        - 0 */1 * * * * = Once every Minute\n        - 0 0 22 * * * = Once every day at 10:00PM\n     \n# Run\n\nRestart the function once the configuration has completed. you can do so at the Function App-->Overview Page:\n\n![image info](./images/13.png) \n\nYou can view the log tail by navigating to Function App-->Monitoring-->Log Stream and click \"start\":\n\n![image info](./images/14.png) \n\nCheck for errors. \n\nDone: You should see Console Information showing in the Log Stream as Well as Teams Messages in the channel you setup your incoming webhook if all is correct!\n\n# Build / Debug\n\n```bash\n\ngit clone https://github.com/microsoft/pstn-callrecord-plan-alerts.git\n\n[C#]\ndotnet build\nfunc start\n\n\n```\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n# Notes\n\n**Call Plan Limits and Configuration Settings**\n\n![image info](./images/5.png) \n\n![image info](./images/6.png) \n\nQuick links:\n\n**MS Graph API Call(s)**\n- [callRecord: getPstnCalls](https://learn.microsoft.com/en-us/graph/api/callrecords-callrecord-getpstncalls?view=graph-rest-1.0&tabs=http)\n\n\n## Where do I file issues\n\nThis is the correct repo to file [issues](https://github.com/microsoft/pstn-callrecord-plan-alerts/issues)\n\n\n## Contribute\n\nWe enthusiastically welcome contributions and feedback. You can clone the repo and start contributing now. Read our [Contribution Guide](contributing.md) for more information.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Security Library\n\nThis library controls how users sign-in and access services. We recommend you always take the latest version of our library in your app when possible. We use [semantic versioning](http://semver.org) so you can control the risk associated with updating your app. As an example, always downloading the latest minor version number (e.g. x.*y*.z) ensures you get the latest security and feature enhancements but our API surface remains the same. You can always see the latest version and release notes under the Releases tab of GitHub.\n\n## Security Reporting\n\nIf you find a security issue with our libraries or services please report it to [secure@microsoft.com](mailto:secure@microsoft.com) with as much detail as possible. Your submission may be eligible for a bounty through the [Microsoft Bounty](http://aka.ms/bugbounty) program. Please do not post security issues to GitHub Issues or any other public site. We will contact you shortly upon receiving the information. We encourage you to get notifications of when security incidents occur by visiting [this page](https://technet.microsoft.com/en-us/security/dd252948) and subscribing to Security Advisory Alerts.\n\n## Data Collection. \n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\nSee the [wiki](https://github.com/AzureAD/microsoft-authentication-library-for-dotnet/wiki/MSAL.NET-telemetry-solution-overview) for an example of the telemetry collected by MSAL.NET.\n\n## Trademarks.\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\nCopyright (c) Microsoft Corporation.  All rights reserved. Licensed under the MIT License (the \"License\");\n", "repo_name": "pstn-callrecord-plan-alerts", "org_name": "microsoft", "org_repo": "microsoft/pstn-callrecord-plan-alerts", "platform_org_repo": "github+microsoft/pstn-callrecord-plan-alerts", "link_to_repo": "https://github.com/microsoft/pstn-callrecord-plan-alerts", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Cookiecutter Python repository demonstrating how to construct and layer your monorepo\n\nThis is a lightweight, reusable Python Cookiecutter template, containing main folder structure you would need for any Python project.\n\nProject contains a basic Python folder structure, Architecture Decision Record & documentation structure, pull request template, as well as CI/CD pipeline with super-linter.\n\n## Quickstart\n\nInstall the latest Cookiecutter if you haven't installed it yet (this requires Cookiecutter 1.4.0 or higher):\n\n```pip install -U cookiecutter```\n\nGenerate a Python package project:\n\n```https://github.com/microsoft/cookiecutter_template_for_python```\n\nNavigate to the directory created by the Cookiecutter and initialize it as git repo:\n\n```git init -b main```\n\nNow you can commit and push the newly created folder and repo into your GitHub.\n\n## Cookiecutter Documentation\n\n- [cookiecutter-library](https://pypi.org/project/cookiecutter-python/)\n- [cookie-cutter-documentation](https://cookiecutter.readthedocs.io/en/stable/)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cookiecutter_template_for_python", "org_name": "microsoft", "org_repo": "microsoft/cookiecutter_template_for_python", "platform_org_repo": "github+microsoft/cookiecutter_template_for_python", "link_to_repo": "https://github.com/microsoft/cookiecutter_template_for_python", "platform": "github", "language": "Bicep", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Dynamics 365 Patterns and Practices\n\nWelcome to the repository for patterns, practices, business process guides, and other types of guidance content for Microsoft Dynamics 365! This repo provides a way for you to actively contribute to the Dynamics 365 guidance content, and we welcome your contributions. Learn more at [Contribute to Microsoft's content for Dynamics 365](https://learn.microsoft.com/en-us/dynamics365/get-started/contribute).  \n\nThe *main* branch is the default branch with approved templates and other artifacts.\n\nIf you have any questions, you can submit feedback as an Issue or a pull request.\n\n## Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dynamics365patternspractices", "org_name": "microsoft", "org_repo": "microsoft/dynamics365patternspractices", "platform_org_repo": "github+microsoft/dynamics365patternspractices", "link_to_repo": "https://github.com/microsoft/dynamics365patternspractices", "platform": "github", "language": null, "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# Microsoft Azure Cloud HSM SDK Private Preview\n\n\n", "repo_name": "MicrosoftAzureCloudHSM", "org_name": "microsoft", "org_repo": "microsoft/MicrosoftAzureCloudHSM", "platform_org_repo": "github+microsoft/MicrosoftAzureCloudHSM", "link_to_repo": "https://github.com/microsoft/MicrosoftAzureCloudHSM", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# HvLoader Overview\nHvLoader.efi is an EFI application for loading an external hypervisor loader.\n\nHvLoader.efi loads a given hypervisor loader binary (DLL, EFI, etc.), and calls\nit's entry point passing HvLoader.efi ImageHandle. This way the hypervisor loader \nbinary has access to HvLoader.efi's command line options, and use those as \nconfiguration parameters. The first HvLoader.efi command line option is the \npath to hypervisor loader binary.\n\nCurrently HvLoader.efi resides in the efi partition, i.e. /boot/efi, and has\naccess to that partition only. Thus, any path arguments need to be relative to\n/boot/efi partition. This restriction may be relaxed in future updates.\n\nA typical HvLoader.efi grub command may look like the following:\n\n_chainloader /HvLoader.efi \\\\lxhvloader.dll MSHV_ROOT=\\\\Windows MSHV_ENABLE=1 MSHV_SCHEDULER_TYPE=0 ..._\n\n## Setting up the build environment\nHvLoader.efi is built in [TianoCore EDK2](https://github.com/tianocore/edk2).\nThe build environment was initially tested on a Ubuntu 22.04.1 LTS, but can be\nset up on Microsoft Windows, macOS, and Unix.  \nFor more information, please refer to [Getting Started with EDK2](https://github.com/tianocore/tianocore.github.io/wiki/Getting-Started-with-EDK-II).\n\nOn Linux, please follow the instructions at [Using EDK2 with Native GCC](https://github.com/tianocore/tianocore.github.io/wiki/Using-EDK-II-with-Native-GCC), and\nfinally clone the repo and build according to [Common EDK II Build Instructions for Linux](https://github.com/tianocore/tianocore.github.io/wiki/Common-instructions).\n\n## Building HvLoader.efi\nHvLoader.efi lives as an application in MdeModulePkg, i.e. <edk2>/MdeModulePkg/Application/HvLoader.\n\nAfter a successful build of EDK2, you can build HvLoader.efi following the steps below,   \nrunning from the root location of edk2:\n1. _pushd MdeModulePkg/Application_\n2. _git clone https://github.com/asherkariv/HvLoader.git_\n3. _popd_\n4. Add HvLoader to MdeModulePkg:\n   * Edit _MdeModulePkg/MdeModulePkg.dsc_\n   * Add _MdeModulePkg/Application/HvLoader/HvLoader.inf_ in the [Components] section, for example:        \n     **[Components]**   \n        MdeModulePkg/Application/HelloWorld/HelloWorld.inf\n        **MdeModulePkg/Application/HvLoader/HvLoader.inf**\n        MdeModulePkg/Application/DumpDynPcd/DumpDynPcd.inf\n        MdeModulePkg/Application/MemoryProfileInfo/MemoryProfileInfo.inf\n        ...\n\n5. Build HvLoder.efi:   \n   _build -p MdeModulePkg/MdeModulePkg.dsc -m MdeModulePkg/Application/HvLoader/HvLoader.inf_\n6. Based on your target configuration (DEBUG/RELEASE), HvLoader.efi is produced at \n   _Build/MdeModule/DEBUG_GCC5/X64/HvLoader.efi_   \n   or   \n   _Build/MdeModule/RELEASE_GCC5/X64/HvLoader.efi_   \n\n## Signing HvLoader.efi\nHvLoader.efi needs to be signed for secure boot.\n\n\n\n\n", "repo_name": "HvLoader", "org_name": "microsoft", "org_repo": "microsoft/HvLoader", "platform_org_repo": "github+microsoft/HvLoader", "link_to_repo": "https://github.com/microsoft/HvLoader", "platform": "github", "language": "C", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Light up Azure &ndash; Azure Playlists Season 3\n\n\uc560\uc800 \ud50c\ub808\uc774\ub9ac\uc2a4\ud2b8 \uc2dc\ub9ac\uc988\uc758 \uc138\ubc88\uc9f8 \uc2dc\uc98c, **Light up Azure**! \uc544\ub798\uc640 \uac19\uc740 \ub0b4\uc6a9\uc73c\ub85c \uc900\ube44\ud588\uc2b5\ub2c8\ub2e4.\n\n1. **S03E01**: \uc560\uc800 \ub124\ud2b8\uc6cc\ud06c \uae30\ubcf8 \uc124\uacc4<br>\n   \u27a1\ufe0f \uc815\ubcf4\ub78c | \ud074\ub77c\uc6b0\ub4dc \uc194\ub8e8\uc158 \uc544\ud0a4\ud14d\ud2b8 | Microsoft\n\n    <div>\n      <a href=\"https://aka.ms/lua/ep1\" target=\"_blank\"><img src=\"./assets/thumbnail.anna.jeong.png\" style=\"width: 360px;\" alt=\"\uc378\ub124\uc77c - \uc815\ubcf4\ub78c\"></a>\n    </div>\n\n2. **S03E02**: \uc560\uc800 \uac00\uc0c1\uba38\uc2e0 \ud655\uc7a5 \uc9d1\ud569\uacfc \uc2a4\ucf00\uc77c\ub9c1 \uc2dc\ub098\ub9ac\uc624 \ud655\uc778\ud558\uae30<br>\n   \u27a1\ufe0f \ub098\uc720\uac15 | \ud074\ub77c\uc6b0\ub4dc \uc194\ub8e8\uc158 \uc544\ud0a4\ud14d\ud2b8 | Microsoft\n\n    <div>\n      <a href=\"https://aka.ms/lua/ep2\" target=\"_blank\"><img src=\"./assets/thumbnail.youkhang.rha.png\" style=\"width: 360px;\" alt=\"\uc378\ub124\uc77c - \ub098\uc720\uac15\"></a>\n    </div>\n\n3. **S03E03**: \uc560\uc800 \uc11c\ubc84\ub9ac\uc2a4\ub85c \uc11c\ubc84 \uc5c6\uc774 \uac04\ub2e8\ud55c \uc6f9 \uc571 \ub9cc\ub4e4\uae30<br>\n   \u27a1\ufe0f \uc5b4\uac70\uc2a4\ud2b8 \ub9ac | \ud074\ub77c\uc6b0\ub4dc \uc194\ub8e8\uc158 \uc544\ud0a4\ud14d\ud2b8 | Microsoft\n\n    <div>\n      <a href=\"https://aka.ms/lua/ep3\" target=\"_blank\"><img src=\"./assets/thumbnail.august.lee.png\" style=\"width: 360px;\" alt=\"\uc378\ub124\uc77c - \uc5b4\uac70\uc2a4\ud2b8 \ub9ac\"></a>\n    </div>\n\n4. **S03E04**: \uc560\uc800 \uc2a4\ud2b8\ub9bc \uc560\ub110\ub9ac\ud2f1\uc2a4 \uae30\ubc18 \uc2e4\uc2dc\uac04 \uc2a4\ud2b8\ub9ac\ubc0d \ub370\uc774\ud130 \ubd84\uc11d<br>\n   \u27a1\ufe0f \uacf5\ud61c\uc6d0 | \ud074\ub77c\uc6b0\ub4dc \uc194\ub8e8\uc158 \uc544\ud0a4\ud14d\ud2b8 | Microsoft\n\n    <div>\n      <a href=\"https://aka.ms/lua/ep4\" target=\"_blank\"><img src=\"./assets/thumbnail.hyewon.kong.png\" style=\"width: 360px;\" alt=\"\uc378\ub124\uc77c - \uacf5\ud61c\uc6d0\"></a>\n    </div>\n\n\uc5d0\ud53c\uc18c\ub4dc\ub294 \uacc4\uc18d \ucd94\uac00\ub420 \uc608\uc815\uc785\ub2c8\ub2e4. \ub9ce\uc740 \uad00\uc2ec \ubd80\ud0c1 \ub4dc\ub9bd\ub2c8\ub2e4!\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "light-up-azure", "org_name": "microsoft", "org_repo": "microsoft/light-up-azure", "platform_org_repo": "github+microsoft/light-up-azure", "link_to_repo": "https://github.com/microsoft/light-up-azure", "platform": "github", "language": "Shell", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Linked Objects Transformer (LOBSTR)\n\nLOBSTR is an image-to-graph model built on top of the Detection Transformer (DETR).\n\nWe provide a base architecture LOBSTR that can be trained directly for simple graphs.\n\nWe also provide a projects folder that shows how you can extend LOBSTR yourself to handle more complex graphs (for example, graphs with multiple node properties, graphs with multiple edge properties, or graphs involving 2D point objects instead of bounding boxes).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "LOBSTR", "org_name": "microsoft", "org_repo": "microsoft/LOBSTR", "platform_org_repo": "github+microsoft/LOBSTR", "link_to_repo": "https://github.com/microsoft/LOBSTR", "platform": "github", "language": null, "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# ACE UX Components\n\nACE-UX-Components provides a library for resuable components based on Microsoft's Sharepoint Adaptive Cards Extension (ACE).\n\nThe following table will help you navigate between the 2 projects in this repository.\n\n|          | UX-Components                                               | Dashboard                                                 |\n| -------- | ----------------------------------------------------------- | --------------------------------------------------------- |\n| Overview | Contains the ACE UX components published as an npm package. | Contains a sample card to test out the ACE UX Components. |\n| README   | [README.md](./ux-components/README.md)                      | [README.md](./dashboard/README.md)                        |\n\n## Using the ACE UX Components\n\nFrom NPM:\nTo install the `@microsoft/ace-ux-components` library on your project, use npm as follows:\n\n```bash\nnpm install --save @microsoft/ace-ux-components\n```\n\nThis will add the package as a dependency in your `package.json` file and download it under `node_modules/@microsoft/ace-ux-components`.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ace-ux-components", "org_name": "microsoft", "org_repo": "microsoft/ace-ux-components", "platform_org_repo": "github+microsoft/ace-ux-components", "link_to_repo": "https://github.com/microsoft/ace-ux-components", "platform": "github", "language": "TypeScript", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "<h1 align=\"center\">   <img src=\"https://microsoft.github.io/augmented-interpretable-models/auggam_gif.gif\" width=\"25%\"> Automated explanations <img src=\"https://microsoft.github.io/augmented-interpretable-models/auggam_gif.gif\" width=\"25%\"></h1>\n<p align=\"center\"> Explaining black box text modules in natural language with language models (<a href=\"https://arxiv.org/abs/2305.09863\">arXiv 2023</a>)\n</p>\n\nThis repo contains code to reproduce the experiments in [the SASC paper]([https://arxiv.org/abs/2209.11799](https://arxiv.org/abs/2305.09863)). SASC takes in a text module and produces a natural explanation for it (see Fig below).\n\n<img src=\"https://microsoft.github.io/automated-explanations/fig.svg?sanitize=True&kill_cache=1\" width=\"90%\">\n\nSASC is similar to the very nice [concurrent paper](https://github.com/openai/automated-interpretability) by OpenAI, but simplifies explanations to describe the function rather than produce token-level activations. This makes it simpler/faster, and makes it more effective at describing semantic functions from limited data (e.g. fMRI voxels) but worse at finding patterns that depend on sequences / ordering.\n\n\nFor a simple scikit-learn interface to use SASC, use the [imodelsX library](https://github.com/csinva/imodelsX). Install with `pip install imodelsx` then the below shows a quickstart example.\n\n```python\nfrom imodelsx import explain_module_sasc\n# a toy module that responds to the length of a string\nmod = lambda str_list: np.array([len(s) for s in str_list])\n\n# a toy dataset where the longest strings are animals\ntext_str_list = [\"red\", \"blue\", \"x\", \"1\", \"2\", \"hippopotamus\", \"elephant\", \"rhinoceros\"]\nexplanation_dict = explain_module_sasc(\n    text_str_list,\n    mod,\n    ngrams=1,\n)\n```\n\n# Reference\n- See related [fMRI experiments](https://github.com/csinva/fmri)\n- Built from [this template](https://github.com/csinva/cookiecutter-ml-research)\n\n```r\n@misc{singh2023explaining,\n      title={Explaining black box text modules in natural language with language models}, \n      author={Chandan Singh and Aliyah R. Hsu and Richard Antonello and Shailee Jain and Alexander G. Huth and Bin Yu and Jianfeng Gao},\n      year={2023},\n      eprint={2305.09863},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```\n", "repo_name": "automated-explanations", "org_name": "microsoft", "org_repo": "microsoft/automated-explanations", "platform_org_repo": "github+microsoft/automated-explanations", "link_to_repo": "https://github.com/microsoft/automated-explanations", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Imitating human behaviour with diffusion models\n\nCode from ICLR 2023 paper 'Imitating human behaviour with diffusion models' - https://arxiv.org/abs/2301.10677\n\n<img height=\"250\" src=\"overview_01.png\">\n\nThis code currently only replicates the claw environment experiments of the paper. We plan to update this with other experiments soon.\n\n## Setup\n\n- Install Python (ran on Python 3.9) and/or create a fresh environment\n- Install requirements `pip install -r requirements.txt`\n\n## Running\n\n### Mini script\n\nAs a lightweight entry-point to the code, we provide `claw_mini_script.py`, which trains a diffusion model and runs the Diffusion-X sampling procedure, on the claw dataset. The dataset must first be created locally, by running `python make_dataset.py`, taking about 1GB of space. `claw_mini_script.py` then runs in around 5 minutes, outputting `code/figures/claw_mini_diffusion_eg.png`. Note this is a smaller network than that used to generate the paper figures.\n\n### Claw experiments\n\nTo recreate the claw machine figures in the paper, run `./run.sh` or alternatively run each step separately:\n\n```\npython make_dataset.py\npython train.py\npython plot.py\n```\n\nThis creates a `figures` directory containing the main claw machine result figures shown in the paper. This cycles through all methods, and uses the transformer denoising network, resulting in a run time >24 hours.\n\nFor nice labels when plotting, set `IS_USE_LATEX=True`, and ensure you have a valid tex/latex installation. To get it on Ubuntu run, `sudo apt install texlive texlive-latex-extra texlive-latex-recommended dvipng cm-super msttcorefonts`.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## License\nCode is licensed under MIT, data and all other content is licensed under Microsoft Research License Agreement (MSR-LA). See LICENSE.", "repo_name": "Imitating-Human-Behaviour-w-Diffusion", "org_name": "microsoft", "org_repo": "microsoft/Imitating-Human-Behaviour-w-Diffusion", "platform_org_repo": "github+microsoft/Imitating-Human-Behaviour-w-Diffusion", "link_to_repo": "https://github.com/microsoft/Imitating-Human-Behaviour-w-Diffusion", "platform": "github", "language": "Python", "stargazers_count": 61, "watchers_count": 61}, {"README_text": "# Microsoft Commercial Marketplace API Emulator\n\nThis repository contains a Node.js implementation of an emulator for the Microsoft commercial marketplace SaaS Fulfillment APIs.\n\nTo make hosting the emulator as simple as possible, the repo includes a Dockerfile for a building a containerised version.\n\n## What challenges does this project address?\n\nIntegrating with the commercial marketplace has a few scaffolding requirements; a barrier to getting up and running quickly. The emulator breaks that dependency, allowing teams to start building for marketplace with zero friction. Specifically:\n\n- Remove dependency on Partner Center\n  - Partner Center onboarding and permissions can take some time to setup correctly. You can get started right away without waiting for this process.\n  - Ordinarily, testing API flows against plans can only happen after they have been created in Partner Center. Emulated plans allow development to happen in parallel. Only final testing is required against Partner Center plans.\n- Remove the AAD requirement\n  - A SaaS transactable offer, or at least the integration with marketplace, must be built with AAD. The integration with the marketplace APIs requires the app to be registered in AAD and API calls are secured with OAuth2. Similarly, your landing page must support AAD SSO. The emulator removes these requirements to make it simpler (and quicker) to start implementing your integration flows.\n\n## Design goals\n\n- Keep things simple\n- Implement the minimum required whilst being fully functional out of the box\n- Be consistent with the **behaviour** of APIs, note differences from the documented behaviour\n- Support both unauthenticated use for simplicity and authenticated use for closer fidelity with the marketplace APIs.\n- Flexible implementation that offers multiple hosting options to suit the user\n- Offer configuration options to tailor the behaviour\n- Provide a UI to step through and visualise certain actions\n\n## Getting started\n\nThe emulator is a Node.js application designed to be run as a Docker container for portability. Other hosting options are also possible. For more details see the relevant section:\n\n- [Hosting the emulator](./docs/launching.md)\n- [Configuring the emulator](./docs/config.md)\n- [Usage scenarios](./docs/scenarios.md)\n\n## Using the emulator\n\nWith the emulator running, you can connect to it using a browser and standard tools such as the [REST Client extension for VS Code](https://github.com/Huachao/vscode-restclient), [Postman](https://www.postman.com/) etc.\n\nThe URL and port will depend on [your chosen deployment method](./docs/launching.md). eg if you're running the emulator locally using `docker run`, you would likely connect on `http://localhost:8080`.\n\n1. Run the emulator using your [chosen method](./docs/launching.md)\n1. With a browser, connect to `http://<domain>:<port>` (domain, port depend on your run method)\n   - You should be presented with a page for configuring a synthetic marketplace purchase token\n1. Configure a purchase token\n   1. (Optionally) configure properties on the purchase token (otherwise defaults will be populated)\n   1. Click the `Generate Token` button\n   1. Observe the generated JSON result\n1. You can now either\n   1. Use the emulator's simple, built-in landing page implementation to resolve & activate a subscription\n   1. Exercise the APIs manually (eg using the VS Code REST client or Postman)\n\n### Use the Emulator's built-in landing page\n\n1. Click on the `Post to landing page` button in the Token area\n1. You will be taken to the emulator's built-in landing page\n1. The purchase token is passed to the landing page as a query string parameter\n1. When it loads, the landing page automatically calls the `resolve API` to decode the token\n1. Key token properties are displayed on the page\n1. Click the `Activate subscription` button to call the `activate API`\n1. You should see a message indicating a `200 OK` status response\n1. Navigate to the `Subscriptions` page to see your new subscription has been activated\n\n### A word about the Publisher ID\n\nSubscriptions need to be associated with a publisher. In the marketplace, the publisher is derived using claims from the AAD bearer token used to authenticate.\n\nFor the emulator, to keep things simple, we removed the AAD requirement. Instead, you can define the Publisher ID in one of two ways:\n\n1. A `publisherId` query string parameter. This is set to \"FourthCoffee\" as a default in the emulator\n   1. To see an example of this in action, take a look at the sample REST calls: [subscriptions-apis.http](./rest_calls/subscription-apis.http)\n   1. To modify this Publisher ID for the built-in emulator functions, set the `PUBLISHER_ID` environment variable\n1. A Publisher ID constructed from the publisher tenant and app IDs.\n   1. This is useful to mirror the behaviour of the marketplace. The tenant and app ID from the app registration you use to authenticate with the marketplace APIs will also work with the emulator\n   1. To modify this Publisher ID for the built-in emulator functions, set the `PUBLISHER_TENANT_ID` and `PUBLISHER_APP_ID` environment variables. Set these to the tenant and app ID from the app registration used to authenticate with the marketplace APIs. eg from the Azure Portal (see below) or Partner Center offer page.\n\n   ![Screenshot of tenant id and app id](./docs/images/readme-publisherid-1.png)\n\nYou can use either approach (but not both at the same time!). The former is useful for early testing as you don't need to register with AAD, the latter is useful in the latter stages of emulator testing as it is directly compatible with the marketplace.\n\nFor more information on configuration see [Configuring the emulator](./docs/config.md)\n\n### Exercise the APIs manually\n\n1. Click the `Copy to clipboard` button in the Token area (**not** the JSON result)\n1. This copies the Base64 encoded purchase token to the clipboard\n1. Call the `resolve API` to resolve (decode) the purchase token\n   1. This repo includes helpers to call the emulated APIs using the REST Client extension for VS Code\n   1. Open this repo in VS Code\n   1. Install the [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)\n   1. Open [subscription-apis.http](rest_calls/subscription-apis.http) from the `\\rest_calls` folder\n   1. In `subscription-apis.http` set\n      - `publisherId [string]` simulates the Publisher ID (can be anything you like eg Contoso Corp)\n      - `baseUrl [sting]` format as above eg `http://localhost:8080`\n      - `purchaseToken [string]` the Base64 encoded purchase token (paste from the clipboard)\n   1. Click `Send Request` (under ### Resolve)\n      - You should see a Response appear with a `200 OK` status\n      - The payload will include the decoded purchase token\n1. Call the `activate API` to activate the purchase\n   1. In `subscription-apis.http`\n   1. Update the `planId` on the `activate` request to **match a valid planId** as set in the purchase token\n   1. Click on `Send Request` on the `activate API` call\n   1. You should see a `200 OK` status\n   1. The payload will be `OK`\n\nAt this point you have\n\n- Created a (synthetic) purchase token\n- Resolved that purchase token to its properties\n- Activated the purchase\n\nYou can now call other APIs to see their response\n\n- eg the `GetSubscriptions` API would return a list of all subscriptions\n- Having completed the above steps, this will return a collection with one item, the subscription you activated\n\nThere are helpers for all available SaaS Fulfillment v2 APIs in\n\n- [subscription-apis.http](rest_calls/subscription-apis.http)\n- [operations-apis.http](rest_calls/operations-apis.http)\n\nFor more details on the available APIs and their operation, see\n\n- [SaaS subscription life cycle](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-life-cycle)\n- [Subscription APIs](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-subscription-api)\n- [Operations APIs](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-operations-api)\n\n## Functionality\n\nThe solution emulates all operations on the [SaaS Fulfillment APIs v2](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-life-cycle) closely. It is close-to, but not 100% fidelity (see [Limitations](#limitations) below.\n\nAny solution should be [thoroughly tested against the marketplace APIs](https://learn.microsoft.com/azure/marketplace/test-saas-overview) before final release.\n\nCapabilities include the following:\n\n- Generate a synthetic [purchase identification token](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-subscription-api#resolve-a-purchased-subscription)\n- This token has customisable subscription properties that can be resolved by the emulated `resolve` API\n- Publisher ID can be set by query string parameter (`publisherId`) or authorization header (bearer token)\n- Testing the following flows:\n  - Landing page flow\n  - Activation flow\n  - Update flow\n  - Suspend and reinstate flow\n  - Webhook flows\n\nThe format of the marketplace [purchase identification token](https://learn.microsoft.com/azure/marketplace/partner-center-portal/pc-saas-fulfillment-subscription-api#resolve-a-purchased-subscription) is not documented. In the emulator, this has been simulated with a Base64 encoded JSON payload.\n\n## Limitations\n\n- The activate call does not validate the publisher (as the 'faux' purchase token isn't associated with a specific publisher. This may be implemented in future.\n\n## Accelerator Integration\n\nThere are two Open Source projects from teams at Microsoft, the SaaS Accelerator and MONA SaaS that have been updated to work with the Emulator in place of the Azure Marketplace. Documentation: [Integrations](/docs/integration.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Commercial-Marketplace-SaaS-API-Emulator", "org_name": "microsoft", "org_repo": "microsoft/Commercial-Marketplace-SaaS-API-Emulator", "platform_org_repo": "github+microsoft/Commercial-Marketplace-SaaS-API-Emulator", "link_to_repo": "https://github.com/microsoft/Commercial-Marketplace-SaaS-API-Emulator", "platform": "github", "language": "TypeScript", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# vscode-perf-bot\nA command line tool to run VS Code performance tests and write results to Slack.\n\n[![Build Status](https://dev.azure.com/monacotools/Monaco/_apis/build/status/npm/vscode/vscode-perf-bot?repoName=microsoft%2Fvscode-perf-bot&branchName=main)](https://dev.azure.com/monacotools/Monaco/_build/latest?definitionId=457&repoName=microsoft%2Fvscode-perf-bot&branchName=main)\n\n## Requirements\n\n- [Node.js](https://nodejs.org/en/) at least `16.x.x`\n\n## Usage\n\n```sh\nnpx @vscode/vscode-perf-bot [ARGS]\n```\n\n## Help\n\n```sh\nnpx @vscode/vscode-perf-bot --help\n```\n\n`vscode-perf-bot` is meant to be only used as a command line tool.\n", "repo_name": "vscode-perf-bot", "org_name": "microsoft", "org_repo": "microsoft/vscode-perf-bot", "platform_org_repo": "github+microsoft/vscode-perf-bot", "link_to_repo": "https://github.com/microsoft/vscode-perf-bot", "platform": "github", "language": "TypeScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "marketplace-automation-templates", "org_name": "microsoft", "org_repo": "microsoft/marketplace-automation-templates", "platform_org_repo": "github+microsoft/marketplace-automation-templates", "link_to_repo": "https://github.com/microsoft/marketplace-automation-templates", "platform": "github", "language": null, "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Confidential AI\n\nThe growing adoption of AI has raised concerns regarding security and privacy of underlying datasets and models. \nThis repository contains samples demonstrating the use of Azure Confidential Computing for hosting AI workloads with hardware-based security guarantees. \nThe samples are provided on an As-Is basis. They are work-in-progress and should not be used in production. \n\n## Confidential Computing Platform\nThese samples are based on several confidential computing platform components and services. \n\n- Confidential Containers\n- Confidential GPUs\n- Microsoft Azure Attestation (MAA)\n- Azure Key Vault mHSM \n\n## Workloads\nSamples include the following workloads.\n\n- [Confidential Inferencing](inference/README.md)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "confidential-ai", "org_name": "microsoft", "org_repo": "microsoft/confidential-ai", "platform_org_repo": "github+microsoft/confidential-ai", "link_to_repo": "https://github.com/microsoft/confidential-ai", "platform": "github", "language": "Shell", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Hack Together: Microsoft Graph and .NET \ud83e\udd92\n\n<p align=\"center\">\n  <img src=\"./assets/banner.png\" alt=\"Hack Together banner\"/>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://aka.ms/hack-together/survey\"><img src=\"https://img.shields.io/badge/hackathon-survey-green?style=for-the-badge\" alt=\"Hackathon survey\" border=\"0\" /></a>\n</p>\n\n## Hack Together: Microsoft Graph and .NET \ud83e\udd92\n\nFrom March 1-15, 2023 we ran **Hack Together: Microsoft Graph and .NET** - a virtual hackathon for beginners to get started building scenario-based apps using .NET and Microsoft Graph.\n\nIn this hackathon, participants learned how to build apps with Microsoft Graph based on top Microsoft Graph scenarios, and met Microsoft Graph Product Group Leaders, Cloud Advocates, MVPs and Student Ambassadors. Each submitted app had a chance to win exciting prizes.\n\n## We're excited to share the hackathon winners\n\n- \ud83e\udd47 1st place: [Magic Note app to plan the day efficiently with AI & Microsoft Graph](https://github.com/microsoft/hack-together/issues/92)\n- \ud83e\udd48 2nd place: [ScheduleEase](https://github.com/microsoft/hack-together/issues/178)\n- \ud83e\udd493rd place: [magi \u2013 An AI wizard to answer all your queries](https://github.com/microsoft/hack-together/issues/140)\n\n**Congratulations to the winners and a huge thank you to all participants!**\n\nRead the [full announcement](https://devblogs.microsoft.com/microsoft365dev/announcing-the-hack-together-microsoft-graph-and-net-winners/)\n\n## Continue your journey\n\nWhile the hackathon is finished, your journey doesn\u2019t have to end. [Join the Microsoft 365 and Power Platform Community](https://pnp.github.io/) to continue building with Microsoft Graph and .NET. **Find like-minded people, attend community calls, and explore resources to see what else you can build using Microsoft Graph!**\n\n[![Microsoft 365 and Power Platform Community banner](./assets/banner-m365-community.png)](https://pnp.github.io/)\n\n## Hack Together Roadmap \ud83d\uddfa\ufe0f\n\n![Hack Together Roadmap](./assets/HackTogetherRoadmap.png)\n\nFollow the steps below to successfully complete the hackathon.\n\n### Watch the sessions \ud83c\udfa5\n\n* **[March 1st - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together/session01):** Kickoff Hack Together: Microsoft Graph and .NET! What can you do with Microsoft Graph .NET SDK?\n\n* **[March 2nd - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together/session02):** Microsoft Graph Product Managers will show you how to get started with Microsoft Graph .NET SDK!\n\n* **[March 8th - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together/session03):** Microsoft Graph Product team and .NET Advocates join the Ask the Experts session to answer your questions. Get to know them!\n\n* **[March 15th - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together/session04):** We're at the end of Hack Together, but the journey doesn't end here. What's next: learn how you can join the community!\n\n## Recommended Learning Materials\n\n* [Learn Path - Explore Microsoft Graph scenarios for ASP.NET Core development](https://learn.microsoft.com/training/paths/m365-msgraph-dotnet-core-scenarios/)\n* [Tutorial - Build .NET apps with Microsoft Graph](https://learn.microsoft.com/graph/tutorials/dotnet?tabs=aad)\n* [Tutorial: Create a Blazor Server app that uses the Microsoft identity platform for authentication](https://learn.microsoft.com/azure/active-directory/develop/tutorial-blazor-server)\n* [Tutorial: Call the Microsoft Graph API from a Universal Windows Platform (UWP) application](https://learn.microsoft.com/azure/active-directory/develop/tutorial-v2-windows-uwp)\n* [Tutorial: Create a .NET MAUI app using the Microsoft Graph SDK](https://learn.microsoft.com/windows/apps/windows-dotnet-maui/tutorial-graph-api)\n* [Documentation - Overview of Microsoft Graph](https://learn.microsoft.com/graph/overview)\n\n## Templates\n\nIf you are looking for a code template to start your project, we have the following templates available for you in this repository:\n\n* [Console App](https://github.com/microsoft/hack-together/tree/main/templates/dotnet-console-app-microsoft-graph)\n* [Blazor Server App](https://github.com/microsoft/hack-together/tree/main/templates/dotnet-blazor-server-app-microsoft-graph)\n* [UWP (Universal Windows Platform) App](https://github.com/microsoft/hack-together/tree/main/templates/dotnet-uwp-app-microsoft-graph)\n* [MAUI (Multi-platform App UI) App](https://github.com/microsoft/hack-together/tree/main/templates/dotnet-maui-app-microsoft-graph)\n* [.NET Core MVC (Model - View - Controller) Web App](https://github.com/microsoft/hack-together/tree/main/templates/dotnet-core-mvc-web-app-microsoft-graph)\n\n---\n\n<p align=\"center\">\n  <img src=\"./assets/footer.png\" alt=\"Hack Together footer\"/>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://aka.ms/hack-together/survey\"><img src=\"https://img.shields.io/badge/hackathon-survey-green?style=for-the-badge\" alt=\"Hackathon survey\" border=\"0\" /></a>\n</p>\n", "repo_name": "hack-together", "org_name": "microsoft", "org_repo": "microsoft/hack-together", "platform_org_repo": "github+microsoft/hack-together", "link_to_repo": "https://github.com/microsoft/hack-together", "platform": "github", "language": "C#", "stargazers_count": 520, "watchers_count": 520}, {"README_text": "# Azure Devops Codespaces Authentication\n\n- This VSCode extension is used for authenticating to Azure Devops in GitHub Codespaces.\n- It authenticates using in-built microsoft auth provider to authenticate to ADO using AAD login.\n- User is prompted for login on opening a codespace with this extension installed.\n- The OAuth access token is then shared with the codespace using a credential helper which is installed at `~/ado-auth-helper`. The credential helper supports two commands\n  - `get` - This command is used by git credential helper to get auth credentials for git. You can configure the helper by running `git config --global credential.helper '<absolutePathToHelper>'`.\n  - `get-access-token` - This command will print an access token to stdout. Other tools can integrate this for getting ADO credentials, for eg, authenticating to ADO Artifact Feeds (NPM, Nuget). \n- This extension is not recommended to be installed by itself. You should instead use the [external-repository](https://github.com/microsoft/codespace-features/tree/main/src/external-repository) and [artifacts-helper](https://github.com/microsoft/codespace-features/tree/main/src/artifacts-helper) devcontainer features which will ensure this extension is preinstalled on your Codespace with proper configuration.\n\n### New in version 1.1\n- Credential helper for managed identities, installed at `~/azure-auth-helper`.\n- This one allows specifying custom scopes for the access token, like so:\n```bash\n$ ~/azure-auth-helper get-access-token \"https://management.azure.com/.default\"\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ado-codespaces-auth", "org_name": "microsoft", "org_repo": "microsoft/ado-codespaces-auth", "platform_org_repo": "github+microsoft/ado-codespaces-auth", "link_to_repo": "https://github.com/microsoft/ado-codespaces-auth", "platform": "github", "language": "TypeScript", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Azure Container Apps Dev Day\n\nThe Azure Container Apps Dev Day labs include:\n1. A lab that covers basic communication between services without Dapr\n2. A lab that covers deploying an Azure Container Apps environment using Github Actions\n3. A lab that covers communication between services using Dapr\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aca-dev-day", "org_name": "microsoft", "org_repo": "microsoft/aca-dev-day", "platform_org_repo": "github+microsoft/aca-dev-day", "link_to_repo": "https://github.com/microsoft/aca-dev-day", "platform": "github", "language": "JavaScript", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# eslint-plugin-fluentui-jsx-a11y [![npm version](https://img.shields.io/npm/v/@microsoft/eslint-plugin-fluentui-jsx-a11y.svg?style=flat)](https://www.npmjs.com/package/@microsoft/eslint-plugin-fluentui-jsx-a11y)\n\nA set of eslint rules against [FluentUI](https://github.com/microsoft/fluentui) to prevent common accessibility issues.\n\nRules for FluentUI v9 end in `-v9`. [Fluent UI React v9](https://react.fluentui.dev/?path=/docs/concepts-introduction--page)\n\nRules for v8 have no ending.\n\nDeveloped and maintained by the Microsoft Research Ireland Accessibility V-Team.\n\n## Installation\n\nYou'll first need to install [ESLint](https://eslint.org/):\n\n```sh\n# npm\nnpm install eslint --save-dev\n\n# yarn\nyarn add eslint --dev\n```\n\nNext, install @microsoft/eslint-plugin-fluentui-jsx-a11y:\n\n```sh\n# npm\nnpm install @microsoft/eslint-plugin-fluentui-jsx-a11y --save-dev\n\n# yarn\nyarn add @microsoft/eslint-plugin-fluentui-jsx-a11y --dev\n```\n\nOr add this package to your `package.json` file:\n\n```sh\n\"devDependencies\": {\n    \"@microsoft/eslint-plugin-fluentui-jsx-a11y\": \"1.0.0\"\n  }\n```\n\nAnd then you can run\n\n```sh\n  npm install\n```\n\n## Usage\n\nYou will need to add the plugin to your `.eslintrc` configuration file.\nAs we support both v8 and v9 right now, you will need to add the rules individually to the rules section.\n\nV9 Suggested Configuration:\n\n```json\n{\n  \"root\": true,\n  \"parser\": \"@typescript-eslint/parser\",\n  \"parserOptions\": {\n    \"ecmaVersion\": 2018,\n    \"sourceType\": \"module\"\n  },\n  \"plugins\": [\n    \"@typescript-eslint\",\n    \"react-hooks\",\n    \"@microsoft/fluentui-jsx-a11y\"\n  ],\n  \"extends\": [\n    \"eslint:recommended\",\n    \"plugin:@typescript-eslint/eslint-recommended\",\n    \"plugin:react/recommended\",\n    \"plugin:@typescript-eslint/recommended\"\n  ],\n  \"rules\": {\n    \"react-hooks/rules-of-hooks\": \"error\",\n    \"react-hooks/exhaustive-deps\": \"warn\",\n    \"react/prop-types\": \"off\",\n    \"@microsoft/fluentui-jsx-a11y/no-empty-buttons\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/checkbox-needs-labelling-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/image-link-missing-aria-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/input-missing-label-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/switch-needs-labelling-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/text-area-missing-label-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/image-button-missing-aria-v9\": \"error\",\n    \"@microsoft/fluentui-jsx-a11y/toolbar-missing-aria-v9\": \"error\"\n  },\n\n```\n\n## Why?\n\nThis plugin does a static code analysis of the React JSX to spot accessibility issues in apps built with FluentUI. That way, common accessibility issues are detected before the pull request stage and will be prevented from being checked into a code base.\n\nAs the plugin can only catch errors in static source code, please use it in combination with [@axe-core/react](https://github.com/dequelabs/axe-core-npm/tree/develop/packages/react) to test the accessibility of the rendered DOM. Consider these tools just as one step of a larger a11y testing process and always test your apps with assistive technology.\n\nWe also recomend that you use this plugin in conjuction with the [eslint-plugin-jsx-a11y](https://github.com/jsx-eslint/eslint-plugin-jsx-a11y) plugin. Please configure [eslint-plugin-jsx-a11y](https://github.com/jsx-eslint/eslint-plugin-jsx-a11y) to work with FluentUI components.\n\nExample:\n\n```json\n{\n    \"settings\": {\n        \"jsx-a11y\": {\n            \"components\": {\n                \"Button\": \"button\",\n                \"Image\": \"img\"\n            }\n        }\n    }\n}\n```\n\n## ESLint\n\nThis plugin was generated with The ESLint generator for Yeoman: [eslint/generator-eslint](https://github.com/eslint/generator-eslint).\n\n### eslint:plugin\n\n```sh\nyo eslint:plugin\n```\n\n### eslint:rule\n\nIf you want to create a new ESLint rule, make sure you're in the top-level directory of an ESLint repo clone or an ESLint plugin and type:\n\n```sh\nyo eslint:rule\n```\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Rules\n\n<!-- begin auto-generated rules list -->\n\n\ud83d\udd27 Automatically fixable by the [`--fix` CLI option](https://eslint.org/docs/user-guide/command-line-interface#--fix).\n\n| Name\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0                                                                | Description                                                                                                                                                                                                            | \ud83d\udd27 |\n| :----------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :- |\n| [checkbox-needs-labelling-v9](docs/rules/checkbox-needs-labelling-v9.md)                                     | Accessibility: Checkbox without label must have an accessible and visual label: aria-labelledby                                                                                                                        |    |\n| [icon-text-content-button-does-not-need-aria](docs/rules/icon-text-content-button-does-not-need-aria.md)     | Accessibility: an image button with text content does not need aria labelling. The button already has an accessible name and the aria-label or aria-labelledby will override the text content for screen reader users. |    |\n| [image-button-missing-aria](docs/rules/image-button-missing-aria.md)                                         | Accessibility: Image buttons must have accessible labelling: aria-label, aria-labelledby, aria-describedby                                                                                                             |    |\n| [image-button-missing-aria-v9](docs/rules/image-button-missing-aria-v9.md)                                   | Accessibility: Image buttons must have accessible labelling: title, aria-label, aria-labelledby, aria-describedby                                                                                                      |    |\n| [image-button-prefer-aria-over-title-attribute](docs/rules/image-button-prefer-aria-over-title-attribute.md) | Accessibility: prefer wai-aria over title or placeholder attributes. Title/placeholder can be used in addition to wai-aria. aria-label, aria-labelledby, aria-describedby                                              |    |\n| [image-link-missing-aria-v9](docs/rules/image-link-missing-aria-v9.md)                                       | Accessibility: Image links must have an accessible name                                                                                                                                                                | \ud83d\udd27 |\n| [input-missing-label-v9](docs/rules/input-missing-label-v9.md)                                               | Accessibility: Inputs must have accessible labelling: aria-label, aria-labelledby or an associated label                                                                                                               |    |\n| [no-empty-buttons](docs/rules/no-empty-buttons.md)                                                           | Accessibility: buttons must either text content or accessible labelling                                                                                                                                                |    |\n| [object-literal-button-no-missing-aria](docs/rules/object-literal-button-no-missing-aria.md)                 | Accessibility: Object literal image buttons must have accessible labelling: aria-label, aria-labelledby, aria-describedby                                                                                              |    |\n| [switch-needs-labelling-v9](docs/rules/switch-needs-labelling-v9.md)                                         | Accessibility: Switch must have an accessible label                                                                                                                                                                    |    |\n| [text-area-missing-label-v9](docs/rules/text-area-missing-label-v9.md)                                       | Accessibility: Textarea must have an accessible name                                                                                                                                                                   |    |\n| [text-content-button-does-not-need-aria](docs/rules/text-content-button-does-not-need-aria.md)               | Accessibility: a button with text content does not need aria labelling. The button already has an accessible name and the aria-label will override the text content for screen reader users.                           |    |\n| [toolbar-missing-aria-v9](docs/rules/toolbar-missing-aria-v9.md)                                             | Accessibility: Toolbars need accessible labelling: aria-label or aria-labelledby                                                                                                                                       |    |\n\n<!-- end auto-generated rules list -->\n", "repo_name": "eslint-plugin-fluentui-jsx-a11y", "org_name": "microsoft", "org_repo": "microsoft/eslint-plugin-fluentui-jsx-a11y", "platform_org_repo": "github+microsoft/eslint-plugin-fluentui-jsx-a11y", "link_to_repo": "https://github.com/microsoft/eslint-plugin-fluentui-jsx-a11y", "platform": "github", "language": "JavaScript", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "![dev-home-readme-header](https://github.com/microsoft/devhome/blob/main/src/Assets/Preview/StoreDisplay-150.png)\n\n# Welcome to the Dev Home repo\n\nThis repository contains the source code for:\n\n* [Dev Home](https://aka.ms/devhome)\n* Dev Home core widgets\n\nRelated repositories include:\n\n* [Dev Home GitHub Extension](https://github.com/microsoft/devhomegithubextension)\n\n## Installing and running Dev Home\n\n> **Note**: Dev Home requires Windows 11 21H2 (build 22000) or later.\n\n### Microsoft Store [Recommended]\n\nInstall [Dev Home from the Microsoft Store](https://aka.ms/devhome).\nThis allows you to always be on the latest version when we release new builds with automatic upgrades.\n\nThis is our preferred method.\n\n### Other install methods\n\n#### Via GitHub\n\nFor users who are unable to install Dev Home from the Microsoft Store, released builds can be manually downloaded from this repository's [Releases page](https://github.com/microsoft/devhome/releases).\n\n#### Via Windows Package Manager CLI (aka winget)\n\n[winget](https://github.com/microsoft/winget-cli) users can download and install the latest Dev Home release by installing the `Microsoft.DevHome` package:\n\n```powershell\nwinget install --id Microsoft.DevHome -e\n```\n\n---\n\n## Dev Home roadmap\n\nThe plan for Dev Home will be posted shortly and will be updated as the project proceeds.\n\n---\n\n## Dev Home overview\n\nPlease take a few minutes to review the overview below before diving into the code:\n\n### Dashboard\n\nThe Dev Home dashboard displays Windows widgets. These widgets are built using the Windows widget platform, which relies on Adaptive Cards.\n\n### Machine configuration\n\nThe machine configuration tool utilizes the Dev Home GitHub Extension, but isn't required to clone and install apps. The app installation tool is powered by winget.\n\n#### Popular apps\n\nThe machine configuration tool provides a list of popular apps when selecting applications to install. This is currently a hard-coded list of applications that have been popular with developers on Windows. Popularity was determined by high levels of installation and usage. As this is a moment in time, we are not accepting submissions for this list. We're looking to improve the experience with [Suggested Apps](https://github.com/microsoft/devhome/issues/375) so the list can be optimized for developers.\n\n---\n\n## Documentation\n\nDocumentation for Dev Home can be found at https://aka.ms/devhomedocs.\n\n---\n\n## Contributing\n\nWe are excited to work alongside you, our amazing community, to build and enhance Dev Home!\n\n***BEFORE you start work on a feature/fix,*** please read & follow our [Contributor's Guide](CONTRIBUTING.md) to help avoid any wasted or duplicate effort.\n\n## Communicating with the team\n\nThe easiest way to communicate with the team is via GitHub issues.\n\nPlease file new issues, feature requests, and suggestions but **DO search for similar open/closed preexisting issues before creating a new issue.**\n\nIf you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:\n\n* [Kayla Cinnamon](https://github.com/cinnamon-msft), Product Manager: [@cinnamon_msft](https://twitter.com/cinnamon_msft)\n* [Clint Rutkas](https://github.com/crutkas), Senior Product Manager: [@clintrutkas](https://twitter.com/clintrutkas) \n* [Leeza Mathew](https://github.com/mathewleeza), Engineering Lead: [@leezamathew](https://twitter.com/leezamathew)\n* [Ujjwal Chadha](https://github.com/ujjwalchadha), Developer: [@ujjwalscript](https://twitter.com/ujjwalscript)\n\n## Developer guidance\n\n* You must be running Windows 11 21H2 (build >= 10.0.22000.0) to run Dev Home\n* You must [enable Developer Mode in the Windows Settings app](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development)\n\n## Building the code\n\n1. Clone the repository\n2. Configure your system, please use the [configuration file](.configurations/configuration.dsc.yaml). This can be applied by either:\n   * Dev Home's machine configuration tool\n   * WinGet configuration. If you have the experimental feature enabled, run `winget configure .configurations/configuration.dsc.yaml` from the project root so relative paths resolve correctly\n\n## Running & debugging\n\nIn Visual Studio, you should be able to build and debug Dev Home by hitting <kbd>F5</kbd>. Make sure to select either the \"x64\" or the \"x86\" platform and set DevHome as the selected startup project.\n\n---\n\n## Code of conduct\n\nWe welcome contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "devhome", "org_name": "microsoft", "org_repo": "microsoft/devhome", "platform_org_repo": "github+microsoft/devhome", "link_to_repo": "https://github.com/microsoft/devhome", "platform": "github", "language": "C#", "stargazers_count": 2020, "watchers_count": 2020}, {"README_text": "# DevBox + Deployment Environments Workshop\nThis lab will guide you through setting up an end-to-end demo environment for the Azure Deployment\nEnvironments and Dev Box services.\n\n## Table of content\n- [DevBox + Deployment Environments Workshop](#devbox--deployment-environments-workshop)\n  - [Table of content](#table-of-content)\n  - [Deployment Environments](#deployment-environments)\n    - [Create a resource group](#create-a-resource-group)\n    - [Create a Dev Center](#create-a-dev-center)\n    - [Create a KeyVault and Store a GitHub personal access token there](#create-a-keyvault-and-store-a-github-personal-access-token-there)\n    - [Add a catalog to the Dev Center](#add-a-catalog-to-the-dev-center)\n    - [Set up environment types](#set-up-environment-types)\n    - [Set Up an Azure Deployment Environments project](#set-up-an-azure-deployment-environments-project)\n    - [Create and access Environments](#create-and-access-environments)\n    - [Use the Dev Center CLI to configure Environments](#use-the-dev-center-cli-to-configure-environments)\n    - [Use the Developer Portal to create and manage environments](#use-the-developer-portal-to-create-and-manage-environments)\n    - [Clean Up](#clean-up)\n  - [DevBox](#devbox)\n    - [Create Dev box definition](#create-dev-box-definition)\n    - [Create a Network Connection](#create-a-network-connection)\n    - [Add a permissions to dev center project](#add-a-permissions-to-dev-center-project)\n    - [Create a Dev box pool](#create-a-dev-box-pool)\n    - [Test the Environment](#test-the-environment)\n    - [Clean Up](#clean-up-1)\n\n## Deployment Environments\n\n<img src=\"./assets/deploymentEnvironments.png\" alt=\"deploymentEnvironments\" width=\"1000\"/>\n\n### Create a resource group\n\n1. If you are not signed in already, sign into the Azure Portal and make sure you can access the subscription that you\u2019d like to use for the duration of the lab. You should be an owner of this subscription.\n2. Select the option to \u201cCreate a resource\u201d, type \u201cresource group\u201d, and select the \u201cResource Group\u201d option in the catalog.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/D023411A-E4CF-4D51-AF33-8C2182393AC2_2/9kUx01gg5wA1BnRWB91OAwZM8VJ6WnIgRrv2CVai2X8z/Image.png)\n\n3. Confirm the creation with \u201cCreate\u201d and pick a name for the resource group that you\u2019ll use for duration of the lab.\n   - We picked \u201cmtc-devportal-group\u201d for the name\n   - Ensure that you select a region where Dev Center resources are currently available and use this region throughout the lab\n      - At the time of writing these regions are: Australia East, Japan East, Canada Central, UK South, West Europe, East US, East US 2, South Central US, West US 3\n4. Confirm the creation of the resource by clicking \u201cReview + create\u201d and then \u201cCreate\u201d on the next screen.\n\n### Create a Dev Center  \n\n1. Immediately return to the \u201cCreate a resource\u201d screen and search for \u201cDev center\u201d and select the \u201cDev Center\u201d resource\n2. Choose a name descriptive name for the Dev Center and place it in the same region as your resource group. Select the resource group you created earlier.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/64B6B426-EB8A-408D-A56C-B89099DC8438_2/qyexv24tiq7Hxzf8xxlnC6m3lnFmZqMWEVACaqEC29cz/Image.png)\n\n3. Confirm the details and the creation of the resource.\n4. Once the Dev Center creation is complete navigate to the resource group that you created earlier and find the Dev Center resource\n5. Click the Dev Center resource to open its resource blade\n6. To allow the Dev Center to manage a catalog of environment templates and associated permissions, we need to assign a system managed identity to the resource.\n7. To do this select \u201cIdentity\u201d in the side bar and then switch the Dev Center\u2019s system assigned identity on.\n8. Confirm the process by clicking \u201cSave\u201d. You will see a pop up informing you that the resource will be registered in the Azure AD. Confirm the prompt and wait for the system-assigned identity object to be created.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/3483A360-DEF1-4488-B566-0B2510E87F03_2/8SfFTgxjWVkzOfPFNod9ay8ywZN19p4Z1LWEehIfS58z/Image.png)\n\n9. The screen will now update to show an object id and a role assignment button\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/9A8120C1-2F9E-4646-979D-903C00B34EE1_2/iOyZRS4A92qYCeTjfo82VFy9DAyPVcJbLCXsBWOtewcz/Image.png)\n\n10. Click the \u201cAzure role assignments\u201d button. This will take you to a new blade where you can manage the permission assignments for the identity.\n11. On the \u201cAzure role assignments\u201d blade click \u201cAdd role assignment\u201d\n12. Specify a Scope of \u201cSubscription\u201d\n13. As the subscription target pick your current subscription\n   - The Dev Center needs to have access to all the subscriptions you want to deploy to\n14. As a role pick \u201cOwner\u201d\n   - This allows the Dev Center to fully manage any environments that are created\n15. Click \u201cSave\u201d to confirm\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/0479F43E-ABA2-4FB6-AA02-0EAEC49968D2_2/yZ2yJOrxYYjOhu6G00uldtTAm6nrQNHTFZZR9VRmkJQz/Image.png)\n\nOnce the assignment is complete a line confirming the assignment will show up in the \u201cAzure role assignments\u201d blade.\n\n16. Use the links at the top of the page to navigate back to your Dev Center.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/FA60BEB4-E0D8-478C-BC25-CEFC2B8ABD28_2/ev38MUxIPZkOH07Uo0Q8IYwqCroy4Hiib81Y1PUWItUz/Image.png)\n\n### Create a KeyVault and Store a GitHub personal access token there\n\nBefore you add a catalog, we need to store a personal access token as a key vault secret in Azure Key Vault and copy the secret identifier. For the purpose of our lab, we will use a GitHub personal access token but Azure DevOps is also supported. We will use the GitHub token to check the status of and consume content from a public repo on GitHub.\n\n- Go to [https://github.com/settings/tokens](https://github.com/settings/tokens)\n- If necessary, log in with your GitHub account\n- Click on \u201cGenerate new token\u201d and then select \u201cGenerate new token (classic)\u201d\n- On the next screen select a relatively short expiration time\n- Select the following scopes\n   - repo:status\n   - repo_deployment\n   - public_repo\n- Add a note \"mtc lab use\" to give the token a descriptive name\n- Click \"Generate token\"\n\nOn the next page make sure to copy your personal access token. It will only be displayed until you navigate away from the page. If you have connected your GitHub account to one or more of Microsoft\u2019s GitHub organizations, then the organization where the repo is hosted will require you to configure an additional authorization on the token. To do this click \u201cConfigure SSO\u201d next to the token and then \u201cAuthorize\u201d next to the Azure organization. If you are not a member of the \u201cAzure\u201d organization you can use the \u201cAuthorize\u201d option with any other Microsoft GitHub organization in the list. While the token remains the same after this step, completing this process enables it to access repos that you have read access to in this organization. You may need to repeat this step if your organizational token expires\n\nNow it is time to return to the Azure Portal\n\nYou might want to temporarily paste the personal access token into Notepad while you complete the next steps to avoid loosing it.\n\n- Once back in the Azure portal go to the \u201cCreate a resource\u201d screen and then type \u201cKey Vault\u201d in the search field.\n- On the search result screen select the option to create a KeyVault.\n- Place the KeyVault into the same resource group and region that you have been using so far.\n- Pick a descriptive name as well as the \u201cStandard\u201d pricing tier\n- You will need to choose a unique name that is all lowercase in order to create the resource.\n   - All other options can remain as specified by default\n- Click \u201cReview + create\u201d and confirm the creation of the vault\n\nThe Key Vault resource should only take a few moments to show up in your existing resource group. Once it appears, click the resource to perform the next steps in its resource blade.\n\n- First we need to configure an access policy that allows your Dev Center to read the secret that we are going to store in the KeyVault\n- To do this select \u201cAccess policies\u201d in the side bar and click \u201cCreate\u201d\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/7558F114-E2DE-46FD-8E02-92848CC0FB4C_2/LF1YOxBxqJMTkgeyxoVlOhACkI9Cxnqc5NZ4RmvDDuUz/Image.png)\n\n- On the \u201cPermissions\u201d screen select \u201cSecret permissions > Get\u201d and click \u201cNext\u201d\n- On the \u201cPrincipal\u201d screen search for the name of your Dev Center resource and select it. Click \u201cNext\u201d\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/D3B4D836-FE2F-4AD7-AB66-9786FBC30202_2/pFZWUcfU7FnC2seIFT4hoJfWSrs8qGUGhN2X6ExeX2Iz/Image.png)\n\n- Skip the \u201cApplication\u201d screen by clicking \u201cNext\u201d\n- Finally on the last screen click \u201cCreate\u201d\n\nOnce back on the previous blade you should see that the Dev Center now has access to read secrets from the Key Vault.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/33B067A0-FE2E-4FBD-BC70-76BD0F929872_2/ItGMy6rmnDI8NyQpVSgcyCUvQhTF5a3zx6sflBhUBIQz/Image.png)\n\nNow it is time to place our PAT token into the key vault.\n\n- Make sure you have still got the PAT token copied.\n- Select \u201cSecrets\u201d in the side bar.\n- Click \u201cGenerate/Import\u201d in the Secrets blade.\n- On the next screen configure the following options\n   - Upload options: Manual\n   - Name: GHPAT\n   - Secret Value: copy your PAT here\n   - Set activation date: no need to tick\n   - Set expiration date: you can optionally set an expiration date for a time a few days out when the PAT will no longer be valid and/or necessary\n   - Enabled: Yes\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/64023FE6-2DAB-41D9-89CB-E3C793F65A9C_2/Y2GO2W9Gz45YOXSWSs45QYqTC6nblTTHxMb24g0g81Yz/Image.png)\n\n- Click \u201cCreate\u201d and ensure that the new secret shows up in the Secrets blade\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/CE2D29CA-7B71-42AA-918E-6D78D88E0671_2/RuGA2yJCg4oQisgP62e0pJGSPDpFM26OEOuIo0xTOAEz/Image.png)\n\n### Add a catalog to the Dev Center\n\nReturn to your Dev Center in the Azure Portal and select the \u201cCatalogs\u201d blade in the side bar. Click \u201cAdd\u201d to add a new catalog.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/126D9DC7-9773-4CC8-9D95-66A4C1B98EFF_2/Ro3TRWKQnbFbomSy93W81E0H8W7IDPW3dK3xpu6YqW4z/Image.png)\n\nFor the lab we will use the Azure [Deployments Community catalog](https://github.com/arikbidny/deployment-environments). It already comes preconfigured with the necessary manifest files and templates\n\nConfigure the following options:\n\n- Name: MtcTemplates\n- Git clone URI: [https://github.com/arikbidny/deployment-environments.git](https://github.com/arikbidny/deployment-environments.git)\n- Branch: main\n- Folder Path: Environments\n- Secret Identifier: https://{your KeyVault name}.vault.azure.net/secrets/GHPAT\n\nIf in doubt you can find your Key Vault URI on its resource blade.\n\nConfirm the creation of the catalog. Within a few seconds the new catalog should start showing up on the blade.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/E266C233-5F01-4E30-9D01-13BE3F36BE6C_2/MCHC0YyWdRqZCBR93hKjxyfdDY63ayLt2re1RwRsbyoz/Image.png)\n\n- Select the item and click \u201cSync\u201d to make the catalog items from the repository available to use.\n- Take some time to [review the requirements for creating catalog items](https://learn.microsoft.com/en-us/azure/deployment-environments/configure-catalog-item) while the repository syncs with your Dev Center.\n\n### Set up environment types\n\nYou can use environment types to categorize the Azure Deployment Environments that your development teams deploy into. You can apply different settings for each environment type.\n\n- Select \u201cEnvironment types\u201d in the side bar to bring up the \u201cEnvironment types\u201d blade\n   - Click \u201cCreate\u201d to create an environment type\n   - Call your environment type \u201cSandbox\u201d\n   - Repeat this process three more times creating three more environment types\n      - \u201cDev\u201d\n      - \u201cTest\u201d\n      - \u201cStaging\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/7CC8C8D6-884B-4A46-BB32-A2BA2FE22064_2/0Ksp69Oq9XXAqzv9CUbfkz6cDC8WNqlx4TbZFo1IqJAz/Image.png)\n\nAn environment type that you add to your dev center is available in each project in the dev center, but environment types aren't enabled by default. When you enable an environment type at the project level, the environment type determines the managed identity and subscription that are used to deploy environments.\n\n### Set Up an Azure Deployment Environments project\n\nThe next step is to create a project in your Dev Center that we can attach an environment type to. This in turn enables development teams to deploy templated infrastructure from the catalog to these environments.\n\n- On your Dev Center resource blade select \u201cProjects\u201d in the side bar.\n- Click \u201cCreate\u201d to create a new project.\n- On the next blade make sure the dev center resource that you have been using so far is selected and give your project a descriptive name.\n- Confirm the creation of the resource.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/5C2D2F17-7317-4BAD-9C29-B5198761CEC3_2/rtqxXACFgnb4xqZ1QRxm7QCllGyL0ehGwtW4yBlYzG0z/Image.png)\n\nYou will land on a deployment screen. Select the resource group name to return to your resource group and select the Dev Center to return to the blade where you started.\n\nYour project should now show up in the \u201cProjects\u201d blade. You can find the link to it in the side bar of your Dev Center\u2019s resource blade. Click the project name in the balde to configure the project further\n\nYou will land on the resource blade for the project.\n\n- In the sidebar select \u201cEnvironment types\u201d to configure the environment that the project has access to.\n- Click **\"Add\"** to proceed\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/AA878D5B-694D-498F-BD70-81FCE22C1175_2/vBxYfERTPxI2ZZgpRiOCMuzjzhrE79uZO8t1fZ5DtZgz/Image.png)\n\n- For the purpose of this lab we\u2019ll select the \u201cSandbox\u201d environment type\n- At least one identity (system-assigned or user-assigned) must be enabled for deployment identity. The identity is used to perform the environment deployment on behalf of the developer\n- When it comes to Environment creator role(s) select \u201cContributor\u201d. This will give developers deploying into the environment contributor access to the resources that they create.\n- You can assign additional access via the \u201cAdditional access\u201d drop downs if needed. This is not required for the lab.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/8FCB6F12-EA1D-46D6-9C93-E4EF663B4681_2/62fpKJk4ogomod7W6uqAblL5aQ3bSPbIvkZNZHNsPiAz/Image.png)\n\nDevelopers can only use the deployment environment specified in the project if they are given a specific RBAC role. For the purpose of this lab we will assign this role to you.\n\nDevelopers can only use the deployment environment specified in the project if they are given a specific RBAC role. For the purpose of this lab we will assign this role to you.\n\n- In the side bar select \u201cAccess control (IAM)\u201d\n- Select \u201cAdd > Add role assignment\u201d.\n- In Add role assignment, enter the following information, and then select Save:\n   - On the Role tab, select Deployment Environments user.\n      - A Dev Center project admin role is available for Dev Managers who need to also review the status of the project resource in the Azure portal.\n   - On the Members tab, select your user identity.\n- Click \u201cReview + assign\u201d and confirm the assignment on the summary page.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/D2FAAD63-ABC0-46B1-9306-205FBC9A1800_2/PWFE1uMAgxlleUhY5mByCP29n5ssLLnuLuUEFK6gWQIz/Image.png)\n\n### Create and access Environments\n\nIn this exercise you will explore the steps to create and manage developer environments both from the developer portal and the Azure CLI\n\n### Use the Dev Center CLI to configure Environments\n\nFor the next step you will require the Azure CLI, as well as a Dev Center extension package for it. Some of the steps outlined are not yet supported in Cloud Shell. We therefore recommend that you complete the following steps with a local installation of the Azure CLI.If you need to install or update Azure CLI, you can find instructions here: [https://learn.microsoft.com/en-us/cli/azure/install-azure-cli](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n- Open a command prompt.\n   - Use Admin mode if you\u2019d like to install the extension for all users.\n- Run the following command in Azure CLI once both services are set up on your machine:\n\n`az extension add --source` [`https://fidalgosetup.blob.core.windows.net/cli-extensions/devcenter-0.1.0-py3-none-any.whl`](https://fidalgosetup.blob.core.windows.net/cli-extensions/devcenter-0.1.0-py3-none-any.whl)\n\nThe path might be easier to copy from here: [https://learn.microsoft.com/en-us/azure/deploymentenvironments/how-to-configure-use-cli](https://learn.microsoft.com/en-us/azure/deploymentenvironments/how-to-configure-use-cli)\n\n- Type \u201cy\u201d to confirm that you\u2019d like to install the extension and wait for the process to complete.\n- Use the \u201caz login\u201d command to log in with the user account that you have been using in the lab so far.\n\nFollow the instructions on screen to complete the login process.\n\n- Once logged in you can use the following command to list all the Azure Deployment Environments projects that you have access to\n\n`az graph query -q \"Resources | where type =~ 'microsoft.devcenter/projects'\"`\n\n- If the project you created in the previous exercise does not show up use the following command to change your subscription\n\n`az account set --subscription`\n\n- You can use the following commands to list the environment type sand catalog items available to you in a specific project respectively\n\n`az devcenter dev environment-type list --dev-center <name> --project-name <name>  -o table`\n\n`az devcenter dev catalog-item list --dev-center <name> --project-name <name> -o table`\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/35FAF514-8E99-4BC0-AF2C-69EE2723E080_2/HVmvEQGNLqEtZ7ZfEIzLpyGXEDrbLNwI6YKzDLWx6agz/Image.png)\n\n- Use the following command to create a Web App resource in a new environment in your project. Feel free to pick your environment name and use the \u201cSandbox\u201d type.\n\naz devcenter dev environment create --dev-center-name <devcenter-name>  --project-name <project-name> -n  --environment-type Sandbox --catalog-item-name WebApp\u2013catalog-name CommunityTemplates\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/18FF761B-05A5-4B4D-9735-89EAC911EC87_2/AYf5xejAAkjyplyHQxEIrqtV6znQmXqGjlKxuR3Qdawz/Image.png)\n\n- By using the following command developers can access the endpoints of resources that are deployed in Azure Deployment Environments via the command line.\n\n`az devcenter dev environment list --dev-center <devcenter-name> --project-name <project-name>`\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/F2547994-1F48-4F4B-9CB8-AA0CF1C61DAA_2/5f9J9xWvjdGSPMwgkww8cn9tyw9HR5cg8i2VffJxs7wz/Image.png)\n\n- Take a moment to review other CLI commands available to you: [https://learn.microsoft.com/en-us/azure/deployment-environments/how-to-configure-use-cli](https://learn.microsoft.com/en-us/azure/deployment-environments/how-to-configure-use-cli)\n\nReturn to the Azure Portal and review your resource groups\n\n- You should be able to see a resource group for the environment deployment that you just completed on the command line\n- Return to the resource group you have used throughout the lab and select the project resources that you created earlier\n- Select Environments in the side bar. All environments both failed and succeeded will show up here.\n- Clicking \u201cView Cost\u201d for an environment takes you to the \u201cCost analysis and management\u201d blades for the resource group used in conjunction with the environment.\n\nYou can delete environments via the CLI. A sample command is shown in Exercise 4 and in the CLI documentation linked on the previous page.\n\n### Use the Developer Portal to create and manage environments\n\nTo deploy and manage environments using the developer portal head to:\n\n[https://devportal.microsoft.com/](https://devportal.microsoft.com/)\n\nWhen prompted to sign in use the account that you have been using for throughout this lab. You are accessing this portal as if you were a developer who has been assigned permissions to the environment.\n\nOnce singed in check if you can see the dropdown that allows you to pick between creating a Dev Box and an Azure Deployment Environment. If this is not visible use the link reference above again after logging in.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/0978F133-6021-4603-B703-8A74219DDC09_2/PXq4cRXYLxb5XhaiiGMzg5uk0rWCytpZxgPEKbjzHR8z/Image.png)\n\nIf you have already created an environment you will see it shown in this dashboard. Click \u201cEnvironment Resources\u201d to navigate to the environment.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/FAD1B0E6-959C-4DD5-B19B-D57E0C7943F5_2/hRtP2exK0xlbmaQDwxyjbNzcPnxGvXH4FvrjWx4yosYz/Image.png)\n\nCreate a new environment by clicking the \u201c+ New\u201d drop down and choosing \u201cNew environment\u201d.\n\nA pop up appears asking you to provide an environment name and to select a catalog item. You can use the link to \u201cview all catalog items\u201d to browse your options.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/F5327088-A7C0-41E1-96FB-1AFD0E388C40_2/yBhMdJMW55dSMQcJEBubONzNpwNCJN7B0RDxZKh1gQwz/Image.png)\n\nSelect the \u201cFunctionApp\u201d catalog item and call your environment \u201cMyFunc\u201d. Click \u201cCreate\u201d to confirm.\n\nYour environment will take a few moments to create. If you are logged into the Azure Portal in a different tab you can follow its creation in the portal.\n\n- Clicking \u201cView Cost\u201d for an environment takes you to the \u201cCost analysis and management\u201d blades for the resource group used in conjunction with the environment.\n\nReturn to the developer portal where you started the creation of your environment. To delete an environment from the Developer Portal, use the ellipsis menu \u201c\u2026\u201d and the \u201cDelete\u201d option. Confirm that you would like to delete the environment in the following pop up.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/85C9B869-DCC7-444B-911C-AB41BBB5610E_2/c4kP4VAH0P3Vi8nGQRVylaOplD4yoxNVcsVD906NuNAz/Image.png)\n\n### Clean Up\n\nTo keep costs to a minimum we recommend that you delete any environments that you have created during this lab.\n\nYou can use the following CLI command to do this\n\naz devcenter dev environment delete --dev-center  <devcenter-name> --project-name <project-name>  --user-id \"me\" --name <environment-name>\n\nYou need to confirm the action by typing \"y\".\n\nAt this time, you can also clean up the Dev Center resources in your subscription. However, you may want to keep them around as they do not cost anything to maintain.\n\n----\n\n## DevBox\n\n<img src=\"./assets/devbox.png\" alt=\"devbox\" width=\"1000\"/>\n\n### Create Dev box definition\n\n- On your Dev Center resource blade select \u201cDev box definitions\u201d in the side bar.\n- Select \"Create\"\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/14127273-9AA4-48AB-B3C6-EAAC46C574BC_2/QA8HURr7sjJHLC5bxdbI1TlsqKHcCm27xeUBXCZI8n8z/Image.png)\n\n- In \"Create dev box definition\", enter the following information, and then select Create:\n   - Name: Choose a name\n   - Image: Choose image from the image list\n   - Image version: Latest\n   - Compute: 4 vCPU, 16 GB RAM\n   - Storage: 256 GB SSD\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/09C64BC3-71D1-4AE0-BF84-A60FE4AC3B14_2/HNVpFeHm5Rpkvhwm3GnQCWJssOUtmrt2wBg8iaDt3ysz/Image.png)\n\n### Create a Network Connection\n\n- Go to your resource group\n- Select +Create\n- Search for Virtual network and select Create\n- In \"Create virtual network\", enter the following information, and then select \"Review + create\"\n   - Resource group: select the same resource group as the dev center\n   - Name: Choose a name\n   - Region: Choose the same region as the dev center\n- In \"Search resources\" search for Network Connections\n- Select \"+Create\"\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/E92D72FB-BCED-40A0-B077-00090ADFDFF8_2/YWByB3wNH0lOLtxZrgU115V9rTxEXUHdX2DOcRy4kMgz/Image.png)\n\n- In \"Create a network connection\", enter the following information, and then select \"Review + create\"\n   - Domain join type: \"Azure active directory join\"\n   - Subscription: Choose your subscription\n   - Resource Group: select the same resource group as the dev center\n   - Name: Choose a name\n   - Virtual network and subnet: Choose a vnet you created in last step\n- On your Dev Center resource blade select \u201cNetworking\u201d in the side bar.\n- Select \"+Add\"\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/B83BCDEF-73E4-43B8-AC5B-9CE06A043B12_2/Z9EuZVwG3GuAgSNFT8YOX6gmiVWSqiudsDG6PGlQCPcz/Image.png)\n\n- Select your connection ( Created in previous step )\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/9669F95E-300A-4C20-8C84-D0182675E587_2/eyEg4ZAhRvEa0Qz5mKwmuUjOeCrnjadHtCEMi7GoELkz/Image.png)\n\n- Click \"Add\"\n- Wait until the \"Running checks\" finished\n\n### Add a permissions to dev center project\n\n- On your Dev Center resource blade select \u201cProjects\u201d in the side bar and choose your project.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/678A2086-F8D2-4EDF-A648-33B3924362EA_2/9PaxWDqp8ejy0tMu4sf8FhpuLfoYGXQEUNyrYWpjnSgz/Image.png)\n\n- On your Project blade, in the side bar select \"Access control (IAM)\"\n   - Choose \"+Add\" and select \"Add role assignment\"\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/5F5904F2-6885-462F-A69C-6C7C1E226280_2/M4mxtH8FgtmAX36NNtLJtzd4ybFVoqLUgVHu0x6lWhEz/Image.png)\n\n      - Choose \"DevCenter Dev Box User\"\n      - Select your \"User\" from \"+Select members\"\n      - Hit \"Select\"\n      - Hit \"Review + assign\"\n\n### Create a Dev box pool\n\n- On your Project resource blade select \u201cDev box pools\u201d in the side bar.\n- Hit \"+Create\"\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/7F202305-3E34-400E-AB56-8BCC356D66B0_2/Nroou1G2GqqHXJ9mCFIzacQOHme43bqCwTvbWoIBbBsz/Image.png)\n\n- In \"Create a dev box pool\", enter the following information, and then select \"Create\"\n   - Name: Choose a dev box pool name\n   - Dev box definition: Choose the definition you created\n   - Network connection: Choose a network connection you created\n   - Dev box Creator privileges: Local Administrator\n   - Enable Auto-stop: Select \"Yes\" and choose a Stop time\n   - Licensing: check the box\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/034C88A6-4DB1-4699-9AF3-FB1579174C84_2/ZTXmGFXboovxfnztlnrqKxDWppYQn3M0EnEqD7Vp2t0z/Image.png)\n\n### Test the Environment\n\nTo deploy and manage environments using the developer portal head to:\n\n[https://devportal.microsoft.com/](https://devportal.microsoft.com/)\n\nWhen prompted to sign in use the account that you have been using for throughout this lab. You are accessing this portal as if you were a developer who has been assigned permissions to the environment.\n\nOnce singed in check if you can see the dropdown that allows you to pick between creating a Dev Box and an Azure Deployment Environment. If this is not visible use the link reference above again after logging in.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/4520D653-B99D-43FC-A9DF-47439078020C_2/TuBwpe9Qwb7IyZOOdBWPyxaHl4FZGZJP0RIPep3buqcz/Image.png)\n\nChoose the dev box:\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/B3B56CED-BFD6-4228-9712-2D13A054572E_2/7ZWNzM1rkQatNRg1MFwFTGk7yz5MzZOTIawowxlUb8gz/Image.png)\n\nHit \"Create\"\n\n### Clean Up\n\nReturn to the developer portal where you started the creation of your dev box. To delete a dev box from the Developer Portal, use the ellipsis menu \u201c\u2026\u201d and the \u201cDelete\u201d option. Confirm that you would like to delete the dev box in the following pop up.\n\n![Image.png](https://res.craft.do/user/full/93958d6f-1340-6147-fc3c-1be83d5bfef9/doc/49E7F73D-1E7D-429B-9AE0-074421D68164/7643BE84-2A69-482B-9117-6341D4237F35_2/VBbGiPjRcAuNVa4JOoS61Gy9sfvvd2AvbxbFwFOevSoz/Image.png)\n\n", "repo_name": "MTC_IL_WORKSHOP_DevBox_DeploymentEnvironments", "org_name": "microsoft", "org_repo": "microsoft/MTC_IL_WORKSHOP_DevBox_DeploymentEnvironments", "platform_org_repo": "github+microsoft/MTC_IL_WORKSHOP_DevBox_DeploymentEnvironments", "link_to_repo": "https://github.com/microsoft/MTC_IL_WORKSHOP_DevBox_DeploymentEnvironments", "platform": "github", "language": null, "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-form-php", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-form-php", "platform_org_repo": "github+microsoft/kiota-serialization-form-php", "link_to_repo": "https://github.com/microsoft/kiota-serialization-form-php", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-form-python", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-form-python", "platform_org_repo": "github+microsoft/kiota-serialization-form-python", "link_to_repo": "https://github.com/microsoft/kiota-serialization-form-python", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# <image src=\"https://github.com/microsoft/teams-ai/assets/14900841/972a9a1b-679a-4725-bfc0-a1e76151a78a\" height=\"10%\" width=\"10%\" /> Teams AI Library\nWelcome to the Teams AI Library! This SDK is specifically designed to assist you in creating bots capable of interacting with Teams and Microsoft 365 applications. It is constructed using the [Bot Framework SDK](https://github.com/microsoft/botbuilder-js) as its foundation, simplifying the process of developing bots that interact with Teams' artificial intelligence capabilities.\n\n<p>\n<figure>\n<img src=\"https://github.com/microsoft/teams-ai/assets/14900841/154353ff-bafe-4423-abcd-6dc5a8680fe9\" />\n<figcaption>This is a diagram of the Teams-AI flow. Teams AI SDK hooks into the Teams SDK and Azure OpenAI SDK to provide a seamless experience for developers.</figcaption>\n</figure>\n</p>\nThe SDK is currently available for JavaScript/TypeScript applications in the <a href=\"./js\" ><code>js</code></a> folder and via the <a href=\"https://www.npmjs.com/package/@microsoft/teams-ai\">teams-ai package on NPM</a>. We are actively developing parity for .NET, which will be available soon.\n\n## Getting Started\n\n\n> ### \ud83d\udd87\ufe0f Jump right in\u2757\ufe0f \ud83d\udcce \n> If you want to jump immediately into AI, try out the [04.ai.a.teamsChefbot](./js/samples/04.ai.a.teamsChefBot) sample. This sample is a simple bot that uses the OpenAI GPT model to build a Teams app. Just load it up in Visual Code and hit F5! \ud83c\udf89\n\n### Start with our getting started guides\n\n0. [Teams AI Library documentation](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams%20conversational%20ai/teams-conversation-ai-overview) - Take a look our documentation!\n1. [Migration](getting-started/00.MIGRATION.md) - if you have an existing bot, this guide will help you migrate to this SDK.\n2. **\\*** [AI Setup](getting-started/01.AI-SETUP.md) - if you're starting from scratch, this guide will help you <large>**get started with AI**</large>.\n3. [API-Reference](getting-started/02.API-REFERENCE.md) - this guide will help you understand the API reference.\n4. [Prompt injection](getting-started/03.PROMPT-INJECTION.md) - this guide will help you learn the basics of avoiding prompt injection in your bot.\n\n# Join us for the Global Teams Hackathon event\n\nTo sign up, visit [https://aka.ms/hack-together-teams](https://aka.ms/hack-together-teams).\n\n> HackTogether is your playground for coding and experimenting with Microsoft Teams. With mentorship from Microsoft experts and access to the latest tech, you will learn how to build Teams apps based on the top Microsoft Teams app scenarios. The possibilities are endless for what you can create... plus you can submit your hack for a chance to win exciting prizes! \ud83e\udd73\n\n## Capabilities\n\n### Teams-centric component scaffolding\n\n> For examples of the below, browse through the [JS](./js/samples/) folders.\n> Simple scaffolding for any conversational app component, including:\n\n- Chat bots\n- Message Extensions\n- Link unfurling\n- Adaptive Cards\n\n### Natural Language Modelling\n\nThe SDK is built to leverage OpenAI Large Language Models so you don't have to create your own. This saves you the complexity of processing natural language, and allows your users to talk to your app with their own words.\n\n### Prompt Engineering\n\nWith a simple text file written in human language, you can describe the functionality of your app to cue OpenAI to focus on the right user intentions and provide relevant responses.\n\n### Moderation\n\nA configurable API call to filter inappropriate content for input content, output content, or both. (See [OpenAIModerator.ts](./js/src/openai/OpenAIModerator.ts))\n\n### Predictive Engine to Map Intents to Actions\n\nLeveraging provided prompts and topic filters, it's simple to create a predictive engine that detects user intents and map them to relevant app actions, where you can focus your business logic. These actions are even possible to chain together to make building complex workflows easy.\n\n### Conversational Session History\n\nThe state of your user's session is not lost, allowing conversations to flow freely and arrive quickly at right outcome.\n\n### Localization\n\nBecause OpenAI's models are trained on the open internet, they're tuned to any language, saving you the cost of localization.\n\n### LLM modularity\n\nWhile the SDK handles OpenAI's GPT models out of the box, you can choose to swap to the LLM of your choice without touching any of your conversational app code.\n\n## License\n\nThis SDK is licensed under the MIT License. This SDK includes tools to use APIs provided by third parties. These APIs are provided under their own separate terms.\n\n- OpenAI API. Use of the OpenAI API requires an API key, which can be obtained from OpenAI. By using this SDK, you agree to abide by the OpenAI API Terms of Use and Privacy Policy. You can find them at [OpenAI Terms of Use](https://openai.com/policies/terms-of-use)\n- Azure OpenAI Service. Use of the Azure OpenAI API requires an API key. By using this SDK, you agree to abide by the Azure OpenAI API terms. You can find them at [Azure OPENAI TOS](https://www.microsoft.com/licensing/terms/productoffering/MicrosoftAzure/MCA#ServiceSpecificTerms), and associated documentation at [Azure Cognitive Services](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/).\n\n---\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nFor more details, see [./CONTRIBUTING.md](./CONTRIBUTING.md).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "teams-ai", "org_name": "microsoft", "org_repo": "microsoft/teams-ai", "platform_org_repo": "github+microsoft/teams-ai", "link_to_repo": "https://github.com/microsoft/teams-ai", "platform": "github", "language": "TypeScript", "stargazers_count": 70, "watchers_count": 70}, {"README_text": "# `aiida-pyscf`\n[![PyPI version](https://badge.fury.io/py/aiida-pyscf.svg)](https://badge.fury.io/py/aiida-pyscf)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/aiida-pyscf.svg)](https://pypi.python.org/pypi/aiida-pyscf)\n[![CI](https://github.com/microsoft/aiida-pyscf/workflows/ci/badge.svg)](https://github.com/microsoft/aiida-pyscf/actions/workflows/ci.yml)\n\nAn [AiiDA](https://www.aiida.net) plugin for the [Python-based Simulations of Chemistry Framework (PySCF)](https://pyscf.org/index.html).\n\n1. [Installation](#installation)\n2. [Requirements](#requirements)\n3. [Setup](#setup)\n4. [Examples](#examples)\n    * [Mean-field calculation](#mean-field-calculation)\n    * [Customizing the structure](#customizing-the-structure)\n    * [Optimizing geometry](#optimizing-geometry)\n    * [Writing Hamiltonian to FCIDUMP files](#writing-hamiltonian-to-fcidump-files)\n    * [Writing orbitals to CUBE files](#writing-orbitals-to-cube-files)\n    * [Restarting unconverged calculations](#restarting-unconverged-calculations)\n    * [Automatic error recovery](#automatic-error-recovery)\n\n## Installation\n\nThe recommended method of installation is through [`pip`](https://pip.pypa.io/en/stable/):\n\n    pip install aiida-pyscf\n\n## Requirements\n\nTo use `aiida-pyscf` a configured AiiDA profile is required.\nPlease refer to the [documentation of `aiida-core`](https://aiida.readthedocs.io/projects/aiida-core/en/latest/intro/get_started.html) for detailed instructions.\n\n## Setup\n\nTo run a PySCF calculation through AiiDA using the `aiida-pyscf` plugin, the computer needs to be configured where PySCF should be run.\nPlease refer to the [documentation of `aiida-core`](https://aiida.readthedocs.io/projects/aiida-core/en/latest/howto/run_codes.html#computer-setup) for detailed instructions.\n\nThen the PySCF code needs to be configured.\nThe following YAML configuration file can be taken as a starting point:\n```yaml\nlabel: pyscf\ndescription: PySCF\ncomputer: localhost\nfilepath_executable: python\ndefault_calc_job_plugin: pyscf.base\nuse_double_quotes: false\nwith_mpi: false\nprepend_text: ''\nappend_text: ''\n\n```\nWrite the contents to a file named `pyscf.yml`, making sure to update the value of `computer` to the label of the computer configured in the previous step.\nTo configure the code, execute:\n```bash\nverdi code create core.code.installed --config pyscf.yml -n\n```\nThis should now have created the code with the label `pyscf` that will be used in the following examples.\n\n## Examples\n\n### Mean-field calculation\n\nThe default calculation is to perform a mean-field calculation.\nAt a very minimum, the structure and the mean-field method should be defined:\n```python\nfrom ase.build import molecule\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code\n\nbuilder = load_code('pyscf').get_builder()\nbuilder.structure = StructureData(ase=molecule('H2O'))\nbuilder.parameters = Dict({'mean_field': {'method': 'RHF'}})\nresults, node = run.get_node(builder)\n```\nThis runs a Hartree-Fock calculation on the geometry of a water molecule.\n\nThe main results are stored in the `parameters` output, which by default contain the computed `total_energy` and `forces`, details on the molecular orbitals, as well as some timing information:\n```python\nprint(results['parameters'].get_dict())\n{\n    'mean_field': {\n        'forces': [\n            [-6.4898366104394e-16, 3.0329042995656e-15, 2.2269765466236],\n            [1.122487932593e-14, 0.64803103141326, -1.1134882733107],\n            [-1.0575895664886e-14, -0.64803103141331, -1.1134882733108]\n        ],\n        'forces_units': 'eV/\u212b',\n        'molecular_orbitals': {\n            'labels': [\n                '0 O 1s',\n                '0 O 2s',\n                '0 O 2px',\n                '0 O 2py',\n                '0 O 2pz',\n                '1 H 1s',\n                '2 H 1s'\n            ],\n            'energies': [\n                -550.86280025028,\n                -34.375426862456,\n                -16.629598134599,\n                -12.323304634736,\n                -10.637428057751,\n                16.200273277782,\n                19.796075801491\n            ],\n            'occupations': [2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0]\n        },\n        'total_energy': -2039.8853743664,\n        'total_energy_units': 'eV',\n    },\n    'timings': {\n        'total': 1.3238215579768, 'mean_field': 0.47364449803717\n    },\n}\n```\n\n### Customizing the structure\n\nThe geometry of the structure is fully defined through the `structure` input, which is provided by a `StructureData` node.\nAny other properties, e.g., the charge and what basis set to use, can be specified through the `structure` dictionary in the `parameters` input:\n```python\nfrom ase.build import molecule\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code\n\nbuilder = load_code('pyscf').get_builder()\nbuilder.structure = StructureData(ase=molecule('H2O'))\nbuilder.parameters = Dict({\n    'mean_field': {'method': 'RHF'},\n    'structure': {\n        'basis ': 'sto-3g',\n        'charge': 0,\n    }\n})\nresults, node = run.get_node(builder)\n```\nAny attribute of the [`pyscf.gto.Mole` class](https://pyscf.org/user/gto.html) which is used to define the structure can be set through the `structure` dictionary, with the exception of the `atom` and `unit` attributes, which are set automatically by the plugin based on the `StructureData` input.\n\n### Optimizing geometry\n\nThe geometry can be optimized by specifying the `optimizer` dictionary in the input `parameters`.\nThe `solver` has to be specified, and currently the solvers `geometric` and `berny` are supported.\nThe `convergence_parameters` accepts the parameters for the selected solver (see [PySCF documentation](https://pyscf.org/user/geomopt.html?highlight=geometry+optimization) for details):\n```python\nfrom ase.build import molecule\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code\n\nbuilder = load_code('pyscf').get_builder()\nbuilder.structure = StructureData(ase=molecule('H2O'))\nbuilder.parameters = Dict({\n    'mean_field': {'method': 'RHF'},\n    'optimizer': {\n        'solver': 'geometric',\n        'convergence_parameters': {\n            'convergence_energy': 1e-6,  # Eh\n            'convergence_grms': 3e-4,    # Eh/Bohr\n            'convergence_gmax': 4.5e-4,  # Eh/Bohr\n            'convergence_drms': 1.2e-3,  # Angstrom\n            'convergence_dmax': 1.8e-3,  # Angstrom\n        }\n    }\n})\nresults, node = run.get_node(builder)\n```\nThe optimized structure is returned in the form of a `StructureData` under the `structure` output label.\nThe structure and energy of each frame in the geometry optimization trajectory, are stored in the form of a `TrajectoryData` under the `trajectory` output label.\nThe total energies can be retrieved as follows:\n```python\nresults['trajectory'].get_array('energies')\n```\n\n### Writing Hamiltonian to FCIDUMP files\n\nTo instruct the calculation to dump a representation of the Hamiltonian to FCIDUMP files, add the `fcidump` dictionary to the `parameters` input:\n```python\nfrom ase.build import molecule\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code\n\nbuilder = load_code('pyscf').get_builder()\nbuilder.structure = StructureData(ase=molecule('N2'))\nbuilder.parameters = Dict({\n    'mean_field': {'method': 'RHF'},\n    'fcidump': {\n        'active_spaces': [[5, 6, 8, 9]],\n        'occupations': [[1, 1, 1, 1]]\n    }\n})\nresults, node = run.get_node(builder)\n```\nThe `active_spaces` and `occupations` keys are requires and each take a list of list of integers.\nFor each element in the list, a FCIDUMP file is generated for the corresponding active spaces and the occupations of the orbitals.\nThe shape of the `active_spaces` and `occupations` array has to be identical.\n\nThe generated FCIDUMP files are attached as `SinglefileData` output nodes in the `fcidump` namespace, where the label is determined by the index of the corresponding active space in the list:\n```python\nprint(results['fcidump']['active_space_0'].get_content())\n &FCI NORB=   4,NELEC= 4,MS2=0,\n  ORBSYM=1,1,1,1,\n  ISYM=1,\n &END\n  0.5832127121682998       1    1    1    1\n  0.5359642500498074       1    1    2    2\n -2.942091015256668e-15    1    1    3    2\n  0.5381290185905914       1    1    3    3\n -3.782672959584676e-15    1    1    4    1\n  ...\n```\n\n### Writing orbitals to CUBE files\n\nTo instruct the calculation to dump a representation of molecular orbitals to CUBE files, add the `cubegen` dictionary to the `parameters` input:\n```python\nfrom ase.build import molecule\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code\n\nbuilder = load_code('pyscf').get_builder()\nbuilder.structure = StructureData(ase=molecule('N2'))\nbuilder.parameters = Dict({\n    'mean_field': {'method': 'RHF'},\n    'cubegen': {\n        'indices': [5, 6],\n        'parameters': {\n            'nx': 40,\n            'ny': 40,\n            'nz': 40,\n        }\n    }\n})\nresults, node = run.get_node(builder)\n```\nThe `indices` key has to be specified and takes a list of integers, indicating the indices of the molecular orbitals that should be written to file.\nAdditional parameters can be provided in the `parameters` subdictionary (see the [PySCF documentation](https://pyscf.org/pyscf_api_docs/pyscf.tools.html?highlight=fcidump#module-pyscf.tools.cubegen) for details).\n\nThe generated CUBE files are attached as `SinglefileData` output nodes in the `cubegen` namespace, where the label is determined by the corresponding molecular orbital index:\n```python\nprint(results['cubegen']['mo_5'].get_content())\nOrbital value in real space (1/Bohr^3)\nPySCF Version: 2.1.1  Date: Sun Apr  2 15:59:19 2023\n    2   -3.000000   -3.000000   -4.067676\n   40    0.153846    0.000000    0.000000\n   40    0.000000    0.153846    0.000000\n   40    0.000000    0.000000    0.208599\n    7    0.000000    0.000000    0.000000    1.067676\n    7    0.000000    0.000000    0.000000   -1.067676\n -1.10860E-04 -1.56874E-04 -2.16660E-04 -2.92099E-04 -3.84499E-04 -4.94299E-04\n -6.20809E-04 -7.62048E-04 -9.14724E-04 -1.07439E-03 -1.23579E-03 -1.39331E-03\n  ...\n```\n\n### Restarting unconverged calculations\n\nThe plugin will automatically instruct PySCF to write a checkpoint file.\nIf the calculation did not converge, it will finish with exit status `410` and the checkpoint file is attached as a `SinglefileData` as the `checkpoint` output node.\nThis node can then be passed as input to a new calculation to restart from the checkpoint:\n```python\nfailed_calculation = load_node(IDENTIFIER)\nbuilder = failed_calculation.get_builder_restart()\nbuilder.checkpoint = failed_calculation.outputs.checkpoint\nsubmit(builder)\n```\nThe plugin will write the checkpoint file of the failed calculation to the working directory such that PySCF can start of from there.\n\n\n### Automatic error recovery\n\nThere are a variety of reasons why a PySCF calculation may not finish with the intended result.\nExamples are the self-consistent field cycle not converging or the job getting killed by the scheduler because it ran out of the requested walltime.\nThe `PyscfBaseWorkChain` is designed to try and automatically recover from these kinds of errors whenever it can potentially be handled.\nIt is a simple wrapper around the `PyscfCalculation` plugin that automatically restarts a new `PyscfCalculation` if the previous iterations failed.\nLaunching a `PyscfBaseWorkChain` is almost identical to launching a `PyscfCalculation` directly; the inputs just have to be \"nested\" inside the `pyscf` namespace:\n```python\nfrom aiida.engine import run\nfrom aiida.orm import Dict, StructureData, load_code, load_node\nfrom aiida_pyscf.workflows.base import PyscfBaseWorkChain\nfrom ase.build import molecule\n\nbuilder = PyscfBaseWorkChain.get_builder()\nbuilder.pyscf.code = load_code('pyscf')\nbuilder.pyscf.structure = StructureData(ase=molecule('H2O'))\nbuilder.pyscf.parameters = Dict({\n    'mean_field': {\n        'method': 'RHF',\n        'max_cycle': 3,\n    }\n})\nresults, node = run.get_node(builder)\n```\nIn this example, we purposefully set the maximum number of iterations in the self-consistent field cycle to 3 (`'mean_field.max_cycle' = 3`), which will cause the first iteration to fail to reach convergence.\nThe `PyscfBaseWorkChain` detects the error, indicated by exit status `410` on the `PyscfCalculation`, and automatically restarts the calculation from the saved checkpoint.\nAfter three iterations, the calculation converges:\n```console\n$ verdi process status IDENTIFIER\nPyscfBaseWorkChain<30126> Finished [0] [2:results]\n    \u251c\u2500\u2500 PyscfCalculation<30127> Finished [410]\n    \u251c\u2500\u2500 PyscfCalculation<30132> Finished [410]\n    \u2514\u2500\u2500 PyscfCalculation<30137> Finished [0]\n```\nThe following error modes are currently handled by the `PyscfBaseWorkChain`:\n\n* `120`: Out of walltime: The calculation will be restarted from the last checkpoint if available, otherwise the work chain is aborted\n* `140`: Node failure: The calculation will be restarted from the last checkpoint\n* `410`: Electronic convergence not achieved: The calculation will be restarted from the last checkpoint\n* `500`: Ionic convergence not achieved: The geometry optmizization did not converge, calculation will be restarted from the last checkpoint and structure\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aiida-pyscf", "org_name": "microsoft", "org_repo": "microsoft/aiida-pyscf", "platform_org_repo": "github+microsoft/aiida-pyscf", "link_to_repo": "https://github.com/microsoft/aiida-pyscf", "platform": "github", "language": "Python", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Mixed Reality Industrial Robot Arm Control\n\nProgramming industrial robot arms can be a tedious and unintuitive process with conventional control methods, such as joysticks or 2D GUIs.\n\nThis repository contains both a Unity application and ROS packages that enable users to control a Kinova Jaco 2 robot arm using the Microsoft HoloLens 2 through a Mixed Reality experience. Besides being able to intuitively set pose targets for the robot directly in task space, there is also a demonstration for planning a simple pick and place task. Furthermore, the application utilizes the spatial awareness capabilities of the HoloLens 2 to provide information on obstacles that are in the robot workspace, enabling collision-aware motion planning of robot movements.\n\n## Installation\n### Hardware\nRequired hardware:\n* Kinova Jaco 2 Robot\n* Microsoft HoloLens 2\n* External PC (Ubuntu 20.04)\n* Printed QR codes for robot localization and object detection\n\n#### Kinova Jaco 2 Robot\n1. Mount the robot securely to a flat surface\n2. Connect the power cable and the controller to the robot\n3. Power on the robot and send the arm to the home position by holding the middle button on the controller\n\n#### Microsoft HoloLens 2\nMake sure the HoloLens 2 device is in [developer mode](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/advanced-concepts/using-visual-studio?tabs=hl2#enabling-developer-mode). This is required in order to deploy the Unity application to the device.\n\n#### External PC (Ubuntu 20.04)\nAs the robot arm does not have an internal computer capable of running the ROS components, a separate machine is required. This machine can be anything capable of running Ubuntu 20.04, such as an Intel NUC or a laptop. Note that running WSL2 on Windows also works but to our knowledge only when using the Kinova Arm in the Gazebo simulation. The Kinova Ethernet/USB integration to WSL is non-trivial though.\n\n##### Ethernet Connection\nConnect the Ethernet cable to the port on the robot and to a port on the PC. So that the PC can connect to the robot driver via the Ethernet connection, the wired connection has to be configured in the following way:\n\n\n<img src=\"doc/images/WiredConnection.png\" width=40% height=40%>\n\n##### ROS\nThis project was developed using ROS Noetic. It may work with other ROS releases, but it is recommended to set up a Noetic environment on the external PC, using these [instructions](http://wiki.ros.org/noetic/Installation/Ubuntu).\n\n#### QR Codes\nPrint out the [QR codes](doc/images/qr_codes/) used for robot and object localization. Generally, the detection improves with increased QR code size.\n\nPlace the QR code for robot localization on the flat surface that the robot is fixed to. When starting the application for the first time, detect the QR code by looking directly at it and the robot model should appear next to it. Move the QR code until the holographic robot model lines up with the real robot. You can then fix the QR code to the surface with tape.\n\nThe object used for the Pick and Place demo also requires a QR code to be attached, so that the HoloLens can localize it. The object used in the demo videos is a cylinder with a diameter of 7.5cm and a height of 13cm. Attach the QR code to the top of the cylinder as centrally as possible.\n\n### Unity\n1. Clone this repository and open the MRIRAC_Unity folder from the Unity Hub (tested with Unity 2020.3.40f1 - we suggest using this exact Unity version, as version up/down-grades sometimes break Unity projects)\n2. In the Unity editor, open the ROS Settings from the Robotics menu and set the ROS IP Address to the IP of the external machine connected to the robot arm.\n3. Build and deploy the application to your HoloLens 2 device, following these [instructions](https://learn.microsoft.com/en-us/windows/mixed-reality/develop/unity/build-and-deploy-to-hololens) \n### ROS\n1. In the src/ folder of a new or existing catkin_ws, clone this repository\n2. Clone the following repositories into the same catkin_ws\n```\ncd ~/catkin_ws/src\ngit clone --branch noetic-devel git@github.com:Kinovarobotics/kinova-ros.git\ngit clone git@github.com:Unity-Technologies/ROS-TCP-Endpoint.git\n```\n3. Open `kinova-ros/kinova_bringup/launch/config/robot_parameters.yaml` in your favorite text editor, and change the `connection_type` parameter from `USB` to `Ethernet` (line 13)\n4. Make sure you have [rosdep](http://wiki.ros.org/rosdep) installed and configured and use it to install the required dependencies\n```\ncd ~/catkin_ws\nrosdep install --from-paths src --ignore-src -r -y\n```\n5. Build and source the workspace with `catkin build && source devel/setup.bash`\n6. Test the setup by running `roslaunch mrirac kinova_real.launch` or alternatively (if you don't have a Kinova arm available or setup) you can launch the Gazebo simulation using `bash MRIRAC_ROS/mrirac/launch_sim.bash`.\n \nIf the robot is connected correctly and the setup was successful, you should be able to set a pose goal using the rviz interface and the robot will move to that position once `plan and execute` is pressed.\n\n## Usage\nThis section aims to provide a brief guide on how to use the core functions of the Mixed Reality experience.\n\nA collection of demonstrations is available here: https://youtube.com/playlist?list=PLihM5VMGCK942veA_fDujf4P7h3gCeHFv\n\n### Starting the Applications\n#### External PC\nStart the ROS components:\n```\nroslaunch mrirac kinova_real.launch\n```\n#### HoloLens\nFrom the Application Menu, start the `MRIRAC` app\n\n### Hand Menu UI\n<img src=\"doc/images/HandMenu.jpg\" width=80% height=80%>\n\n* **Home Arm**: Send arm to 'Home' position\n* **Summon Target**: Summon pose target hologram\n* **Trajectory Lines**: Toggle display of trajectory lines\n* **Obstacles**: Open Collision Obstacle UI\n* **Show Mesh**: Toggle display of spatial awareness mesh\n* **Pick and Place**: Open Pick and Place UI\n\n### Sending Robot to a Target Pose\nhttps://user-images.githubusercontent.com/48822654/217340937-584c06a8-cff5-4365-ae4e-be78d66b367a.mp4\n\n### Collision Obstacle UI\n\n<img src=\"doc/images/CollisionObjectsUI.png\" width=80% height=80%>\n\n* **Sphere**: Instantiate a spherical hologram obstacle\n* **Cube**: Instantiate a cubic hologram obstacle\n* **Cylinder**: Instantiate a cylindrical hologram obstacle\n* **Clear Obstacles**: Remove all obstacles from the planning scene\n* **Toggle Spatial Obstacles**: Toggle transmission of spatial awareness mesh information\n\nhttps://user-images.githubusercontent.com/48822654/217341050-a7dd433c-f838-47ca-af97-89ff8a9c1102.mp4\n\nhttps://user-images.githubusercontent.com/48822654/217341097-46c50b44-8e57-4228-8b9d-82c9a70898a5.mp4\n\n### Pick and Place UI\n\n<img src=\"doc/images/PickAndPlaceUI.png\" width=80% height=80%>\n\n* **Plan Pick and Place**: Plan pick and place mission\n* **Execute Pick and Place**: Execute planned pick and place mission\n* **Reset**: Reset detection of pick target (removes added obstacle)\n\nhttps://user-images.githubusercontent.com/48822654/217341190-aaafd8b8-9d07-454b-926e-0d94b50aba9a.mp4\n\n## Credits\nThis project was developed as part of the Semester Thesis for my (Matthew Hanlon) MSc. Robotics, Systems and Control at ETH Zurich. The project was supervised by Eric Vollenweider (Microsoft Mixed Reality and AI Lab Zurich), in collaboration with the [Computer Vision and Geometry Group](https://cvg.ethz.ch/).\n\n# Trademark Notice\nTrademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n", "repo_name": "mixed-reality-robot-arm-control-demo", "org_name": "microsoft", "org_repo": "microsoft/mixed-reality-robot-arm-control-demo", "platform_org_repo": "github+microsoft/mixed-reality-robot-arm-control-demo", "link_to_repo": "https://github.com/microsoft/mixed-reality-robot-arm-control-demo", "platform": "github", "language": "C#", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# AQAfer\nA repo to make it easy to run AQAvit verification against different OpenJDK binaries.  \n\n\"AQAfer you, AQAfer me, AQAfer everyone!\"\n", "repo_name": "aqafer", "org_name": "microsoft", "org_repo": "microsoft/aqafer", "platform_org_repo": "github+microsoft/aqafer", "link_to_repo": "https://github.com/microsoft/aqafer", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# PromptCraft-Robotics\n\nThe PromptCraft-Robotics repository serves as a community for people to test and share interesting prompting examples for large language models (LLMs) within the robotics domain. We also provide a sample [robotics simulator](https://github.com/microsoft/PromptCraft-Robotics/tree/main/chatgpt_airsim) (built on Microsoft AirSim) with ChatGPT integration for users to get started.\n\nWe currently focus on OpenAI's [ChatGPT](https://openai.com/blog/chatgpt/), but we also welcome examples from other LLMs (for example open-sourced models or others with API access such as [GPT-3](https://openai.com/api/) and Codex).\n\nUsers can contribute to this repository by submitting interesting prompt examples to the [Discussions](https://github.com/microsoft/PromptCraft-Robotics/discussions) section of this repository. A prompt can be submitted within different robotics categories such as [Manipulation](https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-manipulation), [Home Robotics](https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-home-robots), [Physical Reasoning](https://github.com/microsoft/PromptCraft-Robotics/discussions/categories/llm-physical-reasoning), among many others.\nOnce submitted, the prompt will be reviewed by the community (upvote your favorites!) and added to the repository by a team of admins if it is deemed interesting and useful.\nWe encourage users to submit prompts that are interesting, fun, or useful. We also encourage users to submit prompts that are not necessarily \"correct\" or \"optimal\" but are interesting nonetheless.\n\nWe encourage prompt submissions formatted as markdown, so that they can be easily transferred to the main repository. Please specify which LLM you used, and if possible provide other visuals of the model in action such as videos and pictures.\n\n## Paper, videos and citations\n\nBlog post: <a href=\"https://aka.ms/ChatGPT-Robotics\" target=\"_blank\">aka.ms/ChatGPT-Robotics</a>\n\nPaper: <a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf\" target=\"_blank\">ChatGPT for Robotics: Design Principles and Model Abilities\n\nVideo: <a href=\"https://youtu.be/NYd0QcZcS6Q\" target=\"_blank\">https://youtu.be/NYd0QcZcS6Q</a>\n\nIf you use this repository in your research, please cite the following paper:\n\n```\n@techreport{vemprala2023chatgpt,\nauthor = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},\ntitle = {ChatGPT for Robotics: Design Principles and Model Abilities},\ninstitution = {Microsoft},\nyear = {2023},\nmonth = {February},\nurl = {https://www.microsoft.com/en-us/research/publication/chatgpt-for-robotics-design-principles-and-model-abilities/},\nnumber = {MSR-TR-2023-8},\n}\n```\n\n## ChatGPT Prompting Guides & Examples\n\nThe list below contains links to the different robotics categories and their corresponding prompt examples. We welcome contributions to this repository to add more robotics categories and examples. Please submit prompt examples to the [Discussions](https://github.com/microsoft/PromptCraft-Robotics/discussions) page, or submit a pull request with your category and examples.\n\n* Embodied agent \n  * [ChatGPT - Habitat, closed loop object navigation 1](examples/embodied_agents/visual_language_navigation_1.md)\n  * [ChatGPT - Habitat, closed loop object navigation 2](examples/embodied_agents/visual_language_navigation_2.md)\n  * [ChatGPT - AirSim, object navigation using RGBD](examples/embodied_agents/airsim_objectnavigation.md)\n* Aerial robotics\n  * [ChatGPT - Real robot: Tello deployment](examples/aerial_robotics/tello_example.md) | [Video Link](https://youtu.be/i5wZJFb4dyA)\n  * [ChatGPT - AirSim turbine Inspection](examples/aerial_robotics/airsim_turbine_inspection.md) | [Video Link](https://youtu.be/38lA3U2J43w)\n  * [ChatGPT - AirSim solar panel Inspection](examples/aerial_robotics/airsim_solarpanel_inspection.md)\n  * [ChatGPT - AirSim obstacle avoidance](examples/aerial_robotics/airsim_obstacleavoidance.md) | [Video Link](https://youtu.be/Vn6NapLlHPE)\n* Manipulation\n  * [ChatGPT - Real robot: Picking, stacking, and building the MSFT logo](examples/manipulation/pick_stack_msft_logo.md) | [Video Link](https://youtu.be/wLOChUtdqoA)\n  * [ChatGPT - Manipulation tasks](examples/manipulation/manipulation_zeroshot.md)\n* Spatial-temporal reasoning\n  * [ChatGPT - Visual servoing with basketball](examples/spatial_temporal_reasoning/visual_servoing_basketball.md)\n\n\n## ChatGPT + Robotics Simulator\n\nWe provice a sample [AirSim](https://github.com/microsoft/AirSim) environment for users to test their ChatGPT prompts. The environment is a binary containing a sample inspection environment with assets such as wind turbines, electric towers, solar panels etc. The environment comes with a drone and interfaces with ChatGPT such that users can easily send commands in natural language. [[Simulator Link]](chatgpt_airsim/README.md)\n\nWe welcome contributions to this repository to add more robotics simulators and environments. Please submit a pull request with your simulator and environment.\n\n## Related resources\n\nBeyond the prompt examples here, we leave useful and related links to the use of large language models below:\n\n* [Read about the OpenAI APIs](https://openai.com/api/)\n* [Azure OpenAI service](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service)\n* [OPT language model](https://huggingface.co/docs/transformers/model_doc/opt)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PromptCraft-Robotics", "org_name": "microsoft", "org_repo": "microsoft/PromptCraft-Robotics", "platform_org_repo": "github+microsoft/PromptCraft-Robotics", "link_to_repo": "https://github.com/microsoft/PromptCraft-Robotics", "platform": "github", "language": "Python", "stargazers_count": 1163, "watchers_count": 1163}, {"README_text": "<p align=\"center\">\n  <img width=\"600\" height=\"150\" src=\"assets/MT-GPT.png\">\n</p>\n<hr />\n\n## How Good Are GPT Models at Machine Translation? \n## A Comprehensive Evaluation\n\nPaper: https://arxiv.org/abs/2302.09210\n\n## Introduction\nIn this work, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation, all accompanied with an extensive analysis of the differential aspects of translations produced by GPT. We experiment with 18 different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality.\n\n## Quick Installation\n```bash\n$ git clone https://github.com/microsoft/gpt-MT.git\n$ cd tools\n$ conda create -n gpt-mt-eval python=3.10\n$ conda activate gpt-mt-eval\n$ pip install --upgrade pip\n$ pip install -r requirements.txt\n$ git clone https://github.com/Unbabel/COMET.git\n$ cd COMET\n$ git checkout fc2f2b3 \n$ poetry install\n```\n\n## Data Shots and System Outputs\nWe have released all selected shots in our experiments including the sentence-level shots (RR, QR and QS) and the document-level shots (DR and DF). These shots have been organized under [data-shots](./data-shots/).\n\nMoreover, To make reproducing all results an easy task, all system outputs have been released under [system-output](./evaluation/system-outputs/) in addition the WMT official test sets along with document-separated and domain-separated files.\n\n## Prompt Template Examples\n- zero-shot\n>Translate this into 1. [target language]:\n>\n>[input]\n>\n>1.\n\n- 1-shot\n>Translate this into 1. [target language]:\n> \n>[shot 1 source input]\n> \n>1. [shot 1 reference]\n> \n>Translate this into 1. [target language]:\n>\n>[input]\n>\n>1.\n\n- 5-shot\n>Translate this into 1. [target language]:\n> \n>[shot 1 source input]\n> \n>1. [shot 1 reference]\n> \n>Translate this into 1. [target language]:\n> \n>[shot 2 source input]\n> \n>1. [shot 2 reference]\n> \n>....\n> \n>Translate this into 1. [target language]:\n> \n>[shot 5 source input]\n> \n>1. [shot 5 reference]\n> \n>Translate this into 1. [target language]:\n>\n>[input]\n>\n>1.\n\n\n## Reproducing Results\nTo reproduce the reported results in the paper, you need to run the evaluation script [evaluate.py](./tools/evaluate.py).\n### CLI Usage:\n```bash\n$ python evaluate.py -h\nusage: evaluate.py [-h] --testset TESTSET [--docids DOCIDS] --hypotheses HYPOTHESES [HYPOTHESES ...] --directions DIRECTIONS [DIRECTIONS ...]\n                   [--comet-models COMET_MODELS [COMET_MODELS ...]] [--gpus GPUS] --metrics METRICS [METRICS ...] [--save-name SAVE_NAME]\n                   [--sliding-window SLIDING_WINDOW] [--context-length CONTEXT_LENGTH]\n\noptions:\n  -h, --help            show this help message and exit\n  --testset TESTSET     A path to the test set directory containing references and sources for each language pair. Must contain\n                        {src_lang}{tgt_lang}/test.{src_lang}-{tgt_lang}.{tgt_lang} and {src_lang}{tgt_lang}/test.{src_lang}-{tgt_lang}.{src_lang}\n  --docids DOCIDS       A path to the directory containing doc-ids corresponding to testset for each language pair. Must contain\n                        {src_lang}{tgt_lang}/test.{src_lang}-{tgt_lang}.docids\n  --hypotheses HYPOTHESES [HYPOTHESES ...]\n                        A path to the model output files. must contain {src_lang}{tgt_lang}/test.{src_lang}-{tgt_lang}.{tgt_lang}\n  --directions DIRECTIONS [DIRECTIONS ...]\n                        Language directions to evaluate on e.g. \"en-de de-en\"\n  --comet-models COMET_MODELS [COMET_MODELS ...]\n                        A list of COMET models to use for evaluation\n  --gpus GPUS           Number of GPUs to use with COMET\n  --metrics METRICS [METRICS ...]\n                        A list of metrics to use for evaluation, options [\"bleu\", \"comet\", \"doc-comet\", \"chrf\", \"doc-bleu\", \"doc-chrf\"]\n  --save-name SAVE_NAME\n                        name of the output files/folders\n  --sliding-window SLIDING_WINDOW\n                        The stride step over document\n  --context-length CONTEXT_LENGTH\n                        The number of sentences in a single context\n```\nFor example:\n- To reproduce `GPT 5-Shot QR` results in `Table3`:\n```bash\n$ cd ./tools\n$ python evaluate.py \\\n    --testset ../evaluation/testset/wmt-testset \\\n    --directions de-en en-de cs-en en-cs ja-en en-ja zh-en en-zh ru-en en-ru uk-en en-uk is-en en-is ha-en en-ha fr-de de-fr \\\n    --metrics comet chrf bleu \\\n    --comet-models wmt22-comet-da wmt22-cometkiwi-da \\\n    --hypotheses ../evaluation/system-outputs/text-davinci-003/QR/5-shot\n``` \n- To reproduce `GPT Doc ZS w=16` results in `Table5`: \n```bash\n$ cd ./tools\n$ python evaluate.py \\\n    --testset ../evaluation/testset/wmt-testset \\\n    --docids ../evaluation/testset/wmt-testset-docids \\\n    --directions de-en en-de \\\n    --metrics comet doc-comet chrf bleu doc-bleu \\\n    --comet-models wmt22-comet-da wmt22-cometkiwi-da \\\n    --hypotheses ../evaluation/system-outputs/text-davinci-003-doc-level/Doc-W16/zeroshot\n``` \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Citation\n\nIf this work is helpful for your research, please consider citing the following BibTeX entry.\n\n```\n@article{gpt-mt-2023,\n      title={How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation}, \n      author={Amr Hendy and Mohamed Abdelrehim and Amr Sharaf and Vikas Raunak and Mohamed Gabr and Hitokazu Matsushita and Young Jin Kim and Mohamed Afify and Hany Hassan Awadalla},\n      journal={arXiv preprint arXiv:2302.09210},\n      year={2023}\n}\n```    \n\n", "repo_name": "gpt-MT", "org_name": "microsoft", "org_repo": "microsoft/gpt-MT", "platform_org_repo": "github+microsoft/gpt-MT", "link_to_repo": "https://github.com/microsoft/gpt-MT", "platform": "github", "language": "Ruby", "stargazers_count": 60, "watchers_count": 60}, {"README_text": "# Cookiecutter actix simple clean architecture\nThis is a reusable Rust Cookiecutter template. The project is based on Actix web in combination with Diesel ORM.\n\nComplete list of features the template provides:\n* Onion architecture\n* Actix Web \n* Maintenance window support\n* Diesel ORM\n* Database migrations\n* Local postgres database docker support\n* Test containers integration for testing\n\n## Getting started\nTo start a new project, run the following command:\n```bash\ncookiecutter -c v1 https://github.com/microsoft/cookiecutter-rust-actix-clean-architecture\n```\nThis will prompt you for some information about your project. The information\nyou provide will be used to populate the files in the new project directory.\n\nYou can then build the project locally.\n```bash\ncargo build\n```\n\n## Architecture \nThe application follows the Onion Architecture pattern. An article is written \nabout our experience integrating an onion architecture with actix web in combination with diesel ORM that can \nbe found [here](./docs/onion-architecture-article.md).\n\nThis architecture is a design pattern that organizes the codebase of a software application into multiple layers, where the innermost layer \nis the domain layer and the outermost layer is the application layer. Each layer depends only on the layers inside of it and not on the layers outside of it, \ncreating a separation of concerns, allowing for a more maintainable and scalable codebase.\n\nFor this template we suggest using a service-repository design pattern. For example implementations you can have a look at \n\n\n## Running the application locally\nTo run the application locally, you need to have a Postgres database running.\nYou can use the `run_postgres.sh` script in the `scripts` directory to run a Postgres container.\n```bash\n./scripts/run_postgres.sh\n```\n\nYou can then run the application.\n```bash\ncargo run\n```\n\n## Testing support\nAll tests are can be found under the `src/tests` folder. When using the template\nyou can place all you tests in this folder.\n\nTo run the tests, you can use the following command:\n```bash\ncargo test\n```\nTo run the tests with error output you can run the following command:\n```bash\ncargo test -- --nocapture\n```\nor \n```bash\ncargo test -- --show-output\n```\n\n## Diesel ORM\nThe template uses Diesel ORM for its database connection and database models\nintegration. Its is currently setup with postgres, however you can \nchange it to any other database that is supported by diesel. For other databases \nhave a look at the official Diesel documentation that can be found [here](https://diesel.rs/)\n\n### Database migrations\n\n1) Make sure you have the diesel cli installed. You can install it with the following command:\n    ```bash\n    cargo install diesel_cli --no-default-features --features postgres\n    ```\n2) Add your postgres database url to the .env file:\n    ```bash\n    echo DATABASE_URL=postgres://username:password@localhost/diesel_demo > .env\n    ```\n3) Setup diesel before creating a migration:\n    ```bash\n    diesel setup\n    ```\n4) Create a migration with the following command:\n    ```bash\n    diesel migration generate <migration_name>\n    ```\n5) Apply your migrations:\n    ```bash\n    diesel migration run\n    ```\n\n## Service repository design pattern\n\n### Diesel Repositories\nThe onion architecture is best being used with a repository-service pattern. An example \nrepository can be seen below:\n\n```rust\n// Can be placed under /src/domain/repositories/todo.rs\n#[derive(Debug, Serialize, Deserialize)]\npub struct TodoQueryParams {\n    pub limit: Option<i64>,\n    pub offset: Option<i64>,\n    pub title: Option<String>,\n}\n\nimpl QueryParams for TodoQueryParams {\n    fn limit(&self) -> i64 {\n        self.limit.or(DEFAULT_LIMIT).unwrap_or_default()\n    }\n    fn offset(&self) -> i64 {\n        self.offset.or(DEFAULT_OFFSET).unwrap_or_default()\n    }\n}\n\n#[async_trait]\npub trait TodoRepository: Send + Sync {\n    async fn create(&self, new_todo: &CreateTodo) -> RepositoryResult<Todo>;\n    async fn list(&self, params: TodoQueryParams) -> RepositoryResult<ResultPaging<Todo>>;\n    async fn get(&self, todo_id: i32) -> RepositoryResult<Todo>;\n    async fn delete(&self, todo_id: i32) -> RepositoryResult<()>;\n}\n```\n\n```rust\n// Can be placed under /src/infrastructure/repositories/todo.rs\npub struct TodoDieselRepository {\n    pub pool: Arc<DBConn>\n}\n\nimpl TodoDieselRepository {\n    pub fn new(db: Arc<DBConn>) -> Self {\n        TodoDieselRepository { pool: db }\n    }\n}\n\n#[async_trait]\nimpl TodoRepository for TodoDieselRepository {\n\n    async fn create(&self, new_todo: &CreateTodo) -> RepositoryResult<Todo> {\n        use crate::infrastructure::schema::todos::dsl::todos;\n        let new_todo_diesel: CreateTodoDiesel = CreateTodoDiesel::from(new_todo.clone());\n        let mut conn = self.pool.get().unwrap();\n        let result: TodoDiesel = run(move || diesel::insert_into(todos).values(new_todo_diesel)\n            .get_result(&mut conn))\n            .await\n            .map_err(|v| DieselRepositoryError::from(v).into_inner())?;\n        Ok(result.into())\n    }\n\n    async fn list(&self, params: TodoQueryParams) -> RepositoryResult<ResultPaging<Todo>> {\n        use crate::infrastructure::schema::todos::dsl::todos;\n        let pool = self.pool.clone();\n        let builder = todos.limit(params.limit()).offset(params.offset());\n        let result = run(move || {\n            let mut conn = pool.get().unwrap();\n            builder.load::<TodoDiesel>(&mut conn)\n        })\n            .await\n            .map_err(|v| DieselRepositoryError::from(v).into_inner())?;\n        Ok(ResultPaging {\n            total: 0,\n            items: result.into_iter().map(|v| v.into()).collect()\n        })\n    }\n\n    async fn get(&self, todo_id: i32) -> RepositoryResult<Todo> {\n        use crate::infrastructure::schema::todos::dsl::{id, todos};\n        let mut conn = self.pool.get().unwrap();\n        run(move || todos.filter(id.eq(todo_id)).first::<TodoDiesel>(&mut conn))\n            .await\n            .map_err(|v| DieselRepositoryError::from(v).into_inner())\n            .map(|v| -> Todo { v.into() })\n    }\n\n    async fn delete(&self, todo_id: i32) -> RepositoryResult<()> {\n        use crate::infrastructure::schema::todos::dsl::{id, todos};\n        let mut conn = self.pool.get().unwrap();\n        run(move || diesel::delete(todos).filter(id.eq(todo_id))\n            .execute(&mut conn))\n            .await\n            .map_err(|v| DieselRepositoryError::from(v).into_inner())?;\n        Ok(())\n    }\n}\n```\n\n### Services\nThe onion architecture is best being used with a repository-service pattern. An example \nservice can be seen below:\n```rust\n// Can be placed under /src/services/todo.rs\n#[derive(Clone)]\npub struct TodoServiceImpl {\n    pub repository: Arc<dyn TodoRepository>,\n}\n\nimpl TodoServiceImpl {\n    pub fn new(repository: Arc<dyn TodoRepository>) -> Self {\n        TodoServiceImpl {\n            repository,\n        }\n    }\n}\n\n#[async_trait]\nimpl TodoService for TodoServiceImpl {\n    async fn create(&self, todo: CreateTodo) -> Result<Todo, CommonError> {\n        let mut cloned = todo.clone();\n        self.repository\n            .create(&mut cloned)\n            .await\n            .map_err(|e| -> CommonError { e.into() })\n    }\n\n    async fn list(&self, params: TodoQueryParams) -> Result<ResultPaging<Todo>, CommonError> {\n        self.repository\n            .list(params)\n            .await\n            .map_err(|e| -> CommonError { e.into() })\n    }\n\n    async fn get(&self, todo_id: i32) -> Result<Todo, CommonError> {\n        self.repository\n            .get(todo_id)\n            .await\n            .map_err(|e| -> CommonError { e.into() })\n    }\n\n    async fn delete(&self, todo_id: i32) -> Result<(), CommonError> {\n        self.repository\n            .delete(todo_id)\n            .await\n            .map_err(|e| -> CommonError { e.into() })\n    }\n}\n```\n", "repo_name": "cookiecutter-rust-actix-clean-architecture", "org_name": "microsoft", "org_repo": "microsoft/cookiecutter-rust-actix-clean-architecture", "platform_org_repo": "github+microsoft/cookiecutter-rust-actix-clean-architecture", "link_to_repo": "https://github.com/microsoft/cookiecutter-rust-actix-clean-architecture", "platform": "github", "language": "Rust", "stargazers_count": 33, "watchers_count": 33}, {"README_text": "# Cookiecutter flask clean architecture\nThis is a reusable Python Flask template. The project is based on Flask in combination with SQLAlchemy ORM.\n\nComplete list of features the template provides:\n* [Onion architecture](#onion-architecture)\n* [Maintenance window support](#maintenance-window-support)\n* [SQLAlchemy ORM](#sqlalchemy-orm)\n* [Alembic Database migrations](#alembic-database-migrations)\n* [Local postgres database docker support](#local-postgres-database-docker-support)\n* [Tests and test containers integration](#tests-and-test-containers-integration)\n* [Service prefix](#service-prefix)\n* [Dependency injection](#dependency-injection)\n* [Service-repository design pattern](#service-repository-design-pattern)\n\n## Getting started\nTo start a new project, run the following command:\n```bash\ncookiecutter -c v1 https://github.com/microsoft/cookiecutter-python-flask-clean-architecture\n```\nThis will prompt you for some information about your project. The information\nyou provide will be used to populate the files in the new project directory.\n\n### Running the application locally\nTo run the application locally, you need to have a Postgres database running.\nYou can use the `run_postgres.sh` script in the `scripts` directory to run a Postgres container.\n```bash\n./scripts/run_postgres.sh\n```\nYou can then run the application with flask:\n```bash\nflask --app src/app run \n```\nor with gunicorn:\n```bash\ngunicorn wsgi:app -b  0.0.0.0:7000 --workers=1 --preload\n```\n\n## Onion Architecture \nThe application follows the Onion Architecture pattern. An article is written \nabout our experience integrating an onion architecture with flask in combination with \nSQL Alchemy ORM that can be found [here](./docs/onion-architecture-article.md).\n\nThis architecture is a design pattern that organizes the codebase of a software application into multiple layers, where the innermost layer \nis the domain layer and the outermost layer is the application layer. Each layer depends only on the layers inside of it and not on the layers outside of it, \ncreating a separation of concerns, allowing for a more maintainable and scalable codebase.\n\nFor this template we suggest using a service-repository design pattern. This template also provides \na set of abc meta classes that you can use to create your repositories and services.\nFor example implementations you can have a look at [Service-repository design pattern](#service-repository-design-pattern).\n\n## Maintenance window support\nThis template provides you with a maintenance window mode. To learn more about \nmaintenance windows in your service you can read this article [here](https://devblogs.microsoft.com/cse/2023/02/08/maintenance_window_db_migrations/)\n\nDuring maintenance mode, clients will receive an http 503 status code.\n\n### Activating maintenance mode\nYou can activate maintenance mode in the following ways:\n```bash\ncurl -X PATCH http://localhost:7000/<service-prefix>/v1/service-context -d '{\"maintenance\": true}' -H 'Content-Type: application/json'\n```\nor via the command line:\n```bash\nflask activate_maintenance_mode\n```\n\n### Deactivating maintenance mode\nYou can deactivate maintenance mode in the following ways:\n```bash\ncurl -X PATCH http://localhost:7000/<service-prefix>/v1/service-context -d '{\"maintenance\": false}' -H 'Content-Type: application/json'\n```\nor via the command line:\n```bash\nflask deactivate_maintenance_mode\n```\n\n## SQLAlchemy ORM\nThe template uses SQLAlchemy ORM for its database connection and database models\nintegration. Its is currently setup with postgres, however you can \nchange it to any other database that is supported by SQLAlchemy. For other databases \nhave a look at the official Flask SQLAlchemy documentation \nthat can be found [here](https://flask-sqlalchemy.palletsprojects.com/en/3.0.x/)\n\nThis template provides you with a model base class that you can use to create your models.\n\n```python\nfrom src.infrastructure.models.model_extension import ModelExtension\n\nclass User(ModelExtension):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    attribute_a = Column(String(50), nullable=False)\n    attribute_b = Column(String(50), nullable=False)\n\n    def __repr__(self):\n        return self.repr(id=self.id, attribute_a=self.attribute_a, attribute_b=self.attribute_b)\n```\n\n## Alembic database migrations\n> Note: The application uses a postgres database. Make sure you have a postgres\n> database running before running the following commands. For local development,\n> you can use the run_postgres.sh script to run a postgres container locally.\n\n1) Make sure you have the diesel cli installed. You can install it with the following command:\n    ```bash\n    sh ./scripts/run_postgres.sh\n    ```\n2) Create a database migration\n    ```bash\n    flask db migrate -m <migration-message>\n    ```\n3) Apply the database migration:\n    ```bash\n    flask db upgrade\n    ```\n   \n## Local postgres database docker support \nYou can run a local postgres docker database by using the following script:\n```bash\n sh ./scripts/run_postgres.sh\n ```\n\nThis will run a postgres docker container on port 5432. Also it will create a\n.env file in the root directory of the project. This file contains the database\nconnection string. The service will read this connection string from the .env file \nand use it to connect to the database.\n\n## Tests and test containers integration\nAll tests are can be found under the `tests` folder. When using the template\nyou can place all you tests in this folder.\n\nThe service uses [python unittest](https://docs.python.org/3/library/unittest.html) in combination\nwith [flask testing]\n\nTo run the tests, you can use the following command:\n```bash\npython -m unittest discover -s tests\n```\n\nYou can use the test containers library to run your tests against a postgres database.\nYou do this by ```setup_database()``` in your test class. This will create a postgres container\nand run your tests against it. After the tests are done, the container will be destroyed.\n\nIf you want to run your tests against a different database, you can change the \nsetyp_database method in the test class to use a different database container.\n\n```python\nclass Test(AppTestBase):\n\n    def setUp(self) -> None:\n        super(Test, self).setUp()\n        self.setup_database()\n```\n\n## Service prefix\nThe application can use a service prefix for the endpoints.\nThe service prefix is defined in the config.py file. Given a service prefix \nof 'example', endpoints will be prefixed with '/example/v1/service-context'.\n\n```python\n# config.py\nSERVICE_PREFIX = os.environ.get(SERVICE_PREFIX, '')\n\n# middleware\nclass PrefixMiddleware(object):\n    ROUTE_NOT_FOUND_MESSAGE = \"This url does not belong to the app.\"\n\n    def __init__(self, app, prefix=''):\n        self.app = app\n        self.wsgi_app = app.wsgi_app\n        self.prefix = prefix\n\n    def __call__(self, environ, start_response):\n\n        if environ['PATH_INFO'].startswith(self.prefix):\n            environ['PATH_INFO'] = environ['PATH_INFO'][len(self.prefix):]\n            environ['SCRIPT_NAME'] = self.prefix\n            return self.wsgi_app(environ, start_response)\n        else:\n            start_response('404', [('Content-Type', 'text/plain')])\n            return [self.ROUTE_NOT_FOUND_MESSAGE.encode()]\n```\n\nDuring testing the service prefix is not applied. This allows you\nto test the endpoints without having to add the service prefix to the\nendpoint.\n\nIf the service prefix is not set, the service will not use a service prefix.\n\n\n## Dependency injection\nThis template uses the [dependency_injector](https://pypi.org/project/dependency-injector/) library\nfor dependency injection. The template provides you with a container class that you can use to\nregister your dependencies. The container class is located in the `src/dependency_container.py` file.\n\nYou can add your dependencies to the container and use them \nin your routers, services and repositories.\n\n## Service repository design pattern\nThis template provides you with a repository-service pattern. There are two base classes\nthat you can use to create your repositories and services. These base classes are located\nin the `src/services/repository_service.py` and `src/infrastructure/repositories/repository.py`.\n\n### Repository example\nA repository example can be seen below, this repository is used to query the MyModel model.\nFor custom query params support override the `_apply_query_params` method.\n\n```python\nfrom infrastructure.repositories import Repository\nfrom infrastructure.models import MyModel\n\nclass MyExampleRepository(Repository):\n    base_class = MyModel\n    DEFAULT_NOT_FOUND_MESSAGE = \"MyModel was not found\"\n\n    def _apply_query_params(self, query, query_params):\n        name_query_param = self.get_query_param(\"name\", query_params)\n        \n        if name_query_param:\n           query = query.filter_by(name=name_query_param)\n            \n        return query\n```\n\n### Service example\nA Service example can be seen below, this service expect a repository to be injected in its contuctor.\n\n```python\nfrom services.repository_service import RepositoryService\n\nclass MyExampleService(RepositoryService):\n    # The RepositoryService gives you access to crud repository operations by the inheritance \n    # RepositoryService.\n    pass\n\n# Can be instantiate by injecting the repository\nmy_example_service = MyExampleService(my_example_repository)\n```\n", "repo_name": "cookiecutter-python-flask-clean-architecture", "org_name": "microsoft", "org_repo": "microsoft/cookiecutter-python-flask-clean-architecture", "platform_org_repo": "github+microsoft/cookiecutter-python-flask-clean-architecture", "link_to_repo": "https://github.com/microsoft/cookiecutter-python-flask-clean-architecture", "platform": "github", "language": "Python", "stargazers_count": 36, "watchers_count": 36}, {"README_text": "# Review Hub\n\nReview Hub leverages the power of Power Apps, a low-code platform, to provide an intuitive interface that makes it easy for users to create and share dynamic dashboards, reports, and presentations. The solution eliminates the need for manual data entry and saves time by automating the generation of review content. With Review Hub, users can access a centralized repository of information that provides a comprehensive view of their business performance, allowing them to make data-driven decisions quickly and accurately. \n\nThe solution is designed to support businesses of all sizes, from small startups to large enterprises. It integrates seamlessly with other Microsoft tools, such as Excel, Power BI, Dataverse and SharePoint, allowing users to access and visualize data from a wide range of sources. The platform\u2019s interface makes it easy to customize and personalize dashboards and reports to meet the specific needs of each business. \n\nIn addition to providing an efficient way to conduct business reviews, Review Hub also helps organizations to streamline their review process and increase collaboration among team members. The solution\u2019s collaborative features, such as real-time commenting, shared workspaces, and version control, make it easy for teams to share feedback and work together to achieve their goals. \n\nOverall, Review Hub is a powerful solution that empowers organizations to drive their business forward by providing them with the insights and information they need to make informed decisions. By optimizing performance and streamlining processes, Review Hub enables organizations to focus on what matters most \u2013 growing their business.\n\n# Getting started.\n\nTo utilize review hub you will need a power apps premium plan as some of the connections utilized are premium connectors. And a power apps environment setup where you can import the solution.\n\n## Prerequisites\n\n- Power Apps Environment.\n- Power Apps Premium Plan.\n- A sharepoint site.\n- Azure subscription.\n- Azure Devops.\n\nThe following connections are utilized within the solution. A Data Loss Policy(DLP) which allows the use of all these connections should be assigned. To learn more about DLPs please visit [Data Loss Prevention Policies](https://learn.microsoft.com/en-us/power-platform/admin/wp-data-loss-prevention)\n\n- Azure Devops\n- Power BI\n- Dataverse\n- Azure Blob Storage\n- Azure AD\n- Approvals\n- Office 365 Users\n- Sharepoint\n- Excel Online\n- Outlook\n\n## Installation\n\nTo install the solution, you can download the zipped powerplatform solution from this repo's release. Then import the downloaded zipped file as a solution to your Power Apps Platform environment. Refer to the [installation guide](INSTALLATION.md) for details.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nFor details on how to contribute to the repo see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "review-hub", "org_name": "microsoft", "org_repo": "microsoft/review-hub", "platform_org_repo": "github+microsoft/review-hub", "link_to_repo": "https://github.com/microsoft/review-hub", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Excel Labs, a Microsoft Garage project\n\nExcel Labs is an Office Add-in for Excel that allows the Excel team to release experimental Excel features and gather customer feedback about them. Although some of these features may never be incorporated into Excel, this experimentation and feedback is vital.\n\nExcel Labs currently includes two features:\n\n1. Advanced formula environment: An interface designed for authoring, editing, and reusing formulas.\n1. `LABS.GENERATIVEAI`: A custom function that enables you to send prompts to a generative AI model and return the responses directly to the grid. This is not part of Microsoft 365 Copilot.\n\nThe [Microsoft Garage](https://garage.microsoft.com) is an outlet for experimental projects for you to try.\n\n## Advanced formula environment\n\nAdvanced formula environment makes it easy to create, edit, and reuse formulas and named LAMBDA functions.\n\nWhile Excel Name Manager lets you name and reuse any formula, including functions defined with LAMBDA, the interface makes it difficult to author these formulas. Common features that make programming easier are missing, such as immediate inline errors and syntax highlighting. Advanced formula environment fills this gap. It\u2019s an interface for the Excel Name Manager that is designed for formula authoring. Using the advanced formula environment, you can:\n\n- Write formulas using an editor that supports inline errors, IntelliSense, comments, and more.\n- Indent formulas, making them easier to read.\n- Edit modules of named formulas using a single code editor.\n- Quickly reuse LAMBDA formulas by importing them from GitHub gists, or by copying them for other workbooks.\n\nSee [Advanced formula environment](/advanced-formula-environment/README.md) for more information.\n\n## `LABS.GENERATIVEAI` function\n\nWith the announcement of [Microsoft 365 Copilot](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/), we see great potential in the power of generative AI. `LABS.GENERATIVEAI` is a custom function that enables you to send prompts to a generative AI model and return the responses to the Excel grid.\n\nUsing this function, you can test the benefits of generative AI directly in Excel. `LABS.GENERATIVEAI` is not part of Microsoft 365 Copilot but rather a place for you to play and experiment with generative AI today within the Garage framework.\n\n`LABS.GENERATIVEAI` allows you to send simple or complex prompts to a generative AI model, such as requests to:\n\n- Analyze public information.\n- Process data and define the format.\n- Answer factual or creative questions.\n- Produce a response based on a sample.\n\n`LABS.GENERATIVEAI` allows you to reference other cells in your workbook, and it can be called inside any Excel cell or named formula in the workbook.\n\nSee [LABS.GENERATIVEAI](/labs-generative-ai/README.md) for more information.\n\n## Platform support\n\nExcel Labs works in Excel for Desktop, Mac, and on the web, without installing any additional software. To get started, install the add-in from the Office Store.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Excel-Labs", "org_name": "microsoft", "org_repo": "microsoft/Excel-Labs", "platform_org_repo": "github+microsoft/Excel-Labs", "link_to_repo": "https://github.com/microsoft/Excel-Labs", "platform": "github", "language": "TypeScript", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "Process-based sandbox for Verona\n================================\n\nThis repository is attempting to build the base-line sandboxing mechanism for foreign code in Verona.\nIt has been separated into a separate repository because being able to sandbox libraries may be generally useful beyond the scope of Verona.\n\nAs a design principle, Verona does not permit unsafe code to run outside of a sandbox (with the exception of the small run-time library, which can be carefully audited and possibly replaced with formally verified code in the future).\nThe sandboxing abstractions in the language can be implemented in a variety of different ways, such as:\n\n - Process-based isolation.\n - MMU-based isolation with a non-process abstraction.\n - Software fault isolation (e.g. via a WebAssembly implementation).\n - Novel hardware features (e.g. CHERI)\n\nProcess-based isolation is attractive as a baseline because it does not require a modified OS and depends only on CPU features that have been on all mainstream CPUs for decades.\n\nTerminology\n-----------\n\nThe process-sandbox code uses the following terminology:\n\n - *Parent*: The trusted process that wishes to load one or more untrusted libraries.\n - *Child*: The process that runs the unprivileged library.\n - *Sandbox*: The child process and associated state, including the shared heap.\n - *Sandbox heap*: A region of memory shared between the *parent* and *child* from which both can dynamically allocate memory.\n - *Sandboxed library*: The interface that manages all of the state associated with a sandbox and exposes its functionality to the *parent*.\n - *Sandboxed function*: A function exposed from a *sandboxed library*.\n - *Callback function*: The converse of a *sandboxed function*, a function implemented in the parent and exposed for the child to call.\n - *Sandbox allocator*: The snmalloc allocators that run inside a sandbox and allocate memory from the *sandbox heap*.\n - *Boundary allocator*: The per-sandbox instance of snmalloc that allocates memory in the *sandbox heap* on behalf of the *parent*.\n - *OS sandbox*: The kernel-specific functionality used to restrict the rights of the *child*.\n - *library runner*: The program (the `library_runner` binary, compiled from [library_runner.cc](src/library_runner.cc)) that the child process runs.\n\nHigh-level abstractions\n-----------------------\n\nThe host process is assumed to be trusted and wishes to:\n\n - Load untrusted libraries into a sandbox.\n - Allocate memory accessible to the sandbox.\n - Access data structures in sandbox-owned memory.\n - Provide the library with access to external resources (e.g. files or sockets).\n - Invoke functions / methods within the sandbox.\n - Implement callbacks that the sandboxed code can invoke.\n\nAll memory in the sandbox's heap is mapped into both the host and child at the same address and so pointers to sandbox-heap memory can be used directly in the child and after a range check in the parent.\n\nSecurity\n--------\n\nThe sandboxing code is a critical part of the TCB for Verona and so security is one of the most important parts of the design.\n\n### Threat model\n\nThe sandbox loads a library that is assumed to be untrusted and so the attacker is assumed to have arbitrary-code execution power within the sandbox.\nThe attacker is allowed to corrupt *any* memory owned by the sandbox, including the sandbox heap.\nSimilarly, the attacker is assumed to be able to modify memory in the sandbox heap concurrently with accesses from the parent.\n\nThe attacker must not be able to:\n\n - Directly modify any memory owned by the parent.\n - Cause the parent to modify any non-sandbox memory except as explicitly allowed via safe callbacks.\n - Cause the parent to stop executing (denial of service) in any thread, except for the time that the parent allows for a call into the child to execute.\n - Access any global namespace (filesystem, network, and so on) except as explicitly permitted by the parent.\n\n### Unknown issues\n\nThis is experimental code.\nIt is passed an internal security audit but has not been deployed at scale and subject to attack.\nAs such, there are almost certainly security issues, possibly some that are intrinsic to the design.\n\nDesign\n------\n\nThe process sandbox code creates a child process and configures it to run in a low-privilege mode, with a shared memory region for the sandbox heap.\nThe child receives a small number of handles that allow it to communicate with the parent.\n\nThe core of the design provides a lightweight mechanism for allocating memory in the shared region.\nOther abstractions are built on this fact to provide lightweight single-copy I/O (in some cases zero-copy, but typically a single copy is required to prevent time-of-check-to-time-of-use vulnerabilities).\n\nMemory management\n-----------------\n\nFor each sandbox instance, the parent creates two anonymous shared memory objects.\nOne object is large and will be used for the sandbox's heap.\nThe large object is mapped at the same address in both the parent and a child process, allowing intra-sandbox pointers to have the same values in both.\nThis is mapped read-write in both processes.\n\nThe smaller object is a single page, used for the snmalloc page map that covers the large region.\nThis is mapped read-write in the parent over the region of the pagemap that contains metadata for the large region.\nIt is mapped read-only in the child.\nUpdates to the pagemap are infrequent.\n\nThe parent has a boundary allocator associated with the sandbox that allows allocation in the sandbox's heap.\nThe parent allocator's message queue is in the shared region, but the rest of the state is not (though free lists are stored inline in objects and so must be untrusted).\nWhen the boundary allocator in the parent needs to update the pagemap, it does so directly.\nWhen an sandbox allocator in the child needs to update the parent, it communicates the update via a pipe to the parent.\nThe parent then validates the requested update and performs the write.\n\n### Pagemap updates and chunk allocation\n\nThe back end of the snmalloc memory allocator communicates with a memory provider that is responsible for providing chunks of memory.\nThis is on the slow path - once a chunk (typically 1 MiB) has been allocated to an allocator, memory from that chunk can be allocated very cheaply.\nThe memory provider resides in the parent because that is the only place where trusted code can run.\n\nThe child sends pagemap updates and memory provider requests to the parent via a socket.\nBecause all pagemap updates are done in the parent, allocators do not need to validate the pagemap values on read.\n\nThe socket is inherited from the parent in the file descriptor identified by `PageMapUpdates` (defined in [sandbox_fd_numbers.h](include/process_sandbox/sandbox_fd_numbers.h)).\nThese updates are sent via the trivial RPC protocol described in [host_service_calls.h](src/host_service_calls.h).\nThis is not a general protocol but is designed with the following constraints:\n\n - It must not require memory allocation because it is invoked during memory allocation.\n - It must not require any other dynamically allocated state because it is invoked during early bootstrapping in the child.\n - It does not need to support reentrancy and, in particular, should not because allocation should always make forward progress (possibly by failing if the shared heap address space is exhausted).\n\nWhen the child starts, the libc bootstrapping code invokes `malloc`, which triggers snmalloc to bootstrap (via its slow-path initialisation) by mapping the shared memory region and invoking the RPC to request a chunk from which it can allocate.\nOnce this is done, it's possible for both the child and parent to allocate memory within the shared heap.\n\nNote that, for this to be efficient in the current code, the OS must implement lazy commit so that allocating a large (e.g. 1GiB) shared memory region does not consume 1GiB of physical memory or swap unless it is actually used.\nMemory is not used until the allocator has requested it from the memory provider, and so it would be possible to only commit pages in the shared region as requested (and even to provide each superslab as a separate shared memory object).\n\nStarting the child process\n--------------------------\n\nOn process creation, everything running in the child is trusted code.\nThe OS sandbox abstraction allows policies to be run before or after the `library_runner` process starts, but at the moment the security guarantees are roughly equivalent.\n\nThe child closes the file descriptors used for the shared mapping and opens the requested library.\nThe library is a shared library that provides the interfaces to the untrusted code and links directly to that code.\nAs part of the process of loading the library, the run-time linker will invoke global constructors and so after this point the child is not trusted.\n\nThe loaded library is expected to expose two functions:\n\n - `sandbox_init` is the equivalent of `main`, it is invoked after the library is loaded and all global constructors have run but before any exported functions are invoked.  This function takes no arguments and does not return a value.\n - `sandbox_call` is the function that is invoked whenever the parent calls a sandboxed function.\n\nThe `sandbox_call` function takes an integer and a pointer as arguments.\nThe integer is intended to contain the index of the exported function in a logical vtable for the library, the pointer points to a structure used to marshal the arguments and return values.\n\nAfter the library runner has found the entry points into the library, it enters the runloop where it waits for the parent to call functions.\n\nCalling sandbox functions\n-------------------------\n\nThe call-return mechanism for invoking sandbox functions is richer than the host service call mechanism for the memory allocator.\nIt is free to depend on a working memory allocator, because it is used only after snmalloc's bootstrapping completes and snmalloc does not, itself, use this mechanism.\nIt also must support recursive calls with stack stitching.\nAt a high level, the parent must be able to call into the sandbox, which may then invoke callbacks, which then may invoke the sandbox, and so on, with a single logical call stack.\n\n### Flow control for calls\n\nWhen the parent thread invokes the child, it must sleep until the child either returns, or notifies it to invoke a callback.\nThe core building block for signalling between the two is a one-bit semaphore in the shared memory region.\nThis is used to implement a token-passing mechanism.\nAt almost any time, the child will be blocked on its semaphore waiting for the parent or a parent thread will be blocked on its semaphore waiting for the child to complete.\n\nWhen a child starts, both it and the parent are running.\nIf the run-time linker does not natively support a sandboxing technology (e.g. seccomp-bpf and the glibc `ld-linux.so`) then the parent must be able to handle callbacks from the client to open shared libraries before it can call the first sandboxed function.\nThis causes some slightly complex logic for the initial rendezvous, which will probably be simplified in a future version.\n\nThe structure that defines the static (non-heap) part of the shared memory region is declared in [shared_memory_region.h](include/process_sandbox/shared_memory_region.h).\nThis has a field called `token` that contains a pair of one-bit semaphores and the stack depth of current callback.\nThe depth is incremented in the child before it invokes the parent and decremented in the parent when it returns.\nThis allows each side to sit in a modal runloop in the stack frame responsible for waiting for the return and handle deeper invocations by local recursion.\nWhen a runloop is woken up in the child, it checks whether the depth has been decreased by the parent and, if so, returns, otherwise it handles a new invocation from the parent.\nWhen a runloops is woken up in the parent, it checks whether the depth has been increased by the child and, if so, invokes the correct callback function, otherwise it returns.\n\nThe code for invoking the sandbox is in `sandbox::Library::send` in [libsandbox.cc](src/libsandbox.cc), the code for handling invocations in the child is in the `runloop` function in [library_runner.cc](src/library_runner.cc).\n\n### Data movement for calls\n\nThe mechanism described in the last section transfers control from the parent to the child and back, increasing or decreasing the call depth, but it does not describe how parameters and return values are passed.\nRecall that we can cheaply allocate memory in the shared heap from the parent or the child.\nWe take advantage of this for arguments, allocating a structure that contains the arguments and space for the return value and passing a pointer to it in a fixed location in the shared memory region.\n\nFor example, consider exporting a function such as this from a sandbox:\n\n```C\nint example(struct SomeStruct s);\n```\n\nThe wrapper code (generated by the Verona compiler or by the C++ API) would generate a structure like this (names for illustration only, these would be opaque in the Verona implementation and are tuple elements and not structure fields in the C++ version):\n\n```C\nstruct ExampleArgFrame\n{\n\tint ret;\n\tstruct SomeStruct s;\n};\n```\n\nThe `sandbox_call` function would be passed a pointer to this when invoked with the index of this function.\nIt is then responsible for calling `example` with the argument from the structure and placing the return address inside the structure.\nThe Verona compiler will synthesize something roughly like this:\n\n```C\nvoid sandbox_call(int idx, void *args)\n{\n\tswitch (idx)\n\t{\n\t\t...\n\t\tcase ExampleFunctionNumber: // Arbitray value, must be unique\n\t\t{\n\t\t\tstruct ExampleArgFrame *argframe = args;\n\t\t\targs->ret = example(args->s);\n\t\t}\n\t}\n}\n```\n\nThe functions in [cxxsandbox.h](include/process_sandbox/cxxsandbox.h) provide templates for generating this code directly from C++ function declarations.\nNote that the C++ APIs provide the same abstractions for sandbox code invocation that the Verona compiler is expected to use, but the C++ type system lacks viewpoint adaptation and so cannot help you avoid accidentally following pointers that the attacker is able to manipulate.\n\nCurrently, the in-memory RPC mechanism used to invoke methods in the child is very high latency.\nThis may not be a problem for Verona, where foreign calls are likely to be wrapped in `when` clauses, which can batch multiple operations within the library.\nThe asynchronous operation of `when` clauses hides latency, avoiding the blocking operations in the C++ proof-of-concept.\nThis overhead could be significantly reduced on an OS that supported Spring / Solaris Doors.\n\n### Callbacks and system call emulation\n\nCallbacks are registered with the sandboxed library and are assigned a number.\nCurrently, the callback mechanism sends the equivalent of the two arguments that are passed to the `sandbox_call` function over a socket, rather than using an in-memory transport.\nThis is because you can't send a file descriptor over a UNIX domain socket without also sending some data and callbacks need to be able to both accept and return file descriptors.\nAt some point, the two mechanisms will be unified.\nLinux, for example, provides Windows-like system calls for getting and inserting file descriptors in a child process as of version 5.10, which could be used to fetch and return file descriptors if required rather than requiring the socket.\n\nThe same callback mechanism is used for system call emulation.\nWhen the child wishes to invoke a system call that is not allowed (for example, `open`, which would grant access to the entire filesystem if permitted), this is handled by a callback that takes the arguments and returns either an error value or a file descriptor.\nCurrently, only a small handful of system calls (those required for glibc's `ld-linux.so` to load a library) are proxied but this will grow over time.\n\nOS sandboxing mechanisms\n------------------------\n\nThe OS sandbox abstraction layer will almost certainly change over time.\nIt currently supports Capsicum and seccomp-bpf but could potentially support the macOS sandbox framework, OpenBSD's pledge, and so on.\n\nCapsicum provides two small modifications to the traditional POSIX system call API:\n\n - File descriptors have fine-grained rights that can be removed at any time by an explicit call (but cannot be re-added).\n - A process in Capsicum mode has no access to any global namespace and so cannot issue system calls such as `open` or `bind` at all and can only issue relative calls such as `openat` if it has a file descriptor with the relevant permissions.\n\nConveniently, this is *precisely* the policy that we want for sandboxing libraries.\nThe library running in the child can do whatever it wants with its own heap, but it can only read or write files, sockets, or IPC primitives that the parent explicitly delegates it access to.\nEnforcement of a Capsicum sandbox is very cheap and there is no performance penalty for running in the sandboxed mode.\n\nThe seccomp-bpf is far less useful for this purpose.\nThe model for seccomp-bpf is that userspace can provide a filter program that runs on system call entry that determines whether the system call may proceed.\nThis has several limitations compared to Capsicum:\n\n - There is no way of allowing `openat` in the safe cases, so `openat` must be emulated.\n - The system call filters can make decisions based on their arguments only in the most limited sense:\n   - They cannot attach metadata to a file descriptor and make decisions based on the specific file descriptor.\n   - They cannot (for security reasons) read userspace memory and so cannot inspect any pointer arguments (e.g. paths, flags for `clone3` and so on).\n   - The `libseccomp` library does not expose interfaces for comparing two arguments and so you cannot write filters of the form `base + length > sandbox_heap_start` without opting out of `libseccomp` entirely and writing raw eBPF scripts.\n\nThe seccomp-bpf policy divides system calls into four categories:\n\n - Safe to use in general (e.g. `read` / `write`)\n - Process-related state, safe to use only if the process ID refers to this process, blocked otherwise.\n - Unsafe to use in general but possibly safe in some cases and so emulated with a callback where possible (e.g. `openat` which can be transformed into a callback that may or may not open the file for you, depending on user policy).\n - Unsafe to use in any case (e.g. `settimeofday`), includes intrinsically privileged operations that you should not try to use in a sandbox.\n\n### System-call interposition\n\nIn the common case, POSIX software invokes system calls via libc wrappers.\nELF linkage allows us to preempt those symbols by providing implementations in `library_runner` that will be use in preference to the ones in libc.\nThis makes it possible to intercept calls that would normally result in a call that the operating system would block and issue a callback instead.\nSome software; however, issues system calls directly.\nIn particular, glibc's `ld-linux.so`, which is required for loading the untrusted library *after* the sandbox policy has been applied on GNU/Linux systems, issues system calls directly to open the file.\n\nBoth of the currently supported sandboxing mechanisms provide a mechanism for delivering a signal if the sandbox policy blocks a call.\nThe `library_runner` process registers a handler for this system call and attempts a last-resort emulation for the system calls that we can transform into callbacks.\nThis extracts the system call number and arguments from the signal frame (which include a complete register dump at the time of the signal) and then re-inserts the results before the signal returns.\n\nSignal delivery is generally a very slow path in operating systems and so this mechanism is definitely not the desired way of issuing system calls.\nIt provides a fallback for maximising compatibility but in the common case this mechanism should not be used.\nAll of the complex code for handling this case runs inside the sandbox and so a bug in it would not grant the attacker any power that they do not have within our threat model.\n\nPortability\n-----------\n\nThe current code is expected to be easily portable to any POSIX system that has a process sandboxing framework.\nThe core abstractions provided should be portable to Windows using the Isolated User Mode APIs, though interposing on Windows system calls for privilege elevation may not be possible and there are still a number of questions about how this can be implemented.\n\nThe platform abstractions are all in the [platform](include/process_sandbox/platform/) directory.\nThis currently provides several abstractions that have generic POSIX implementations:\n\n - Child processes, implemented via `vfork`, with a `pdfork` implementation available if supported.\n - Socket pairs with the ability to send and receive file descriptors and data.\n   Windows provides separate mechanisms for sending messages and sending handles but these should be possible to wrap in the same abstraction.\n - Shared memory, implemented with generic POSIX shared memory objects and `mmap`.\n   This is more efficient if the OS can guarantee strongly aligned allocations and if it provides native support for anonymous memory objects.\n\nThe remaining abstractions require per-OS (or OS-family) support.\nThese are:\n\n - A poller, which can wait for data on a set of socket connections and notice when the remote end is closed.\n   This is implemented with `epoll` (Linux) and `kqueue` (pretty much everything that's not Linux).\n   The POSIX `select` and `poll` calls are not quite adequate because they do not portably allow detecting when the remote end of a socket or pipe has been closed.\n - A one-bit semaphore that can exist in memory.\n   This is *probably* safe to implement with a pthread mutex and condition variable but it is not possible to portably reason about the security implications of doing so.\n   There are `futex` (Linux, OpenBSD) and `umtx` (FreeBSD) implementations.\n   The `futex`-based implementation will require some small tweaks to run on OpenBSD.\n   This should be possible to implement on any platform with a mutex primitive that does not store any critical data in userspace other than the lock word.\n - System call frame introspection routines.\n   These are both platform- and architecture-dependent.\n   They are required to pull the system-call number arguments out of a trap frame delivered to a signal and re-inject the return values.\n   This is purely a compatibility feature and is not required if running libraries that only invoke system calls via interposable functions.\n - Finally, and most importantly, the OS sandboxing policy code.\n   Capsicum (FreeBSD) and seccomp-bpf (Linux) implementations are provided.\n   The Capsicum implementation is the simplest, because the kernel provide exactly the policy required (no direct access to the global namespace, restrictions on the operations permitted on delegated file descriptors.\n   The seccomp-bpf policy blocks a number of things that should be safe most of the time and requires callbacks to implement them.\n\nCode layout\n-----------\n\n - [include/process_sandbox](include/process_sandbox) contains the public interfaces.\n   - [cxxsandbox.h](include/process_sandbox/cxxsandbox.h) contains the C++ API, which is primarily used for testing.\n   - [filetree.h](include/process_sandbox/filetree.h) describes the interface for exporting a virtual file tree to the child.\n     This is accessed from the `sandbox::Library` class.\n   - [helpers.h](include/process_sandbox/helpers.h) provides some helpers for managing C memory, extracting argument types from functions, and so on.\n   - [platform/](include/process_sandbox/platform) contains the platform abstraction code.\n   - [callbacks.h](include/process_sandbox/callbacks.h) describes the callback mechanism.\n   - [sandbox_fd_numbers.h](include/process_sandbox/sandbox_fd_numbers.h) contains the file descriptors that are set on child-process creation.\n   - [sandbox.h](include/process_sandbox/sandbox.h) contains the definition of the sandbox library interface.\n   - [shared_memory_region.h](include/process_sandbox/shared_memory_region.h) defines the part of the shared memory region, not including the heap.\n - [src](src) contains the source files\n   - [child_malloc.h](src/child_malloc.h) contains the interfaces for the parts that specialise snmalloc for use in the child.\n   - [host_service_calls.h](src/host_service_calls.h) describes the IPC mechanism used for the snmalloc to request memory and update the sandbox.\n   - [library_runner.cc](src/library_runner.cc) is the program that runs inside the sandbox that loads the library and manages communication with the parent.\n   - [libsandbox.cc](src/libsandbox.cc) is the library that manages sandbox lifecycle.\n - [tests](tests) contains tests.\n   Some are stand-alone unit tests, the files that start `sandbox-` and `sandboxlib-` are the parent / child parts of tests that use the complete sandboxing framework.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "verona-sandbox", "org_name": "microsoft", "org_repo": "microsoft/verona-sandbox", "platform_org_repo": "github+microsoft/verona-sandbox", "link_to_repo": "https://github.com/microsoft/verona-sandbox", "platform": "github", "language": "C++", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# regorus\n\nTHIS REPOSITORY IS IN ACTIVE DEVELOPMENT AND NOT INTENDED FOR PRODUCTION USE.\n\n**Regorus** is a Rego interpreter, analyzer and checker written in Rust.\n**Regorus** also aims to be a rigorous enforcer of formally defined Rego semantics.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "regorus", "org_name": "microsoft", "org_repo": "microsoft/regorus", "platform_org_repo": "github+microsoft/regorus", "link_to_repo": "https://github.com/microsoft/regorus", "platform": "github", "language": "Rust", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# [Visual Studio] VSConfig Finder\n\n![build status: main](https://devdiv.visualstudio.com/DevDiv/_apis/build/status/Setup/VSConfigFinder-CI?branchName=main&label=main)\n[![github release](https://img.shields.io/github/release/Microsoft/vsconfigfinder.svg?logo=github&logoColor=white)](https://github.com/microsoft/VSConfigFinder/releases/latest)\n[![github releases: all](https://img.shields.io/github/downloads/Microsoft/vsconfigfinder/total.svg?logo=github&logoColor=white&label=github)](https://github.com/microsoft/VSConfigFinder/releases)\n\nWhen you want to set up Visual Studio from a new environment, `.vsconfig` files can be very useful as they are easy to be created from your [existing installations](https://learn.microsoft.com/en-us/visualstudio/install/import-export-installation-configurations?view=vs-2022) or from a [working solution](https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/). However, one existing problem with the `.vsconfig` usage is that the Visual Studio Installer supports importing one `.vsconfig` file at a time, so if you have multiple `.vsconfig`s throughout your solution (e.g. you have a monorepo that is consisted of multiple projects) and you want to apply them while setting up your pipeline, you would have to run an installer operation as many times as you'd want to use the different `.vsconfig`s. One way to get around this problem is to generate a single `.vsconfig` yourself that you put on the root of the solution, but this approach still has its own issues: For example, if you're only interested in a subset of the solution, you'll install far more components than the ones you need, resulting in a longer install and subsequent update time.\n\n_VSConfigFinder_ is designed to be a redistributable, single-file executable that can be used in build or deployment scripts to use multiple `.vsconfig`s that exist throughout the solution without having to go through multiple installer operations by recursively finding nested `.vsconfig` files and merging them into a single output file, or an installer command line argument, depending on the customer requirement.\n\n_VSConfigFinder_ is a simple tool and will not be shipped with the Visual Studio Installer, so please feel free to consume the package based on your need and use it for your own deployment setup.\n\n## Example\n\nImagine that you have a solution or a repository with the folder structure as below:\n\n```\nroot\n  - folderA\n  - folderB\n    - .vsconfig (packageE)\n  - folderC\n    - someProject\n      - .vsconfig (packageA, packageB)\n    - folderD\n      - folderE\n        - .vsconfig (pacakgeC)\n      - folderF\n        - .vsconfig (packageD)\n```\n\nIf you want to pass in all the components that are needed to build & run `folderC`, you could run the tool with the following parameters:\n\n`VSConfigFinder.exe --folderpath root\\folderC`\n\nThis will generate the following command as a console output that you can simply pass into the installer.\n\n`--add packageA --add packageB --add packageC --add packageD`\n\nRemember to add your own verb (e.g. `install` or `modify`) in conjunction with the tool output.\n\n## Multi-Root Folders Support\n\nSay if you want to do something similar to the example above, but you want everything under `folderB` AND `folderC`. You cannot pass in one or the other, because the two do not share a common folder (if you pass in the `root`, `folderA` will also be included). Instead, you can simply pass in the topmost folders as a list to achieve your goal.\n\n`VSConfigFinder.exe --folderpath root\\folderC root\\folderB`\n\nThis will generate the following command as a console output that you can simply pass into the installer.\n\n`--add packageA --add packageB --add packageC --add packageD --add packageE`\n\nAgain, remember to add your own verb (e.g. `install` or `modify`) in conjunction with the tool output.\n\n## Alternate Example\n\nAlternatively, you can pass in additional parameters provided by the tool to get the merged `.vsconfig` as a single file:\n\n`VSConfigFinder.exe --folderpath root\\folderC --createfile --configoutputpath c:\\somefolder`\n\nThis will generate an alternate single `.vsconfig` file with all the needed components in the specified `configOutputPath`. If you don't specify a `configOutputPath`, the output directory will default to the current directory.\n\nNote that if you choose to use `--createfile`, the Visual Studio Installer arguments will no longer be output to the console.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\n## License\n\nThis project is licensed under the [MIT license](LICENSE.txt).\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "VSConfigFinder", "org_name": "microsoft", "org_repo": "microsoft/VSConfigFinder", "platform_org_repo": "github+microsoft/VSConfigFinder", "link_to_repo": "https://github.com/microsoft/VSConfigFinder", "platform": "github", "language": "C#", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-tmdl", "org_name": "microsoft", "org_repo": "microsoft/vscode-tmdl", "platform_org_repo": "github+microsoft/vscode-tmdl", "link_to_repo": "https://github.com/microsoft/vscode-tmdl", "platform": "github", "language": null, "stargazers_count": 59, "watchers_count": 59}, {"README_text": "# Welcome to the Win32 app isolation repo\nWin32 app isolation is a new security feature on Windows that helps contain the damage and safeguard user privacy choices in the event of an \napplication compromise. \nWin32 app isolation is built on the foundation of [AppContainers](https://learn.microsoft.com/en-us/windows/win32/secauthz/implementing-an-appcontainer), which offer a security boundary, \nand components that virtualize resources and provide brokered access to other resources. \nThis repo contains the documentation and tools to help you isolate your applications.\n\n## Getting started \n* The first step to isolating your application is to package it to run isolated by following the instructions [here](docs/packaging/msix-packaging-tool.md). \n* Once you have your application packaged, use [Application Capability Profiler](docs/profiler/application-capability-profiler.md) to update the application to grant it access to additional resources.\n* We also have additional documentation about the [fundamentals](docs/fundamentals) including file access consent.\n* You're now ready to deploy and run your application on Windows.\n\nBinaries for the tools used to package applications to run isolated are shared under the [releases](https://github.com/microsoft/win32-app-isolation/releases) section of the repo.\n\nRelease notes for supported Windows builds and tools can be found [here](relnotes/windows-release-notes.md).\n\n## Communicating with the team\nWe'd love to hear your feedback and answer your questions! \nThe best way to communicate with the team is through GitHub [discussions](https://github.com/microsoft/win32-app-isolation/discussions)\nand [issues](https://github.com/microsoft/win32-app-isolation/issues). \nPlease search for similar discussions and issues before creating new ones. \n\n## Resources\nYou can find additional information about Win32 app isolation using the following resources: \n* [Win32 app isolation Build session](https://www.youtube.com/watch?v=w6VwHGPz12w&pp=ygUTd2luMzIgYXBwIGlzb2xhdGlvbg%3D%3D&ab_channel=MicrosoftDeveloper)\n* Coming soon - Win32 app isolation blog\n\n## Contributing\nIf you would like to contribute to the documentation, please familiarize yourself with the Code of Conduct resources below and submit a pull request.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "win32-app-isolation", "org_name": "microsoft", "org_repo": "microsoft/win32-app-isolation", "platform_org_repo": "github+microsoft/win32-app-isolation", "link_to_repo": "https://github.com/microsoft/win32-app-isolation", "platform": "github", "language": null, "stargazers_count": 898, "watchers_count": 898}, {"README_text": "# Welcome to the JDK!\n\nFor build instructions please see the\n[online documentation](https://openjdk.org/groups/build/doc/building.html),\nor either of these files:\n\n- [doc/building.html](doc/building.html) (html version)\n- [doc/building.md](doc/building.md) (markdown version)\n\nSee <https://openjdk.org/> for more information about the OpenJDK\nCommunity and the JDK and see <https://bugs.openjdk.org> for JDK issue\ntracking.\n", "repo_name": "openjdk-jdk20u", "org_name": "microsoft", "org_repo": "microsoft/openjdk-jdk20u", "platform_org_repo": "github+microsoft/openjdk-jdk20u", "link_to_repo": "https://github.com/microsoft/openjdk-jdk20u", "platform": "github", "language": "Java", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "FourthCoffee", "org_name": "microsoft", "org_repo": "microsoft/FourthCoffee", "platform_org_repo": "github+microsoft/FourthCoffee", "link_to_repo": "https://github.com/microsoft/FourthCoffee", "platform": "github", "language": "HTML", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Data Science Editor\n\nPrototype integration of the [Data Science Editor](https://microsoft.github.io/data-science-editor/excel/) in Office for the Web.\n\n-   [Get it from the Office Store](https://appsource.microsoft.com/en-us/product/office/WA200005186?tab=Overview)\n\n![A gif showcasing the app in action.](https://microsoft.github.io/data-science-editor-excel/hosted_files/getting-started.gif)\n\n## Developer Zone\n\nThese instructions will show you how to build and debug the Office App locally.\n\n## Getting Started\n\nThese instructions have been tested on Windows only.\n\n1. Download the latest LTS version of [node.js](https://nodejs.org/en/download/).\n1. Install all dependencies.\n\n> npm run install\n\n### Build\n\nThe following script will build and place assets in the dist directory:\n\n> npm run build\n\n### Lint\n\nRuns prettier over all typescript files\n\n> npm run lint\n\n### Manual Test\n\n1. Run the following script to start the dev server:\n    - `npm run server`\n1. [Manually sideload the add-in to Office on the web](https://learn.microsoft.com/en-us/office/dev/add-ins/testing/sideload-office-add-ins-for-testing#manually-sideload-an-add-in-to-office-on-the-web)\n1. select `manifest-local.xml`\n\n## Testing the hosted data science editor locally\n\n-   clone https://github.com/microsoft/data-science-editor and follow instructions to launch dev server\n-   update `localhost` to true in webpack.config.js and rebuild\n\n## Architecture\n\nThis add-in is a host to an iframe that holds the data science editor. This Add-In provides the interface to allow the data-science editor to interact with Excel.\n\nThis add-in is complete static and hosted on a github pages site.\n\n### Layout\n\nLayout of folders\n\n-   **assets**\n    -   image assets\n-   **src**\n    -   source code for the add-in\n-   **listing**\n    -   descriptions for the add-in store listing\n-   **hosted_files**\n    -   additional hosted files\n-   **scripts**\n    -   development scripts\n-   **config**\n\n    -   tooling configuration files\n\n-   **dist**\n    -   the build site, this is the exact layout hosted\n\n## Add-In Manifest\n\n### Generate production manifest.xml from manifest-local.xml\n\nMake all manifest changes to `manifest-local.xml`.\n\nWhen the local manifest changes run:\n\n> npm run manifest\n\nThe command:\n\n-   check that the local manifest is valid\n-   generates the production `manifest.xml`\n-   checks the production manifest is valid\n\n### Requirement Set\n\nThe Manifest is set to require a specific Excel version to avoid having to support specific outdated browser versions.\n\n[ExcelApi Requirement Sets and Supported Office Versions](https://learn.microsoft.com/en-us/javascript/api/requirement-sets/excel/excel-api-requirement-sets#requirement-set-availability)\n", "repo_name": "data-science-editor-excel", "org_name": "microsoft", "org_repo": "microsoft/data-science-editor-excel", "platform_org_repo": "github+microsoft/data-science-editor-excel", "link_to_repo": "https://github.com/microsoft/data-science-editor-excel", "platform": "github", "language": "TypeScript", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Platform Services Source\n\n[Kalypso](https://github.com/microsoft/kalypso) Platform Services source repo contains the high level manifests and templates for the `dial-tone` platform services that are used by the CI/CD workflows to generate manifests for the GitOps repository.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-svc-src", "org_name": "microsoft", "org_repo": "microsoft/kalypso-svc-src", "platform_org_repo": "github+microsoft/kalypso-svc-src", "link_to_repo": "https://github.com/microsoft/kalypso-svc-src", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Platform Services GitOps\n\n[Kalypso](https://github.com/microsoft/kalypso) Platform Services source repository contains final manifests of sample dial-tone platform services to be deployed across clusters fleet. The manifests are stored in the repository branches that represent rollout environments. Changes to this repository should happen only through the CD workflow.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kalypso-svc-gitops", "org_name": "microsoft", "org_repo": "microsoft/kalypso-svc-gitops", "platform_org_repo": "github+microsoft/kalypso-svc-gitops", "link_to_repo": "https://github.com/microsoft/kalypso-svc-gitops", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "assessment", "org_name": "microsoft", "org_repo": "microsoft/assessment", "platform_org_repo": "github+microsoft/assessment", "link_to_repo": "https://github.com/microsoft/assessment", "platform": "github", "language": "C#", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project codename WALDO : Improve video search using AI\n\nThis repository contains code and documentation of WALDO (**W**eb **A**pplication to **L**ookup **D**igital **O**bjects) which is a solution developed by Microsoft. It aims to improve video search using Artificial Intelligence. It does the following :\n\n- Automates the analysis of videos with Azure Video Indexer,\n- Creates and improves the search index with Video Indexer insights and optional metadata\n- Provides a web UI application to search and see results\n- Logs the behavior and feedback of users (in order for the admin or data science team to improve the search index)\n- Provides a dashboard which displays statistics on platform usage and search performances\n\n[Overview](docs-internal/img/overview-slide.png)\n\n## Documentation\n\nTechnical documentation :\n\n- [Getting started with Terraform](/docs/1-terraform.md)\n- [Main orchestrator](/docs-internal/dotnet/orchestrator.md)\n- [Data orchestrator](/docs-internal/python/orchestrator.md)\n- [function-apps](/docs/2-function-apps.md)\n- [index-creation](/docs/3-index-resources.md)\n- [web-application](/docs/4-web-application.md)\n- [Testing](/docs-internal/dotnet/testing.md)\n\n## Notes\n\n## Limited Access features of Azure Video Indexer\n\nMicrosoft facial recognition services are Limited Access in order to help prevent the misuse of the services in accordance with our [AI Principles](https://www.microsoft.com/ai/responsible-ai?SilentAuth=1&wa=wsignin1.0&activetab=pivot1%3aprimaryr6) and [facial recognition](https://blogs.microsoft.com/on-the-issues/2018/12/17/six-principles-to-guide-microsofts-facial-recognition-work/) principles. The Face Identify and Celebrity Recognition operations in Azure Video Indexer are Limited Access features that require registration.  \n\nPlease go to [this page](https://docs.microsoft.com/en-us/azure/azure-video-indexer/limited-access-features) to get more information on how to enable limited access feature in Video Indexer.\n\n## Contacts\n\n **Company** | **Role** | **Contact** |\n|-|-|-|\n|Microsoft| Technical Program Manager | [Xavier Pouyat](mailto:xpouyat@microsoft.com)|\n|Microsoft| Development Lead | [Yannick Brombach](mailto:yabromba@microsoft.com) |\n|Microsoft| Data Science Lead | [Malvina Matlis](mailto:melmatlis@microsoft.com) |\n\n## Infos\n\nTrademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\n\n- [Microsoft Code Of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Reporting security issues](https://docs.opensource.microsoft.com/releasing/securing-content/reporting-security-issues/)\n", "repo_name": "Video-Indexer-Processor", "org_name": "microsoft", "org_repo": "microsoft/Video-Indexer-Processor", "platform_org_repo": "github+microsoft/Video-Indexer-Processor", "link_to_repo": "https://github.com/microsoft/Video-Indexer-Processor", "platform": "github", "language": "TypeScript", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Microsoft Defender for Cloud Apps\n\n> Welcome to the Microsoft Defender for Cloud Apps repository! This repository contains sampless scripts and code that is intended to serve as a guide to \n> drive automation and adoption of Microsoft Defender for Cloud Apps.\n\n# Resources\n* [Microsoft Defender for Cloud Apps documentation](https://aka.ms/DefenderforCloudAppsdocs)\n* [Microsoft Defender for Cloud Apps Ninja Training](http://aka.ms/MDCANinjaTraining)\n* [Microsoft 365 Defender documentation](https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide)\n* [Security Community Webinars](https://aka.ms/securitywebinars)\n* [Recent Defender for Cloud Apps block posts](https://aka.ms/DefenderforCloudAppsBlogs)\n\nWe value your feedback. Here are some channels to help surface your questions or feedback:\n\nProduct specific feature requests - Upvote or post new on [Microsoft 365 Defender feedback forums](https://aka.ms/M365Defender/Feedback)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Microsoft-Defender-for-Cloud-Apps", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-Defender-for-Cloud-Apps", "platform_org_repo": "github+microsoft/Microsoft-Defender-for-Cloud-Apps", "link_to_repo": "https://github.com/microsoft/Microsoft-Defender-for-Cloud-Apps", "platform": "github", "language": "PowerShell", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Power Automate Documentation\n\nThis is the GitHub repository for the technical product documentation for **Power Automate**. This documentation is published at [Microsoft Power Automate documentation](https://learn.microsoft.com/power-automate).\n\n## How to contribute\n\nThanks for your interest in contributing to [Microsoft Learn](https://learn.microsoft.com/), home of technical content for Microsoft products and services.\n\nTo learn how to make contributions to the content in this repository, start with our [contributor guide](https://learn.microsoft.com/contribute).\n\n## Code of conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "power-automate-docs", "org_name": "microsoft", "org_repo": "microsoft/power-automate-docs", "platform_org_repo": "github+microsoft/power-automate-docs", "link_to_repo": "https://github.com/microsoft/power-automate-docs", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "## Analyzing Leakage of Personally Identifiable Information in Language Models\n\n<p>\n    <a href=\"https://www.python.org/downloads/\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/3.10-Python-blue\">\n    </a>\n    <a href=\"https://pytorch.org\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/1.11-PyTorch-orange\">\n    </a>\n    <a href=\"https://github.com/pytorch/opacus\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/1.12-opacus-orange\">\n    </a>\n</p>\n\nThis repository contains the official code for our IEEE S&P 2023 paper using GPT-2 language models and\nFlair Named Entity Recognition (NER) models.\nIt allows fine-tuning (i) undefended, (ii) differentially-private and (iii) scrubbed language models\non ECHR and Enron and attacking them using the attacks presented in our paper.\n\n\n## Publication\n\n> **Analyzing Leakage of Personally Identifiable Information in Language Models.**\n> Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz and Santiago Zanella-B\u00e9guelin.\n> Symposium on Security and Privacy (S&P '23). San Francisco, CA, USA.\n>\n> [![arXiv](https://img.shields.io/badge/arXiv-2302.00539-green)](https://arxiv.org/abs/2302.00539)\n\n\n## Build & Run\n\nWe recommend setting up a conda environment for this project.\n```shell\n$ conda create -n pii-leakage python=3.10\n$ conda activate pii-leakage\n$ pip install -e .\n```\n\n\n## Usage\n\nWe explain the following functions. The scripts are in the ```./examples``` folder and\nrun configurations are in the ```./configs``` folder.\n* **Fine-Tune**: Fine-tune a pre-trained LM on a dataset (optionally with DP or scrubbing).\n* **PII Extraction**: Given a fine-tuned LM, return a set of PII.\n* **PII Reconstruction**: Given a fine-tuned LM and a masked sentence, reconstruct the most likely PII candidate for the masked tokens.\n* **PII Inference**: Given a fine-tuned LM, a masked sentence and a set of PII candidates, choose the most likely candidate.\n\n\n## Fine-Tuning\n\nWe demonstrate how to fine-tune a ```GPT-2 small``` ([Huggingface](https://huggingface.co/gpt2)) model on the [ECHR](https://huggingface.co/datasets/ecthr_cases) dataset\n(i) without defenses, (ii) with scrubbing and (iii) with differentially private training (\u03b5=8).\n\n**No Defense**\n```shell\n$ python fine_tune.py --config_path ../configs/fine-tune/echr-gpt2-small-undefended.yml\n```\n\n**With Scrubbing**\n\n_Note_: All PII will be scrubbed from the dataset. Scrubbing is a one-time operation that requires tagging all PII in the dataset first\nwhich can take many hours depending on your setup. We do not provide tagged datasets.\n```\n$ python fine_tune.py --config_path ../configs/fine-tune/echr-gpt2-small-scrubbed.yml\n```\n\n**With DP (\u03b5=8.0)**\n\n_Note_: We use the [dp-transformers](https://github.com/microsoft/dp-transformers) wrapper around PyTorch's [opacus](https://github.com/pytorch/opacus) library.\n ```\n$ python fine_tune.py --config_path ../configs/fine-tune/echr-gpt2-small-dp8.yml\n```\n\n\n## Attacks\n\nAssuming your fine-tuned model is located at ```../echr_undefended``` run the following attacks.\nOtherwise, you can edit the ```model_ckpt``` attribute in the ```../configs/<ATTACK>/echr-gpt2-small-undefended.yml``` file to point to the location of the model.\n\n**PII Extraction**\n\nThis will extract PII from the model's generated text.\n```shell\n$ python extract_pii.py --config_path ../configs/pii-extraction/echr-gpt2-small-undefended.yml\n```\n\n**PII Reconstruction**\n\nThis will reconstruct PII from the model given a target sequence.\n```shell\n$ python reconstruct_pii.py --config_path ../configs/pii-reconstruction/echr-gpt2-small-undefended.yml\n```\n\n**PII Inference**\n\nThis will infer PII from the model given a target sequence and a set of PII candidates.\n```shell\n$ python reconstruct_pii.py --config_path ../configs/pii-inference/echr-gpt2-small-undefended.yml\n```\n\n\n## Evaluation\n\nUse the ```evaluate.py``` script to evaluate our privacy attacks against the LM.\n```shell\n$ python evaluate.py --config_path ../configs/evaluate/pii-extraction.yml\n```\nThis will compute the precision/recall for PII extraction and accuracy for PII reconstruction/inference attacks.\n\n\n## Datasets\n\nThe provided ECHR dataset wrapper already tags all PII in the dataset.\nThe PII tagging is done using the Flair NER modules and can take several hours depending on your setup, but is a one-time operation\nthat will be cached in subsequent runs.\n\n\n## Fine-Tuned Models\n\nUnfortunately, we do not provide fine-tuned model checkpoints.\nThis repository does support loading models remotely, which can be done by providing a URL instead of a local path\nin the configuration files for the ```model_ckpt``` attribute.\n\n\n## Citation\n\nPlease consider citing the following paper if you found our work useful.\n\n```\n@InProceedings{lukas2023analyzing,\n  title      = {Analyzing Leakage of Personally Identifiable Information in Language Models},\n  author     = {Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\\'e}guelin, Santiago},\n  booktitle  = {2023 IEEE Symposium on Security and Privacy (SP)},\n  year       = {2023},\n  publisher  = {IEEE Computer Society},\n  pages      = {346-363},\n  doi        = {10.1109/SP46215.2023.00154}\n}\n```\n", "repo_name": "analysing_pii_leakage", "org_name": "microsoft", "org_repo": "microsoft/analysing_pii_leakage", "platform_org_repo": "github+microsoft/analysing_pii_leakage", "link_to_repo": "https://github.com/microsoft/analysing_pii_leakage", "platform": "github", "language": "Python", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "intune-autopkg-recipes", "org_name": "microsoft", "org_repo": "microsoft/intune-autopkg-recipes", "platform_org_repo": "github+microsoft/intune-autopkg-recipes", "link_to_repo": "https://github.com/microsoft/intune-autopkg-recipes", "platform": "github", "language": "Python", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# GAMut Interactively Visualization Generalized Additive Models for Model Understanding\n\nDemo the prototype [here](https://microsoft.github.io/msrgamut)\n\nSee the abstract for this project [here](https://www.microsoft.com/en-us/research/project/msrgamut/)\n\nA paper, presented at SIGCHI 2019 in Glasgow, Scotland is available [here](https://www.microsoft.com/en-us/research/publication/gamut-a-design-probe-to-understand-howdata-scientists-understand-machine-learning-models/)\n\n![Alt](images/gamscreenshot.PNG 'Screenshot')\n\nThis code is based on react, d3, mobx, and typescript. It creates an interactive viewer of the system.\n\n## To run\n\n```\nnpm install\nnpm start\n```\n\n## Dataset Attribution:\n\nThe GAMut demonstration includes GAM (General Additive Models) built from several publicly available datasets:\n\n-   **[Ames Iowa Housing Dataset](http://jse.amstat.org/v19n3/decock.pdf)**: All datasets may be freely used in teaching without contacting the author or JSE for permission.\n\n-   Datasets from the **R data repository**: http://vincentarelbundock.github.io/Rdatasets/datasets.html\n\n    -   **Boston Housing**: http://vincentarelbundock.github.io/Rdatasets/doc/MASS/Boston.htmlHarrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81\u2013102.\n\n        Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n\n    -   **PIMA**: Diabetes in Pima Indian Women: (http://vincentarelbundock.github.io/Rdatasets/doc/MASS/Pima.tr.html)Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and Johannes, R. S. (1988) Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications in Medical Care (Washington, 1988), ed. R. A. Greenes, pp. 261\u2013265. Los Alamitos, CA: IEEE Computer Society Press.\n        Ripley, B.D. (1996) Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.\n\n    -   **Diamonds**: prices of 50000 round cut diamonds: http://vincentarelbundock.github.io/Rdatasets/doc/ggplot2/diamonds.html\n        Diamond data obtained from AwesomeGems.com on July 28, 2005.\n\n-   Datasets from **UCI Machine Learning Repository**: https://archive.ics.uci.edu/ml/index.php (Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.)\n\n    -   **Income**: https://archive.ics.uci.edu/ml/datasets/Census+Income\n\n    -   **Heart-disease**: https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n\n        1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n\n        2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n\n        3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n\n        4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n\n    -   **Red Wine Quality**: https://archive.ics.uci.edu/ml/datasets/wine+quality\n        Paulo Cortez, University of Minho, Guimar\u00e3es, Portugal, http://www3.dsi.uminho.pt/pcortez\n        A. Cerdeira, F. Almeida, T. Matos and J. Reis, Viticulture Commission of the Vinho Verde Region(CVRVV), Porto, Portugal\n        @2009\n\n    -   **Yacht Hydrodynamics**: http://archive.ics.uci.edu/ml/datasets/yacht+hydrodynamics\n        Ship Hydromechanics Laboratory, Maritime and Transport Technology Department, Technical University of Delft.\n\n-   **Titanic Dataset**: Paul Hendricks (2015). titanic: Titanic Passenger Survival Data Set. R package version 0.1.0.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "msrgamut", "org_name": "microsoft", "org_repo": "microsoft/msrgamut", "platform_org_repo": "github+microsoft/msrgamut", "link_to_repo": "https://github.com/microsoft/msrgamut", "platform": "github", "language": "TypeScript", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# BabylonJS + React Project Template \nA template project for creating 3D real-time web apps using BabylonJS and React.\n\n![Hero image for BabylonJS + React tempalte](docs/images/BabylonReactHero.png)\n\nDEMO > https://microsoft.github.io/BabylonJS-React-Template/\n\n# Why Use This Project Template?\nThis project provides a convenient starting point for building 3D web apps with BabylonJS or Cesium.\n\nIt also serves as a reference for how to structure a React/Babylon/Cesium with MSAL authentication, for situations where you prefer setting up your own project.\n\nThe project was created with [`npx create-react-app`](https://reactjs.org/docs/create-a-new-react-app.html) and then [ejected](https://create-react-app.dev/docs/available-scripts/#npm-run-eject) \n\n\n# What's included:\n* Webpack project, with:\n  * [MSAL](https://github.com/AzureAD/microsoft-authentication-library-for-js) for AAD authentication\n  * [Cesium](https://cesium.com/) dependencies for visualing geo-spatial data, with a sample scene to get started\n  * [Babylon](https://www.babylonjs.com/) dependencies for rendering 3D content, with a sample scene to get started\n  * [Redux Toolkit (RTK)](https://redux-toolkit.js.org/) for state management\n* Build scripts for local deployment\n* DevOps Build pipelines\n\n# Getting Started\n## Run locally\n```bash\ncd Apps/babylon-react-bootstrap.js\nnpm start\n```\n\nNavigate to http://localhost:3000\n\n## Build\n```bash\ncd Apps/babylon-react-bootstrap.js\nnpm run build\n```\n\n## Guides\n[How to configure Authentication](./AAD_AUTHENTICATION.md)\n<!-- [How to build in Azure DevOps] -->\n<!-- [] -->\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "BabylonJS-React-Template", "org_name": "microsoft", "org_repo": "microsoft/BabylonJS-React-Template", "platform_org_repo": "github+microsoft/BabylonJS-React-Template", "link_to_repo": "https://github.com/microsoft/BabylonJS-React-Template", "platform": "github", "language": "JavaScript", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Renee: End-to-end training of extreme classification models\n\nOfficial PyTorch implementation for the paper: \"Renee: End-to-end training of extreme classification models\" accepted at MLSys 2023.\n\n\ud83d\udc49 You can find the camera-ready paper [here](Renee-Camera-Ready.pdf).\n\n## DOI\n[![DOI](https://zenodo.org/badge/605004299.svg)](https://zenodo.org/badge/latestdoi/605004299)\n\n## Abstract\n\nThe goal of Extreme Multi-label Classification (XC) is to learn representations that enable mapping input texts to the most relevant subset of labels selected from an extremely large label set, potentially in hundreds of millions.\n\nWe identify challenges in the end-to-end training of XC models and devise novel optimizations that improve training speed over an order of magnitude, making end-to-end XC model training practical. Renee delivers state-of-the-art accuracy in a wide variety of XC benchmark datasets.\n\n## Requirements\n\nRun the below command, this will create a new conda environment with all the dependencies required to run Renee.\n\n```bash\nbash install1.sh\nconda activate renee\nbash install2.sh\n```\n\n## Data Preparation\nYou can download the datasets from the [XML repo](http://manikvarma.org/downloads/XC/XMLRepository.html).\n\nA dataset folder should have the following directory structure. Below we show it for LF-AmazonTitles-131K dataset:\n\n```bash\n\ud83d\udcc1 LF-AmazonTitles-131K/\n    \ud83d\udcc4 trn_X_Y.txt # contains mappings from train IDs to label IDs\n    \ud83d\udcc4 trn_filter_labels.txt # this contains train reciprocal pairs to be ignored in evaluation\n    \ud83d\udcc4 tst_X_Y.txt # contains mappings from test IDs to label IDs\n    \ud83d\udcc4 tst_filter_labels.txt # this contains test reciprocal pairs to be ignored in evaluation\n    \ud83d\udcc4 trn_X.txt # each line contains the raw input train text, this needs to be tokenized\n    \ud83d\udcc4 tst_X.txt # each line contains the raw input test text, this needs to be tokenized\n    \ud83d\udcc4 Y.txt # each line contains the raw label text, this needs to be tokenized\n```\n\nTo tokenize the raw train, test and label texts, we can use the following command (change the path of the dataset folder accordingly):\n```bash\npython -W ignore -u utils/CreateTokenizedFiles.py \\\n--data-dir xc/Datasets/LF-AmazonTitles-131K \\\n--max-length 32 \\\n--tokenizer-type bert-base-uncased \\\n--tokenize-label-texts\n```\n\nTo create a dataset having label-text augmentation, we can use the following command:\n```bash\npython utils/CreateAugData.py \\\n--data-dir xc/Datasets/LF-AmazonTitles-131K \\\n--tokenization-folder bert-base-uncased-32 \\\n--max-len 32\n```\n\nAbove command will create a folder named `xc/Datasets/LF-AmazonTitles-131K-Aug`, now we can refer to this dataset directory in our training script to train with label-text augmentation.\n\n## Training\n\nTrain Renee on LF-AmazonTitles-131K dataset using label-text augmentation, you can use the following command (make sure you modify `data-dir`, `use-ngame-encoder`, `expname` arguments accordingly; also keep in mind that you need to generate label-text augmentation dataset folder first, refer to Data Preparation section of README)\n```bash\npython main.py \\\n--epochs 100 \\\n--batch-size 32 \\\n--lr1 0.05 \\\n--lr2 1e-5 \\\n--warmup 5000 \\\n--data-dir xc/Datasets/LF-AmazonTitles-131K-Aug \\\n--maxlen 32 \\\n--tf sentence-transformers/msmarco-distilbert-base-v4 \\\n--dropout 0.85 \\\n--pre-tok \\\n--wd1 1e-4 \\\n--noloss \\\n--fp16xfc \\\n--use-ngame-encoder xc/ngame_pretrained_models/LF-AmazonTitles-131K/state_dict.pt \\\n--expname lfat-131k-aug-1.0\n```\nTo change hyperparameters, you can refer to the various arguments provided in `main.py` file or you can do `python main.py --help` to list out the all the arguments.\n\nTraining commands for other datasets are provided in `scripts/train_commands.md`.\n\n## License\n\nThis project is licensed under the [Microsoft Research License](https://github.com/microsoft/renee/blob/main/License.docx).\n\n\n## Citation\n\nIf you find our work/code useful in your research, please cite the following:\n\n```bibtex\n@article{renee_2023,\n  title={Renee: End-to-end training of extreme classification models},\n  author={Jain, Vidit and Prakash, Jatin and Saini, Deepak and Jiao, Jian and Ramjee, Ramachandran and Varma, Manik},\n  journal={Proceedings of Machine Learning and Systems},\n  year={2023}\n}\n```\n\n## References\n", "repo_name": "renee", "org_name": "microsoft", "org_repo": "microsoft/renee", "platform_org_repo": "github+microsoft/renee", "link_to_repo": "https://github.com/microsoft/renee", "platform": "github", "language": "Python", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Using Azure OpenAI like a Pro to build powerful AI applications \n\n[Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview) provides REST API access to OpenAI's powerful language models including the GPT-3, Codex and Embeddings model series. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the service through REST APIs, Python SDK, or our web-based interface in the Azure OpenAI Studio.\n\n## Introduction\n\nWelcome to the Azure OpenAI workshop! In this workshop, you will learn how to use the Azure OpenAI service to create AI powered solutions. You will get hands-on experience with the latest AI technologies and will learn how to use Azure OpenAI API. \n\n## Goal\n\n### To understand,\n\n- [Open AI basics on LLMs, APIs and Application scenarios](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/models).\n- [Prompt engineering](https://github.com/microsoft/OpenAIWorkshop/tree/main/scenarios/prompt_engineering).\n  - Common NLP tasks: summarization, classification, entity recognition, sentiment analysis.\n  - Generative tasks: generic content generation, code generation.\n  - Conversational dialog.\n  - Zero shot, few-shot and in-context learning.\n\n### Build,\n\n- [Your first AOAI application with PowerApp](https://github.com/microsoft/OpenAIWorkshop/tree/main/scenarios/powerapp_and_python).\n- Advanced: [A natural language query application on SQL data](https://github.com/microsoft/OpenAIWorkshop/tree/main/scenarios/natural_language_query).\n- Advanced: [An AOAI data pipeline to extract insights from unstructured data](https://github.com/microsoft/OpenAIWorkshop/tree/main/scenarios/openai_batch_pipeline).\n- Advanced: [Make ChatGPT works on your own proprietary dataset](https://github.com/microsoft/OpenAIWorkshop/tree/main/scenarios/openai_on_custom_dataset).\n\n## Format\n\n- All use cases have examples and instructions in a github repo\n- Instructor will run through an overview of solutions and steps\n- Audience will follow and build the solution in their environment\n\n## Audience\n\n- Power Users\n- Software Engineers\n- Data Scientist\n- AI architects and Managers\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks \n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "OpenAIWorkshop", "org_name": "microsoft", "org_repo": "microsoft/OpenAIWorkshop", "platform_org_repo": "github+microsoft/OpenAIWorkshop", "link_to_repo": "https://github.com/microsoft/OpenAIWorkshop", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 136, "watchers_count": 136}, {"README_text": "# SMART: Self-supervised Multi-task pretrAining with contRol Transformers\n\nThis is the official codebase for the ICLR 2023 spotlight paper [SMART: Self-supervised Multi-task pretrAining with contRol Transformers](https://openreview.net/forum?id=9piH3Hg8QEf).\nIf you use this code in an academic context, please use the following citation:\n\n```\n@inproceedings{\nsun2023smart,\ntitle={{SMART}: Self-supervised Multi-task pretrAining with contRol Transformers},\nauthor={Yanchao Sun and Shuang Ma and Ratnesh Madaan and Rogerio Bonatti and Furong Huang and Ashish Kapoor},\nbooktitle={International Conference on Learning Representations},\nyear={2023},\nurl={https://openreview.net/forum?id=9piH3Hg8QEf}\n}\n```\n\n## Setting up\n\n- Using conda\n\n  ```\n  # dmc specific\n  # create env\n  conda env create --file docker/environment.yml\n\n  # activate conda\n  conda activate smart\n  bash scripts/dmc_setup.sh\n\n  # install this repo\n  (smart) $ pip install -e .\n  ```\n\n- Using docker\n\n  ```\n  # build image\n  docker build \\\n        -f Dockerfile_base_azureml_dmc \\\n        --build-arg BASE_IMAGE=openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest \\\n        -t smart:latest .\n\n  # run image\n  docker run -it -d --gpus=all --name=rl_pretrain_dmc_1 -v HOST_PATH:CONTAINER_PATH smart:latest\n\n  # setup the repo (run inside the container)\n  pip install -e .\n  ```\n\n## Downloading data and pre-trained models download from Azure\n\n- Install azcopy\n\n  ```\n  wget https://aka.ms/downloadazcopy-v10-linux\n  tar -xvf downloadazcopy-v10-linux\n  sudo cp ./azcopy_linux_amd64_*/azcopy /usr/bin/\n  rm -rf *azcopy*\n  ```\n\n- Downloading the full dataset (1.18TiB)\n\n  ```\n  # download to data/ directory\n  azcopy copy 'https://smartrelease.blob.core.windows.net/smartrelease/data/dmc_ae' 'data' --recursive\n  ```\n\n- Downloading a subset of the full dataset\n\n  ```\n  # download to data/ directory\n  azcopy copy 'https://smartrelease.blob.core.windows.net/smartrelease/data/dmc_ae/TYPE_DOMAIN_TASK' 'data' --recursive\n  ```\n\n  where\n\n  - `TYPE`: `randcollect`, `fullcollect`\n    Note: `fullcollect` datasets are ~10x larger than `randcollect` datasets)\n\n  - `DOMAIN_TASK`: `cartpole_balance`, `cartpole_swingup`, `cheetah_run`, `finger_spin`, `hopper_hop`, `hopper_stand`, `pendulum_swingup`, `walker_run`, `walker_stand`, or  `walker_walk` (See Table 2 in the paper)\n\n  Example:\n\n  ```\n  # download to data/ directory (~ 9.7 GB each)\n  azcopy copy 'https://smartrelease.blob.core.windows.net/smartrelease/data/dmc_ae/randcollect_walker_walk' 'data' --recursive\n  azcopy copy 'https://smartrelease.blob.core.windows.net/smartrelease/data/dmc_ae/randcollect_cheetah_run' 'data' --recursive\n  ```\n\n- Downloading the pretrained models\n\n  ```\n  # download to pretrained_models/ directory (236.34 MiB)\n  azcopy copy 'https://smartrelease.blob.core.windows.net/smartrelease/pretrained_models' '.' --recursive\n  ```\n\n## Running the code\n\n### Testing on small subset of full dataset\n\nLet us run the code on the aforementioned small subset of `randcollect_walker_walk` and `randcollect_cheetah_run`.\n\n```\npython src/dmc_pretrain.py base=configs/pretrain.yaml \\\n        epochs=10 \\\n        data.num_steps=80000 \\\n        domain_and_task.source_data_type=rand \\\n        data.train_replay_id=1 \\\n        data.data_dir_prefix=data \\\n        model.model_type=naive \\\n        domain_and_task.source_envs=\"{'walker': ['walk'], 'cheetah': ['run']}\" \\\n        output_dir=./outputs/pretrain_explore_subset\n```\n\n### Pretraining on multiple domains and tasks\n\nThe set of pretraining tasks can be specified in the config file as shown below:\n\n- Pretrain with offline data collected by exploratory policies\n\n```\npython src/dmc_pretrain.py base=configs/pretrain.yaml \\\n        epochs=10 \\\n        data.num_steps=80000 \\\n        data.train_replay_id=5 \\\n        data.data_dir_prefix=data \\\n        model.model_type=naive \\\n        domain_and_task.source_data_type=full \\\n        domain_and_task.source_envs=\"{'walker': ['walk'], 'cheetah': ['run']}\" \\\n        output_dir=./outputs/pretrain_explore\n```\n\n- Pretrain with offline data collected by random policies\n\n```\npython src/dmc_pretrain.py base=configs/pretrain.yaml \\\n        epochs=10 \\\n        data.num_steps=80000 \\\n        data.train_replay_id=5 \\\n        data.data_dir_prefix=data \\\n        model.model_type=naive \\\n        domain_and_task.source_data_type=rand \\\n        domain_and_task.source_envs=\"{'walker': ['walk'], 'cheetah': ['run']}\" \\\n        output_dir=./outputs/pretrain_random\n```\n\n### Using pretrained model and finetunes the policy on a specific downstream task:\n\nYou can also download our pretrained models as reported in the paper, using the `azcopy` command in the previous section.\n\n```\n## set the downstream domain and task\nDOMAIN=cheetah\nTASK=run\n\n## behavior cloning as the learning algorithm\npython src/dmc_downstream.py base=configs/downstream.yaml \\\n        epochs=30 \\\n        data.num_steps=1000000 \\\n        domain_and_task.domain=${DOMAIN} \\\n        domain_and_task.task=${TASK} \\\n        model.model_type=naive \\\n        no_load_action=True \\\n        load_model_from=./outputs/pretrain_explore/checkpoints/last.ckpt \\\n        output_dir=./outputs/${DOMAIN}_${TASK}_bc/\n\n## RTG-conditioned learning as the learning algorithm\npython src/dmc_downstream.py \\\n        epochs=30 \\\n        data.num_steps=1000000 \\\n        domain_and_task.domain=${DOMAIN} \\\n        domain_and_task.task=${TASK} \\\n        model.model_type=reward_conditioned\n        data.rand_select=False\n        no_load_action=True \\\n        load_model_from=./outputs/pretrain_explore/checkpoints/last.ckpt \\\n        output_dir=./outputs/${DOMAIN}_${TASK}_bc/\n```\n\nNote that if *--load_model_from* is not specified, the model is trained from scratch.\n", "repo_name": "smart", "org_name": "microsoft", "org_repo": "microsoft/smart", "platform_org_repo": "github+microsoft/smart", "link_to_repo": "https://github.com/microsoft/smart", "platform": "github", "language": "Python", "stargazers_count": 28, "watchers_count": 28}, {"README_text": "![build-test-check](https://github.com/microsoft/android-samples/actions/workflows/build-test-check.yml/badge.svg)\n\n# Microsoft Android Samples\n\nThis repo contains samples that demonstrate how to use various Microsoft APIs in Android apps.\n\nPlease read the [code of conduct](CODE_OF_CONDUCT.md) and [contribution guidelines](CONTRIBUTING.md).\n\n## Getting Started\n\nWhen importing the sample code into Android Studio, use the specific project folder (ex: **TwoNote**) as the base directory of the project.\n\n## Samples\n\n[TwoNote](/TwoNote/): A note taking app that uses the [Microsoft Graph](https://learn.microsoft.com/graph/overview) to sync a user's notes to OneNote.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "android-samples", "org_name": "microsoft", "org_repo": "microsoft/android-samples", "platform_org_repo": "github+microsoft/android-samples", "link_to_repo": "https://github.com/microsoft/android-samples", "platform": "github", "language": "Kotlin", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# dbt-fabric\n\n[dbt](https://www.getdbt.com) adapter for Microsoft Fabric Synapse Data Warehouse.\n\nThe adapter supports dbt-core 1.4 or newer and follows the same versioning scheme.\nE.g. version 1.1.x of the adapter will be compatible with dbt-core 1.1.x.\n\n## Documentation\n\nWe've bundled all documentation on the dbt docs site:\nTODO\n* [Profile setup & authentication](https://docs-getdbt-com.netlify.app/reference/warehouse-setups/fabric-setup)\n* [Adapter documentation, usage and important notes](https://docs-getdbt-com.netlify.app/reference/resource-configs/fabric-configs)\n\n## Installation\n\nThis adapter requires the Microsoft ODBC driver to be installed:\n[Windows](https://docs.microsoft.com/nl-be/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver16#download-for-windows) |\n[macOS](https://docs.microsoft.com/nl-be/sql/connect/odbc/linux-mac/install-microsoft-odbc-driver-sql-server-macos?view=sql-server-ver16) |\n[Linux](https://docs.microsoft.com/nl-be/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16)\n\n<details><summary>Debian/Ubuntu</summary>\n<p>\n\nMake sure to install the ODBC headers as well as the driver linked above:\n\n```shell\nsudo apt-get install -y unixodbc-dev\n```\n\n</p>\n</details>\n\nLatest version: ![PyPI](https://img.shields.io/pypi/v/dbt-fabric?label=latest&logo=pypi)\n\n```shell\npip install -U dbt-fabric\n```\n\n## Changelog\n\nSee [the changelog](CHANGELOG.md)\n\n## Contributing\n\n[![Unit tests](https://github.com/microsoft/dbt-fabric/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/microsoft/dbt-fabric/actions/workflows/unit-tests.yml)\n[![Integration tests on Azure](https://github.com/microsoft/dbt-fabric/actions/workflows/integration-tests-azure.yml/badge.svg)](https://github.com/microsoft/dbt-fabric/actions/workflows/integration-tests-azure.yml)\n[![Publish Docker images for CI/CD](https://github.com/microsoft/dbt-fabric/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/microsoft/dbt-fabric/actions/workflows/publish-docker.yml)\n\nThis adapter is Microsoft-maintained.\nYou are welcome to contribute by creating issues, opening or reviewing pull requests.\nIf you're unsure how to get started, check out our [contributing guide](CONTRIBUTING.md).\n\n## License\n\n[![PyPI - License](https://img.shields.io/pypi/l/dbt-fabric)](https://github.com/microsoft/dbt-fabric/blob/main/LICENSE)\n\n## Code of Conduct\n\nThis project and everyone involved is expected to follow the [Microsoft Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n", "repo_name": "dbt-fabric", "org_name": "microsoft", "org_repo": "microsoft/dbt-fabric", "platform_org_repo": "github+microsoft/dbt-fabric", "link_to_repo": "https://github.com/microsoft/dbt-fabric", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Basin\nA specification and libraries to stream updates to an object using JSONPaths.\nWe use the term \"basin\" because streams flow into a basin.\n\nSee in [npmjs.com](https://www.npmjs.com/package/object-basin).\n\nSee in [NuGet](https://www.nuget.org/packages/ObjectBasin).\n\nThis library supports various ways to update objects or their contents:\n* appending to the end of a string\n* inserting anywhere in a string\n* removing characters from a string\n* appending to the end of a list\n* inserting anywhere in a list\n* overwriting an item in a list\n* deleting items in a list\n* setting a value in an object\n\n# JavaScript / Node.js\nSee examples and installation instructions in the [js folder](js/).\n\n# .NET / C#\nSee examples and installation instructions in the [dotnet folder](dotnet/).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "object-basin", "org_name": "microsoft", "org_repo": "microsoft/object-basin", "platform_org_repo": "github+microsoft/object-basin", "link_to_repo": "https://github.com/microsoft/object-basin", "platform": "github", "language": "C#", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Dynamics 365 Business Central & Dataverse integration\n\nThis repo is being prepared as our one-stop-shop for Business Central & Dataverse integration that includes all types of app/system interactions, such as Data Sync that replicates data between Business Central & Dataverse, virtual tables on Dataverse via Business Central API for (**C**reate/**R**ead/**U**pdate/**D**elete) operations, data change (**CUD**) events, and business events.\n\nIn here, our partners/customers can discuss existing features, preview new ones, try sample code, and submit their feedbacks/questions/issues/ideas, while we can follow up, prioritize, and take them into our roadmap.\n\nIntegrating w/ Dataverse enables Business Central to interact w/ other apps in its ecosystem, see [a review of Business Central & Dataverse integration](https://github.com/microsoft/d365bcdv/blob/main/Review%20of%20Business%20Central%20and%20Dataverse%20integration.pdf).  To preview business events, see [Business Events on Business Central (Preview)](https://github.com/microsoft/d365bcdv/tree/main/samples/Business%20Events/Private%20Preview).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "d365bcdv", "org_name": "microsoft", "org_repo": "microsoft/d365bcdv", "platform_org_repo": "github+microsoft/d365bcdv", "link_to_repo": "https://github.com/microsoft/d365bcdv", "platform": "github", "language": "JavaScript", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Commercial Marketplace Offer Deployment Manager (MODM)\n\nThis marketplace offer deployment manager (MODM) simplifies the deployment of complex managed and packaged applications for the Azure Commercial Marketplace.\n\n\n## How it works\n\n- Create a `Controller` (some call it a driver--same thing) codebase that consumes the MODM client SDK\n- Build your controller binary into a Docker image along with your managed/package app templates\n- Create the createUIDefinition.json as usual, but now have the mainTemplate.json represent the deployment of your controller + MODM (See our starter bicep templates)\n- Create your marketplace package\n\n\n<img src=\"https://github.com/microsoft/commercial-marketplace-offer-deploy/blob/main/docs/img/modm-architecture.png?raw=true\" />\n\n\n## Feature Overview\n\n- Simplified deployment semantics\n- Includes deployment \"Stages\" that are tracked separate\n- Automatical retries of a Deployment and/or Stage\n- Dry Run operation support\n- Async operations built-in\n- Web Hook registration to receive only relevant deployment events\n- Client SDK (Go, C#, Python) \n\nA full description of each feature can be found in the [features](./docs/features.md) documentation.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n### Development Setup\n\n**Prerequisites**\n- Go Version: 1.18+\n- Docker Version: v4+\n- An Azure subscription\n- Azure CLI (latest)\n- IDE that works with Go\n- Ngrok ([Create a free account](https://ngrok.com/))\n* Setup a .env file in /bin (see ./configs for the template)\n\n**Developer activities**\n\n- [Building the Docker image](./docs/docker-image.md)\n- [Running locally](./docs/run-locally.md)\n- [Client SDK usage (Go)](./docs/sdk-usage-go.md)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Credits\n\n- [Ashwin Senthilkumar](https://github.com/ashsenth) (Contributor)\n- [Bob Jacobs](https://github.com/bobjac) (Author)\n- [Kevin M. Gates](https://github.com/kevinmgates) (Contributor)\n- [Kevin Hillinger](https://github.com/kevinhillinger) (Author)", "repo_name": "commercial-marketplace-offer-deploy", "org_name": "microsoft", "org_repo": "microsoft/commercial-marketplace-offer-deploy", "platform_org_repo": "github+microsoft/commercial-marketplace-offer-deploy", "link_to_repo": "https://github.com/microsoft/commercial-marketplace-offer-deploy", "platform": "github", "language": "Go", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Open Data Sheets\n\nThis framework aims to assists in the documentation of datasets to promote transparency and help dataset creators and consumers make informed decisions about whether specific datasets meet their needs and what limitations they need to consider\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "opendatasheets-framework", "org_name": "microsoft", "org_repo": "microsoft/opendatasheets-framework", "platform_org_repo": "github+microsoft/opendatasheets-framework", "link_to_repo": "https://github.com/microsoft/opendatasheets-framework", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Ringmaster\n\nRingmaster is a videoconferencing research platform open-sourced along with our paper\npublished at NSDI '23 \u2014 [Tambur: Efficient loss recovery for videoconferencing via streaming\ncodes](https://www.usenix.org/conference/nsdi23/presentation/rudow), where\nRingmaster serves as the basis for developing and benchmarking forward error\ncorrection (FEC) schemes in videoconferencing released [here](https://github.com/Thesys-lab/tambur).\n\nRingmaster is designed to be a readable and extensible replacement for WebRTC in videoconferencing\nresearch, with the goal of supporting more use cases in the future\n(e.g., congestion control, multiparty conferencing). [Contributions](#contributing) are welcome.\n\nSee [below](#emulating-a-video-call) for the basic usage of Ringmaster that emulates a 1:1\nvideo call. More [documentation](https://github.com/microsoft/ringmaster/wiki/Documentation) is in\nprogress. Please contact the owner [Francis Yan](https://francisyyan.org) with any questions for now.\n\n## Dependencies\n- Required environment: Ubuntu >=18.04\n- Install required packages\n   ```\n   sudo apt install autoconf libvpx-dev libsdl2-dev\n   ```\n\n## Building\nCompile Ringmaster with\n```\n./autogen.sh\n./configure\nmake -j\n```\n\n## Emulating a video call\nDownload a sample raw video\n[ice_4cif_30fps.y4m](https://media.xiph.org/video/derf/y4m/ice_4cif_30fps.y4m),\nwhich has a resolution of 704x576 and a frame rate of 30 fps.\n\nNext, go to `src/app` and execute the following commands in two terminals, respectively\n(run them without any arguments to see the usage):\n```\n./video_sender 12345 ice_4cif_30fps.y4m\n./video_receiver 127.0.0.1 12345 704 576 --fps 30 --cbr 500\n```\nThis emulates a 1:1 video call, where the caller compresses the raw video into VP9-encoded\nvideo frames with an average bitrate of 500 kbps and transmits the packetized frames to the\ncallee over UDP.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ringmaster", "org_name": "microsoft", "org_repo": "microsoft/ringmaster", "platform_org_repo": "github+microsoft/ringmaster", "link_to_repo": "https://github.com/microsoft/ringmaster", "platform": "github", "language": "C++", "stargazers_count": 32, "watchers_count": 32}, {"README_text": "# MSCCL scheduler\n\nMSCCL scheduler selects optimal MSCCL algorithms for MSCCL executors.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "msccl-scheduler", "org_name": "microsoft", "org_repo": "microsoft/msccl-scheduler", "platform_org_repo": "github+microsoft/msccl-scheduler", "link_to_repo": "https://github.com/microsoft/msccl-scheduler", "platform": "github", "language": "C++", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "HAMS-ALT", "org_name": "microsoft", "org_repo": "microsoft/HAMS-ALT", "platform_org_repo": "github+microsoft/HAMS-ALT", "link_to_repo": "https://github.com/microsoft/HAMS-ALT", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Semantic Kernel\n\n[![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n[![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)\n[![dotnet](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci.yml/badge.svg?branch=main)](https://github.com/microsoft/semantic-kernel/actions/workflows/dotnet-ci.yml)\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n> \u2139\ufe0f **NOTE**:This project is just like AI and will evolve quickly.\n> We invite you to join us in developing the Semantic Kernel together!\n> Please contribute by\n> using GitHub [Discussions](https://github.com/microsoft/semantic-kernel/discussions),\n> opening GitHub [Issues](https://github.com/microsoft/semantic-kernel/issues/new/choose),\n> sending us [PRs](https://github.com/microsoft/semantic-kernel/pulls),\n> joining our [Discord community](https://aka.ms/SKDiscord).\n\n**Semantic Kernel (SK)** is a lightweight SDK enabling integration of AI Large\nLanguage Models (LLMs) with conventional programming languages. The SK extensible\nprogramming model combines natural language **semantic functions**, traditional\ncode **native functions**, and **embeddings-based memory** unlocking new potential\nand adding value to applications with AI.\n\nSK supports\n[prompt templating](docs/PROMPT_TEMPLATE_LANGUAGE.md), function\nchaining,\n[vectorized memory](docs/EMBEDDINGS.md), and\n[intelligent planning](docs/PLANNERS.md)\ncapabilities out of the box.\n\nSemantic Kernel supports and encapsulates several design patterns from the latest\nin AI research, such that developers can infuse their applications with  [plugins](https://learn.microsoft.com/semantic-kernel/howto/) like [prompt](docs/PROMPT_TEMPLATE_LANGUAGE.md)\nchaining, recursive reasoning, summarization, zero/few-shot learning, contextual\nmemory, long-term memory, [embeddings](docs/EMBEDDINGS.md), semantic indexing,\n[planning](docs/PLANNERS.md), retrieval-augmented generation and accessing external\nknowledge stores as well as your own data.\n\nBy joining the SK community, you can build AI-first apps faster and have a front-row\npeek at how the SDK is being built. SK has been released as open-source so that more\npioneering developers can join us in crafting the future of this landmark moment\nin the history of computing.\n\n## Get Started with Semantic Kernel \u26a1\n\nSemantic Kernel is available to explore AI and build apps with C# and Python:\n\n<div style=\"display:flex;height:30px;padding:5px 0 5px 10px;\">\n<img src=\"https://user-images.githubusercontent.com/371009/230673036-fad1e8e6-5d48-49b1-a9c1-6f9834e0d165.png\" style=\"margin-right:12px\" height=\"30\"/>\n<a href=\"dotnet/README.md\">Using Semantic Kernel in C#</a>.\n</div>\n\n<div style=\"display:flex;height:30px;padding:5px 0 5px 10px;\">\n<img src=\"https://user-images.githubusercontent.com/371009/230673733-7a447d30-b48e-46e1-bd84-2b321c90649e.png\" style=\"margin-right:12px\" height=\"30\"/>\n<a href=\"python/README.md\">Using Semantic Kernel in Python</a>.\n</div>\n<br/>\n\nSee the [Feature Matrix](FEATURE_MATRIX.md) to see a breakdown of feature parity between C# and Python.\n\nThe quickest way to get started with the basics is to get an API key\n(OpenAI or Azure OpenAI)\nand to run one of the C# or Python console applications/scripts:\n\nFor C#:\n1. Create a new console app.\n2. Add the semantic kernel nuget `Microsoft.SemanticKernel`.\n3. Copy the code from [here](dotnet/README.md) into the app `Program.cs` file.\n4. Replace the configuration placeholders for API key and other params with your key and settings.\n5. Run with `F5` or `dotnet run`\n\nFor Python:\n1. Install the pip package: `python -m pip install semantic-kernel`.\n2. Create a new script e.g. `hello-world.py`.\n3. Store your API key and settings in an `.env` file as described [here](python/README.md).\n4. Copy the code from [here](python/README.md) into the `hello-world.py` script.\n5. Run the python script.\n\n\n## Sample apps \u26a1\n\nThe repository includes some sample applications, with a React frontend and\na backend web service using Semantic Kernel.\n\nFollow the links for more information and instructions about running these apps.\n\n|                                                                         |                                                                                                                                   |\n| ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n| [Simple chat summary](samples/apps/chat-summary-webapp-react/README.md) | Use ready-to-use plugins and get plugins into your app easily.                                                                |\n| [Book creator](samples/apps/book-creator-webapp-react/README.md)        | Use planner to deconstruct a complex goal and envision using the planner in your app.                                             |\n| [Authentication and APIs](samples/apps/auth-api-webapp-react/README.md) | Use a basic connector pattern to authenticate and connect to an API and imagine integrating external data into your app's LLM AI. |\n| [GitHub repository Q&A](samples/apps/github-qna-webapp-react/README.md) | Use embeddings and memory to store recent data and allow you to query against it.                                                 |\n| [Copilot Chat Sample App](samples/apps/copilot-chat-app/README.md)      | Build your own chat experience based on Semantic Kernel.                                                                          |\n\n**Requirements:**\n\n- You will need an\n  [Open AI API Key](https://openai.com/api/) or\n  [Azure Open AI service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=rest-api)\n  to get started.\n- [Azure Functions Core Tools](https://learn.microsoft.com/azure/azure-functions/functions-run-local)\n  are required to run the kernel as a local web service, used by the sample web apps.\n- [.NET 6 SDK](https://dotnet.microsoft.com/download/dotnet/6.0) or [.NET 7 SDK](https://dotnet.microsoft.com/download/dotnet/7.0)\n- [Yarn](https://yarnpkg.com/getting-started/install) is used for installing web apps' dependencies.\n\n## Deploy Semantic Kernel to Azure in a web app service \u2601\ufe0f\n\nGetting Semantic Kernel deployed to Azure as web app service is easy with one-click deployments. Click [here](https://aka.ms/sk-docs-azuredeploy) to learn more on how to deploy to Azure.\n\n## Jupyter Notebooks \u26a1\n\nFor a more hands-on overview, you can also check out the C# and Python Jupyter notebooks, starting\nfrom here:\n* [Getting Started with C# notebook](samples/notebooks/dotnet/00-getting-started.ipynb)\n* [Getting Started with Python notebook](samples/notebooks/python/00-getting-started.ipynb)\n\n**Requirements:** C# notebooks require [.NET 7](https://dotnet.microsoft.com/download)\nand the VS Code [Polyglot extension](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode).\n\n## Contributing and Community\n\nWe welcome your contributions and suggestions to SK community! One of the easiest\nways to participate is to engage in discussions in the GitHub repository.\nBug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with\nus before sending a PR. This is to avoid rejection as we might be taking the core\nin a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://github.com/microsoft/semantic-kernel/blob/main/CONTRIBUTING.md) to the project\n- Join the [Discord community](https://aka.ms/SKDiscord)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n", "repo_name": "semantic-kernel", "org_name": "microsoft", "org_repo": "microsoft/semantic-kernel", "platform_org_repo": "github+microsoft/semantic-kernel", "link_to_repo": "https://github.com/microsoft/semantic-kernel", "platform": "github", "language": "C#", "stargazers_count": 9975, "watchers_count": 9975}, {"README_text": "# Transparency Engine\n\nTransparency Engine aims to detect and communicate the implicit structure of complex activities in real-world problem areas, in ways that support both situational awareness and targeted action. A potential application is to identify and counter adversarial activities (e.g., corruption) hidden within large-scale datasets.\n\nGiven a collection of streaming data sources describing the attributes and actions of real-world entities, it uses a range of graph modeling, joint graph embedding [[1-3]](#references), and other statistical techniques to detect and explain the networks of entities most closely related to each entity. \n\nTo prioritize the expert review of these entity networks, entities can be linked to \"review flags\" that indicate the need for inspection. Review flags may be signs of either opportunities (\"green flags\") or risks (\"red flags\") thought to transfer between closely-related entities. The more review flags in an entity network, the higher the priority of that network for review.\n\nFor each entity in the dataset, Transparency Engine generates a narrative report illustrating the detected relationships and review flags contributing to its overall review priority. In typical use, review of high-priority reports will help inform real-world actions targeted at either the entity or its broader network. \n\nTransparency Engine consists of three components:\n- Pipeline: Python package that uses graph modeling techniques to detect networks of closely-related entities.\n- API: FastAPI interface that supports querying of the entity report using outputs produced by the entity relationship modeling pipeline.\n- Report: React-based application that enables viewing of the entity narrative report and exporting the report to a PDF file.\n\n\n## Getting Started\n\nTo run the pipeline locally, ensure that you have Docker installed and running on your machine. You can find instructions for installing Docker [here](https://docs.docker.com/engine/install/),\n\n### Pipeline\n\nTo configure the pipeline, specify the pipeline configuration using two json files:\n- Pipeline config: Specify steps to be executed in the pipeline. An example pipeline config can be found in `samples/config/pipeline.json`\n- Steps config: Specify configurations for each step of the pipeline. An example steps config can be found in `samples/config/steps.json`.\n\nTo launch the pipeline's Docker container, execute the following command from the root of the project:\n```bash\ndocker build Dockerfile -t transparency-engine\ndocker run -it transparency-engine\n```\n\nTo run the pipeline, once in Docker interactive mode, execute the following command from the `python/transparency-engine` folder:\n```bash\npoetry run python transparency_engine/main.py --config pipeline.json --steps steps.json\n```\n\nThe pipeline can also be launched in Visual Studio Code. To do so, open the project in Visual Studio Code and attach the project to a Docker container by following the instructions [here](https://code.visualstudio.com/docs/remote/containers).\n\nThe pipeline can also be packaged as a wheel file to be installed on Azure Synapse or Databricks. To create the wheel file, execute the following command from the `python/transparency-engine` folder:\n```bash\npoetry build\n```\n\n### API\n\nTo install the dependencies needed for the API, execute the following commands from the `python/api-backend` folder:\n```bash\npip install poetry\npoetry install\n```\n\nTo run the backend API execute from the root of the project:\n```bash\ndocker-compose up backend_api --build\n```\n\n### Report\n\nTo run the UI, you can either use `docker-compose` or install node and yarn and execute the following commands from the root of the project:\n```bash\nyarn\nyarn build\nyarn start # run the webapp locally\n```\nThe webapp will be available at http://localhost:3000\n\n## References\n1. Alexander Modell, Ian Gallagher, Joshua Cape, and Patrick Rubin-Delanchy. \"Spectral embedding and the latent geometry of multipartite networks.\" arXiv preprint arXiv:2202.03945 (2022).\n\n2. Nick Whiteley, Annie Gray, and Patrick Rubin-Delanchy. \"Matrix factorisation and the interpretation of geodesic distance.\" Advances in Neural Information Processing Systems 34 (2021): 24-38.\n\n3. Ian Gallagher, Andrew Jones, and Patrick Rubin-Delanchy. \"Spectral embedding for dynamic networks with stability guarantees.\" Advances in Neural Information Processing Systems 34 (2021): 10158-10170.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n", "repo_name": "transparency-engine", "org_name": "microsoft", "org_repo": "microsoft/transparency-engine", "platform_org_repo": "github+microsoft/transparency-engine", "link_to_repo": "https://github.com/microsoft/transparency-engine", "platform": "github", "language": "Python", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dataflow2text", "org_name": "microsoft", "org_repo": "microsoft/dataflow2text", "platform_org_repo": "github+microsoft/dataflow2text", "link_to_repo": "https://github.com/microsoft/dataflow2text", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# DeviceScript Development Gateway\n\nThis project contains a prototype development gateway implementation\nfor the built-in DeviceScript cloud integration.\n\nThe gateway can be run locally (no cloud dependencies), in GitHub CodeSpaces or deployed to Azure.\n\n-   [Read the documentation](https://microsoft.github.io/devicescript/developer/gateway)\n\n| :exclamation: This implementation is for prototyping only and not meant for production. |\n| --------------------------------------------------------------------------------------- |\n\n## Local development\n\n-   setup Node.JS 18\n\n```bash\nnvm install 18\nnvm use 18\n```\n\n-   download submodules and install dependencies\n\n```bash\nyarn setup\n```\n\n-   start a local instance using azurite\n\n```bash\nyarn dev\n```\n\nThe terminal output will provide the connection string to connect\nto to the gateway from the DeviceScript Visual Studio Code extension.\n\n-   **in GitHub Codespaces**, change the visibility of port `7071` to `Public`\n\nYou can also access the Swagger sandbox locally:\n\n-   after running head to http://127.0.0.1:7071/swagger/ or otherwise the live site\n-   Click Authorize\n-   Use user/password `devstoreaccount1:Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==`\n\n### Github Codespaces, CodeSandbox.io\n\nRunning the gateway in Codespaces/Codesandbox.io will give you an addressable web server that will be reachable by devices which the codespace\nis active. It is an easy to get a development gateway available on the web without having to deal with network issues.\nThe tools will automatically detect Codesandbox.io and self configure.\n\n-   Clone this repository or the fork into a GitHub Codespace to get started\n-   To open this repo in Codesandbox.io, https://codesandbox.io/p/github/microsoft/devicescript-gateway/main\n\n## Azure services\n\nMake sure to follow the provisioning steps in the documentation before trying to run locally.\n\n-   start a local instance using Azure services\n\n```\nyarn dev:azure\n```\n\n-   after running head to http://127.0.0.1:7071/swagger/ or otherwise the live site\n-   Click Authorize\n-   Use user/password from the `passwords` secret in the key vault\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "devicescript-gateway", "org_name": "microsoft", "org_repo": "microsoft/devicescript-gateway", "platform_org_repo": "github+microsoft/devicescript-gateway", "link_to_repo": "https://github.com/microsoft/devicescript-gateway", "platform": "github", "language": "TypeScript", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# WHO/WHDH Chart Data Text Summarization - Extracting Insights from Data for Visually Impaired\n\n[WHO's World Health Data Hub(WHDH)](https://data.who.int/home) provides an interactive digital platform and trusted source for global health data, fulfilling WHO\u2019s\u00a0commitment\u00a0to provide health data as a public good. The platform brings together an ambitious product stack to deliver an end-to-end solution for WHO data processes. From collection to use the platform provides a world class experience leveraging innovative technology to address challenges and pain points.\n\n![WHO WHDH](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/blob/main/Images/WHO%20WHDH%20Screen%20Shot.png)\n\n<br />\n<br />\n<br />\n\n## The Challenge\n\nThe data in the WHDH is provided as primarily as reports with visualizations although there are additionally API endpoints to access the data directly.  The goal of this challenge was to use OpenAI to provide text and audio summarizations of PowerBI or other visualizations, for visually impaired or mobile data users.  A stretch goal was to allow the users to ask questions of the data in a more ad-hoc manner and have the analysis read back.  \n\n<br />\n<br />\n<br />\n\n## The Solution\n\nOur team built two different solutions, one based on Python and the other on C#.  We focused on the \u201csummarization engine\u201d portion of the project and built solutions to record customer data requests, ingest data, summarize key points with GTP and read this back to the user.\n\n<br />\n\n![Architecture](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/blob/main/Images/architecture%20screen%20shot.png)\n\n<br />\n<br />\n<br />\n\n## The Impact\n\nAt a very high level this is a tool to replace visualizations for PowerBI with text summaries enabling visually impaired and mobile users.  This extends further as users are also able to simply ask additional questions of the data.\n\nAs this code is easily called by serverless services such as Azure Functions or Logic Apps, our code is very extensible and easily embedded behind web apps or chat bots or by referencing voice captured through telephony solutions.  While our solutions would both require some modification by customers prior to implementation, this technological lift should be small.\n\nThe output however is a fully accessible replacement to PowerBI visualizations and \"Ask a question of the data\" functionality for both visually impaired as well as mobile or remote scenarios.\n\n<br />\n<br />\n<br />\n\n## How to Replicate the solution\n\nThis use case not unique to nonprofit or the WHO.  These ML models will ingest data and allow for text summarization and user Q&A functionality regardless of industry.  There are certain limitations due to data size, however our data scientists did provide recommendations and sample code for extensions.  This will be further discussed in the technical details below.\n\n<br />\n<br />\n<br />\n\n## Demos\n\n<br />\n\n![Python Demo](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/blob/main/Demos/Python%20Demo%20Final.mp4)\n\n<br />\n\n![C# Demo](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/blob/main/Demos/C%23%20Demo%20Final.mov)\n\n<br />\n\n## Cost Considerations\n\nFor both solutions, cost for our customers was a consideration.\n\nThe Python solution is currently using the following technology.\n\n    Whisperlite Python Library - this is $0.006/minute\n    Azure Function - to execute the Python code\n    Azure Storage - to facilitate data movement\n    Azure App Services - This could be used to host a front end web app.  However this is not a requirement.\n    Azure OpenAI Services\n\nThe C# solution is using the following.\n\n    Azure Cognitive Services - This is currently on free tier pricing.\n    Azure Function or Logic Apps - To execute the C#\n    Azure Storage - To facilitate data movement\n    Azure App Services - This could be used to host a front end web app.  However this is not a requirement.\n    Azure OpenAI Services\n \n Most of these services have a free tier, so development of this technology should be relatively cheap for customers.  Once this solution is in production, the cost of all of these services are based on the number of calls to the system.  Most of the calls to cognitive services are just a few cents per API call, so the cost of Azure would likely be derived more from the Azure infrastructure to support integration between this summarization engine and the front end appliation.\n \n  \n<br />\n<br />\n<br />\n\n## Technical Details\n\nTechnical deals of each individual solution are contained in the readme files in the folders [Python](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/tree/main/Python) and [C#](https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian/tree/main/C%23).\n\n<br />\n\n### Technical Challenges\n\nThe biggest technical challange with this solution was in using OpenAI services for the generation of the data summary.  GPT is great at document summaries and great at language and conversation understanding, however most of this data is column data in the simplest form.  \n\nIn our initial testing, we were simiply asking for a summary or asking specific questions of the data, and specifying the WHO URL for the intended data set.  Surprisingly this generated excellent outputs.  Unfortunately when we examined the numerical output, the values did not corrospond to the current data in the visualizatoins.  Upon review of the models, it would appear that the answers were being genereated from GPT itself, and that either it had been trained on this WHO data or on data that contained similar infomration and that the data was simply old.  The result being that we could use GPT directly if we did not need 100% factual or more likely, up to date data.\n\nOur goal then became to force the models to use the document provided and not the pretrained data in the model itself.  We attempted this in a few different ways.  We experimented with taking each row of columns and turning it into a sentence, and then passing that into the model as a parameter.  We also tried a couple different approaches of passing the values directly in as parameters.\n\nWe had success with each of these methods but with the result of limiting the overall data set sizes that can be utilized in this way.  Essentially you will run out of tokens to pass data and will be limited in the number of rows.  This limitation will be addressed in the future enhancements for this project.\n<br />\n\n### Future Enhancements\n\nOur team did explore different solutions to this token and data limitation of GPT.  One solution would be allowing the model to be trained on the external data sets. Likely this would require additional data preparation such as creating sentences out of column data. \n\nThe other option would be to pre-analyze the data.  In the future enhancements section of this repository, we have included a notebook that would take a numerical data set and extract \"interesting\" features.  These extracted features would then be useful to provide as parameters to the GPT calls and facilitate data set summarization to fulfill the requirements of this challange with larger data sets.  \n\nThe drawback to this method is that \"interesting features\" of the data may be different depending on the type of data being analyzed.  So additional data science work may be required to provide accurate preprocessing for summarization.\n<br />\n\nAdditionally to assist with data engineering and data prep, we did include a sample data pipeline that can extract data from API endpoint and format this in suitable CSV format for preprocessing and tokenization for GTP processing of smaller datasets.\n\n<br />\n<br />\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "OpenAI-Hack-Group-5-Humanitarian", "org_name": "microsoft", "org_repo": "microsoft/OpenAI-Hack-Group-5-Humanitarian", "platform_org_repo": "github+microsoft/OpenAI-Hack-Group-5-Humanitarian", "link_to_repo": "https://github.com/microsoft/OpenAI-Hack-Group-5-Humanitarian", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "\n# Clifford Layers\n\n[![Documentation](https://img.shields.io/badge/docs-passing-brightgreen)](https://microsoft.github.io/cliffordlayers)\n\nFor details about usage please see [documentation](https://microsoft.github.io/cliffordlayers).\nIf you have any questions or suggestions please open a [discussion](https://github.com/microsoft/cliffordlayers/discussions). If you notice a bug, please open an [issue](https://github.com/microsoft/cliffordlayers/issues).\n\n## Installation\n\n```bash\npip install cliffordlayers\n```\n\n\n## Citation\n\nIf you find our work and/or our code useful, please cite us via:\n\n```bibtex\n@article{brandstetter2022clifford,\n  title={Clifford Neural Layers for PDE Modeling},\n  author={Brandstetter, Johannes and Berg, Rianne van den and Welling, Max and Gupta, Jayesh K},\n  journal={arXiv preprint arXiv:2209.04934},\n  year={2022}\n}\n\n@article{ruhe2023geometric,\n  title={Geometric Clifford Algebra Networks},\n  author={Ruhe, David and Gupta, Jayesh K and de Keninck, Steven and Welling, Max and Brandstetter, Johannes},\n  journal={arXiv preprint arXiv:2302.06594},\n  year={2023}\n}\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cliffordlayers", "org_name": "microsoft", "org_repo": "microsoft/cliffordlayers", "platform_org_repo": "github+microsoft/cliffordlayers", "link_to_repo": "https://github.com/microsoft/cliffordlayers", "platform": "github", "language": "Python", "stargazers_count": 80, "watchers_count": 80}, {"README_text": "# Fallible allocation functions for Vec\n\nFallible allocation functions for the Rust standard library's [`alloc::vec::Vec`](https://doc.rust-lang.org/std/vec/struct.Vec.html) type.\n\nThese functions are designed to be usable with `#![no_std]`, `#[cfg(no_global_oom_handling)]` (see\n<https://github.com/rust-lang/rust/pull/84266>) enabled and Allocators (see <https://github.com/rust-lang/wg-allocators>).\n\nBy default this crate requires the nightly compiler, but the stable compiler can be used if all\nfeatures are disabled (i.e., specifying [`default-features = false` for the dependency](https://doc.rust-lang.org/cargo/reference/features.html#the-default-feature)).\n\n## Usage\n\nThe recommended way to add these functions to `Vec` is by adding a `use` declaration for the\n`FallibleVec` trait: `use fallible_vec::FallibleVec`:\n```rust\nuse fallible_vec::{FallibleVec, try_vec};\n\nlet mut vec = try_vec![1, 2]?;\nvec.try_push(3)?;\nassert_eq!(vec, [1, 2, 3]);\n```\n\n## Panic Safety\n\nThese methods are \"panic safe\", meaning that if a call to external code (e.g., an iterator's\n`next()` method or an implementation of `Clone::clone()`) panics, then these methods will leave the\n`Vec` in a consistent state:\n* `len()` will be less than or equal to `capacity()`.\n* Items in `0..len()` will only be items originally in the `Vec` or items being added to the `Vec`.\n  It will never include uninitialized memory, duplicated items or dropped items.\n* Items originally (but no longer) in the `Vec` or being added to (but not yet in) the `Vec` may be\n  leaked - any method that may leak items like this will have a note to specify its behavior.\n\nThe exact behavior of each method is specified in its documentation.\n\n## Code origin\n\nMost of this code is forked from [Rust's Standard Library](https://github.com/rust-lang/rust). While\nwe will attempt to keep the code and docs in sync, if you notice any issues please check if they\nhave been fixed in the Standard Library first.\n\n## This API is incomplete\n\nThere are many more infallible functions on `Vec` which have not been ported yet. If there's a\nparticular API that you're missing feel free to open a PR or file an Issue to get it added.\n\n## Why are these not already in the Standard Library?\n\nThere is a [PR to add these and more](https://github.com/rust-lang/rust/pull/95051) to the Standard\nLibrary, followed by an [RFC to discuss if it's a good idea or not to do so](https://github.com/rust-lang/rfcs/pull/3271).\n\n## Why would I use this crate versus similar crates?\n\nIn general, `fallible_vec` is only useful in situations where `#[cfg(no_global_oom_handling)]` is\nrequired, or if using the Allocator API (functions ending in `_in`). Other crates use APIs that\ndon't exist when `#[cfg(no_global_oom_handling)]` is enabled (like `vec::push`), whereas\n`fallible_vec` reimplements each function to avoid these APIs and builds with `#[cfg(no_global_oom_handling)]`\nin its CI.\n\n`fallible_vec` focuses on `vec` alone, whereas other crates provide support for additional types\n(like `Box` and `HashMap`).\n\nComparing `fallible_vec` to [`fallible_collections`](https://crates.io/crates/fallible_collections):\n\n|                                           | `fallible_vec` v0.3.1 | `fallible_collections` v0.4.7 |\n|-------------------------------------------|:---------------------:|:-----------------------------:|\n| Supports `no_std`                         | X                     | X                             |\n| Supports `#[cfg(no_global_oom_handling)]` | X                     |                               |\n| Requires nightly rust compiler by default | X                     |                               |\n| Supports stable rust compiler             | X                     | X                             |\n| `vec::try_append`                         |                       | X                             |\n| `vec::try_extend`                         | X                     |                               |\n| `vec::try_extend_from_slice`              | X                     | X                             |\n| `vec::try_insert`                         | X                     | X                             |\n| `vec::try_push`                           | X                     | X                             |\n| `vec::try_push_give_back`                 |                       | X                             |\n| `vec::try_resize`                         | X                     | X                             |\n| `vec::try_resize_with`                    | X                     | X                             |\n| `vec::try_splice_in`                      | X                     |                               |\n| `try_collect`                             | X                     | X                             |\n| `try_collect_in`                          | X                     |                               |\n| `try_from_iterator`                       |                       | X                             |\n| `try_with_capacity`                       | X                     |                               |\n| `try_with_capacity_in`                    | X                     |                               |\n| `try_vec!`                                | X                     |                               |\n| `try_vec_in!`                             | X                     |                               |\n| `Box::*`                                  |                       | X                             |\n| `Arc::*`                                  |                       | X                             |\n| `Rc::*`                                   |                       | X                             |\n| `HashMap::*`                              |                       | X                             |\n| `try_format!`                             |                       | X                             |\n\n## Building locally\n\nThe recommended way to build locally is to use the `build.ps1` script: this will build the crate\nusing all feature combinations, run tests, check formatting, run clippy and build with `#[cfg(no_global_oom_handling)]`\nenabled.\n\nIn order to run this script you'll need:\n* [PowerShell 7+](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell)\n* [Rust](https://rustup.rs/)\n  * Including the [`rust-src` component](https://rust-lang.github.io/rustup/concepts/components.html).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "rust_fallible_vec", "org_name": "microsoft", "org_repo": "microsoft/rust_fallible_vec", "platform_org_repo": "github+microsoft/rust_fallible_vec", "link_to_repo": "https://github.com/microsoft/rust_fallible_vec", "platform": "github", "language": "Rust", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Node API for .NET: JavaScript + .NET Interop\n\nThis project enables advanced interoperability between .NET and JavaScript in the same process.\n\n - Load .NET assemblies and call .NET APIs in-proc from a JavaScript application.\n - Load JavaScript packages call JS APIs in-proc from a .NET application.\n\nInterop is high-performance and supports TypeScript type-definitions generation, async\n(tasks/promises), streams, and more. It uses [Node API](https://nodejs.org/api/n-api.html) so\nit is compatible with any Node.js version (without recompiling) or other JavaScript runtime that\nsupports Node API.\n\n:warning: _**Status: In Development** - Core functionality works, but many things are incomplete,\nand it isn't yet all packaged up nicely in a way that can be easily consumed._\n\n[Instructions for getting started are below.](#getting-started)\n\n### Minimal example - JS calling .NET\n```JavaScript\n// JavaScript\nconst Console = require('node-api-dotnet').System.Console;\nConsole.WriteLine('Hello from .NET!');\n```\n\n### Minimal example - .NET calling JS\n```C#\n// C#\ninterface IConsole { void Log(string message); }\n\nvar nodejs = new NodejsPlatform(libnodePath).CreateEnvironment();\nnodejs.Run(() => {\n    var console = nodejs.Import<IConsole>(\"global\", \"console\");\n    console.Log(\"Hello from JS!\");\n});\n```\n\nFor more examples, see the [examples](./examples/) directory.\n\n## Feature Highlights\n - [Load and call .NET assemblies from JS](#load-and-call-net-assemblies-from-js)\n - [Load and call JavaScript packages from .NET](#load-and-call-javascript-packages-from-net)\n - [Generate TS type definitions for .NET APIs](#generate-ts-type-definitions-for-net-apis)\n - [Full async support](#full-async-support)\n - [Error propagation](#error-propagation)\n - [Develop Node.js addons with C#](#develop-nodejs-addons-with-c)\n - [Optionally work directly with JS types in C#](#optionally-work-directly-with-js-types-in-c)\n - [Automatic efficient marshaling](#automatic-efficient-marshaling)\n - [Stream across .NET and JS](#stream-across-net-and-js)\n - [Optional .NET native AOT compilation](#optional-net-native-aot-compilation)\n - [High performance](#high-performance)\n\n### Load and call .NET assemblies from JS\nThe `node-api-dotnet` package manages hosting the .NET runtime in the JS process\n(if not using AOT - see below). The .NET core library types are available immediately;\nadditional .NET assemblies can be loaded by file path:\n```JavaScript\n// JavaScript\nconst dotnet = require('node-api-dotnet');\ndotnet.load('path/to/ExampleAssembly.dll');\nconst exampleObj = new dotnet.ExampleNamespace.ExampleClass(...args);\n```\n\nAll .NET namespaces are projected onto the top-level `dotnet` object. When loading multiple .NET\nassemblies, types from all assemblies are merged into the same namespace hierarchy.\n\n### Load and call JavaScript packages from .NET\nCalling JavaScript from .NET requires hosting a JS runtime such as Node.js in the .NET app.\nThen JS packages can be imported either directly as JS values or by declaring C# interfaces for\nthe JS types and using automatic marshalling.\n\nAll interaction with a JavaScript environment must be from its thread, via the\n`Run()`, `RunAsync()`, or `Post()` methods on the JS environment object.\n```C#\n// C#\ninterface IExample\n{\n    void ExampleMethod();\n}\n\nvar nodejsPlatform = new NodejsPlatform(libnodePath);\nvar nodejs = nodejsPlatform.CreateEnvironment();\n\nnodejs.Run(() => {\n    // Import a module property, then call a function on it.\n    var example1 = nodejs.Import(\"example-npm-package\", \"ExampleObject\");\n    example1.CallMethod(\"exampleMethod\");\n\n    // Import the module property using an interface, and call the same function.\n    var example2 = nodejs.Import<IExample>(\"example-npm-package\", \"ExampleObject\");\n    example2.ExampleMethod();\n});\n```\n\nIn the future, it may be possible to automatically generate .NET API definitions from TypeScript\ntype definitions.\n\n### Generate TS type definitions for .NET APIs\nIf writing TypeScript, or type-checked JavaScript, there is a tool to generate type `.d.ts` type\ndefinitions for .NET APIs. It also generates a small `.js` file that makes it simple to load the\nassembly DLL and type-definitions together.\n```bash\n$ npm exec node-api-dotnet-generator --assembly ExampleAssembly.dll --typedefs ExampleAssembly.d.ts\n```\n```TypeScript\n// TypeScript\nimport './ExampleAssembly.js'; // TS also loads the adjacent .d.ts file.\ndotnet.ExampleNamespace.ExampleClass.ExampleMethod(...args); // This call is type-checked!\n```\n\n(CommonJS modules must use `require()` instead of `import`.)\n\nFor reference, there is a [list of C# type projections to TypeScript](/Docs/typescript.md).\n\n### Full async support\nJavaScript code can `await` a call to a .NET method that returns a `Task`. The marshaller\nautomatically sets up a `SynchronizationContext` so that the .NET result is returned back to the\nJS thread.\n```TypeScript\n// TypeScript\nimport { ExampleClass } from './ExampleAssembly';\nconst asyncResult = await ExampleClass.GetSomethingAsync(...args);\n```\n.NET `Task`s are seamlessly marshaled to & from JS `Promise`s. So JS code can work naturally with\na `Promise` returned from a .NET async method, and a JS `Promise` passed to .NET becomes a\n`JSPromise` that can be `await`ed in the C# code.\n\n### Error propagation\nExceptions/errors thrown in .NET or JS are propagated across the boundary with stack traces.\nAn unhandled .NET exception is thrown back to a JS caller as an `Error` with a stack trace that\nincludes both .NET and JS frames, and source line numbers if symbols are available. For example:\n```\nError: Test error thrown by JS.\n    at Microsoft.JavaScript.NodeApi.TestCases.Errors.ThrowDotnetError(String message) in D:\\node-api-dotnet\\test\\TestCases\\Errors.cs:line 13\n    at Microsoft.JavaScript.NodeApi.Generated.Module.Errors_ThrowDotnetError(JSCallbackArgs __args) in napi-dotnet.NodeApi.g.cs:line 357\n    at Microsoft.JavaScript.NodeApi.JSNativeApi.InvokeCallback[TDescriptor](napi_env env, napi_callback_info callbackInfo, JSValueScopeType scopeType, Func`2 getCallbackDescriptor) in JSNativeApi.cs:line 1070\n    at catchDotnetError (D:\\node-api-dotnet\\test\\TestCases\\errors.js:14:12)\n    at Object.<anonymous> (D:\\node-api-dotnet\\test\\TestCases\\errors.js:41:1)\n```\nSimilarly, an unhandled JS `Error` is thrown back to a .NET caller as a `JSException` with a\ncombined stack trace.\n\n### Develop Node.js addons with C#\nA C# class library project can use the `[JSExport]` attribute to tag (and rename) APIs that are\nexported when the library is built as a JavaScript module. A [C# Source Generator](\nhttps://learn.microsoft.com/en-us/dotnet/csharp/roslyn-sdk/source-generators-overview) runs as\npart of the compilation and generates code to export the tagged APIs and marshal values between\nJavaScript and C#.\n\n```C#\n// C#\n[JSExport] // Export class and all public members to JS.\npublic class ExampleClass { ... }\n\npublic static class ExampleStaticClass\n{\n    [JSExport(\"exampleFunction\")] // Export as a module-level function.\n    public static string StaticMethod(ExampleClass obj) { ... }\n\n    // (Other public members in this class are not exported by default.)\n}\n```\n\nThe `[JSExport]` source generator enables faster startup time because the marshaling code is\ngenerated at build time rather than dynamically emitted at runtime (as when calling a pre-built\nassembly). The source generator also enables building ahead-of-time compiled libraries in C# that\ncan be called by JavaScript without depending on the .NET Runtime. (More on that below.)\n\n### Optionally work directly with JS types in C#\nThe class library includes an object model for the JavaScript type system. `JSValue` represents a\nvalue of any type, and there are more types like `JSObject`, `JSArray`, `JSMap`, `JSPromise`, etc.\nC# code can work directly with those types if desired:\n\n```C#\n// C#\n[JSExport]\npublic static JSPromise JSAsyncExample(JSValue input)\n{\n    // Example of integration between C# async/await and JS promises.\n    string greeter = (string)input;\n    return new JSPromise(async (resolve) =>\n    {\n        await Task.Delay(50);\n        resolve((JSValue)$\"Hey {greeter}!\");\n    });\n}\n```\n\n### Automatic efficient marshaling\nThere are two ways to get automatic marshaling between C# and JavaScript types:\n  1. Compile a C# class library with `[JSExport]` attributes like the examples above. The source\n  generator produces marshaling code that is compiled with the assembly.\n\n  2. Load a pre-built .NET assembly, as in the earlier examples. The loader will use reflection to\n  scan the APIs, then emit marshaling code on-demand for each type that is referenced by JS. The\n  dynamic marshalling code is derived from the same expression trees that are used for compile-time\n  source-generation, but is generated and at runtime and compiled with\n  [`LambdaExpression.Compile()`](https://learn.microsoft.com/en-us/dotnet/api/system.linq.expressions.lambdaexpression.compile).\n  So there is a small startup cost from that reflection and compilation, but subsequent calls to\n  the same APIs may be just as fast as the pre-compiled marshaling code (and are just as likely\n  to be JITted).\n\nThe marshaller uses the strong typing information from the C# API declarations as hints about how to\nconvert values beteen JavaScript and C#. Here's a general summary of conversions:\n  - Primitives (numbers, strings, etc.) are passed by value directy.\n  - C# structs have all properties passed by value (shallow copied).\n  - C# classes are passed by reference. Any JS call to a C# class or interface property or method\n  gets proxied over to the C# instance of the class. (Object GC lifetimes are synchronized\n  accordingly.)\n  - JS code may implement a C# interface, and pass that implementation back to C# code where it\n  becomes a proxy that C# code can use.\n  - C# collections like `IList<T>` and JS collections like `Map<T>` are also passed by reference;\n  access to collection elements is proxied to whichever side the real instance of the collection\n  is on.\n  - JS `TypedArray`s are mapped to C# `Memory<T>` and passed by reference using shared memory\n  (no proxying is needed).\n  - Other types like enums, dates, and delegates are automatically marshaled as one would expect.\n  - Custom marshaling and marshaling hints [may be supported later](\n    https://github.com/jasongin/napi-dotnet/pull/25).\n\n### Stream across .NET and JS\n.NET `Stream`s are automatically marshalled to and from Node.js `Duplex` (or `Readable` or\n`Writable`) streams. That means JS code can seamlessly read from or write to streams created\nby .NET. Or .NET code can read from or write to streams created by JS. Streamed data is\ntransferred using shared memory (without any additional sockets or pipes), so memory allocation\nand copying is minimized.\n\n### Optional .NET native AOT compilation\nThis library supports hosting the .NET Runtime in the same process as the JavaScript runtime.\nAlternatively, it also supports building [native ahead-of-time (AOT) compiled C#](\nhttps://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/) libraries that are\nloadable as a JavaScript module _without depending on the .NET Runtime_.\n\nThere are advantages and disadvantages to either approach:\n|                     | .NET Runtime | .NET Native AOT |\n|---------------------|--------------|-----------------|\n| API compatibility   | Broad compatibility with .NET APIs | Limited compatibility with APIs designed to support AOT |\n| Ease of deployment  | Requires a matching version of .NET to be installed on the target system | A .NET installation is not required (though some platform libs may be required on Linux/Mac)\n| Size of deployment  | Compact - only IL assemblies need to be deployed | Larger due to bundling necessary runtime code - minimum ~3 MB per platform |\n| Performance         | Slightly slower startup (JIT) | Slightly faster startup (no JIT) |\n| Runtime limitations | Full .NET functionality | Some .NET features like reflection and code-generation aren't supported |\n\n### High performance\nThe project is designed to be as performant as possible when bridging between .NET and JavaScript.\nTechniques benefitting performance include:\n - Automatic marshaling avoids any use of JSON serialization, and uses generated code to avoid\n reflection.\n - Automatic marshalling uses shared memory or proxies when possible to minimize the amount of\n data transferred across the boundary.\n - Simple calls between JS and C# require **_almost_** zero memory allocation. (Maybe it will be\n zero eventually.)\n - Most JavaScript values are represented in C# as small structs (basically containing just a\n handle to the JS value), which helps avoid memory allocation.\n - Marshaling code uses modern C# performance features like `Span<T>` and `stackalloc` to minimize\n heap allocations and copying.\n\nThanks to these design choices, JS to .NET calls are [more than twice as fast](\n    https://github.com/jasongin/napi-dotnet/pull/23) when compared to `edge-js` using\n    [that project's benchmark](https://github.com/tjanczuk/edge/wiki/Performance).\n\n## Getting Started\n#### Requirements\n - .NET 6 or later\n    - .NET 7 or later is required for AOT support.\n    - .NET Framework 4.7.2 or later is supported at runtime,\n      but .NET 6 SDK is still required for building.\n - Node.js v16 or later\n    - Other JS runtimes may be supported in the future.\n - OS: Windows, Mac, or Linux\n    - It should work on any platform where .NET 6 is supported.\n\n#### Instructions\nFor calling .NET from JS, choose between one of the following scenarios:\n - [Dynamically invoke .NET APIs from JavaScript](./Docs/dynamic-invoke.md)<br/>\n   Dynamic invocation is simpler to set up: all you need is the `node-api-dotnet` npm package and\n   the path to a .NET assembly you want to call. But it has some limitations (not all kinds of APIs\n   are supported), and is not quite as fast as a C# module, because marshalling code must be\n   generated at runtime.\n - [Develop a Node module in C#](./Docs/node-module.md)<br/>\n   A C# Node module is appropriate for an application that has more advanced interop needs. It can\n   be faster because marshalling code can be generated at compile time, and the shape of the APIs\n   exposed to JavaScript can be adapted with JS interop in mind.\n\nFor calling JS from .NET, more documentation will be added soon. For now, see the\n[`winui-fluid` example code](./examples/winui-fluid/).\n\nGenerated TypeScript type definitions can be utilized with any of these aproaches.\n\n## Development\nFor information about building, testing, and contributing changes to this project, see\n[README-DEV.md](./README-DEV.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or\ncomments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of\nMicrosoft trademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion\nor imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those\nthird-party's policies.\n\n<br/>\n<br/>\n\n![.NET + JS scene](./Docs/images/dotnet-bot_scene_coffee-shop.png)\n", "repo_name": "node-api-dotnet", "org_name": "microsoft", "org_repo": "microsoft/node-api-dotnet", "platform_org_repo": "github+microsoft/node-api-dotnet", "link_to_repo": "https://github.com/microsoft/node-api-dotnet", "platform": "github", "language": "C#", "stargazers_count": 131, "watchers_count": 131}, {"README_text": "# TaskMatrix\n\n**TaskMatrix** connects ChatGPT and a series of Visual Foundation Models to enable **sending** and **receiving** images during chatting.\n\nSee our paper: [<font size=5>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models</font>](https://arxiv.org/abs/2303.04671)\n\n<a src=\"https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue\" href=\"https://huggingface.co/spaces/microsoft/visual_chatgpt\">\n    <img src=\"https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue\" alt=\"Open in Spaces\">\n</a>\n\n<a src=\"https://colab.research.google.com/assets/colab-badge.svg\" href=\"https://colab.research.google.com/drive/1P3jJqKEWEaeNcZg8fODbbWeQ3gxOHk2-?usp=sharing\">\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\">\n</a>\n\n## Updates:\n- Now TaskMatrix supports [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) and [segment-anything](https://github.com/facebookresearch/segment-anything)! Thanks **@jordddan** for his efforts. For the image editing case, `GroundingDINO` is first used to locate bounding boxes guided by given text, then `segment-anything` is used to generate the related mask, and finally stable diffusion inpainting is used to edit image based on the mask. \n    - Firstly, run `python visual_chatgpt.py --load \"Text2Box_cuda:0,Segmenting_cuda:0,Inpainting_cuda:0,ImageCaptioning_cuda:0\"`\n    - Then, say `find xxx in the image` or `segment xxx in the image`. `xxx` is an object. TaskMatrix will return the detection or segmentation result!\n\n\n- Now TaskMatrix can support Chinese! Thanks to **@Wang-Xiaodong1899** for his efforts.\n- We propose the **template** idea in TaskMatrix!\n    - A template is a **pre-defined execution flow** that assists ChatGPT in assembling complex tasks involving multiple foundation models. \n    - A template contains the **experiential solution** to complex tasks as determined by humans. \n    - A template can **invoke multiple foundation models** or even **establish a new ChatGPT session**\n    - To define a **template**, simply adding a class with attributes `template_model = True`\n- Thanks to **@ShengmingYin** and **@thebestannie** for providing a template example in `InfinityOutPainting` class (see the following gif)\n    - Firstly, run `python visual_chatgpt.py --load \"Inpainting_cuda:0,ImageCaptioning_cuda:0,VisualQuestionAnswering_cuda:0\"`\n    - Secondly, say `extend the image to 2048x1024` to TaskMatrix!\n    - By simply creating an `InfinityOutPainting` template, TaskMatrix can seamlessly extend images to any size through collaboration with existing `ImageCaptioning`, `Inpainting`, and `VisualQuestionAnswering` foundation models, **without the need for additional training**.\n- **TaskMatrix needs the effort of the community! We crave your contribution to add new and interesting features!**\n<img src=\"./assets/demo_inf.gif\" width=\"750\">\n\n\n## Insight & Goal:\nOn the one hand, **ChatGPT (or LLMs)** serves as a **general interface** that provides a broad and diverse understanding of a\nwide range of topics. On the other hand, **Foundation Models** serve as **domain experts** by providing deep knowledge in specific domains.\nBy leveraging **both general and deep knowledge**, we aim at building an AI that is capable of handling various tasks.\n\n\n## Demo \n<img src=\"./assets/demo_short.gif\" width=\"750\">\n\n##  System Architecture \n\n \n<p align=\"center\"><img src=\"./assets/figure.jpg\" alt=\"Logo\"></p>\n\n\n## Quick Start\n\n```\n# clone the repo\ngit clone https://github.com/microsoft/TaskMatrix.git\n\n# Go to directory\ncd visual-chatgpt\n\n# create a new environment\nconda create -n visgpt python=3.8\n\n# activate the new environment\nconda activate visgpt\n\n#  prepare the basic environments\npip install -r requirements.txt\npip install  git+https://github.com/IDEA-Research/GroundingDINO.git\npip install  git+https://github.com/facebookresearch/segment-anything.git\n\n# prepare your private OpenAI key (for Linux)\nexport OPENAI_API_KEY={Your_Private_Openai_Key}\n\n# prepare your private OpenAI key (for Windows)\nset OPENAI_API_KEY={Your_Private_Openai_Key}\n\n# Start TaskMatrix !\n# You can specify the GPU/CPU assignment by \"--load\", the parameter indicates which \n# Visual Foundation Model to use and where it will be loaded to\n# The model and device are separated by underline '_', the different models are separated by comma ','\n# The available Visual Foundation Models can be found in the following table\n# For example, if you want to load ImageCaptioning to cpu and Text2Image to cuda:0\n# You can use: \"ImageCaptioning_cpu,Text2Image_cuda:0\"\n\n# Advice for CPU Users\npython visual_chatgpt.py --load ImageCaptioning_cpu,Text2Image_cpu\n\n# Advice for 1 Tesla T4 15GB  (Google Colab)                       \npython visual_chatgpt.py --load \"ImageCaptioning_cuda:0,Text2Image_cuda:0\"\n                                \n# Advice for 4 Tesla V100 32GB                            \npython visual_chatgpt.py --load \"Text2Box_cuda:0,Segmenting_cuda:0,\n    Inpainting_cuda:0,ImageCaptioning_cuda:0,\n    Text2Image_cuda:1,Image2Canny_cpu,CannyText2Image_cuda:1,\n    Image2Depth_cpu,DepthText2Image_cuda:1,VisualQuestionAnswering_cuda:2,\n    InstructPix2Pix_cuda:2,Image2Scribble_cpu,ScribbleText2Image_cuda:2,\n    SegText2Image_cuda:2,Image2Pose_cpu,PoseText2Image_cuda:2,\n    Image2Hed_cpu,HedText2Image_cuda:3,Image2Normal_cpu,\n    NormalText2Image_cuda:3,Image2Line_cpu,LineText2Image_cuda:3\"\n\n```\n\n## GPU memory usage\nHere we list the GPU memory usage of each visual foundation model, you can specify which one you like:\n\n| Foundation Model        | GPU Memory (MB) |\n|------------------------|-----------------|\n| ImageEditing           | 3981            |\n| InstructPix2Pix        | 2827            |\n| Text2Image             | 3385            |\n| ImageCaptioning        | 1209            |\n| Image2Canny            | 0               |\n| CannyText2Image        | 3531            |\n| Image2Line             | 0               |\n| LineText2Image         | 3529            |\n| Image2Hed              | 0               |\n| HedText2Image          | 3529            |\n| Image2Scribble         | 0               |\n| ScribbleText2Image     | 3531            |\n| Image2Pose             | 0               |\n| PoseText2Image         | 3529            |\n| Image2Seg              | 919             |\n| SegText2Image          | 3529            |\n| Image2Depth            | 0               |\n| DepthText2Image        | 3531            |\n| Image2Normal           | 0               |\n| NormalText2Image       | 3529            |\n| VisualQuestionAnswering| 1495            |\n\n## Acknowledgement\nWe appreciate the open source of the following projects:\n\n[Hugging Face](https://github.com/huggingface) &#8194;\n[LangChain](https://github.com/hwchase17/langchain) &#8194;\n[Stable Diffusion](https://github.com/CompVis/stable-diffusion) &#8194; \n[ControlNet](https://github.com/lllyasviel/ControlNet) &#8194; \n[InstructPix2Pix](https://github.com/timothybrooks/instruct-pix2pix) &#8194; \n[CLIPSeg](https://github.com/timojl/clipseg) &#8194;\n[BLIP](https://github.com/salesforce/BLIP) &#8194;\n\n## Contact Information\nFor help or issues using the TaskMatrix, please submit a GitHub issue.\n\nFor other communications, please contact Chenfei WU (chewu@microsoft.com) or Nan DUAN (nanduan@microsoft.com).\n\n## Trademark Notice\n\nTrademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft\u2019s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.", "repo_name": "TaskMatrix", "org_name": "microsoft", "org_repo": "microsoft/TaskMatrix", "platform_org_repo": "github+microsoft/TaskMatrix", "link_to_repo": "https://github.com/microsoft/TaskMatrix", "platform": "github", "language": "Python", "stargazers_count": 33199, "watchers_count": 33199}, {"README_text": "# Clarity\n\n[Clarity](https://clarity.microsoft.com/) is a behavioral analysis tool that helps you understand how users view and use your application. Understanding how users navigate and interact with your application can provide new insights about your users. Empathizing with your users and seeing where features fail or succeed can help improve your product, grow revenue and improve user retention.\n\nCurrently Clarity supports Android including popular cross-platform frameworks (React Native, Cordova, Ionic). You can find our libraries here:\n- [Android](https://central.sonatype.com/artifact/com.microsoft.clarity/clarity/)\n- [ReactNative](https://www.npmjs.com/package/react-native-clarity)\n- [Ionic/Cordova](https://www.npmjs.com/package/cordova-clarity)\n\nPlease feel free to use this repository to suggest any enhancements or file any issues.\n", "repo_name": "clarity-apps", "org_name": "microsoft", "org_repo": "microsoft/clarity-apps", "platform_org_repo": "github+microsoft/clarity-apps", "link_to_repo": "https://github.com/microsoft/clarity-apps", "platform": "github", "language": null, "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# greenlands\n\nThis repo contains the code for `greenlands`, a platform that can be used to run\ngames between _AI agents_ and human players in Minecraft, and collect metrics\nthroughout the games. \n\nThe code in this repo is _almost exactly_ the code that was used for the\nplatform used to collect human evaluation data for the [IGLU\n2022](https://www.aicrowd.com/challenges/neurips-2022-iglu-challenge)\ncompetition, and while useful, the goal for it is to be a reference\nimplementation that you can check if you're interested in building something\nsimilar, or just want to see how things work. \n\n**NOTE:** The code in this repo is provided as-is in the hopes that it can be\nuseful to the open source community. However, it is not actively maintained\nright now. But PRs are welcome, and we'll try to respond to issues as soon as we\ncan.\n\nWe're still in the process of cleaning up the documentation (and this README),\nbut the code of the platform is complete. The existing documentation can be\nfound [here](https://github.com/microsoft/greenlands/blob/main/Docs/Home.md),\nand it should give you an idea of the architecture and functionality of the\ndifferent components.\n\nHere you can see an example of a human giving instructions to an agent and how\nsaid agent attempts to fulfill the human's requests (see full video\n[here](https://www.youtube.com/watch?v=PWrvLhQDybw)).\n\n[3452346.webm](https://user-images.githubusercontent.com/3422347/232140380-4605b2f8-2533-45d4-b389-d49f3c0ced1e.webm)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## References\n\nThe greenwolds was inspired as a playground for human in the loop evaluation for the competition [IGLU:Interactive Grounded Language Understanding in a Collaborative Environment](https://www.aicrowd.com/challenges/neurips-2022-iglu-challenge), which is described in the following papers:\n\n```\n@inproceedings{kiseleva2022interactive,\n  title={Interactive grounded language understanding in a collaborative environment: Iglu 2021},\n  author={Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and others},\n  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},\n  pages={146--161},\n  year={2022},\n  organization={PMLR}\n}\n```\n\n```\n@article{kiseleva2022iglu,\n  title={Iglu 2022: Interactive grounded language understanding in a collaborative environment at neurips 2022},\n  author={Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C{\\^o}t{\\'e}, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and others},\n  journal={arXiv preprint arXiv:2205.13771},\n  year={2022}\n}\n```\n\n```\n@article{zholus2022iglu,\n  title={IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents},\n  author={Zholus, Artem and Skrynnik, Alexey and Mohanty, Shrestha and Volovikova, Zoya and Kiseleva, Julia and Szlam, Artur and Cot{\\'e}, Marc-Alexandre and Panov, Aleksandr I},\n  journal={arXiv preprint arXiv:2206.00142},\n  year={2022}\n}\n```\n\n", "repo_name": "greenlands", "org_name": "microsoft", "org_repo": "microsoft/greenlands", "platform_org_repo": "github+microsoft/greenlands", "link_to_repo": "https://github.com/microsoft/greenlands", "platform": "github", "language": "Python", "stargazers_count": 43, "watchers_count": 43}, {"README_text": "# Cognitive Services Utilities\n\nThis repo hosts a variety of tools and utilities and E2E soltions and proptypes for scenarions that use the Cognitive Services APIs. \n\n## Solutions and Utiltities\n\n1. [Offline Speech to Speech Dubbing solution with Human Verification](OfflineDubbing): This is an E2E process that includes Speech to Text (STT), Translation and Text to Speech (TTS) services from Cognitive Services. The pipeline enables intermediate file production, which could be edited by human editors to correct errors at each stage of the pipeline. The pipeline does enable highlighting the potential points where human intervention could make sense, and highlights them in the intermediate file. Details of this pipeline and the appraoches used in this implementation can be found in the Offline Audio Dubbing Solution Guide (pending publishing)\n\n[//]: # (https://learn.microsoft.com/azure/architecture/guide/media/offline-audio-dubbing)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n", "repo_name": "Cognitive_Service_Utilities", "org_name": "microsoft", "org_repo": "microsoft/Cognitive_Service_Utilities", "platform_org_repo": "github+microsoft/Cognitive_Service_Utilities", "link_to_repo": "https://github.com/microsoft/Cognitive_Service_Utilities", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Welcome to PyCon 2023\n\n<img src=\"assets/Bit%20Steps.png\" style=\"max-height: 400px\" alt=\"Bit our Cloud Mascot travelling through Utah\">\n\n---\n\n- [Take the Azure Django Deployment Challenge :car: \ud83d\udca8](#take-the-azure-django-deployment-challenge-car-)\n- [What's New in Microsoft and Python](#whats-new-in-microsoft-and-python)\n- [PyCon 2023 Presentation Schedule](#pycon-2023-presentation-schedule)\n- [The Team at PyCon](#the-team-at-pycon)\n- [Hang out with us virtually](#hang-out-with-us-virtually)\n- [Code of Conduct](#code-of-conduct)\n- [Trademarks](#trademarks)\n\nLearn more about all the things Microsoft is working on in the Python community!\n\n## Take the Azure Django Deployment Challenge :car: \ud83d\udca8\n\nPython on Azure is faster than ever! That includes everything from deploying it to its performance! To prove it, we challenge you to take the Azure deployment speedrun!\n\nSee how fast you can deploy a webapp to azure. Our top runners will recieve a $50 Gift Card to [The GitHub Shop](https://www.thegithubshop.com/)!\n\n[Learn how to participate](azure-speedrun.md)\n\n## What's New in Microsoft and Python\n### Python 3.11 now available on Azure\n\nPython 3.11 is now available on Azure! \n\n### Check out our Ask the Expert Sessions\n- [Deploying a Python 3.11 Application to Azure App Service - Youtube](https://www.youtube.com/watch?v=lwNzb5pRn08)\n- [Deploying a Python Project with Azure Container Apps - Youtube](https://www.youtube.com/watch?v=8JwyQ6hb2Xc)\n\n### Deploying to Azure is Easier Than Ever with AZD\n\nYou can now deploy your Python application to Azure with a simple command!\nLearn more about the [Azure Developer CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).\n\nWant to get started quickly? Check out [the awesome AZD template library](https://azure.github.io/awesome-azd/) from us and the community!\n\n### VS Code and Python\n\nVS Code is great for Code Editor and we are working hard to make it even better!\n\n- Learn about the latest [Python Features in VS Code](https://devblogs.microsoft.com/python/python-in-visual-studio-code-march-2023-release/).\n- Learn about our updates to the [Jupyter Notebook extension](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) in VS Code and the all new [Data Wrangler](https://www.youtube.com/watch?v=WByQs82d29Y)!\n- Make Contributing faster and easier with [devcontainers](https://containers.dev) and [GitHub Codespaces](https://github.com/features/codespaces)!\n- [GitHub Copilot](https://github.com/features/copilot/) makes for an awesome pair programmer!\n\n### Microsoft is Helping Make Python Faster\n\nLearn more about the Faster CPython Team and how they are contributing to the Python Community!\n\n- [About the Faster CPython Team](https://devblogs.microsoft.com/python/python-311-faster-cpython-team/)\n- Check out [what's in store for Python 3.12](https://github.com/faster-cpython/ideas/blob/main/3.12/README.md)\n\n## PyCon 2023 Presentation Schedule\n\n### PyCon Talks\n\nCatch some of the talks from our team members presenting at PyCon!\n\n|Title|Presenter|Time (All Times Local to Salt Lake City)|Location |\n|---|---|---|---|\n|[Inside CPython 3.11's new specializing, adaptive interpreter.](https://us.pycon.org/2023/schedule/presentation/6/)| Brandt Bucher|Friday 21 April @ 11:30|355DEF|\n|[A Per-Interpreter GIL: Concurrency and Parallelism with Subinterpreters](https://us.pycon.org/2023/schedule/presentation/54/)| Eric Snow |21 April @ 15:30| 355ABC |\n|[Python's Syntactic Sugar](https://us.pycon.org/2023/schedule/presentation/41/)|Brett Cannon|Saturday 22 April @ 11:30|355ABC|\n|[How we are making CPython faster. Past, present and future](https://us.pycon.org/2023/schedule/presentation/73/)| Mark Shannon|Saturday 22 April @ 14:30|355ABC|\n\n### Presentations at the Microsoft Booth\n\nIf you're in the Expo Hall check out some of our special presentations and enter to win a pair of Surface Earbuds!\n\n| Talk | Presenter | Time (All Times Local to Salt Lake City) |\n| --- | --- | --- |\n|Talk Python Live | Michael Kennedy & Jay Miller| Friday 21 April @ 10:00 |\n| Deploying a Django Application to Azure Using VS Code | Dawn Wages | Friday 21 April @ 13:00 |\n| Supercharge your VS Code Session | Luciana Abud | Friday 21 April @ 15:30 |\n| Python Testing with Playwright | Andrew Knight (Applitools) | Saturday 22 April @ 10:20 |\n| Setting up AZD for our Django Application | Savannah Ostrowski | Saturday 22 April @ 13:00 |\n| Ask Brett (a)Bout Anything (ABBA) | Brett Cannon | Saturday 22 April @ 15:15 |\n\n## The Team at PyCon\n\nVisit us at the Microsoft Booth (booth 206) or out in the Hallway Track!\n\n### Microsoft booth team\n\n|Conference Booth Team|||\n|---|---|---|\n|Lucianna Abud (She/Her)|Eryn Clark (She/Her)|Rohit Ganguly (He/Him)|\n|Jay Miller (They/Them)|Krista Pratico (She/Her)|Savannah Ostrowski (She/Her)|\n|Dawn Wages (She/Her)|||\n\n|PyCon Virtual Team|\n|---|\n|Aaron Wislang (He/Him)|\n|Sarah Kaiser (She/Her)|\n|Pamela Fox (She/Her)|\n\nWe have a lot of great things to show you! Check out our [schedule](#schedule) below!\n\n## Hang out with us virtually\n\n![BIT our Cloud Mascot Enjoying his time at a coworking space](/assets/bit_cropped_coworking.png)\n\n### Join the Microsoft Python Discord\n\nJoin our [Microsoft Python Discord](https://aka.ms/python-discord) and connect with other Python developers in the Microsoft Community.\n\nCome say hi to our virtual team members,share your photos with us and make plans to connect with one another in our [#pycon](https://discord.com/channels/702724176489873509/1096501197965361282) channel. \n\n### Check out the Python Pulse\n\n[![Python Pulse](assets/python_pulse_banner_1MB-1024x576.png)](https://youtube.com/playlist?list=PLj6YeMhvp2S4aIxuGH0NaGXQZlVUBsH3E)\n\nTune into our monthly [Python Pulse](https://youtube.com/playlist?list=PLj6YeMhvp2S4aIxuGH0NaGXQZlVUBsH3E) Series to get updated on the latest Python news and announcements around Microsoft!\n\n\n## Code of Conduct\n\nWhile at PyCon US, we expect all Booth attendees and staff to follow the [PyCon US Code of Conduct](https://us.pycon.org/2023/about/code-of-conduct/) as well as the [Microsoft Events Code of Conduct](https://aka.ms/codeofconduct).\n\nIf you have any questions or concerns while at the Microsoft Booth, feel free to do any of the following:\n\n- Find and speak to a member of the Microsoft Booth Code of Conduct Team (Eryn Clark & Jay Miller)\n- Reach out to the PyCon US Code of Conduct Team (email: pycon-us-report@python.org)\n- Email our Events Code of Conduct buscond@microsoft.com\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "pycon", "org_name": "microsoft", "org_repo": "microsoft/pycon", "platform_org_repo": "github+microsoft/pycon", "link_to_repo": "https://github.com/microsoft/pycon", "platform": "github", "language": null, "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# INTeractive learning via REPresentatIon Discovery\n\nVersion: 0.0.1 beta\n\nINTREPID (stands for INTeractive learning via REPresentatIon Discovery) is a library that contains various interactive learning algorithms that learn a representation (or a latent state) from observational data in order to complete their tasks. \n\nA list of algorithms, environments, and utils are given below. For full details see [Wiki](https://github.com/microsoft/Intrepid/wiki) of this repository.\n\n## What is Interactive Learning and Representation Discovery\n\nConsider any agent, also called decision maker, (e.g., a bot, robot, LLM) that is taking actions in an environment (e.g., a place, an OS). The world changes as a effect of the agent's action and also because of other noise in (e.g., a person maybe moving in the background or an OS may receive a notification unrelated to what the bot did). The goal of this agent is to solve a task, e.g., navigate safetly to a given location, or compose an email and send it off. The agent maybe take a series of actions to accomplish its goal. This is called an *Interactive Learning* task as the agent interacts with the world.\n\nTypically, the observes the world in the form of a complex representation (e.g., an image, a large piece of text). It is useful for solving the task to map this representation into a more manageable representation from which it is easier to learn how to take actions, or model the dynamics of the world. A large-language-model, for example, does this internally by predicting the next token using a large corpus of data. However, for a robot or software, there can be other approaches for learning this representation. This is called the *Representation Discovery* problem.\n\nAs the name suggests, INTREPID is designed to discover representation for the purpose of interactive learning. INTREPID is designed to contain state-of-the-art algorithms for mapping the observation into representation for the purpose of taking actions, learning the world model, debugging, or visualization. INTREPID also contains various interactive learning algorithms that use this representation to optimize reward, and learn a policy. This is an continual evolving repository with a lot of features to come over time. To contribute, see the contributing section below.\n\n## Algorithms Currently Supported\n\n1. **Homer**. Learns latent state/representation using temporal constrastive learning loss. Provably explores and optimizes reward in Block MDP setting. \n          \n   **Citation**: Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning, _Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, John Langford_ [\\[ICML 2020\\]](http://proceedings.mlr.press/v119/misra20a/misra20a.pdf)\n        \n2. **PPE: Path Prediction and Elimination**. Learns latent state/representation using a variant of multi-step inverse dynamics where the model predicts the identity of the path (sequence of actions) used to reach a given observation. Provably ignores noisy TV like temporal distractions. A very fast and scalable algorithm for near-deterministic problems.\n\n   **Citation**: Provable RL with Exogenous Distractors via Multistep Inverse Dynamics, _Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford_ [\\[ICLR 2022 Oral\\]](https://openreview.net/pdf?id=RQLLzMCefQu)\n\n3. **RicHID**: Algorithm designed for control problems where the latent dynamics are [LQR](https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator) but the LQR state is obfuscated by rich observational noise. Provably explores and extracts the latent state by predicting past actions from observation.\n\n    **Citation**: Learning the Linear Quadratic Regulator from Nonlinear Observations, _Zakaria Mhammedi, Dylan J. Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay Krishnamurthy, Alexander Rakhlin, John Langford_ [\\[NeurIPS 2020\\]](https://papers.nips.cc/paper/2020/file/a70145bf8b173e4496b554ce57969e24-Paper.pdf)\n\n4. **FactoRL (pron. Factorel)**: Algorithm designed for settings where the latent representation is factorized over a set of states with sparse dynamical evolution (Factored MDP dynamics). The latent state is extracted by performing independence test over pairs of atoms (e.g., pixels/tokens) of the observation. This is followed by performing contrastive learning similar to in-painting. The algorithm has guarantee of success under certain assumptions. However, the algorithm is quite computationally expensive to run.\n\n     **Citation**: Provable Rich Observation Reinforcement Learning with Combinatorial Latent States, _Dipendra Misra, Qinghua Liu, Chi Jin, John Langford_ [\\[ICLR 2021\\]](https://openreview.net/pdf?id=hx1IXFHAw7R) [\\[RL Theory Seminar\\]](https://www.youtube.com/watch?v=SEE5Snqhd40&ab_channel=RLtheoryseminars)\n\n5. **Sabre**: Sabre is an algorithm for Safe Reinforcement Learning that assumes a safety function that can only provide binary feedback (safe/unsafe). This safety function is unknown but can be queried. The algorithm works by performing a sequence of active learning queries for safety while ensuring possible coverage so safety can be learned everywhere. Under certain assumptions, the algorithm can guarantee never taking any unsafe action even during training, optimizing calls to safety, and finding safest optimal policy.\n\n    **Citation**: Provable Safe Reinforcement Learning with Binary Feedback, _Andrew Bennett, Dipendra Misra, and Nathan Kallus_ [\\[AISTATS 2023\\]](https://arxiv.org/pdf/2210.14492.pdf)\n    \nIn addition to the above, there is also support for frequently used tabular RL methods (e.g., Q-learning with Bonus [\\[Jin et al. 2018\\]](https://arxiv.org/pdf/1807.03765.pdf)) and policy search methods (e.g., Fitted Q-Iteration [\\[Nan Jiang's note on FQI\\]](https://nanjiang.cs.illinois.edu/files/cs598/note5.pdf) and Policy Search by Dynamic Programming [\\[Bagnell et al., 2003\\]](https://papers.nips.cc/paper_files/paper/2003/hash/3837a451cd0abc5ce4069304c5442c87-Abstract.html)). \n\nWe hope to include more algorithms in the future particularly those for representation discovery via self-supervised learning, and any RL algorithm that has provable regret or PAC guarantees which are typically unavailable in popular DeepRL repositories.\n\n## Environments currently supported\n\n1. Challenging Block MDP environments: This includes Diabolical Combination Lock [\\[Misra et al., 2020\\]](http://proceedings.mlr.press/v119/misra20a/misra20a.pdf)\n2. Simple Newtonian Mechanics LQR problem\n3. Wrappers for OpenAI Gym, Matterport Simulator, Minigrid, and AI2Thor. You will need to install these packages on your own. We provide no guarantee for these repositories. See Wiki for details. \n\n## Basic Usage in under 1 minute\n\nThe code is built primarily using Python with PyTorch and is regularly tested on OSX and Linux Systems. However, the code should also work on any other system which supports these base dependencies. To run a sample code in under a minute do the following.\n\n1. Git clone the repository `git clone https://github.com/microsoft/Intrepid.git`\n\n2. Go to the Intrepid folder in a terminal.\n\n3. Install requirements. If you are using pip then you can install them as `python3 -m pip install requirements.txt`. \n\n3. Run a sample code as `sh local_runs/run_homer.sh`. This will run the Homer algorithm on a toy task and should generate results inside the `./results` folder.\n\nFor full functionality please see the [wiki](https://github.com/microsoft/Intrepid/wiki) of this repository.\n\n## Citing this repository\n\nIf you use this repository in your research and find it helpful, then please cite the usage as:\n\n``` \n@software{Intrepid,\n\ntitle = \"Intrepid: INTeractive REPresentatIon Discovery, a library for decision making algorithms that learn latent state representation\",\n\nauthors = \"Dipendra Misra, Rajan Chari, Alex Lamb, Anurag Koul, Byron Xu, Akshay Krishnamurthy, Dylan Foster, John Langford\",\n\nyear = \"2023\"\n\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA. \nPlease use [PEP](https://peps.python.org/pep-0008/) standards for python programming if you send us pull request.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Intrepid", "org_name": "microsoft", "org_repo": "microsoft/Intrepid", "platform_org_repo": "github+microsoft/Intrepid", "link_to_repo": "https://github.com/microsoft/Intrepid", "platform": "github", "language": "Python", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Bootstrapped Transformer\nSource code for NeurIPS 2022 paper *[Bootstrapped Transformer for Offline Reinforcement Learning](https://seqml.github.io/bootorl/)*.\n\n## Abstract\n> Offline reinforcement learning (RL) aims at learning policies from previously collected static trajectory data without interacting with the real environment. Recent works provide a novel perspective by viewing offline RL as a generic sequence generation problem, adopting sequence models such as Transformer architecture to model distributions over trajectories, and repurposing beam search as a planning algorithm. However, the training datasets utilized in general offline RL tasks are quite limited and often suffer from insufficient distribution coverage, which could be harmful to training sequence generation. In this paper, we propose a novel algorithm named Bootstrapped Transformer, which incorporates the idea of bootstrapping and leverages the learned model to self-generate more offline data to further boost the sequence model training. We conduct extensive experiments on two offline RL benchmarks and demonstrate that our model can largely remedy the existing offline RL training limitations and beat other strong baseline methods. We also analyze the generated pseudo data and the revealed characteristics may shed some light on offline RL training. The codes and supplementary materials are available at https://seqml.github.io/bootorl.\n\n## Dependencies\n\nPython dependencies are listed in [`./environment.yml`](./environment.yml).\n\nWe also provides an extra dockerfile as [`./Dockerfile`](./Dockerfile) for reproducibility. \n\n## Usage\n\nTo train the model, run with \n```\npython main/train.py --dataset hopper-medium-replay-v2 \\\n                     --bootstrap True \\\n                     --bootstrap_type once \\\n                     --generation_type autoregressive\n```\nor\n```\npython main/train.py --dataset hopper-medium-replay-v2 \\\n                     --bootstrap True \\\n                     --bootstrap_type repeat \\\n                     --generation_type teacherforcing\n```\ndepending on your choice of hyperparameters and bootstrap schemes. All default hyperparameters used in our experiments are placed at [`./utils/argparser.py`](`./utils/argparser.py`). You can find it in `DEFAULT_ARGS` at the beginning of this file. By default, training logs and saved models are output to `./logs/<environment>-<dataset_level>/` directory.\n\nTo evaluate the performance of trained model, run with\n```\npython main/plan.py --dataset hopper-medium-replay-v2 \\\n                    --checkpoint <checkpoint_directory> \\\n                    --suffix <output_directory_suffix>\n```\nwhere `checkpoint_directory` should be the directory containing your model `state_*.pt`. By default, evaluation results are output to `./logs/<environment>-<dataset_level>/<suffix>` directory.\n\n\n## Acknowledgements\nSome source codes of this work have been implemented on top of *Trajectory Transformer* (https://arxiv.org/abs/2106.02039).\n*Trajectory Transformer* uses GPT implementation from Andrej Karpathy's *minGPT* repo.\n\n## Citation\nYou are more than welcome to cite our paper:\n```\n@article{wang2022bootstrapped,\n  title={Bootstrapped Transformer for Offline Reinforcement Learning},\n  author={Wang, Kerong and Zhao, Hanye and Luo, Xufang and Ren, Kan and Zhang, Weinan and Li, Dongsheng},\n  journal={arXiv preprint arXiv:2206.08569},\n  year={2022}\n}\n```", "repo_name": "BooTORL", "org_name": "microsoft", "org_repo": "microsoft/BooTORL", "platform_org_repo": "github+microsoft/BooTORL", "link_to_repo": "https://github.com/microsoft/BooTORL", "platform": "github", "language": "Python", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Project\n\n# Introduction \nThis repo supplements the Azure OpenAI one-pager by providing code examples of some of the functionalities of the OpenAI GPT models e.g. call anaytics, sentiment anaysis, ChatGPT, text summarisation, NLP to code, text classification, text similarity, document search and more. \n\nNew Updates: ChatGPT.\nComing soon: Change Summary use cases.\n\n# Getting Started\n1.\tcreate a virtual environment locally\n2.  pip install -r .\\requirements.txt\n3.\treplace OpenAI API key with your own (when running locally)\n\n# Build and Test\nRun the app.py and load the website in your browser.\nThere are 10 pages, each has sample input pre-loaded into the input forms. Sample input & corresponding output can be seen in samples.txt.\n1.  Call Centre Analytics uses Azure Speech to Text Service and OpenAI's GPT-3 model to perform analytics.\n2.  NLP to SQL - uses OpenAI's codex model to transform natural language to an SQL query.\n3.  NLP to Python - uses OpenAI's codex model to tranform natural language to Python.\n4.  Synthetic Data Generation - Uses GPT-3's ability to follow natural language to create synthetic data.\n5.  User Stories - uses OpenAI's GPT-3 model to capture the IT requirements and main data attributes required by a technical system that encapsulates various user scenarios.\n6.  Text Generation -  Uses GPT-3's ability to follow natural language to create text.\n7.  Classify Text -  GPT-3 classifies text into one of multiple categories provided by the user.\n8.  Similarity Embedding - compares two pieces of text by transforming each into an embedding vector & using cosine similarity to generate a similarity score.\n9.  Text Search - creates embeddings for a set of Amazon food reviews (already done - just loaded into memory). compares these embeddings to the embedded user input and returns the top 3 reviews most similar to what the user asked.\n10. Entity extraction - demonstrating zero-shot vs one-shot learning. An article is input and the names of companies as well as the names of people & their titles are extracted and returned to the user. Results should show improved accuracy with one-shot learning over zero-shot.\n11. Micrsoft Assistant Chatbot (ChatGPT) - demonstrating ChatGPT's abilities when integrated into a website. Model is prompted to be a Microsoft assistant chatbot using its own knowledge. This could be extended with fine-tuning, or using a source of external knowledge such as Azure Cognitive Search. \n\nNew Updates: ChatGPT (MS Assistant)\nComing soon: Change Summary use cases.\n\n# Contribute\nPlease reach out to clodaghlynch@microsoft.com, or henrytaylor@microsoft.com, for any questions, suggestions, or improvements. Thank you!\n\n# Resources\nhttps://microsofteur-my.sharepoint.com/:o:/g/personal/clodaghlynch_microsoft_com/Eg7NwxxSiBpDmyFOqlNmekIBcO_G6-_kG7SbOV7Y7pJIdg \n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dstoolkit-AOAIFlaskApp", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-AOAIFlaskApp", "platform_org_repo": "github+microsoft/dstoolkit-AOAIFlaskApp", "link_to_repo": "https://github.com/microsoft/dstoolkit-AOAIFlaskApp", "platform": "github", "language": "CSS", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# SeqRecord\n\nModern custom effcient data pipelines for high throughput training on sequential data in the cloud.\n\n## Installation\n\n```\npip install seqrecord\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "seqrecord", "org_name": "microsoft", "org_repo": "microsoft/seqrecord", "platform_org_repo": "github+microsoft/seqrecord", "link_to_repo": "https://github.com/microsoft/seqrecord", "platform": "github", "language": "Python", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Application Insights - Reproduction examples\n\nDo you have an issue with Application Insights? It may be a bug or a configuration issue. A reproduction example in this repository would make it easier for the Application Insights maintainers to help you! \n\n_Be careful not to include personal information such as a connection string in the examples._\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ApplicationInsights-Java-Repros", "org_name": "microsoft", "org_repo": "microsoft/ApplicationInsights-Java-Repros", "platform_org_repo": "github+microsoft/ApplicationInsights-Java-Repros", "link_to_repo": "https://github.com/microsoft/ApplicationInsights-Java-Repros", "platform": "github", "language": "Java", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# habitat-simulation\n\nRun agents in habitat simulation enviroment (habitat-lab), and collect data for training.\n\n## Run Locally \nFirst install conda environment in `docker/`:\n* `env.yaml` contains habitat and pytorch lightning\n* `env_habitat.yaml` contains only habitat (more friendly maybe)\n\nThen install habitat-lab locally:\n```\ngit clone https://github.com/shuhangchen/habitat-lab.git\ncd habitat-lab\npip install -e habitat-lab  # install habitat_lab\n```\n\nInstall utilities packages [here](https://github.com/AutonomousSystemsResearch/utils_package/tree/shc).\n## run on aml\nThe current `jobs/amlt.yaml` is already set up for aml. Note that the docker image at `docker.io:shuhangchen/habitat:latest` contains both habitat (sim and lab) and pytorch lightning, and it does not depend on the conda environments in `docker`. \n\n", "repo_name": "habitat-data-collection", "org_name": "microsoft", "org_repo": "microsoft/habitat-data-collection", "platform_org_repo": "github+microsoft/habitat-data-collection", "link_to_repo": "https://github.com/microsoft/habitat-data-collection", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# perception-benchmark\n\n## Overview\n\nA benchmark that evaluates and analyzes multi-modal perception models.\n\n\n## Usage\n\n#### Run locally\n1. install the conda environment with `docker/env.yml`\n2. install packages in (utils_package)[https://github.com/AutonomousSystemsResearch/utils_package].\n3. run `pip install -e .` in the root directory of this repo\n4. update configs files. For example, mounting directory for azure storage.\n5. run `python tasks/train.py base=configs/multiMAE.yaml` to pretrain multi-MAE model over habitat dataset \n\n#### Run aml\nThe `jobs/amlt.yaml` is already set up.\n## Note\n\nCertain directories in this repo are cloned from exist repos as submodules. When you first `git clone` this repo, those folders of submodules will be empty.You must run two commands: `git submodule init` to initialize your local configuration file, and `git submodule update` to fetch all the data from that project and check out the appropriate commit listed in your superproject. Or you can pass --recurse-submodules to the `git clone` command, it will automatically initialize and update each submodule in the repository, including nested submodules if any of the submodules in the repository have submodules themselves.\n\nThose submodules are usually official repo of open-sourced codes, you should avoid modifying them unless necessary.\n", "repo_name": "perception-benchmark", "org_name": "microsoft", "org_repo": "microsoft/perception-benchmark", "platform_org_repo": "github+microsoft/perception-benchmark", "link_to_repo": "https://github.com/microsoft/perception-benchmark", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "\r\n## Building ADRL on Linux\r\n- Building ADRL is similar to [building AirSim](https://microsoft.github.io/AirSim/build_linux.html), apart from a couple of 3rd party libs. \r\n- [Setup ssh keys for Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/repos/git/use-ssh-keys-to-authenticate?view=azure-devops)\r\n- In the `git clone` step, let's clone it with submodules:\r\n  ```\r\n  git clone --recurse-submodules -j8 bizair@vs-ssh.visualstudio.com:v3/bizair/Research/airsim_drone_racing_lab\r\n  ```\r\n  - We need to check out the `linux-autobuild` branch in 3 submodules:\r\n    ```\r\n    cd external/gflags_airsim;\r\n    git checkout linux-autobuild;\r\n    cd ../;\r\n    cd external/glog_airsim;\r\n    git checkout linux-autobuild;\r\n    cd external/nlopt_airsim;\r\n    git checkout linux-autobuild;\r\n    ```\r\n  \r\n  - Now, we can carry on with the normal process of building AirSim on linux. \r\n     let's proceed with first building AirLib;\r\n    ```\r\n    cd airsim_drone_racing_lab;\r\n    ./setup.sh;\r\n    ./build.sh;\r\n    ```\r\n\r\n  - [then setup unreal environment](https://microsoft.github.io/AirSim/build_linux.html#build-unreal-environment)\r\n\r\n\r\n## Last announcement\r\n\r\nThe AirSim team is trying to get a better understanding of how AirSim and other simulation engines are being used across the community. If you have a couple of minutes, please take a look and respond to the AirSim survey as it does have an impact on the features and direction of the platform:\r\n\r\n[Survey link](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR4J_69QtqUREtdPd5aLYTsFURDJPQUFSNDI3T0VENzJHUjVQVkpVWlRFNC4u)\r\n\r\n\r\n# Welcome to AirSim\r\n\r\nAirSim is a simulator for drones, cars and more, built on [Unreal Engine](https://www.unrealengine.com/) (we now also have an experimental [Unity](https://unity3d.com/) release). It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, we have an experimental release for a Unity plugin.\r\n\r\nOur goal is to develop AirSim as a platform for AI research to experiment with deep learning, computer vision and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim also exposes APIs to retrieve data and control vehicles in a platform independent way.\r\n\r\n**Check out the quick 1.5 minute demo**\r\n\r\nDrones in AirSim\r\n\r\n[![AirSim Drone Demo Video](docs/images/demo_video.png)](https://youtu.be/-WfTr1-OBGQ)\r\n\r\nCars in AirSim\r\n\r\n[![AirSim Car Demo Video](docs/images/car_demo_video.png)](https://youtu.be/gnz1X3UNM5Y)\r\n\r\n\r\n## How to Get It\r\n\r\n### Windows\r\n[![Build Status](https://github.com/microsoft/AirSim/actions/workflows/test_windows.yml/badge.svg)](https://github.com/microsoft/AirSim/actions/workflows/test_windows.yml)\r\n* [Download binaries](https://github.com/Microsoft/AirSim/releases)\r\n* [Build it](https://microsoft.github.io/AirSim/build_windows)\r\n\r\n### Linux\r\n[![Build Status](https://github.com/microsoft/AirSim/actions/workflows/test_ubuntu.yml/badge.svg)](https://github.com/microsoft/AirSim/actions/workflows/test_ubuntu.yml)\r\n* [Download binaries](https://github.com/Microsoft/AirSim/releases)\r\n* [Build it](https://microsoft.github.io/AirSim/build_linux)\r\n\r\n### macOS\r\n[![Build Status](https://github.com/microsoft/AirSim/actions/workflows/test_macos.yml/badge.svg)](https://github.com/microsoft/AirSim/actions/workflows/test_macos.yml)\r\n* [Build it](https://microsoft.github.io/AirSim/build_linux)\r\n\r\nFor more details, see the [use precompiled binaries](docs/use_precompiled.md) document. \r\n\r\n## How to Use It\r\n\r\n### Documentation\r\n\r\nView our [detailed documentation](https://microsoft.github.io/AirSim/) on all aspects of AirSim.\r\n\r\n### Manual drive\r\n\r\nIf you have remote control (RC) as shown below, you can manually control the drone in the simulator. For cars, you can use arrow keys to drive manually.\r\n\r\n[More details](https://microsoft.github.io/AirSim/remote_control/)\r\n\r\n![record screenshot](docs/images/AirSimDroneManual.gif)\r\n\r\n![record screenshot](docs/images/AirSimCarManual.gif)\r\n\r\n\r\n### Programmatic control\r\n\r\nAirSim exposes APIs so you can interact with the vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on. The APIs are exposed through the RPC, and are accessible via a variety of languages, including C++, Python, C# and Java.\r\n\r\nThese APIs are also available as part of a separate, independent cross-platform library, so you can deploy them on a companion computer on your vehicle. This way you can write and test your code in the simulator, and later execute it on the real vehicles. Transfer learning and related research is one of our focus areas.\r\n\r\nNote that you can use [SimMode setting](https://microsoft.github.io/AirSim/settings#simmode) to specify the default vehicle or the new [ComputerVision mode](https://microsoft.github.io/AirSim/image_apis#computer-vision-mode-1) so you don't get prompted each time you start AirSim.\r\n\r\n[More details](https://microsoft.github.io/AirSim/apis/)\r\n\r\n### Gathering training data\r\n\r\nThere are two ways you can generate training data from AirSim for deep learning. The easiest way is to simply press the record button in the lower right corner. This will start writing pose and images for each frame. The data logging code is pretty simple and you can modify it to your heart's content.\r\n\r\n![record screenshot](docs/images/record_data.png)\r\n\r\nA better way to generate training data exactly the way you want is by accessing the APIs. This allows you to be in full control of how, what, where and when you want to log data.\r\n\r\n### Computer Vision mode\r\n\r\nYet another way to use AirSim is the so-called \"Computer Vision\" mode. In this mode, you don't have vehicles or physics. You can use the keyboard to move around the scene, or use APIs to position available cameras in any arbitrary pose, and collect images such as depth, disparity, surface normals or object segmentation.\r\n\r\n[More details](https://microsoft.github.io/AirSim/image_apis/)\r\n\r\n### Weather Effects\r\n\r\nPress F10 to see various options available for weather effects. You can also control the weather using [APIs](https://microsoft.github.io/AirSim/apis#weather-apis). Press F1 to see other options available.\r\n\r\n![record screenshot](docs/images/weather_menu.png)\r\n\r\n## Tutorials\r\n\r\n- [Video - Setting up AirSim with Pixhawk Tutorial](https://youtu.be/1oY8Qu5maQQ) by Chris Lovett\r\n- [Video - Using AirSim with Pixhawk Tutorial](https://youtu.be/HNWdYrtw3f0) by Chris Lovett\r\n- [Video - Using off-the-self environments with AirSim](https://www.youtube.com/watch?v=y09VbdQWvQY) by Jim Piavis\r\n- [Reinforcement Learning with AirSim](https://microsoft.github.io/AirSim/reinforcement_learning) by Ashish Kapoor\r\n- [The Autonomous Driving Cookbook](https://aka.ms/AutonomousDrivingCookbook) by Microsoft Deep Learning and Robotics Garage Chapter\r\n- [Using TensorFlow for simple collision avoidance](https://github.com/simondlevy/AirSimTensorFlow) by Simon Levy and WLU team\r\n\r\n## Participate\r\n\r\n### Paper\r\n\r\nMore technical details are available in [AirSim paper (FSR 2017 Conference)](https://arxiv.org/abs/1705.05065). Please cite this as:\r\n```\r\n@inproceedings{airsim2017fsr,\r\n  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},\r\n  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},\r\n  year = {2017},\r\n  booktitle = {Field and Service Robotics},\r\n  eprint = {arXiv:1705.05065},\r\n  url = {https://arxiv.org/abs/1705.05065}\r\n}\r\n```\r\n\r\n### Contribute\r\n\r\nPlease take a look at [open issues](https://github.com/microsoft/airsim/issues) if you are looking for areas to contribute to.\r\n\r\n* [More on AirSim design](https://microsoft.github.io/AirSim/design)\r\n* [More on code structure](https://microsoft.github.io/AirSim/code_structure)\r\n* [Contribution Guidelines](CONTRIBUTING.md)\r\n\r\n### Who is Using AirSim?\r\n\r\nWe are maintaining a [list](https://microsoft.github.io/AirSim/who_is_using) of a few projects, people and groups that we are aware of. If you would like to be featured in this list please [make a request here](https://github.com/microsoft/airsim/issues).\r\n\r\n## Contact\r\n\r\nJoin our [GitHub Discussions group](https://github.com/microsoft/AirSim/discussions) to stay up to date or ask any questions.\r\n\r\nWe also have an AirSim group on [Facebook](https://www.facebook.com/groups/1225832467530667/). \r\n\r\n\r\n## What's New\r\n\r\n- [Python wrapper for Open AI gym interfaces.](https://github.com/microsoft/AirSim/pull/3215)\r\n- [Python wrapper for Event camera simulation](https://github.com/microsoft/AirSim/pull/3202)\r\n- [Voxel grid construction](https://github.com/microsoft/AirSim/pull/3209)\r\n- [Programmable camera distortion](https://github.com/microsoft/AirSim/pull/3039)\r\n- [Wind simulation](https://github.com/microsoft/AirSim/pull/2867)\r\n- [Azure development environment with documentation](https://github.com/microsoft/AirSim/pull/2816)\r\n- ROS wrapper for [multirotor](https://github.com/microsoft/AirSim/blob/master/docs/airsim_ros_pkgs.md) and [car](https://github.com/microsoft/AirSim/pull/2743).\r\n\r\nFor complete list of changes, view our [Changelog](docs/CHANGELOG.md)\r\n\r\n## FAQ\r\n\r\nIf you run into problems, check the [FAQ](https://microsoft.github.io/AirSim/faq) and feel free to post issues in the  [AirSim](https://github.com/Microsoft/AirSim/issues) repository.\r\n\r\n## Code of Conduct\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n\r\n## License\r\n\r\nThis project is released under the MIT License. Please review the [License file](LICENSE) for more details.\r\n\r\n\r\n", "repo_name": "AirSim-Drone-Racing-Lab-Source", "org_name": "microsoft", "org_repo": "microsoft/AirSim-Drone-Racing-Lab-Source", "platform_org_repo": "github+microsoft/AirSim-Drone-Racing-Lab-Source", "link_to_repo": "https://github.com/microsoft/AirSim-Drone-Racing-Lab-Source", "platform": "github", "language": "C++", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Modulegarage\n\nA repo that serves as test bed for new computational blocks/modules/components. Currently has\n * FlashAttention and fused operations for transformer model", "repo_name": "modulegarage", "org_name": "microsoft", "org_repo": "microsoft/modulegarage", "platform_org_repo": "github+microsoft/modulegarage", "link_to_repo": "https://github.com/microsoft/modulegarage", "platform": "github", "language": "Python", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Autonomous Systems Research Group: ML Template\n\nThis document serves as an onboarding document as well as a template repository to quickstart machine learning experimentation at the [Autonomous Systems Research Group at Microsoft](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/)\n\n**Note** Use the table of contents icon <img src=\"./assets/images/table-of-contents.png\" width=\"25\" height=\"25\" /> on the top left corner of this document to get to a specific section quickly.\n\n### Using this template to generate a repository:\n\n- Click on the green colored box titled **Use this template** top right, and name your new repository.\n- You can clone your repo when it looks like [example_repo_generated_from_ml_template](https://github.com/AutonomousSystemsResearch/example_repo_generated_from_ml_template).\n\n> **Note** that after you create the template, it will take about **20 seconds** for an automated github action to clean up the generated repository using an auto-commit. Please ensure your repository looks like [example_repo_generated_from_ml_template](https://github.com/AutonomousSystemsResearch/example_repo_generated_from_ml_template) before cloning it.\n\n## Introduction\n\nFor the template repository, we will use:\n\n- [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/)\n  - For minimizing boilerplate code\n- [OmegaConf](https://omegaconf.readthedocs.io/)\n  - Please go through [OmegaConf's github readme](https://github.com/omry/omegaconf#releases) for tutorials.\n  - For config management\n    > **Note**: we have an [archived branch called `hydra`](https://github.com/AutonomousSystemsResearch/ml_template/tree/hydra) which uses [hydra](https://hydra.cc/) for config management.\n- Logging\n  - We primarily use tensorboard. Amulet automatically patches tensorboard scalars to MLFlow for viewing metrics in Azure ML Studio.\n- Conda and Docker\n  - For development\n\n## Using this repository\n\n### **Running locally**\n\n#### Setup\n\n- **VSCode**\n\n  - Extensions:\n\n    - Hit `Ctrl+Shift+P` and type `Show Recommended Extensions` and install them from the sidebar.\n      Or click \"yes\" when you get a VS Code pop up to install the recommended extensions, which are specified in [.vscode/extensions.json](.vscode/extensions.json).\n      Follow [this doc](https://code.visualstudio.com/docs/editor/extension-marketplace#_recommended-extensions) for more details.\n    - `Python`, `Pylance`, `Docker`, `GitLens`, `YAML`, and the [Remote development extension pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) are strongly recommended.\n\n  - Debugging:\n\n    - Please follow [VSCode docs and tutorials](https://code.visualstudio.com/docs/python/debugging) on Python debugging\n    - A minimal debugging configuration has been provided in [.vscode/launch.json](.vscode/launch.json). Please see VSCode docs on [launch.json configs](https://code.visualstudio.com/docs/python/debugging#_additional-configurations) and [config options](https://code.visualstudio.com/docs/python/debugging#_set-configuration-options).\n\n- **Conda**\n\n  - Recommended for local development and debugging.\n  - Note: For CUDA 11.6, see `Creating the conda environment from scratch (click to expand)` below.\n\n  ```\n  # create env\n  conda env create --file docker/environment.yml\n\n  # activate it\n  conda activate ml_template\n\n  # install this repo\n  (ml_template) $ pip install -e .\n\n  # install pre-commit (recommended). Scroll down to the #Developing section for details.\n  (ml_template) $ pre-commit install\n  ```\n\n  > **Note** If you install additional packages in your environment manually, you should update the `environment.yml` correspondingly by doing a `$ conda env export | grep -v \"^prefix: \" > docker/environment.yml`.\n\n  <details>\n      <summary>\n      Creating the conda environment from scratch (click to expand)\n      </summary>\n\n  ```\n  conda update -n base -c defaults conda\n  conda create --name ml_template python=3.9\n  conda activate ml_template\n  conda install pip\n  conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n  conda install pytorch-lightning -c conda-forge\n  pip install omegaconf \\\n    pytest \\\n    sh \\\n    pre-commit \\\n    mlflow \\\n    azureml-mlflow \\\n    azureml-core \\\n    torch_tb_profiler \\\n    opencv-python \\\n    black isort flake8 \\\n    psutil \\\n    rich\n  conda env export | grep -v \"^prefix: \" > docker/environment.yml\n  pre-commit install\n  pre-commit run --all-files\n  pip install -e .\n  ```\n\n  For CUDA 11.6:\n\n  ```\n  conda update -n base -c defaults conda\n  conda create --name ml_template_cu116 python=3.9\n  conda activate ml_template_cu116\n  conda install pip\n  conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n  pip install pytorch-lightning\n  pip install omegaconf \\\n    pytest \\\n    sh \\\n    pre-commit \\\n    mlflow \\\n    azureml-mlflow \\\n    azureml-core \\\n    torch_tb_profiler \\\n    opencv-python \\\n    black isort flake8 \\\n    psutil \\\n    rich\n  conda env export | grep -v \"^prefix: \" > docker/environment_cu116.yml\n  pre-commit install\n  pre-commit run --all-files\n  pip install -e .\n  ```\n\n  </details>\n\n  <details>\n      <summary>\n      Upgrading pytorch and cudatoolkit (click to expand)\n      </summary>\n\n  ```\n  conda remove pytorch torchvision torchaudio cudatoolkit\n  # then follow pytorch installation steps, for example:\n  conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n  # then update pytorch lightning:\n  pip install pytorch-lightning --upgrade\n  pip install pytorch-lightning[extra] --upgrade\n  pip install -U jsonargparse[signatures] --upgrade\n  ```\n\n  </details>\n\n- **Docker**\n\n  - While submitting jobs to AzureML, we take our local conda environment and overlay them on an appropriate docker base image. For a new project / a custom conda environment, you can build the docker image locally as explained in a note later in this section. Optionally, the docker image building can be automated by CI (as explained later) if your project has a frequently update conda environment.\n\n  - For `ml_template`, we have [three docker images](docker/) built automatically on each commit to `main` branch or a branch corresponding to a Pull Request.\n    Docker images are pushed to [PRIVATEAZURECONTAINERREGISTRYNAME](https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource/subscriptions/964a24a8-8835-43c1-9633-7d78841facf1/resourceGroups/research_team/providers/Microsoft.ContainerRegistry/registries/PRIVATEAZURECONTAINERREGISTRYNAME/repository) container registory under [ml_template](https://ms.portal.azure.com/#view/Microsoft_Azure_ContainerRegistries/RepositoryBlade/id/%2Fsubscriptions%2F964a24a8-8835-43c1-9633-7d78841facf1%2FresourceGroups%2Fresearch_team%2Fproviders%2FMicrosoft.ContainerRegistry%2Fregistries%25PRIVATEAZURECONTAINERREGISTRYNAME/repository/ml_template).\n    To automate this for your generated repository from this template, please follow make an Azure Pipelines which will `azure-pipelines.yml`\n\n  - The following tags correspond to the the *latest commit on the main branch.*\n\n|                      Tag                      |                        Dockerfile                         |                                  docker pull command                                  |                                                                                       Base Image                                                                                       |\n| :-------------------------------------------: | :-------------------------------------------------------: | :-----------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n|         `latest`  or `latest-azureml`         |         [azureml](docker/Dockerfile_base_azureml)         |     `docker pull PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest`     | [mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest](https://github.com/Azure/AzureML-Containers/tree/master/base/gpu/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04) |\n| `latest-nightly`  or `latest-azureml-nightly` | [azureml_nightly](docker/Dockerfile_base_azureml_nightly) | `docker pull PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-nightly` | [mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest](https://github.com/Azure/AzureML-Containers/tree/master/base/gpu/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04) |\n|                `latest-nvidia`                |          [nvidia](docker/Dockerfile_base_nvidia)          | `docker pull PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-nvidia`  |                                  [nvcr.io/nvidia/pytorch:22-06-py3](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html)                                  |\n\n- Building docker images and running docker containers locally - can be useful to reproduce issues which might occur while submitting to AzureML on your local machine. Please peruse public documentation on docker + vscode.\n\n```\n# pull image with [azureml image](https://hub.docker.com/_/microsoft-azureml?tab=description) as base with docker/environment.yml on top\ndocker pull PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest\n\n# (optional) pull image with nvidia pytorch image as base\ndocker pull PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-nvidia (for nvidia pytorch base image. See the note below for more details.)\n\n# run image\ndocker run -it --gpus=all -v <PATH_TO_THIS_REPO>:<PATH_TO_THIS_REPO> PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest\n\n# (optional) recommended give a name to your container\ndocker run -it --rm --name=MYFANCYCONTAINERNAME --gpus=all -v <PATH_TO_THIS_REPO>:<PATH_TO_THIS_REPO> PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest\n\n# setup the repo (run inside the container)\npip install -e .\n\n# install pre-commit (recommended). Scroll down to the \"Developing\" section for details.\npre-commit install\n```\n\n<details>\n  <summary>\n  More details on docker image tags for Pull Request and main branch builds (click to expand)\n  </summary>\nSimilar to the `main` branch, for each pull request, we have:\n\n- `PR-<#pr_number>-latest` aka `PR-<pr_number>-latest-azureml`\n- `PR-<#pr_number>-latest-nightly` aka `PR-<pr_number>-latest-azureml-nightly`\n- `PR-<#pr_number>-latest-nvidia`\n\nAnd finally for both `main` and PR branches, we have tags corresponding to git commit hashes\n\n- `main-<gitcommithash>-azureml` and `PR-<pr_number>-<gitcommithash>-azureml`\n- `main-<gitcommithash>-azureml-nightly` and `PR-<pr_number>-<gitcommithash>-azureml-nightly`\n- `main-<gitcommithash>-nvidia` and `PR-<pr_number>-<gitcommithash>-nvidia`\n\nFor example:\n\n- `main-7fadad2b-azureml`, `main-7fadad2b-azureml-nightly`, `main-7fadad2b-nvidia`: correspond to [commit 7fadad2b](https://github.com/AutonomousSystemsResearch/ml_template/commit/7fadad2b1391cdbbc46422a6865caaf0300b9af8) on `main` branch with our three different dockerfiles\n- `PR-50-latest-azureml`, `PR-50-latest-azureml-nightly`, `PR-50-latest-nvidia`: correspond to latest commit on [PR#50](https://github.com/AutonomousSystemsResearch/ml_template/pull/50) with our three different dockerfiles\n- `PR-50-eef3b90-azureml`, `PR-50-eef3b90-azureml-nightly`, `PR-50-eef3b90-nvidia`: correspond to [commit eef3b90](https://github.com/AutonomousSystemsResearch/ml_template/pull/50/commits/eef3b900fc956614c7d45eac6fa9245b57f7bd72) on [PR#50](https://github.com/AutonomousSystemsResearch/ml_template/pull/50) with our three different dockerfiles\n\n</details>\n<details>\n    <summary>\n    Building and understanding our Dockerfiles (click to expand)\n    </summary>\n\n- We have three docker files:\n\n  - azureml base:\n    - [docker/Dockerfile_base_azureml](docker/Dockerfile_base_azureml)\n    - [docker/Dockerfile_base_azureml_latest](docker/Dockerfile_base_azureml_latest)\n  - nvidia pytorch base:\n    - [docker/Dockerfile_base_nvidia](docker/Dockerfile_base_nvidia).\n\n- Both of the azureml base images grabs a base image from [here](https://github.com/Azure/AzureML-Containers/tree/master/base/gpu), and put the user's conda environment ([docker/environment.yml](docker/environment.yml)) on top of the base page.\n\n- In the `latest-azureml` version, packages in your local conda environment should match the docker image exactly.\n\n- In the `latest-azureml-nightly` image, pytorch (including cudatoolkit) and pytorch lightning are updated to the nightly versions.\n\n- The nvidia pytorch base image grabs a base image from [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags) ([here](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html) for details), which already has the latest version of pytorch.\n  Instead of using user's conda environment, this docker file uses `pip` to install pytorch lightning and other dependencies on top of base image. So this image can have different versions of  packages as compared to your conda environment.\n\nAll docker images accept a build argument to update the base image version easily:\n\n- azureml images:\n  - take base azure image name's suffix **and** tag. see available options [here](https://github.com/Azure/AzureML-Containers/tree/master/base/gpu):\n    - examples: `openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest`, `openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest`, and so on.\n- nvidia pytorch image:\n  - takes base nvidia image name's tag only.\n  - see [available tags here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags) and [the release notes for their contents](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html)\n  - examples: `22.06-py3`, `22.05-py3`, and so on.\n\nPlease review the arguments in the dockerfiles carefully. These can also be seen by reading through [azure-pipelines.yml](azure-pipelines.yml).\n\nBuilding the azure-ml base + conda env images locally:\n\n```\ncd docker;\n\ndocker build \\\n  -f Dockerfile_base_azureml \\\n  --build-arg BASE_IMAGE=openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04:latest \\\n  -t PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-azureml .\n\n# note that in the PRIVATEAZURECONTAINERREGISTRYNAME acr, latest is equivalent to latest-azureml tag. So, we can just re-tag the image:\ndocker tag PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-azureml PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest\n```\n\nFor the CUDA 11.6 version:\n\n```\ncd docker;\n\ndocker build \\\n  -f Dockerfile_base_azureml_cu116 \\\n  --build-arg BASE_IMAGE=openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04:latest \\\n  -t PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-azureml-cu116 .\n\n# note that in the PRIVATEAZURECONTAINERREGISTRYNAME acr, latest is equivalent to latest-azureml tag. So, we can just re-tag the image:\ndocker tag PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-azureml-cu116 PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-cu116\n```\n\nBuilding the nvidia-pytorch image locally:\n\n```\n# building nvidia-pytorch image with locally.\ncd docker;\n\ndocker build \\\n  -f Dockerfile_base_nvidia \\\n  --build-arg BASE_IMAGE=22.06-py3 \\\n  -t PRIVATEAZURECONTAINERREGISTRYNAME.azurecr.io/ml_template:latest-nvidia .\n```\n\n</details>\n\n<details>\n    <summary>\n    Developing inside docker containers with VSCode: (click to expand)\n    </summary>\n\n- [Attach to a docker container](https://code.visualstudio.com/docs/remote/attach-container)\n\n- [Devcontainer](https://code.visualstudio.com/docs/remote/containers)\n\n  > **Note**: This method can be used on an Azure VM or locally with no change and uses docker\n\n  Follow the steps below:\n\n  - Connect to your remote Azure VM using VS Code\n  - Open the workspace within a docker container for development, either using the popup as shown in the animation above, or by searching for `(Re)Build and (Re)open in container` in the  command palette (hit `Ctrl+Shift+P` to open the command palette)\n  - After setup is complete, it is time to set up the repository:\n    ```\n      pip install -e .\n      pre-commit install\n    ```\n  - > **Note**: By default, the devcontainer uses the [azureml-conda base image](docker/Dockerfile_base_azureml). We can also use the [nvidia base image](docker/Dockerfile_base_nvidia) by modifying the `dockerfile` line in [devcontainer.json](.devcontainer/devcontainer.json). Similarly, we can edit the docker files build argument therein itself.\n\n</details>\n\n#### Running MNIST example\n\n- Understanding OmegaConf and config files\n\n  - Please review OmegaConf's [github readme](https://github.com/omry/omegaconf#releases) for [their documentation](https://omegaconf.readthedocs.io/en/2.2_branch/), [slides (for ver 2.1)](https://docs.google.com/presentation/d/e/2PACX-1vT_UIV7hCnquIbLUm4NnkUpXvPEh33IKiUEvPRF850WKA8opOlZOszjKdZ3tPmf8u7hGNP6HpqS-NT5/pub?start=false&loop=false&delayms=3000&slide=id.p), and a [live tutorial](https://github.com/omry/omegaconf#live-tutorial).\n\n- Single GPU\n\n  ```\n  python src/train.py base=configs/train.yaml trainer.num_nodes=1 trainer.devices=1\n  ```\n\n- Multiple GPUs\n\n  ```\n  python src/train.py base=configs/train.yaml trainer.num_nodes=1 trainer.devices=4\n  ```\n\n### Running on Azure\n\nNote: This section used internal tools for job submission to Azure ML workspaces. This section is not supported publicly at the time of writing. However, one may peruse existing public documentation on azure ml.\n\n### Developing\n\n#### Tests\n\nThe template has some basic tests in `tests/` directory. To run them, run:\n\n```\n# run all tests\npytest\n\n# run single test\npytest tests/test_dev_fast_run.py\n```\n\nList of tests implemented:\n\n- [fast_dev_run](https://pytorch-lightning.readthedocs.io/en/stable/common/debugging.html#fast-dev-run):  a simple check to run your trainer on single batch of train, valid, and test datase.\n  It can also be useful to quickly check your code works by running while adding new features:\n  ```\n  python src/train.py base=configs/train.yaml --fast_dev_run=True\n  ```\n\n#### Code formatting and Linting\n\nWe use:\n\n- [black](https://black.readthedocs.io/en/stable/) for code formatting\n\n- [isort](https://pycqa.github.io/isort/) for import ordering\n\n- [pycln](https://hadialqattan.github.io/pycln/#/) for removing unused imports\n\n- Running locally:\n\n  ```\n  $ cd ml_template;\n  $ black .\n  $ isort .\n  $ pycln --all .\n  ```\n\n#### Pre-commit Hooks: Automating Code formatting and Linting\n\n[pre-commit](https://pre-commit.com/) hooks automate black autoformatting and ensuring PEP8 compliance.\n\n- Setting up:\n\n  ```\n  $ cd ml_template;\n  $ pre-commit install\n  ```\n\n- Running:\n\n  After the above step, `pre-commit` will run **automatically** when you `git commit`.\n  If the run fails with errors in red, you can check the edits made by `pre-commit` by `git diff`.\n  If the changes look good, (1) `git add` those files again, and then (2) run `git commit` again.\n\n  Optionally, you can also run pre-commit manually by:\n\n  ```\n  $ pre-commit run --all-files\n  ```\n\n- Updating hooks:\n  Use the `autoupdate` command to keep the versions of formatters in `.pre-commit-config.yaml` up to date.\n\n  ```\n  $ pre-commit autoupdate\n  ```\n\n### Continuous Integration\n\n- **Github Actions**\n\n  - [Pre-commit checks](.github/workflows/pre-commit.yml)\n  - [Template cleanup](.github/workflows/template-cleanup.yml):\n    When a new repository is generated using this template, this action replace `README.md` with `README_template.md` to keep microsoft links internal.\n\n- **Azure Pipelines**\n\n  - Create an azure devops pipeline for your repository.\n    This automates building of your docker images, and also run pytests on them.\n\n  - The azure pipeline logs can be seen at Azure DevOps webpage, but not on with github UI directly.\n\n    Pull Request example:\n\n    - You can click `View more details on Azure Pipelines` under the `Checks` section of a github PR.\n    - See [PR#6/checks](https://github.com/AutonomousSystemsResearch/ml_template/pull/6/checks) for an example.\n\n    <br>\n\n  - [Docker Build and Push Image](azure-pipelines.yml)\n\n    See the job `BuildDockerImageAndPush` in [azure-pipelines.yml](azure-pipelines.yml). It will build the image in [docker/Dockerfile](docker/Dockerfile) and push it to a private azure container registry\n\n    See docker section under #running-locally for details\n\n### Contributing\n\n- conda `environment.yml` update:\n\n  If you install packages in conda, update the `docker/environment.yml` by `conda env export | grep -v \"^prefix: \" > docker/environment.yml`, and send a PR.\n\n## Reference Repositories\n\n- Pytorch Lightning:\n\n  - Pytorch v/s Pytorch Lightning\n\n    - [PyTorch Lightning for Dummies - A Tutorial and Overview\n      ](https://www.assemblyai.com/blog/pytorch-lightning-for-dummies/)\n    - [PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers\n      ](https://dev.to/krypticmouse/pytorch-lightning-datamodules-callbacks-tpu-and-loggers-4nhb)\n\n  - Template / reference repositories\n\n    - https://github.com/ashleve/lightning-hydra-template\n    - https://github.com/lkhphuc/lightning-hydra-template\n    - [Pytorch lightning bolts](https://lightning-bolts.readthedocs.io/en/latest/)\n      - Look inside the code for datamodules, datasets, models, etc: https://github.com/PyTorchLightning/lightning-bolts/tree/master/pl_bolts\n\n- Pytorch Geometric:\n\n  - [lightning-examples](https://github.com/pyg-team/pytorch_geometric/tree/d451d6d20287b03cbe5036e5c53ee5f633f3c429/examples/pytorch_lightning)\n  - [torch_geometric.data.lightning_datamodule](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/lightning_datamodule.html)\n  - [Graph Gym](https://pytorch-geometric.readthedocs.io/en/latest/notes/graphgym.html)\n\n- Pytorch data, datapipes, dataloaders:\n\n  - https://pytorch.org/data/main/examples.html\n  - https://github.com/tcapelle/torchdata\n  - https://github.com/pytorch/data\n", "repo_name": "auto-sys-ml-template", "org_name": "microsoft", "org_repo": "microsoft/auto-sys-ml-template", "platform_org_repo": "github+microsoft/auto-sys-ml-template", "link_to_repo": "https://github.com/microsoft/auto-sys-ml-template", "platform": "github", "language": "Python", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Introduction \nMicrosoft Virtual File System for Perforce (**P4VFS**)\n\nThe P4VFS is a Windows service that allows you to sync files from Perforce quickly using almost no disk space. The contents of a file will be download automatically on-demand when first needed. You can seamlessly mix any use of P4VFS virtual sync with regular Perforce sync's using P4V and p4.exe.\n\nFor example, you can use the P4VFS to perform a \"virtual sync\" to a Perforce filespec and it will complete relatively quickly, and the files will exist on machine as usual. However, the actual size of the files on disk will be zero, and the file contents will be downloaded on-demand during the first read operation.\n\n# Installation\nYou can install the latest signed release of P4VFS from here:\n> [P4VFS.Setup](https://github.com/microsoft/p4vfs/releases/download/v1.25.0.0/P4VFS.Setup-1.25.0.0.exe)\n\nThe entire history of release notes is included with the installer.\n> [Release Notes](https://github.com/microsoft/p4vfs/blob/main/source/P4VFS.Console/P4VFS.Notes.txt)\n\n### Supported operating systems\n* Windows 10\n* Windows 11\n* Windows Server 2022 \n* Windows Server 2019\n* Windows Server 2016\n* Windows 8.1 (P4VFS 1.24.0.0 and earlier)\n\n# Technical Overview\nP4VFS is a Windows service, driver, and console application that allows us to sync files from Perforce immediately, and then actually download the file contents on-demand. It introduces the concept of a \"virtual\" sync, where a file revision can be sync'ed from Perforce and will exist locally on disk with a correct size, but zero disk-size, until accessed. When first opened, the Windows NTFS file system will automatically download file's contents, and the file will be read as expected.\n\nThe P4VFS is intended to work perfectly seamless with regular p4 and p4v usage. There is no need for special perforce server configuration, special workspaces, or any other client settings. You can feel free to sync files immediately using **p4.exe**, or use **p4vfs.exe** to virtual sync zero sized \"offline\" files with contents downloaded on-demand.\n\n# Basic Usage\nThe main program that you'll use to do a virtual sync, and possibly other P4VFS operations, is **p4vfs.exe**. \n\n    C:\\Program Files\\P4VFS\\p4vfs.exe\n\n The tool has a very similar interface to **p4.exe**. If you ever need to terminate **p4vfs.exe** while in-progress, simply use Ctrl+C or Ctrl+Break from the terminal, or just terminate the process with the Windows Task Manager. A p4vfs sync will always terminate gracefully and always leave your client consistent with Perforce. Take a look at the command help:\n\n    p4vfs.exe help\n    p4vfs.exe help sync\n \nTry syncing as usual: \n\n    p4vfs.exe sync //depot/tools/dev/...\n    p4vfs.exe sync //depot/tools/dev/...@600938\n    p4vfs.exe sync //depot/tools/dev/...#head  //depot/tools/release/...@599820\n\nThe tool respects P4CONFIG file usage, as well as supports typical configuration settings (just like **p4.exe**)\n \n    p4vfs.exe -c joe-pc-depot -u contonso\\joe -p p4-contoso:1666 sync //depot/tools/dev/...\n\n# Build and Test\n### Build Requirments:\n\n1. Visual Studio 2022 version 17.5.0 or later\n1. Windows SDK version 10.0.22621.0 (22H2) \n1. Windows WDK version 10.0.22621.382 (22H2)\n\nDetails for installing Visual Studio 2022, the Windows Software Development Kit (SDK), and the Windows Driver Kit (WDK) can be found here: \n> [Download the Windows Driver Kit](https://learn.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk)\n\n### Instructions:\n1. Using Visual Studio 2022, open solution file P4VFS.sln\n1. Build solution configurations Release/Debug for regular user mode development including the current production signed driver binaries. Build solution configuration Release-Dev/Debug-Dev for local built, test signed, driver binaries.\n1. Run P4VFS.Setup project to install local build.\n\n### Testing:\n1. Full suite of unit tests can be run locally by first opening Visual Studio as Administrator, and building the P4VFS.sln solution\n1. In the P4VFS.Console project, set the debug command arguments to:\n\n       p4vfs.exe test\n\n1. See the [Development.md](doc/Development.md) for details.\n\n\n", "repo_name": "p4vfs", "org_name": "microsoft", "org_repo": "microsoft/p4vfs", "platform_org_repo": "github+microsoft/p4vfs", "link_to_repo": "https://github.com/microsoft/p4vfs", "platform": "github", "language": "C#", "stargazers_count": 211, "watchers_count": 211}, {"README_text": "# Project\n\nThis repository contains code for a series of research projects about [Machine Intelligence on Sequence Data](https://seqml.github.io/).\n\n## News\n\n- 2023.4.17 [Learning Decomposed Spatial Relations for Multi-Variate Time-Series Modeling](https://seqml.github.io/srd/) is now available in [SRD](https://github.com/Arthur-Null/SRD/tree/main).\n- 2023.4.8 [SIMPLE: Specialized Model-Sample Matching for Domain Generalization](https://seqml.github.io/simple/) is now available in [SIMPLE](SIMPLE).\n- 2023.4.8 [Towards Inference Efficient Deep Ensemble Learning](https://seqml.github.io/irene/) is now available in [IRENE](IRENE).\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SeqML", "org_name": "microsoft", "org_repo": "microsoft/SeqML", "platform_org_repo": "github+microsoft/SeqML", "link_to_repo": "https://github.com/microsoft/SeqML", "platform": "github", "language": "Python", "stargazers_count": 30, "watchers_count": 30}, {"README_text": "# zeromq-prebuilt\n\n[![Build Status](https://dev.azure.com/monacotools/Monaco/_apis/build/status/Extensions/ms-toolsai/zeromq-prebuilt?branchName=main)](https://dev.azure.com/monacotools/Monaco/_build/latest?definitionId=466&branchName=main)\n\nBuilds [zeromq](https://github.com/zeromq/zeromq.js) on Azure Pipelines for multiple platforms, used by the [Jupyter Extension for VS Code](https://github.com/microsoft/vscode-jupyter).\n\n## Details\n\nReads `config.json` to determine which repo to clone and which tag to check out. If the build was triggered by a tag, each job will package the build outputs and publish them as pipeline artifacts, and the last job will upload those artifacts to the Github release. The Github release is consumed by the [Jupyter Extension](https://github.com/microsoft/vscode-jupyter).\n\n* zeromq.js is patched, see [patch.js](https://github.com/microsoft/zeromq-prebuilt/blob/main/patch.js) file for details\n* As and when a new release of zeromq.js has been published, we will need to\n\t* Update the [config.json](https://github.com/microsoft/zeromq-prebuilt/blob/main/config.json) file to point to the new upstream release\n    * Update the patches accordingly\n    * Update the build pipeline (if necessary, e.g. if there are new dependencies that need to be installed)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "zeromq-prebuilt", "org_name": "microsoft", "org_repo": "microsoft/zeromq-prebuilt", "platform_org_repo": "github+microsoft/zeromq-prebuilt", "link_to_repo": "https://github.com/microsoft/zeromq-prebuilt", "platform": "github", "language": "JavaScript", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# dataloader-benchmark\n\n## Benchmarking dataloader\n\n### local\n- generate smaller json by hand :\n\n  - copy first n lines:\n    ```\n    head -17411 /datadrive/commondatasets/tartanair-release1/train_ann_debug.json  > /datadrive/commondatasets/tartanair-release1/train_ann_debug_ratnesh.json\n    head -810 /datadrive/commondatasets/tartanair-release1/train_ann_debug.json  > /datadrive/commondatasets/tartanair-release1/train_ann_debug_ratnesh_100_frames.json\n    ```\n  - add some closing braces manually\n\n- generate smaller json with a script\n\n- Usage\n\n  ```\n  python src/benchmarks/tartanair_og.py  -h\n  ```\n\n- single worker debug on single environment\n\n  ```\n  python src/benchmarks/tartanair_og.py \\\n  --train_ann_file /datadrive/commondatasets/tartanair-release1/train_ann_debug_ratnesh_100_frames.json \\\n  --benchmark_results_file benchmark_results.csv \\\n  --modalities image_left depth_left flow_flow \\\n  --train_transform TartanAirVideoTransformWithAugmentation \\\n  --batch_size 1 \\\n  --num_workers 0 \\\n  --num_seq 1 \\\n  --seq_len 1\n  ```\n\n- pytorch bottleneck\n\n  ```\n  python -m torch.utils.bottleneck \\\n  src/benchmarks/tartanair_og.py \\\n  --train_ann_file /datadrive/commondatasets/tartanair-release1/train_ann_debug_ratnesh_100_frames.json \\\n  --benchmark_results_file benchmark_results.csv \\\n  --modalities image_left depth_left flow_flow \\\n  --train_transform TartanAirVideoTransformWithAugmentation \\\n  --batch_size 1 \\\n  --num_workers 0 \\\n  --num_seq 1 \\\n  --seq_len 1\n  ```\n\n- cProfile:\n  uncomment `cprofile(args)` and comment `main(args)` in `src/benchmarks_dataloader.py`\n\n- line_profiler\n\n  - install\n\n    ```\n    conda install -c anaconda line_profiler\n    ```\n\n  - add @profile macro before function.\n    For an example, uncomment @profile in `build.py -> class TartanAirVideoTransformWithAugmentation -> __call__` and `tartanair_video.py -> class TartanAirVideoDataset -> `__getitem__\\`\n\n  - run:\n\n    - step 1\n\n      ```\n      kernprof -l \\\n          src/benchmarks/tartanair_og.py \\\n          --train_ann_file /datadrive/commondatasets/tartanair-release1/train_ann_debug_ratnesh_100_frames.json \\\n          --benchmark_results_file benchmark_results.csv \\\n          --modalities image_left \\\n          --batch_size 1 \\\n          --num_workers 0 \\\n          --num_seq 16 \\\n          --seq_len 1\n      ```\n\n    - step 2\n\n      ```\n      python -m line_profiler benchmark_tartanair_dataloader.py.lprof\n      ```\n\n      Example results:\n\n      ```\n      (dataloader-benchmark) azureuser@linux-nc24rsv3-0:/datadrive/projects/dataloader-benchmark$ python -m line_profiler benchmark_tartanair_dataloader.py.lprof\n      Timer unit: 1e-06 s\n\n      Total time: 3.40855 s\n      File: /datadrive/projects/dataloader-benchmark/src/data/build.py\n      Function: __call__ at line 407\n\n      Line #      Hits         Time  Per Hit   % Time  Line Contents\n      ==============================================================\n        407                                               @profile\n        408                                               def __call__(self, item):\n        409                                                   # TODO: Need a better visualization of these augmentations.\n        410                                                   # 1. Color jittering\n        411       101        452.0      4.5      0.0          if self.do_color_jitter:\n        412                                                       # Asymmetric.\n        413       101        888.0      8.8      0.0              if np.random.rand() < self.asymmetric_color_aug_prob:\n        414        27     721222.0  26711.9     21.2                  images = [np.array(self.color_jitter(x)) for x in item[\"image_left\"]]\n        415                                                       # Symmetric.\n        416celse:\n        417       148       5898.0     39.9      0.2                  image_stack = np.concatenate(\n        418        74      37050.0    500.7      1.1                      [np.array(x) for x in item[\"image_left\"]], axis=0\n        419                                                           )  # Shape: [H,W,C]*D -> [D*H,W,C].\n        420       148      35557.0    240.2      1.0                  image_stack = np.array(\n        421        74    1944922.0  26282.7     57.1                      self.color_jitter(Image.fromarray(image_stack)), dtype=np.uint8\n        422                                                           )\n        423       148       5475.0     37.0      0.2                  images = np.split(\n        424        74        254.0      3.4      0.0                      image_stack, len(item[\"image_left\"]), axis=0\n        425                                                           )  # Shape: [D*H,W,C] -> [H,W,C]*D.\n        426                                                   else:\n        427                                                       images = [np.array(x) for x in item[\"image_left\"]]\n        428\n        429                                                   # 2. Flipping\n        430       101        411.0      4.1      0.0          if \"flow_flow\" in self.modalities:\n        431                                                       flows = item[\"flow_flow\"]\n        432       101        227.0      2.2      0.0          if \"depth_left\" in self.modalities:\n        433                                                       depths = item[\"depth_left\"]\n        434       101        215.0      2.1      0.0          if \"seg_left\" in self.modalities:\n        435                                                       segs = item[\"seg_left\"]\n        436\n        437       101        227.0      2.2      0.0          if self.do_flip:\n        438       101        785.0      7.8      0.0              if np.random.rand() < self.h_flip_prob:  # h-flip\n        439        54        323.0      6.0      0.0                  images = [x[:, ::-1] for x in images]  # Shape: [H,W,C].\n        440        54        120.0      2.2      0.0                  if \"flow_flow\" in self.modalities:\n        441                                                               flows = [\n        442                                                                   x[:, ::-1] * [-1.0, 1.0] for x in item[\"flow_flow\"]\n        443                                                               ]  # Shape: [H,W,2].\n        444        54        113.0      2.1      0.0                  if \"depth_left\" in self.modalities:\n        445                                                               depths = [x[:, ::-1] for x in item[\"depth_left\"]]  # Shape: [H,W,1].\n        446        54        138.0      2.6      0.0                  if \"seg_left\" in self.modalities:\n        447                                                               segs = [x[:, ::-1] for x in item[\"seg_left\"]]  # Shape: [H,W,1].\n        448       101        300.0      3.0      0.0              if np.random.rand() < self.v_flip_prob:  # v-flip\n        449        10         49.0      4.9      0.0                  images = [x[::-1, :] for x in images]\n        450        10         24.0      2.4      0.0                  if \"flow_flow\" in self.modalities:\n        451                                                               flows = [x[::-1, :] * [1.0, -1.0] for x in item[\"flow_flow\"]]\n        452        10         22.0      2.2      0.0                  if \"depth_left\" in self.modalities:\n        453                                                               depths = [x[::-1, :] for x in item[\"depth_left\"]]\n        454        10         21.0      2.1      0.0                  if \"seg_left\" in self.modalities:\n        455                                                               segs = [x[::-1, :] for x in item[\"seg_left\"]]\n        456\n        457                                                   # 3. Standard transformations\n        458       101     291773.0   2888.8      8.6          images = [Image.fromarray(x) for x in images]\n        459\n        460       101        315.0      3.1      0.0          transformed_item = {}\n        461       202      11404.0     56.5      0.3          transformed_item[\"image_left\"] = torch.stack(\n        462       101     349196.0   3457.4     10.2              [self.image_transform(x) for x in images], dim=1\n        463                                                   )  # Shape: [H,W,C]*D -> [C,H,W]*D -> [C,D,H,W].\n        464       101        445.0      4.4      0.0          if \"flow_flow\" in self.modalities:\n        465                                                       transformed_item[\"flow_flow\"] = torch.stack(\n        466                                                           [self.flow_transform(x) for x in flows], dim=1\n        467                                                       )  # Shape: [H,W,2]*D -> [2,H,W]*D -> [2,D,H,W].\n        468       101        252.0      2.5      0.0          if \"depth_left\" in self.modalities:\n        469                                                       transformed_item[\"depth_left\"] = torch.stack(\n        470                                                           [self.depth_transform(x) for x in depths], dim=1\n        471                                                       )  # Shape: [H,W,2]*D -> [2,H,W]*D -> [2,D,H,W].\n        472       101        234.0      2.3      0.0          if \"seg_left\" in self.modalities:\n        473                                                       transformed_item[\"seg_left\"] = torch.stack(\n        474                                                           [self.depth_transform(x) for x in segs], dim=1\n        475                                                       )  # Shape: [H,W,2]*D -> [2,H,W]*D -> [2,D,H,W].\n        476\n        477       101        236.0      2.3      0.0          return transformed_item\n      ```\n\n## Benchmarking ffcv\n\n### Installing:\n\n```\nsudo apt install libturbojpeg-dev libopencv-dev\n\n# cupy has pre-compiled binaries for your cuda version.\n\n# find your version and modify the command below accordingly\n\n# docs: https://docs.cupy.dev/en/stable/install.html#installing-cupy-from-pypi\n\npip install cupy-cuda116\n\n# could be pip install cupy-cuda117\n\npip install ffcv numba opencv-python\n\n```\n", "repo_name": "dataloader-benchmark", "org_name": "microsoft", "org_repo": "microsoft/dataloader-benchmark", "platform_org_repo": "github+microsoft/dataloader-benchmark", "link_to_repo": "https://github.com/microsoft/dataloader-benchmark", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Crea un Dataset desde cero con Python\nEn este repositorio encontrar\u00e1s un tutorial para crear un _dataset_ desde cero con Python. Tendras que procesar un archivo de texto, limiparlo, ordernalo con el fin de crear un archivo de CSV (comma separated values) que podras utilizar para entrenar un modelo de Machine Learning.\n\nEste tutorial se basa en un **caso real**, donde una comunidad de voluntarios usa una pagina web para anunciar si las rutas de bicicleta de monta\u00f1a estan abiertas o cerradas. Hay dos rutas: Valenciana y Borja. El objetivo de este tutorial es crear un CSV con Python que resulte en un ejemplo como este:\n\n```csv\nstatus, valenciana, borja\ntodas las rutas estan abiertas, 1, 1\ntodas las rutas estan cerradas, 0, 0\nla ruta de valenciana esta abierta, 1, 0\nla ruta de borja esta abierta, 0, 1\n```\n\n\ud83e\udd14 Si deseas aprender con una leccion usa el siguiente video:\n\n[![Introducci\u00f3n a Ciencia de Datos con Python](https://img.youtube.com/vi/zQ_AA5u4pOU/0.jpg)](https://youtu.be/zQ_AA5u4pOU \"Introducci\u00f3n a Ciencia de Datos con Python\")\n\n## Completa el reto\n\nEmpieza esta leccion abriendo el archivo [main.py](./main.py) y completando todos los pasos.\n\nEl archivo incluye comentarios que te guiaran en el proceso y la funcion `main()` que se encargara de ejecutar el codigo. No te olvides de actualizar la funcion para usar los datos limpios que vas agregando!\n\nSi tienes dudas, usa el video como guia.\n\n### Objetivos\n\n1. Limpiar los datos, linea por linea con Python\n2. Reducir el numero de datos identificando duplicados\n3. Normalizar el texto con min\u00fasculas \n4. Identificar si las rutas estan abiertas o cerradas, usando un 0 para cerrado y un 1 para abierto\n5. Crear un archivo CSV con los datos limpios y ordenados\n\n\n## Recursos\n\n- [Leccion en vivo del Mes de Datos](https://youtu.be/zQ_AA5u4pOU)\n- [Ruta de aprendizaje de Microsoft Learn](https://learn.microsoft.com/es-es/training/paths/beginner-python/?WT.mc_id=academic-0000-alfredodeza)\n- [Azure para estudiantes con credito incluido](https://azure.microsoft.com/free/students/?WT.mc_id=academic-91581-alfredodeza)\n- [GitHub Copilot Gratis para estudiantes](https://aka.ms/Copilot4Students)\n", "repo_name": "crea-un-dataset", "org_name": "microsoft", "org_repo": "microsoft/crea-un-dataset", "platform_org_repo": "github+microsoft/crea-un-dataset", "link_to_repo": "https://github.com/microsoft/crea-un-dataset", "platform": "github", "language": "Python", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# SimpleRacerResearchPlatform\n\n>  A simple racer research platform that showcases various imitation learning models with a web racing game.\n\n> Author group: Galena V Team from Microsoft project:[ Galena](https://www.microsoft.com/en-us/research/project/project-galena/)\n\n> Contact: kevingao@microsoft.com\n\n\n## Prerequisites\n\n- Supported OS: Windows WSL / Linux / MacOS\n- Install [Nodejs](https://nodejs.org/en/)\n\n## Project Structures\n\n```\n\u2514\u2500\u2500\u2500src:  Python notebooks and web demo UI files\n    \u251c\u2500\u2500\u2500data: Training data with [README](/src/data/README.md)\n    \u2514\u2500\u2500\u2500models: Pre-trained models (.onnx) with [README](/src/models/README.md)\n```\n\n## Getting Started\n\n(Below instruction is based on Ubuntu 22.04.1 LTS GNU/Linux 5.15.90.1-microsoft-standard-WSL2 x86_64)\n\n- Start your WSL:\n- > $ cd SimpleRacerResearchPlatform/src/\n- > $ node server\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a  CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n", "repo_name": "SimpleRacerResearchPlatform", "org_name": "microsoft", "org_repo": "microsoft/SimpleRacerResearchPlatform", "platform_org_repo": "github+microsoft/SimpleRacerResearchPlatform", "link_to_repo": "https://github.com/microsoft/SimpleRacerResearchPlatform", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# vscode-zeromq\n\nThis is an npm module for using zeromq binaries build in Azure Pipelines in a Node project. It is used by the [Jupyter Extension for VS Code](https://github.com/microsoft/vscode-jupyter-internal).\nThe binaries can be found in the releases section of the [zeromq-prebuilt](https://github.com/microsoft/zeromq-prebuilt) repo.\n\n# How it works\n\n* The npm package `zeromq` is installed as a node dependency.\n* This npm package is installed as a dev dependency.\n* As part of the post install, the exported function `downloadZMQ` should be invoked as follows:\n```typescript\nconst { downloadZMQ } = require('vscode-zeromq');\n// Download all binaries for all platforms and all architectures into the `node_modules/zeromq/prebuilds` folder.\nawait downloadZMQ()\n```\n\t* This will replace all of the binaries found in the `zeromq` module with the prebuilt binaries.\n\n\n# Usage example\nRun the following as a `postinstall` step (with the desired arguments).\n\n```typescript\nconst { downloadZMQ } = require('vscode-zeromq');\n// Download all binaries for all platforms and all architectures\nawait downloadZMQ()\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-zeromq", "org_name": "microsoft", "org_repo": "microsoft/vscode-zeromq", "platform_org_repo": "github+microsoft/vscode-zeromq", "link_to_repo": "https://github.com/microsoft/vscode-zeromq", "platform": "github", "language": "JavaScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# HAMS ALT\n\n![LICENSE](https://img.shields.io/github/license/microsoft/HAMS?style=for-the-badge) [![Dashboard](https://img.shields.io/website?down_message=Dashboard%20Offline&style=for-the-badge&up_color=green&up_message=Dashboard&url=https%3A%2F%2Fhams-dashboard.westus3.cloudapp.azure.com%2F)](https://hams-dashboard.westus3.cloudapp.azure.com) [![Documentation](https://img.shields.io/badge/docs-Documentation-blue?style=for-the-badge&logo=appveyor)](https://microsoft.github.io/HAMS) \n\n[![Docs](https://github.com/microsoft/HAMS/actions/workflows/gh_pages.yml/badge.svg)](https://github.com/microsoft/HAMS/actions/workflows/gh_pages.yml)\n\nThis project contains the core components of the Automated License Testing(ALT) system from the HAMS group at Microsoft Research, India.\n\n## Installation\n\n1. Run `pip install git+https://github.com/microsoft/HAMS` to install the latest version\n2. Downlaod the binaries from the [HAMS Releases](https://github.com/microsoft/HAMS/releases)\n3. Refer to additional requirements and instructions on each of the modules in the `Tutorials` section of the [documentation](https://microsoft.github.io/HAMS).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "HAMS", "org_name": "microsoft", "org_repo": "microsoft/HAMS", "platform_org_repo": "github+microsoft/HAMS", "link_to_repo": "https://github.com/microsoft/HAMS", "platform": "github", "language": "Python", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# ResiDual Transformer\n**The code implementation of [ResiDual: Transformer with Dual Residual Connections](https://arxiv.org/abs/2304.14802).**\n\n## Enviroment setup\nPlease use the anaconda to setup the environment by `conda env create -f environment.yaml`.\nThen you can activate the environment by `conda activate resi_dual`.\n\n## Model training\n\nFor model training, please follow the [guide](https://github.com/facebookresearch/fairseq/tree/main/examples/translation) to process data, and use the command below\n```bash\nLAYER=6 # or 12\nDATA_PATH=THE_PATH_TO_YOUR_DATA_BIN # the data should be fairseq data-bin format\nCONFIG=iwslt.yaml # or other config under hydra_config folder\n\nfairseq-hydra-train \\\ncommon.user_dir=$(pwd)/resi_dual \\\ntask.data=${DATA_PATH} \\\nmodel.encoder.layers=${LAYER} \\\nmodel.decoder.layers=${LAYER} \\\n--config-dir hydra_config \\\n--config-name ${CONFIG}\n```\n\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Cite\n```\n@article{xie2023residual,\n  title={ResiDual: Transformer with Dual Residual Connections},\n  author={Xie, Shufang and Zhang, Huishuai and Guo, Junliang and Tan, Xu and Bian, Jiang and Awadalla, Hany Hassan and Menezes, Arul and Qin, Tao and Yan, Rui},\n  journal={arXiv preprint arXiv:2304.14802},\n  year={2023}\n}\n```\n", "repo_name": "ResiDual", "org_name": "microsoft", "org_repo": "microsoft/ResiDual", "platform_org_repo": "github+microsoft/ResiDual", "link_to_repo": "https://github.com/microsoft/ResiDual", "platform": "github", "language": "Python", "stargazers_count": 66, "watchers_count": 66}, {"README_text": "# Adaptive Cards\n\n![logo](assets/adaptive-card-200.png)\n\n[Adaptive Cards](https://adaptivecards.io) are a new way for developers to exchange content in a common and consistent way. Get started today by putting Adaptive Cards into Microsoft Teams, Outlook Actionable Messages, Cortana Skills, or Windows Timeline -- or render cards inside your own apps by using our SDKs.\n\n## Dive in\n\n* [Documentation](https://adaptivecards.io/documentation/)\n* [Roadmap](https://github.com/microsoft/AdaptiveCards/projects/32)\n* [Schema Explorer](https://adaptivecards.io/explorer/)\n* [Sample Cards](https://adaptivecards.io/samples/)\n* [Designer](https://adaptivecards.io/designer/)\n\n## Install and Build\n\nAdaptive Cards are designed to render anywhere that your users are. The following native platform renderers are under development right now.\n\nPS: Latest Build Status is against `main` branch.\n\n|Platform|Latest Release|Source|Docs|Latest Build Status|\n|---|---|---|---|---|\n| .NET | [![Nuget install](https://img.shields.io/nuget/vpre/AdaptiveCards.svg)](https://www.nuget.org/packages/AdaptiveCards) | [Source](https://github.com/Microsoft/AdaptiveCards/tree/main/source/dotnet)| [Docs](https://docs.microsoft.com/en-us/adaptive-cards/create/libraries/net) | ![Build status](https://img.shields.io/azure-devops/build/Microsoft/56cf629e-8f3a-4412-acbc-bf69366c552c/20596/main.svg) |\n| .NET WPF | [![Nuget install](https://img.shields.io/nuget/vpre/AdaptiveCards.Rendering.Wpf.svg)](https://www.nuget.org/packages/AdaptiveCards.Rendering.Wpf) | [Source](https://github.com/Microsoft/AdaptiveCards/tree/main/source/dotnet)| [Docs](https://docs.microsoft.com/en-us/adaptive-cards/display/libraries/net-wpf) | ![Build status](https://img.shields.io/azure-devops/build/Microsoft/56cf629e-8f3a-4412-acbc-bf69366c552c/20596/main.svg) |\n| .NET HTML | [![Nuget install](https://img.shields.io/nuget/vpre/AdaptiveCards.Rendering.Html.svg)](https://www.nuget.org/packages/AdaptiveCards.Rendering.Html) | [Source](https://github.com/Microsoft/AdaptiveCards/tree/main/source/dotnet) | [Docs](https://docs.microsoft.com/en-us/adaptive-cards/display/libraries/net-html) | ![Build status](https://img.shields.io/azure-devops/build/Microsoft/56cf629e-8f3a-4412-acbc-bf69366c552c/20596/main.svg) |\n\n## End User License Agreement for our binary packages\nConsumption of the AdaptiveCards binary packages are subject to the Microsoft EULA (End User License Agreement). Please see the relevant terms as listed below:\n- [.NET](https://github.com/microsoft/AdaptiveCards/blob/main/source/EULA-Windows.txt)\n\nNOTE: All of the source code, itself, made available in this repo as well as our NPM packages, continue to be governed by the open source [MIT license](https://github.com/microsoft/AdaptiveCards/blob/main/LICENSE).\n\n## Contribute\n\nThere are many ways to [contribute](https://github.com/Microsoft/AdaptiveCards/blob/main/.github/CONTRIBUTING.md) to Adaptive Cards.\n* [Submit bugs](https://github.com/Microsoft/AdaptiveCards/issues) and help us verify fixes as they are checked in.\n* Review the [source code changes](https://github.com/Microsoft/AdaptiveCards/pulls).\n* Engage with Adaptive Cards users and developers on [StackOverflow](http://stackoverflow.com/questions/tagged/adaptive-cards). \n* Join the [#adaptivecards](https://twitter.com/hashtag/adaptivecards?f=tweets&vertical=default) discussion on Twitter.\n* [Contribute bug fixes](https://github.com/Microsoft/AdaptiveCards/blob/main/.github/CONTRIBUTING.md).\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see \nthe [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "AdaptiveCards-.NET", "org_name": "microsoft", "org_repo": "microsoft/AdaptiveCards-.NET", "platform_org_repo": "github+microsoft/AdaptiveCards-.NET", "link_to_repo": "https://github.com/microsoft/AdaptiveCards-.NET", "platform": "github", "language": "C#", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Adaptive Cards\n\n![logo](assets/adaptive-card-200.png)\n\n[Adaptive Cards](https://adaptivecards.io) are a new way for developers to exchange content in a common and consistent way. Get started today by putting Adaptive Cards into Microsoft Teams, Outlook Actionable Messages, Cortana Skills, or Windows Timeline -- or render cards inside your own apps by using our SDKs.\n\n## Dive in\n\n* [Documentation](https://adaptivecards.io/documentation/)\n* [Roadmap](https://github.com/microsoft/AdaptiveCards/projects/32)\n* [Schema Explorer](https://adaptivecards.io/explorer/)\n* [Sample Cards](https://adaptivecards.io/samples/)\n* [Designer](https://adaptivecards.io/designer/)\n\n## Install and Build\n\nAdaptive Cards are designed to render anywhere that your users are. The following native platform renderers are under development right now.\n\nPS: Latest Build Status is against `main` branch.\n\n|Platform|Latest Release|Source|Docs|Latest Build Status|\n|---|---|---|---|---|\n| Android | [![Maven Central](https://img.shields.io/maven-central/v/io.adaptivecards/adaptivecards-android.svg)](https://search.maven.org/#search%7Cga%7C1%7Ca%3A%22adaptivecards-android%22) | [Source](https://github.com/Microsoft/AdaptiveCards/tree/main/source/android) | [Docs](https://docs.microsoft.com/en-us/adaptive-cards/display/libraries/android) | ![Build status](https://img.shields.io/azure-devops/build/Microsoft/56cf629e-8f3a-4412-acbc-bf69366c552c/37913/main.svg)\n| iOS | [![CocoaPods](https://img.shields.io/cocoapods/v/AdaptiveCards.svg)](https://cocoapods.org/pods/AdaptiveCards) | [Source](https://github.com/Microsoft/AdaptiveCards/tree/main/source/ios) | [Docs](https://docs.microsoft.com/en-us/adaptive-cards/display/libraries/ios) |  ![Build status](https://img.shields.io/azure-devops/build/Microsoft/56cf629e-8f3a-4412-acbc-bf69366c552c/37917/main.svg) |\n\n## Code format\n\nWe require the C++ code inside this project to follow the clang-format. If you change them, please make sure your changed files are formatted correctly.\n\nMake sure clang-format version 12.0.0 and above version is used.\n\n### IDE integration\nClangFormat describes a set of tools that are built on top of LibFormat. It can support your workflow in a variety of ways including a standalone tool and editor integrations. For details, refer to https://clang.llvm.org/docs/ClangFormat.html\n\n### Format with script\nTwo scripts are provided to help you format files.\n- Windows user only: use FormatSource.ps1. This script use clang-format.exe which is built into Visual Studio by default.\n\n\tExecute below command in the root folder of the project\n\n\t```\n\tPowerShell.exe -ExecutionPolicy Bypass scripts\\FormatSource.ps1 -ModifiedOnly $False\n\t```\n\nIf it's the first time to run the script, make sure clang-format version 12.0.0 or above in the output. Otherwise you may need to upgrade Visual Studio or use your own clang-format binaries.\n```\n[clang-format] Version is:\nclang-format version 12.0.0\n```\n\n- Both Windows and MAC users: Use clang-format npmjs package\n\n\tExecute below command in source/nodejs\n\n\t```\n\tnpm run format\n\t``` \n\nMake sure `npm install` is run before.\n\n### Use Git pre-commit hook\n`git pre-commit hook` is an optional process. When you run `git commit`, it will automatically do the format check and auto fix the format if error detected.\n\nFirst make sure clang-format binary is installed in your dev enviroment.\nThen modify scripts/hooks/pre-commit to make sure clangFormat is point to the correct path.\nAnd finally setup the git hook.\n\nTwo ways to setup the hook:\n1. Copy `scripts/hooks/pre-commit` to `.git/hooks`\n2. `git config --local core.hooksPath scripts/hooks`\n\n## End User License Agreement for our binary packages\nConsumption of the AdaptiveCards binary packages are subject to the Microsoft EULA (End User License Agreement). Please see the relevant terms as listed below:\n- [Android/iOS](https://github.com/microsoft/AdaptiveCards/blob/main/source/EULA-Non-Windows.txt)\n\nNOTE: All of the source code, itself, made available in this repo as well as our NPM packages, continue to be governed by the open source [MIT license](https://github.com/microsoft/AdaptiveCards/blob/main/LICENSE).\n\n## Contribute\n\nThere are many ways to [contribute](https://github.com/Microsoft/AdaptiveCards/blob/main/.github/CONTRIBUTING.md) to Adaptive Cards.\n* [Submit bugs](https://github.com/Microsoft/AdaptiveCards/issues) and help us verify fixes as they are checked in.\n* Review the [source code changes](https://github.com/Microsoft/AdaptiveCards/pulls).\n* Engage with Adaptive Cards users and developers on [StackOverflow](http://stackoverflow.com/questions/tagged/adaptive-cards). \n* Join the [#adaptivecards](https://twitter.com/hashtag/adaptivecards?f=tweets&vertical=default) discussion on Twitter.\n* [Contribute bug fixes](https://github.com/Microsoft/AdaptiveCards/blob/main/.github/CONTRIBUTING.md).\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see \nthe [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "repo_name": "AdaptiveCards-Mobile", "org_name": "microsoft", "org_repo": "microsoft/AdaptiveCards-Mobile", "platform_org_repo": "github+microsoft/AdaptiveCards-Mobile", "link_to_repo": "https://github.com/microsoft/AdaptiveCards-Mobile", "platform": "github", "language": "C++", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "asrr_tools", "org_name": "microsoft", "org_repo": "microsoft/asrr_tools", "platform_org_repo": "github+microsoft/asrr_tools", "link_to_repo": "https://github.com/microsoft/asrr_tools", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Azure Partner center GitHub action\n\nThis action update the artifact of a plan within the Azure partner center offer.\n\nIt supports both Application offer and Azure Virtual Machine offer.\n\n## Prerequisites\n\nTo have the action works, you will need to setup three repository secrets for your pipeline(you can also pass them as parameters but it is not recommended):\n\n* CLIENT_ID: Client ID for an Azure AD application.\n* SECRET_VALUE: Secret value of the application.\n* TENANT_ID: Tenant ID you'd like to run pipeline against.\n\nHere are the steps to get those credentials:\n\n1. [Complete prerequisites for using the Partner Center submission API](https://learn.microsoft.com/en-us/azure/marketplace/azure-app-apis#how-to-associate-an-azure-ad-application-with-your-partner-center-account).\n\n1. [Quickstart: Register an application with the Microsoft identity platform](https://learn.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#changing-the-application-registration-to-support-multi-tenant)\n\n1. [Associate an existing Azure AD tenant with your Partner Center account](https://learn.microsoft.com/en-us/windows/apps/publish/partner-center/associate-existing-azure-ad-tenant-with-partner-center-account).\n\n## Inputs\n\n### `clientId`\n\n**Required** Client ID for an Azure AD application.\n\n### `secretValue`\n\n**Required** Secret value of the application.\n\n### `tenantId`\n\n**Required** Tenant ID you'd like to run pipeline against.\n\n### `offerId`\n\n**Required** The id of the offer.\n\n### `planId`\n\n**Required** The id of the plan.\n\n### `offerType`\n\n**Required** The type of the offer, supported values are `application_offer` and `vm_image_offer`.\n\n### `filePath`\n\n**Required for Application Offer** The path to the artifact(ZIP file).\n\n### `artifactVersion`\n\n**Required for Application Offer** The new version of the artifact.\n\n### `imageVersionNumber`\n\n**Required for Azure Virtual Machine Offer** The new version of the image.\n\n### `imageType`\n\n**Required for Azure Virtual Machine Offer** The type of the image, supported value examples are `x64Gen1`, `x64Gen2` etc.\n\n### `osDiskSasUrl`\n\n**Required for Azure Virtual Machine Offer** The OS Disk SAS URL.\n\n### `dataDiskSasUrl`\n\n**Required for Azure Virtual Machine Offer** The Data Disk SAS URL.\n\n### `operatingSystemFamily`\n\n**Required for Azure Virtual Machine Offer** The OS family like `linux`.\n\n### `operatingSystemType`\n\n**Required for Azure Virtual Machine Offer** The OS type like `redHat`.\n\n### `verbose`\n\n**If and only if `true`, output additional debugging information.\n\n## Outputs\n\n## Example usage\n\n### For Application Offer\n\n```terminal\nuses: microsoft/microsoft-partner-center-github-action@v3.1\nwith:\n  offerId: offerId\n  planId: planId\n  offerType: 'application_offer'\n  filePath: filePath\n  artifactVersion: artifactVersion\n  clientId: clientId\n  secretValue: secretValue\n  tenantId: tenantId\n```\n\n### For Virtual Machine Offer\n\n```terminal\nuses: microsoft/microsoft-partner-center-github-action@v3.1\nwith:\n  offerId: offerId\n  planId: planId\n  offerType: 'vm_image_offer'\n  imageVersionNumber: imageVersionNumber\n  osDiskSasUrl: osDiskSasUrl\n  dataDiskSasUrl: dataDiskSasUrl\n  imageType: imageType\n  operatingSystemFamily: operatingSystemFamily\n  operatingSystemType: operatingSystemType\n  clientId: clientId\n  secretValue: secretValue\n  tenantId: tenantId\n```\n", "repo_name": "microsoft-partner-center-github-action", "org_name": "microsoft", "org_repo": "microsoft/microsoft-partner-center-github-action", "platform_org_repo": "github+microsoft/microsoft-partner-center-github-action", "link_to_repo": "https://github.com/microsoft/microsoft-partner-center-github-action", "platform": "github", "language": "Shell", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\n\n## :fire: News\n* **[2023.04.12]** Incoming changes by 2023.04.20: Update LLM to [Azure OpenAI API for GPT4.0](https://openai.com/product/gpt-4)\n* **[2023.03.27]** Incoming changes by 2023.03.31: Update LLM to [Azure OpenAI API for GPT3.5 Turbo](https://azure.microsoft.com/en-in/blog/chatgpt-is-now-available-in-azure-openai-service/)\n* **[2023.03.21]** We build MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action.\n* **[2023.03.21]** Feel free to explore various demo videos on our [website](https://multimodal-react.github.io/)!\n* **[2023.03.21]** Try our [live demo](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react)!\n\n## :notes: Introduction\n![MM-REACT teaser](https://multimodal-react.github.io/images/teaser.png)\nMM-REACT allocates specialized vision experts with ChatGPT to solve challenging visual understanding tasks through multimodal reasoning and action.\n\n### MM-ReAct Design\n![design](https://multimodal-react.github.io/images/model_figure_2.gif)\n* To enable the image as input, we simply use the file path as the input to ChatGPT. The file path functions as a placeholder, allowing ChatGPT to treat it as a black box.\n* Whenever a specific property, such as celebrity names or box coordinates, is required, ChatGPT is expected to seek help from a specific vision expert to identify the desired information.\n* The expert output is serialized as text and combined with the input to further activate ChatGPT.\n* If no external experts are needed, we directly return the response to the user.\n\n## Getting Started\nMM-REACT code is bases on langchain.\n\nPlease refer to [langchain](https://github.com/hwchase17/langchain) for [instructions on installation](https://github.com/hwchase17/langchain#quick-install) and [documentation](https://github.com/hwchase17/langchain#-documentation).\n\n### Additional packages needed for MM-REACT\n\n```bash\npip install PIL imagesize\n```\n\n### Here are the list of resources you need to set up in Azure and their environment variables\n\n1. Computer Vision service, for Tags, Objects, Faces and Celebrity.\n\n```bash\nexport IMUN_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/vision/v3.2/analyze\"\nexport IMUN_PARAMS=\"visualFeatures=Tags,Objects,Faces\"\nexport IMUN_CELEB_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/vision/v3.2/models/celebrities/analyze\"\nexport IMUN_CELEB_PARAMS=\"\"\nexport IMUN_SUBSCRIPTION_KEY=\n```\n\n2. Computer Vision service for dense captioning. With a potentially different subscription key (e.g. westus region supports this)\n\n```bash\nexport IMUN_URL2=\"https://yourazureendpoint.cognitiveservices.azure.com/computervision/imageanalysis:analyze\"\nexport IMUN_PARAMS2=\"api-version=2023-02-01-preview&model-version=latest&features=denseCaptions\"\nexport IMUN_SUBSCRIPTION_KEY2=\n```\n\n3. Form Recogizer (OCR) prebuilt services\n\n```bash\nexport IMUN_OCR_READ_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-read:analyze\"\nexport IMUN_OCR_RECEIPT_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-receipt:analyze\"\nexport IMUN_OCR_BC_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-businessCard:analyze\"\nexport IMUN_OCR_LAYOUT_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-layout:analyze\"\nexport IMUN_OCR_INVOICE_URL=\"https://yourazureendpoint.cognitiveservices.azure.com/formrecognizer/documentModels/prebuilt-invoice:analyze\"\nexport IMUN_OCR_PARAMS=\"api-version=2022-08-31\"\nexport IMUN_OCR_SUBSCRIPTION_KEY=\n```\n\n4. Bing search service\n\n```bash\nexport BING_SEARCH_URL=\"https://api.bing.microsoft.com/v7.0/search\"\nexport BING_SUBSCRIPTION_KEY=\n```\n\n5. Bing visual search service (available on a separate pricing)\n\n```bash\nexport BING_VIS_SEARCH_URL=\"https://api.bing.microsoft.com/v7.0/images/visualsearch\"\nexport BING_SUBSCRIPTION_KEY_VIS=\n```\n\n6. Azure OpenAI service\n\n```bash\nexport OPENAI_API_TYPE=azure\nexport OPENAI_API_VERSION=2022-12-01\nexport OPENAI_API_BASE=https://yourazureendpoint.openai.azure.com/\nexport OPENAI_API_KEY=\n```\n\nNote: At the time of writing, we use and test against private endpoint. The public endpoint is now released and we plan to add support for it later.\n\n7. Photo editting local service\n\n```bash\nexport PHOTO_EDIT_ENDPOINT_URL=\"http://127.0.0.1:123/\"\nexport PHOTO_EDIT_ENDPOINT_URL_SHORT=127.0.0.1\n```\n\n### Sample code to run conversational-mm-assistant agent against an image\n\n[conversational-mm-assistant sample](sample.py)\n\n\n## Acknowledgement\n\nWe are highly inspired by [langchain](https://github.com/hwchase17/langchain).\n\n\n## Citation\n```\n@article{yang2023mmreact,\n  author      = {Zhengyuan Yang* and Linjie Li* and Jianfeng Wang* and Kevin Lin* and Ehsan Azarnasab* and Faisal Ahmed* and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang^},\n  title       = {MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},\n  publisher   = {arXiv},\n  year        = {2023},\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MM-REACT", "org_name": "microsoft", "org_repo": "microsoft/MM-REACT", "platform_org_repo": "github+microsoft/MM-REACT", "link_to_repo": "https://github.com/microsoft/MM-REACT", "platform": "github", "language": "Python", "stargazers_count": 693, "watchers_count": 693}, {"README_text": "# [PREME: Preference-based Meeting Exploration through an Interactive Questionnaire](https://arxiv.org/pdf/2205.02370)\nThis repo contains [the annotated data for PREME paper(https://github.com/microsoft/preme/blob/main/data.tsv).\n\nThe data contains 1000 annotated questions with their ```subject``` and ```aspect``` or ```what was said about  the subjest``` by 2 annotators. \n\nThe format of the annotated data is as follows where the query comes with a json file including:\n```\n- subject1\n- subject2  (if exists) \n- person (if exists)\n```\n\nFor example:\n```\nQuery<\\t>data\n How does the new system work with HMRC?  {\"Subject1\":\"new system\",\"Subject2\":\"HMRC\",\"What-Was-Said-About-the-Subject\":\"work with\"} # Annotators 1 \n How does the new system work with HMRC?\t{\"Subject1\":\"HMRC\",\"What-Was-Said-About-the-Subject\":\"new system work\"} # Annotator 2\n Which energy source is worth looking into?\t{\"Subject1\":\"energy source\",\"What-Was-Said-About-the-Subject\":\"worth looking into\"} # Annotators 1 \n Which energy source is worth looking into?\t{\"Subject1\":\"energy source\",\"What-Was-Said-About-the-Subject\":\"worth looking into\"} # Annotator 2\n```\n \nFor more information about the data, we encourage the read the [PREME paper](https://arxiv.org/abs/2205.02370) published in EACL2023. \n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Reference:\n\nIf you used PREME data in your work, please consider citing the following paper:\n```\n@@inproceedings{arabzadeh2022preme,\n      title={PREME: Preference-based Meeting Exploration through an Interactive Questionnaire}, \n      author={Negar Arabzadeh and Ali Ahmadvand and Julia Kiseleva and Yang Liu and Ahmed Hassan Awadallah and Ming Zhong and Milad Shokouhi},\n      year={2023},\n      booktitle={EACL}\n}\n```\n", "repo_name": "preme", "org_name": "microsoft", "org_repo": "microsoft/preme", "platform_org_repo": "github+microsoft/preme", "link_to_repo": "https://github.com/microsoft/preme", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# IGLU Datasets\n\nInteractive Grounded Language Understanding in a Collaborative Environment\n--\n\nThe primary goal of the IGLU project is to approach the problem of how\nto develop interactive agents that learn to solve a task while provided with grounded natural language\ninstructions in a collaborative environment.\n\nFollowing this objective, the project has collected several datasets with different types of interactions during a block building task. The data and scripts used to collect it will be progressively released in this repository.\n\nDue to the complexity of a general interactive task, the problem is simplified to interactions inside of a finite, Minecraft-like world of blocks. The goal of the interaction is to build a structure using a limited number of block types, which can vary in complexity. Examples of possible target structures to build are:\n\n![Shots of three possible target structures to build](./resources/imgs/voxelworld_combined_shots.png)\n\n Two roles arise: the Architect is provided with a target structure that needs to be built by the Builder. The Architect provides instructions to the Builder on how to create the target structure and the Builder can ask clarifying questions to the Architect if an instruction is unclear.\n\nThe progression of each game is recorded, corresponding to the construction of a target structure\nby an Architect and Builder pair, as a discrete sequence of game observations. Each observation\ncontains the following information: 1) a time stamp, 2) the chat history up until that point in time,\n3) the Builder\u2019s position (a tuple of real-valued x, y, z coordinates as well as pitch and yaw angles,\nrepresenting the orientation of their camera), 4) the Builder\u2019s block inventory, 5) the locations of\nthe blocks in the build region.\n\n<img src=\"./resources/imgs/voxelwrold_building_dialog.gif\" width=\"420\" height=\"280\" alt=\"Gif with interactions between Architect and Builder\"/>\n\n## Installation\n\nTo install the stable version of the IGLU-datasets API, please use the following command:\n\n```sh\npip install git+https://github.com/microsoft/iglu-datasets.git@master\n```\n\nAlternatively, you can download the repo and install it manually:\n\n```sh\ngit clone https://github.com/microsoft/iglu-datasets.git\ncd iglu-datasets && python setup.py install\n```\n\nThe requirements to install the API are the followings:\n\n```\ntqdm\nPILLOW\nopencv-python==4.5.5.64\npandas\nnumpy\nfilelock\nrequests\n```\n\n## Working with IGLU datasets \n\n\nThe IGLU-datasets library prives an easy and flexible way for for with Single and Multi turn datasets.\nHere is an example of how to use it:\n\n```python\nfrom iglu_datasets import MultiturnDataset, SingleturnDataset\n\n# leave dataset_version empty to access the most recent version of the dataset.\n# create a multiturn dataset instance\ndataset = MultiturnDataset(dataset_version='v1.0') \n\n# create a singleturn dataset instance\ndataset = SingleturnDataset(dataset_version='v1.0') \n```\n\nOn creation, this class will automatically download the dataset from this repository and parse (might take a few minutes) the raw data to store in a format that is fast and convienient to load. \nThere are two ways for accessing the underlying data.\nFirst, the `.sample()` method can be used. This method simply retrieves one random example from the dataset. The example is \nwrapped into `Task` format which has the following fields:\n\n```python\nsample = dataset.sample() # here a `Task` instance will be returned\nprint('Previous dialog\\n', sample.dialog) # the whole prior conversation EXCLUDING the current instruction.\nprint('Instruction\\n', sample.instruction) # the current instruction to execute by the builder. \n# If builder asks for clarifying questions, this will be a tuple (instruction, question, answer) \n# concatenated to a string.\nprint('Target grid\\n', sample.target_grid) # 3D numpy array of shape (9, 11, 11). \n# Represents the volume snapshot of the target blocks world that correspond to the builder's \n# result in responce to the instruction.\nprint('Starting grid\\n', sample.starting_grid) # 3D numpy array of shape (9, 11, 11). \n# Represents the volume snapshot of the starting blocks world with which the builder \n# starts executing the instruction.\n```\n\nThe multiturn dataset consists of structures that represent overall collaboration goals. For each structure, we have several collaboration sessions that pair architects with builders to build each particular structure. Each session consists of a sequence of \"turns\". Each turn represents an *atomic* instruction and corresponding changes of the blocks in the world. The structure of a `Task` object is following:\n\n  * `target_grid` - target blocks configuration that needs to be built\n  * `starting_grid` - optional, blocks for the environment to begin the episode with.\n  * `dialog` - full conversation between the architect and builder, including the most recent instruction\n  * `instruction` - last utterance of the architect\n\nSometimes, the instructions can be ambiguous and the builder asks a clarifying question which the architect answers. In the latter case, `instruction` will contain three utterances: an instruction, a clarifying question, and an answer to that question. Otherwise, `instruction` is just one utterance of the architect.\n\nHere is an example of task (the target structure is shown on the left and blocks to add are on the right):\n\n<img src=\"./resources/vids/output.gif\" width=\"640\" height=\"392\" alt=\"Gif with task vis\"/>\n\n\nTo represent collaboration sessions, the `Subtasks` class is used. This class represents a sequence of dialog utterances and their corresponding goals (each of which is a partially completed structure). On `.sample()` call, it picks a random turn and returns a `Task` object, where starting and target grids are consecutive partial structures and the dialog contains all utterances up until the one corresponding to the target grid.\n\nIn the example above, the dataset object is structured as follows:\n\n```python\n# .tasks is a dict mapping from structure to a list of sessions of interaction\ndataset.tasks \n# each value contains a list corresponding to collaboration sessions.\ndataset.tasks['c73']\n# Each element of this list is an instance of `Subtasks` class\ndataset.tasks['c73'][0]\n```\n\nThe `.sample()` method of `MultiturnDataset` does effectively the following:\n\n```python\ndef sample(dataset):\n  task_id = random.choice(dataset.tasks.keys())\n  session = random.choice(dataset.tasks[task_id])\n  subtask = session.sample() # Task object is returned\n  return subtask\n```\n\nThis behavior can be customized simply by overriding the reset method in a subclass:\n\n```python\nfrom iglu_datasets import MultiturnDataset\n\nclass MyDataset(MultiturnDataset):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.my_task_id = 'c73'\n    self.my_session = 0\n  \n  def sample(self):\n    return self.tasks[self.my_task_id][self.my_session].sample()\n\nmy_dataset = MyDataset(dataset_version='v1.0')\n# do training/sampling\n```\n\nOn the first creation, the dataset is downloaded and parsed automatically. Below you will find the structure of the dataset:\n\n```\ndialogs.csv\nbuilder-data/\n  ...\n  1-c118/ # session id - structure_id\n    step-2\n  ...\n  9-c118/\n    step-2\n    step-4\n    step-6\n  1-c120/\n    step-2\n  ...\n  23-c126/\n    step-2\n    step-4\n    step-6\n    step-8\n```\n\nHere, `dialog.csv` contains the utterances of architects and builders solving different tasks in \ndifferent sessions. The `builder-data/` directory contains builder behavior recorded by the voxel.js engine. Right now we extract only the resulting grids and use them as targets.\n\n### Singleturn dataset\n\nThe `SingleturnDataset` has the same structure of each sample. The main difference compared to the `MultiturnDataset` is \nthat here tasks are not structured into a chain but rather branch out from random chain steps. \n\n\nBelow you will find the structure of the single turn dataset:\n\n```\nsingle_turn_instructions.csv\nmulti_turn_dialogs.csv\ninitial_world_states/\n  builder-data/\n    <same as in multiturn>\ntarget_world_states/\n    actionHit/\n      <tree structure with game sessions>\n    <tree structure with game sessions>\n```\n\nHere, `multi_turn_dialogs.csv` and `initial_world_states/` is just the copy of the multiturn dataset under a different name. In `single_turn_instructions.csv` you can find the single turn instructions, and references to game sessions where the block states can be restored. \n\n### Grid prediction score calculation\n\n\nGiven a predicted grid and a target one, the intersection score is calculated based on their similarity. The score is determined regardless of global spatial position of currently placed blocks, it only takes into account how much the built blocks are similar to the target structure. To make it possible, at each step we calculate the intersection between the built and the target structures for each spatial translation within the horizontal plane and rotation around the vertical axis. Then we take the maximal intersection value among all translation and rotations. To calculate the score, we compare the maximal intersection size from the current step with the one from the previous step. The resulting intersection size can serve as a true positive rate for the `f1`/`precision`/`recall` score calculations, also it can be used as a reward function for a reinforcement learning agent. A visual example is shown below.\n\n<img src=\"./resources/imgs/intersections.png\" width=\"256\">\n\nSpecifically, we run the code that is equivalent to the following one. Note that our version is much more optimized, while the version is given for reference:\n\n```python\ndef maximal_intersection(grid, target_grid):\n  \"\"\"\n  Args:\n    grid (np.ndarray[Y, X, Z]): numpy array snapshot of a built structure\n    target_grid (np.ndarray[Y, X, Z]): numpy array snapshot of the target structure\n  \"\"\"\n  maximum = 0\n  # iterate over orthogonal rotations\n  for i in range(4):\n    # iterate over translations\n    for dx in range(-X, X + 1):\n      for dz in range(-Z, Z + 1):\n        shifted_grid = translate(grid, dx, dz)\n        intersection = np.sum( (shifted_grid == target) & (target != 0) )\n        maximum = max(maximum, intersection)\n    grid = rotate_y_axis(grid)\n  return maximum\n```\n\nIn practice, a more optimized version is used. There is a way to convert this score into a reward function for a reinforcement learning agent. To do that, we can calculate the reward based on the temporal difference between maximal intersection of the two consecutive grids. Formally, suppose `grids[t]` is a built structure at timestep `t`. The reward is then calculated as:\n\n```python\ndef calc_reward(prev_grid, grid, target_grid, , right_scale=2, wrong_scale=1):\n  prev_max_int = maximal_intersection(prev_grid, target_grid)\n  max_int = maximal_intersection(grid, target_grid)\n  diff = max_int - prev_max_int\n  prev_grid_size = num_blocks(prev_grid)\n  grid_size = num_blocks(grid)\n  if diff == 0:\n    return wrong_scale * np.sign(grid_size - prev_grid_size)\n  else:\n    return right_scale * np.sign(diff)\n```\n\nIn other words, if a recently placed block strictly increases or decreases the maximal intersection, the reward is positive or negative and is equal to `+/-right_scale`. Otherwise, its absolute value is equal to `wrong_scale` and the sign is positive if a block was removed or negative if added. This reward function is implemented in the [embodied IGLU environment](https://github.com/iglu-contest/gridworld).\n\n## References\n\nThe described datasets are collected as a part of [IGLU:Interactive Grounded Language Understanding in a Collaborative Environment](https://www.aicrowd.com/challenges/neurips-2022-iglu-challenge), which is described in the following papers:\n\n```\n@article{mohanty2023transforming,\n  title={Transforming Human-Centered AI Collaboration: Redefining Embodied Agents Capabilities through Interactive Grounded Language Instructions},\n  author={Mohanty, Shrestha and Arabzadeh, Negar and Kiseleva, Julia and Zholus, Artem and Teruel, Milagro and Awadallah, Ahmed and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur},\n  journal={arXiv preprint arXiv:2305.10783},\n  year={2023}\n}\n```\n\n```\n@article{mohanty2022collecting,\n  title={Collecting Interactive Multi-modal Datasets for Grounded Language Understanding},\n  author={Mohanty, Shrestha and Arabzadeh, Negar and Teruel, Milagro and Sun, Yuxuan and Zholus, Artem and Skrynnik, Alexey and Burtsev, Mikhail and Srinet, Kavya and Panov, Aleksandr and Szlam, Arthur and others},\n  journal={arXiv preprint arXiv:2211.06552},\n  year={2022}\n}\n```\n\n```\n@inproceedings{kiseleva2022interactive,\n  title={Interactive grounded language understanding in a collaborative environment: Iglu 2021},\n  author={Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and others},\n  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},\n  pages={146--161},\n  year={2022},\n  organization={PMLR}\n}\n```\n\n```\n@article{kiseleva2022iglu,\n  title={Iglu 2022: Interactive grounded language understanding in a collaborative environment at neurips 2022},\n  author={Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C{\\^o}t{\\'e}, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and others},\n  journal={arXiv preprint arXiv:2205.13771},\n  year={2022}\n}\n```\n\nConsider citing the papers above if you use the assets for your research.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "iglu-datasets", "org_name": "microsoft", "org_repo": "microsoft/iglu-datasets", "platform_org_repo": "github+microsoft/iglu-datasets", "link_to_repo": "https://github.com/microsoft/iglu-datasets", "platform": "github", "language": "Python", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# Debugpy extension for Visual Studio Code\n\nA [Visual Studio Code](https://code.visualstudio.com/) [extension](https://marketplace.visualstudio.com/VSCode) that supports Python debugging with debugpy. debugpy provides a seamless debugging experience by allowing you to set breakpoints, step through code, inspect variables, and perform other essential debugging tasks. The debugy extension offers debugging support for various types of Python applications including scripts, web applications, remote processes, and multi-threaded processes. \n\nNote: \n- The Python extension offers the debugpy extension as an optional installation, including it during the setup process.\n- This extension is supported for all [actively supported versions](https://devguide.python.org/#status-of-python-branches) of the Python language (i.e., Python >= 3.7).\n\n## Usage\n\nOnce installed in Visual Studio Code, debugpy will be automatically activated when you open a Python file.\n\n## Disabling the Debugpy extension\nIf you want to disable the Debugpy extension, you can [disable this extension](https://code.visualstudio.com/docs/editor/extension-marketplace#_disable-an-extension) per workspace in Visual Studio Code.\n\n## Commands\n\n| Command                | Description                       |\n| ---------------------- | --------------------------------- |\n| Debugpy: viewOutput | Show the debugpy extension output. |\n| Debugpy: clearCacheAndReload | Allows you to clear the global values set in the extension. |\n| Debugpy: debugInTerminal | Allows you to debug a simple Python file in the terminal. |\n\n## Data and telemetry\nThe Debubpy Extension for Visual Studio Code collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://privacy.microsoft.com/privacystatement) to learn more. This extension respects the `telemetry.enableTelemetry` setting which you can learn more about at https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting.\n", "repo_name": "vscode-python-debugger", "org_name": "microsoft", "org_repo": "microsoft/vscode-python-debugger", "platform_org_repo": "github+microsoft/vscode-python-debugger", "link_to_repo": "https://github.com/microsoft/vscode-python-debugger", "platform": "github", "language": "TypeScript", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Mypy extension for Visual Studio Code\n\nA Visual Studio Code extension with support for the `mypy` linter. The extension ships with `mypy=1.1.1`.\n\nFor more information on `mypy`, see https://www.mypy-lang.org/.\n\nNote:\n\n-   This extension is supported for all [actively supported versions](https://devguide.python.org/#status-of-python-branches) of the `python` language (i.e., python >= 3.7).\n-   The bundled `mypy` is only used if there is no installed version of `mypy` found in the selected `python` environment.\n-   Minimum supported version of `mypy` is `1.0.0`.\n\n## Usage\n\nOnce installed in Visual Studio Code, mypy will be automatically executed when you open a Python file.\n\nIf you want to disable mypy, you can [disable this extension](https://code.visualstudio.com/docs/editor/extension-marketplace#_disable-an-extension) per workspace in Visual Studio Code.\n\n## Settings\n\n| Settings              | Default                                       | Description                                                                                                                                                                                                                                                       |\n| --------------------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| mypy.args             | `[]`                                          | Custom arguments passed to `mypy`. E.g `\"mypy.args\" = [\"--config-file=<file>\"]`                                                                                                                                                                                   |\n| mypy.severity         | `{ \"error\": \"Error\", \"note\": \"Information\" }` | Controls mapping of severity from `mypy` to VS Code severity when displaying in the problems window. You can override specific `mypy` error codes `{ \"error\": \"Error\", \"note\": \"Information\", \"name-defined\": \"Warning\" }`                                        |\n| mypy.path             | `[]`                                          | Setting to provide custom `mypy` executable. This will slow down linting, since we will have to run `mypy` executable every time or file save or open. Example 1: `[\"~/global_env/mypy\"]` Example 2: `[\"conda\", \"run\", \"-n\", \"lint_env\", \"python\", \"-m\", \"mypy\"]` |\n| mypy.interpreter      | `[]`                                          | Path to a python interpreter to use to run the linter server.                                                                                                                                                                                                     |\n| mypy.importStrategy   | `useBundled`                                  | Setting to choose where to load `mypy` from. `useBundled` picks mypy bundled with the extension. `fromEnvironment` uses `mypy` available in the environment.                                                                                                      |\n| mypy.showNotification | `off`                                         | Setting to control when a notification is shown.                                                                                                                                                                                                                  |\n\n## Commands\n\n| Command              | Description                       |\n| -------------------- | --------------------------------- |\n| Mypy: Restart Server | Force re-start the linter server. |\n\n## Logging\n\nFrom the command palette (View > Command Palette ...), run the `Developer: Set Log Level...` command. From the quick pick menu, select `Mypy` extension from the `Extension logs` group. Then select the log level you want to set.\n", "repo_name": "vscode-mypy", "org_name": "microsoft", "org_repo": "microsoft/vscode-mypy", "platform_org_repo": "github+microsoft/vscode-mypy", "link_to_repo": "https://github.com/microsoft/vscode-mypy", "platform": "github", "language": "Python", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Azure Openai Democenter for China partners- owned by Micrsoft GPS China CSA Team:wave:\n    \nPlease visit the website URL :point_right: for this repository to complete the setup of this repository and configure access controls.\n", "repo_name": "gps-china-democenter", "org_name": "microsoft", "org_repo": "microsoft/gps-china-democenter", "platform_org_repo": "github+microsoft/gps-china-democenter", "link_to_repo": "https://github.com/microsoft/gps-china-democenter", "platform": "github", "language": null, "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# Embedded Chat\n\n|  [Architecture](/docs/architecture.md#overall-architecture) |  [Deployment Guide](/docs/deployment-guide.md#Deployment-Guide) |\n| ---- | ---- |\n\nEmbedded Chat is a solution to embed a Microsoft Teams Chat into a web application. A new chat can be automatically created on behalf of the user or the user can access existing Teams chats that they are members of.\n\nAs an example use case, you may want to embed a Teams chat into your Support Desk Ticketing system where the team working with service desk tickets do not have to leave the application to chat with others about the ticket. When a help desk team member opens the ticket in the ticketing system, the embedded chat could automatically create a new Teams Chat with the Service Desk Ticket Number as the chat group name and add the user who opened the ticket as a participant of the chat. The help desk team member can not chat directly in the Service Desk Ticket system with the user needing support. The user who is requesting support will be able to chat and receive notifications all from the Teams client. Below is an example of a chat that shows the chat in Microsoft Teams on the right and the embedded chat in a web application on the left.\n\n![sample chat](./images/embeddedchat%20example.png)\n\n## Supported Features\n\n1. Create a new chat\n1. Show latest chat messages (the message count fetched is configurable)\n1. Send a message to the chat\n1. Add participants\n1. Receive chats in near real time\n1. Receive participant changes in near real time\n1. Automatic subscription renewal for messages and participants\n\nEmbedded Chat can easily be embedded in any JavaScript enabled webpage by using this code sample.\n\n```html\n<div id=\"embed\"></div>\n<script src=\"/dist/graph/embeddedchat.min.js\" />\n\n<script>\nteamsEmbeddedChat.renderEmbed(\n    document.getElementById(\"embed\"),\n    {\n        entityId: \"987654\",\n        topicName: \"Invoice 987654\",\n    }\n);\n</script>\n```\n\n### Configurable Options\n\nChat conversations can be initiated by selecting a number of configuration options.\n\n| **Field**      | **Mandatory** | **Default Value**     | **Description** |\n| :---        |    :----:   |                  :--- |                 :--- |\n| Notification Source      | Yes       | Microsoft Graph  | Notification engine for delivering real time notifications with option to select either Microsoft Graph or ACS w/ Online Meeting. |\n| Entity ID   | Yes        | ' '     | Unique identifier that is used to identify a specific chat conversation. |\n| Chat Topic   | No        | Entity ID      | Subject or topic of a specific chat conversation.|\n| Chat Context Card JSON   | No        | ' '     | Information that describes a particular chat conversation in the form of an adaptive card JSON payload. |\n| Participants   | No        | ' '      | Participants Email address used to identify and reference a specific participant in the context of a chat conversation. |\n| Disable add participants   | No        | False    | Option to disable further addition of participants to a chat conversation from embedded chat UI, once chat has been initiated. |\n\n\n> **Note**   \n> The current version of the embedded chat does not support interactive elements such as actions in adaptive cards.\n> Feature to send Adaptive card in a ***chat message*** is currently not supported(Except chat context card). The adaptive card will, however, be rendered if sent via Teams.\n\n</em>\n\n#### Sample Configuration\n\n```text\nentityId: CR-786543\ntopicName: Product development and testing requests- ES\ncontextCard: { \"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\", \"type\": \"AdaptiveCard\", \"version\": \"1.5\", \"body\": [ { \"type\": \"TextBlock\", \"text\": \"Hi, can we chat about this Change Request?\", \"size\": \"Large\", \"weight\": \"Bolder\" }, { \"type\": \"TextBlock\", \"text\": \"This is a sample Chat Context Adaptive Card sent from Embedded Chat.\", \"wrap\": true }, { \"type\": \"Image\", \"url\": \"https://www.logodesignlove.com/wp-content/uploads/2012/08/microsoft-logo-02.jpeg\", \"size\": \"Medium\" } ] }\nparticipants: alex@mngenvmcap773902.onmicrosoft.com\ndisableAddParticipants: true\n\n```\n\n## Resources Needed\n\nEmbedded Chat runs as SPA in an existing browser application and uses the following resources.\n\n- Azure Storage Account (Table Storage)\n- Azure Web App\n- Azure Key Vault\n- Azure Communication Services (ACS) instance or [Graph Notification Broker (GNB)](https://github.com/microsoft/GraphNotificationBroker) instance\n- Microsoft Teams\n<em>\n\n> **Note**\n> The above Azure resources will be created & configured by the deployment script as mentioned in Deployment section. The GNB will need to be deployed prior to deploying this Embedded Chat application.\n</em>\n\n## Getting Started\n\nEmbedded chat requires the user to log into the Embedded Chat Azure AD application and grant consent to the required permissions before being able to create or access an existing chat. Embedded chat will create a new chat in Teams, fetch existing chat messages on initial load and send new messages using the Microsoft Graph API. All near real time notifications will be delivered depending on which 'Notification Source' was chosen. Currently we support the following Notification Sources:\n\n- Azure Communication  (ACS)\n- [Graph Notification Broker (GNB)](https://github.com/microsoft/GraphNotificationBroker)\n\nEmbedded Chat contains a sample application for testing ./wwwroot/index.html. Before you can use the sample application, the application dependencies will need to be deployed. If you plan to use the GNB as your notification source, you will need an existing instance that you have access to or will need to deploy that first. Refer to the [Graph Notification Broker (GNB)](https://github.com/microsoft/GraphNotificationBroker) repo for installation instructions.\n", "repo_name": "EmbeddedTeamsChat", "org_name": "microsoft", "org_repo": "microsoft/EmbeddedTeamsChat", "platform_org_repo": "github+microsoft/EmbeddedTeamsChat", "link_to_repo": "https://github.com/microsoft/EmbeddedTeamsChat", "platform": "github", "language": "TypeScript", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# App innovation with Azure Openai In-A-Day --Microsoft China GPS Tech \n\n ![architecture](./media/1.png)\n## In-A-Day\u4ecb\u7ecd \nApp innovation with Azure Openai In-a-day Workshop\u5229\u7528\u4e00\u5929\u65f6\u95f4\u4e0e\u5ba2\u6237\u4e00\u8d77\u4ea4\u6d41Azure Openai\u6280\u672f\u53ca\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6f14\u793a\u57fa\u4e8e\u573a\u666f\u7684openai demo\uff0c\u5e76\u6839\u636e\u5ba2\u6237\u9700\u6c42\u5b9a\u5236\u5316\u4e00\u4e9b\u5185\u5bb9\uff0c\u6bd4\u5982\u53ef\u80fd\u589e\u52a0\u52a8\u624b\u5b9e\u9a8c\u73af\u8282\u3002\u6240\u9700\u57fa\u672c\u5ba2\u6237\u4ea4\u6d41\u7684deck\u5728[\u8fd9\u91cc](./Workshop%20Content/)\n\n\n### \u4ee5\u4e0b\u662f\u5728In-A-Day Workshop\u4e0a\u91cd\u70b9\u5b8c\u6210\u7684\u52a8\u624b\u5b9e\u9a8c\uff1a\n- [\u5feb\u901f\u5165\u95e8\uff1a\u4f53\u9a8cChatGPT\u5e76\u901a\u8fc7\u7a0b\u5e8f\u8c03\u7528\u5b83\u7684API](https://learn.microsoft.com/zh-cn/azure/cognitive-services/openai/chatgpt-quickstart?tabs=command-line&pivots=programming-language-studio)\n- [\u591a\u6a21\u6001\u4f01\u4e1a\u6570\u636e\u77e5\u8bc6\u5e93\u667a\u80fd\u641c\u7d22](https://github.com/teo-ma/azure-open-ai-embeddings-qna)\n- [\u5173\u7cfb\u578b\u6570\u636eSQL\u667a\u80fd\u4ea4\u4e92](https://github.com/teo-ma/AzureSQLChatGPTDemo)\n\n\n### \u4ee5\u4e0b\u662f\u4f9b\u53c2\u8003\u7684sample\uff0c\u52a8\u624b\u5b9e\u9a8c\u6307\u5357\u6216\u6587\u6863\uff1a\n\n- [\u90e8\u7f72\u4f01\u4e1a\u81ea\u5df1\u7684Chat GPT-openai\u5b98\u65b9Chatgpt\u7684\u6700\u5c0f\u514b\u9686](https://github.com/teo-ma/cosmosdb-chatgpt)\n- [\u4f7f\u7528 OpenAI API \u6784\u5efa\u667a\u80fd\u804a\u5929\u673a\u5668\u4eba\u5e76\u5c06\u5e76\u90e8\u7f72\u5230 Azure App Service \u548cMicrosoft Teams App](https://github.com/microsoft/gps-csa-tech-stack/tree/main/Create-A-ChatGPT-Bot-APP-and-Deploy-To-Azure-APP-Service-or-Teams-APP)\n- [\u63a2\u7d22 Azure OpenAI \u670d\u52a1\u5d4c\u5165\u548c\u6587\u6863\u641c\u7d22](https://learn.microsoft.com/zh-cn/azure/cognitive-services/openai/tutorials/embeddings?tabs=command-line)\n- [\u4f7f\u7528\u8bed\u97f3\u4e0e Azure OpenAI \u5bf9\u8bdd](https://learn.microsoft.com/zh-cn/azure/cognitive-services/speech-service/openai-speech)\n- [\u5305\u542b\u591a\u4e2a\u4e0d\u540c\u573a\u666f\u7684sample\u8bf7\u53c2\u8003Openai\u5b98\u65b9Cookbook](https://github.com/openai/openai-cookbook)\n- [Azure Openai\u5b98\u65b9\u6587\u6863](https://learn.microsoft.com/zh-cn/azure/cognitive-services/openai/)\n\n\n## \u5728\u7ebf\u4f53\u9a8c\u4e0d\u540c\u573a\u666f\u7684Azure Openai Demo\uff08\u5fae\u8f6f\u4e2d\u56fdGPS CSA\u56e2\u961f\u63d0\u4f9b\uff09\u8bf7\u8bbf\u95ee[Openai Democenter](https://agreeable-flower-0968eb610.2.azurestaticapps.net/)\n\n ![architecture](./media/democenter.jpg)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "gpscsa-china-openai-in-a-day", "org_name": "microsoft", "org_repo": "microsoft/gpscsa-china-openai-in-a-day", "platform_org_repo": "github+microsoft/gpscsa-china-openai-in-a-day", "link_to_repo": "https://github.com/microsoft/gpscsa-china-openai-in-a-day", "platform": "github", "language": null, "stargazers_count": 38, "watchers_count": 38}, {"README_text": "# Project\n\nThis repository contains terraform environment configuration for privatelinks and services endpoint suffixes.\n\n## Getting started\n\nTo use this project in your terraform scripts, source the module as follows:\n\n```terraform\nmodule \"azurerm_environment_configuration\" {\n  source          = \"github.com/microsoft/terraform-azurerm-environment-configuration\"\n  arm_environment = <arm_environment>\n}\n```\n\n`arm_environment` - stands for [terraform environment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs#environment). Supported values are: `public` and `usgovernment`.\n\n### Privatelink\nTo get privatelink, use as in following example:\n\n```terraform\nmodule.azurerm_environment_configuration.private_links[\"privatelink.monitor.azure.com\"]\n```\n\nNote: provide the public private link as an input. Based on the environment the module will return the privatelink in your environment.\n\nFollowing links are available:\n\n- `privatelink.azurewebsites.net`\n- `privatelink.queue.core.windows.net`\n- `privatelink.table.core.windows.net`\n- `privatelink.monitor.azure.com`\n- `privatelink.oms.opinsights.azure.com`\n- `privatelink.ods.opinsights.azure.com`\n- `privatelink.agentsvc.azure-automation.net`\n- `privatelink.blob.core.windows.net`\n- `privatelink.web.core.windows.net`\n- `privatelink.file.core.windows.net`\n- `privatelink.vaultcore.azure.net`\n- `privatelink.azurecr.io`\n- `privatelink.eventgrid.azure.net`\n- `privatelink.mongo.cosmos.azure.com`\n- `privatelink.mysql.database.azure.com`\n- `privatelink.documents.azure.com`\n- `privatelink.servicebus.windows.net`\n- `privatelink.purview.azure.com`\n- `privatelink.purviewstudio.azure.com`\n- `privatelink.sql.azuresynapse.net`\n- `privatelink.dev.azuresynapse.net`\n- `privatelink.azuresynapse.net`\n- `privatelink.dfs.core.windows.net`\n- `privatelink.azurehealthcareapis.com`\n- `privatelink.dicom.azurehealthcareapis.com`\n- `privatelink.api.azureml.ms`\n- `privatelink.cert.api.azureml.ms`\n- `privatelink.notebooks.azure.net`\n- `privatelink.postgres.database.azure.com`\n- `privatelink.azuredatabricks.net`\n\n### Service Suffix\n\nTo get suffix for a service, use as in following example:\n\n```terraform\nmodule.azurerm_environment_configuration.storage_suffix\n```\n\nThe following suffixes are available:\n- storage_suffix - public cloud example: `core.windows.net`\n- acr_suffix - public cloud example: `azurecr.io`\n- web_app_suffix - public cloud example: `azurewebsites.net`\n\n### Endpoints\n\nTo get an Azure Endpoint, use as in following example:\n\n```terraform\nmodule.azurerm_environment_configuration.active_directory_endpoint\n```\n\nThe following endpoints are available:\n- active_directory_endpoint - public cloud example: `https://login.microsoftonline.com`\n- microsoft_graph_endpoint - public cloud example: `https://graph.microsoft.com`\n- resource_manager_endpoint - public cloud example: `https://management.azure.com`\n- aml_studio_endpoint - Azure Machine Learning Studio Endpoint. public cloud example: `https://ml.azure.com`\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "terraform-azurerm-environment-configuration", "org_name": "microsoft", "org_repo": "microsoft/terraform-azurerm-environment-configuration", "platform_org_repo": "github+microsoft/terraform-azurerm-environment-configuration", "link_to_repo": "https://github.com/microsoft/terraform-azurerm-environment-configuration", "platform": "github", "language": "HCL", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "<p align=\"center\">\n  <a href=\"https://github.com/microsoft/setup-msstore-cli/actions\"><img alt=\"setup-msstore-cli status\" src=\"https://github.com/microsoft/setup-msstore-cli/workflows/build-test/badge.svg\"></a>\n</p>\n\n# Setup MSStore Developer CLI\n\nThis repository contains the source code for the `setup-msstore-cli` GitHub Action as well as the `setup-msstore-cli` Azure DevOps extension.\n\nThis action/extension sets up the [MSStore Developer CLI](https://github.com/microsoft/msstore-cli) on a runner/agent.\nThe MSStore Developer CLI is a command line interface that allows you to manage your Microsoft Store apps and in-app products.\n\nExample (GitHub Action):\n  \n```yaml\nname: MSStore CLI\non: [push]\njobs:\n  build:\n    runs-on: windows-latest\n    steps:\n    - uses: actions/checkout@v3\n    - uses: microsoft/setup-msstore-cli@v1\n    - run: msstore reconfigure --tenantId ${{ secrets.PARTNER_CENTER_TENANT_ID }} --sellerId ${{ secrets.PARTNER_CENTER_SELLER_ID }} --clientId ${{ secrets.PARTNER_CENTER_CLIENT_ID }} --clientSecret ${{ secrets.PARTNER_CENTER_CLIENT_SECRET }}\n    - run: msstore apps list\n```\n\nExample (Azure DevOps extension):\n  \n```yaml\nname: MSStore CLI\ntrigger:\n- main\npool:\n  vmImage: 'windows-latest'\nsteps:\n- checkout: self\n- task: UseMSStoreCLI@0\n- script: msstore reconfigure --tenantId $(PARTNER_CENTER_TENANT_ID) --sellerId $(PARTNER_CENTER_SELLER_ID) --clientId $(PARTNER_CENTER_CLIENT_ID) --clientSecret $(PARTNER_CENTER_CLIENT_SECRET)\n- script: msstore apps list\n```", "repo_name": "setup-msstore-cli", "org_name": "microsoft", "org_repo": "microsoft/setup-msstore-cli", "platform_org_repo": "github+microsoft/setup-msstore-cli", "link_to_repo": "https://github.com/microsoft/setup-msstore-cli", "platform": "github", "language": "JavaScript", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Data Discovery Solution \ud83d\udd2c <img src=\"Project_Preparation/Decision_Guidance/repo_images/form-recognizer-demo-sample1.png\" align=\"right\" alt=\"\" width=\"350\"/>\n\n<img src=\"images/dashboard.gif\" align=\"center\" alt=\"\" width=\"750\"/>\n\n<img src=\"images/concept_graph.gif\" align=\"center\" alt=\"\" width=\"750\"/>\n\nEvery machine learning project requires a deep understanding of the data, to be able to understand whether the data is representative of the problem to be solved, to determine the approaches to be undertaken and indeed for the project to be successful.\n\n<img src=\"images/insights.png\" align=\"right\" alt=\"Power BI insights\" width=\"350\"/>\n\nUnderstanding of the data typically takes place during the Exploratory Data Analysis (EDA) phase. It is complex part of the project where data is attempted to be cleansed, outliers identified and the suitability of the data is assessed to inform hypothesis and experiments.\n\nThe following image illustrates the various phases, their respective complexity and roles during a typical machine learning project:\n\n<img src=\"images/stagesandroles.png\" alt=\"stages and roles in a typical ML project\" width=\"750\"/>\n\n## Project goals \ud83c\udfaf\n\nThe Data Discovery Playbook aims to quickly provide structured views on your text, images and videos, all at scale using Synapse and unsupervised ML techniques that exploit state of the art deep learning models.\n\nThe goal is to present this data to and facilitate discussion with a business user/data owner very quickly via PowerBI visualisation, so that the customer and team can decide the next best action with the data, identify outliers or generate a training data set for a supervised model.\n\nAnother goal is to help simplify and accelerate the complex Exploratory Data Analysis phase of the project by democratising common data science functions and to accelerate your project so that can focus more on the business problem you are trying to solve.\n\nKeep all code assets standalone and as simple as possible for quick usage or adaptation for production usage.\n\n## Overview\n\nThe aim of this Playbook is to illustrate the usage of the tools, alongside guidance, examples and documentation to get rapid insights of your unstructed data, all of which have been applied in real customer solutions.\n\n<img src=\"images/data_discovery_process.png\" align=\"center\" alt=\"The Data Discovery Process\" width=\"750\"/>\n\n## Intended audience\n\nThe intended audience of this Playbook includes:\n\n* Engineering/project leads\n* Data scientists/data engineers\n* Machine learning engineers\n* Software engineers\n\n## General approach for this Playbook\n\nThis Playbook provides code to quickly discover data as part of the Exploratory Data Analysis phase of the project. The overall approach is to take a large unstructured dataset that has no labels available, and to iterate over the data using a variety of techniques to aggregate, cluster and ultimately label the data in a cost effective and timely manner.\n\nThis is achieved by using unsupervised ML clustering algorithms, heuristic approaches and by direct input and validation by a domain expert. Asking questions of the data in natural language is also possible, if text based, using the semantic search feature of Azure Cognitive Search.  \n\nBy combining these approaches, structure and labels can be applied to large datasets so that the data may either be indexed for discovery via a search solution such as Azure Cognitive Search or for a supervised ML model to be trained so that future unseen data can be classified accordingly.\n\nThe following illustrates this approach at a high level for a text based problem where large amounts of unstructured data exists:\n\n1) Cluster and explore the data quickly in the generated interactive PowerBI report\n1) Ask specific questions of your data from within the Synapse notebooks using Azure Cognitive Search with Semantic search and SynapseML\n1) Assess the data to determine whether some simple heuristics may be applied to classify the data with a semantically relevant term - see the Heuristics notebook\n1) Apply the heuristic classification to the underlying data and remove the data from the larger corpus that could be classified\n1) Run Text Clustering in the remaining data and generate Word Clouds - iterate until a ideal number of clustered data appears and the clusters make sense to a Domain Expert\n1) The Domain Expert assesses the Word Clouds in more detail, and makes obvious corrections to Word Clouds by programmaticaly moving terms between clusters or using [PixPlotML](https://github.com/alexhock/pixplotml)\n1) The Domain Expert assess the Concept Graph for network connectivity and relationships\n1) The Domain Expert labels the clusters with a semantically relevant term which is programmatically propagated to the underlying records within the dataset\n1) Merge steps 1 and 6 which now allows for a classification model to be trained\n\n## Using PixPlotML to rapidly visualise and label data for training\n\n[PixPlotML](https://github.com/alexhock/pixplotml) is an interactive and zoomable visualization of your whole dataset. This web-based tool, a modified version of the original Pixplot, is valuable for object detection and classification projects to perform these tasks:\n\n1) Initial investigation and visualization of a labelled (or unlabelled) dataset.\n1) Fixing incorrect classifications and removing invalid or confusing images. (Click on an image and update its label, or flag for removal)\n1) Visualizing false positive bounding boxes to identify why they are occuring.\nImages that look similar are located next to or near each other, making it easy to see where errors occur (in the UMap visualization).\n\nImages that look similar are located next to or near each other, making it easy to see where errors occur (in the UMap visualization).\n\nUMap Visualization            |  Interactive and Zoomable          |   Different Views (by label)                |\n:-------------------------:|:-------------------------:|:-------------------:\n![](images/pixplot_small.png)  |  ![](images/pixplot_detail_small.png) | ![By Category](images/pixplot_by_category_small.png)\n\n## Hypothesis driven development and experiment tracking \ud83e\uddea\n\nCode written during EDA may not make it to production, but treating it as production code is vital as it provides an audit and represents the investment made to determine the correct ML solution as part of a [hypothesis driven development approach](https://www.thoughtworks.com/insights/articles/how-implement-hypothesis-driven-development).\n\nThis allows teams to not only reproduce the experiments but also be able to learn from past lessons learnt, saving time and associated development costs.\n\nAll Synapse notebooks contain full AML and MLFlow experiment tracking to provide lineage on data and parameters used.\n\n## Technology used \ud83e\udd16\n\nThis Playbook aims to provide similar approaches accross a variety of technologies and uses the following components:\n\n* [Azure Synapse](https://azure.microsoft.com/en-gb/services/synapse-analytics/#overview) notebooks and Spark Pools for data processing and compute.\n* [PowerBI](https://powerbi.microsoft.com/en-us/) for rapid and simple interactive data visualisation dashboards\n* [OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview) These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation\n* [Keras Applications](https://keras.io/api/applications/) in particular [InceptionV3](https://keras.io/api/applications/inceptionv3/) for image inference and feature extraction\n* [Transformers Packages](https://huggingface.co/docs/transformers/) for feature extraction\n* [BLIP model](https://huggingface.co/spaces/Salesforce/BLIP) for image captioning via visual and word transformers\n* [Pegasus xsum model](https://huggingface.co/google/pegasus-xsum) for abstractive text summarisation\n* [SparkML pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) for clustering techniques\n* [SKLearn](https://scikit-learn.org/stable/modules/clustering.html) for clustering techniques and others\n* [spaCy](https://spacy.io/) for feature extraction\n* [Azure Cognitive Search](https://azure.microsoft.com/en-us/services/search/) for rapid search of the dataset\n* [PixPlotML](https://github.com/alexhock/pixplotml) for rapidly visualising and labelling datasets\n\n## Getting Started - Synapse\n\n<img src=\"images/dd_deploy.gif\" align=\"center\" alt=\"\" width=\"750\"/>\n\n### If you do not have a Synapse Workspace\n\nA new Synapse workspace and all cluster configuration and notebooks can be deployed from here.\n\n#### Prerequisites\n\n1) Download and install the [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n1) Download and install [jq](https://stedolan.github.io/jq/download/), a lightweight and flexible command-line JSON processor\n\n1) Azure Data Lake Storage Gen2 storage account - The Azure Synapse workspace needs to be able to read and write to the selected ADLS Gen2 account. In addition, for any storage account that you link as the primary storage account, you must have enabled hierarchical namespace at the creation of the storage account, as described on the Create a Storage Account page. More info on creating Azure Data Lake Storage can be found [here](https://learn.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account)\n\n#### Deployment steps\n\n* Login to your Azure Subscription via ``` az login ```\n\n* Clone the Playbook repo:\n\n  ```bash\n  git clone https://github.com/microsoft/data-discovery-toolkit\n\n  cd data-discovery-toolkit/environment_preparation/deployment\n  ```\n\n* Rename the file `vars.sample` to `vars.env`\n* Populate the required variables within the `vars.env` file:\n\n```bash\n# The resource group that your Synapse instance has been provisioned to\nSynapseResourceGroup=\n# The region of the Synapse Resource Group\nRegion=\n# The ADLS Storage Account name\nStorageAccountName=\n# The resource group of the ADLS storage account\nStorageAccountResourceGroup=\n# The name of the file share within the ADLS storage account\nFileShareName=\n# The name of the Synapse Workspace\nSynapseWorkspaceName=\n# The Synapse SQL user\nSqlUser=\n# The Synapse SQL password\nSqlPassword=\n# The Azure subscription id\nSubscriptionId=\n```\n\n* Run the following command:\n\n```bash\nsh deploy.sh\n```\n\n\u2615 Grab some coffee as it will take around 30 minutes \u2615\n\n#### \u2714\ufe0f _Verifying the install_\n\n1) Verify that your resource group contains a provisioned Synapse service\n1) Verify that no permission errors are raised when launching the Synapse Workspace\n1) Verify that are Apache Spark Pool cluster has been provisioned\n1) Verify that packages have been installed against the cluster\n1) Verify that the notebooks have been imported into a folder called `Data Discovery`\n1) Verify that you can associate the Apache Spark Pool cluster with a notebook\n1) Verify that a Spark session is started when executing the notebook\n\n### If you already have a Synapse Workspace\n\nTo use the Synapse components, an [Azure Synapse Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-pool-configurations) is required. Please navigate to [Synapse Environment Preparation](/environment_preparation/synapse/README.md) to configure the cluster for usage.\n\n### To use the Azure Cognitive Search Features\n\nTo use the [Azure Cogitive Search](https://azure.microsoft.com/en-us/services/search/) functionality, a provisioned Search instance must be provisioned and ensure that [Semantic Search is enabled](https://docs.microsoft.com/en-us/azure/search/semantic-search-overview#enable-semantic-search)\n\n## Terminology used in this Playbook \ud83d\udcd6\n\n* `Node` - a single infrastructure VM comprised of compute and memory\n* `Cluster` - a group of nodes\n* `Spark Pool` - a cluster with its associated configuration and sizing\n* `Clustering` - an unsupervised machine learning technique for grouping similar records together\n* `ADLS` - [Azure Data Lake Storage](https://azure.microsoft.com/en-gb/services/storage/data-lake-storage/)\n\n## Text clustering\n\nRefer to the [Text Clustering section](/walkthroughs/text_clustering.md) For more detailed information on clustering documents.\n\n## Code accelerators \ud83d\ude80\n\nThe following code accelerators serve as starting points to try approaches that are known to work for the data discovery phase.\n**Note - these accelerators are not intended for production, they will require amendment to incorporate into a production pipeline**\n\n| Media Type | Scenario | Description | Platform |\n| -------- | ----------- | ------| ----- |\n| Text Documents | [Text Clustering](/Synapse/notebooks/text_clustering/standalone_text_clustering.ipynb) | Extract features with TF-IDF and cluster documents with built in Search and interactive PowerBI report | Synapse\n| Text Documents | [Text Clustering](/Synapse/notebooks/text_clustering/standalone_text_clustering_spaCy.ipynb) | Extract features with [spaCy](https://spacy.io/) and cluster documents with built in Search and interactive PowerBI report | Synapse\n| Text Documents | [Text Clustering](/Synapse/notebooks/text_clustering/standalone_text_clustering_BERT.ipynb) | Extract features with [BERT](https://huggingface.co/docs/transformers/model_doc/bert) and cluster documents with built in Search and interactive PowerBI report| Synapse\n| Text Documents | [Text Clustering](/Synapse/notebooks/text_clustering/standalone_text_clustering_OpenAI.ipynb) | Extract features with [Azure OpenAI](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/) and cluster documents with built in Search and interactive PowerBI report | Synapse\n| Text Documents | [Text Summarisation](/Synapse/notebooks/text_summarisation/standalone_text_summarisation.ipynb) | Generate abstractive text summaries with [Pegasus xsum model](https://huggingface.co/google/pegasus-xsum) with built in Search | Synapse\n| Images and videos     | [Image Clustering](/Synapse/notebooks/image_clustering/standalone_image_clustering.ipynb) | Extract features from images, make an [imagenet](https://www.image-net.org/) prediction and cluster | Synapse\n| Images and videos     | [Image Captioning](/Synapse/notebooks/image_captioning/standalone_image_captioning.ipynb) | Generate a caption for an image and cluster the captions with built in Search and interactive PowerBI report | Synapse\n\n## Tips for Synapse \ud83d\udca1\n\nSee [Environment preparation for Synapse](/environment_preparation/synapse/README.md)\n\n## Example walkthroughs with data \ud83e\udded\n\nThis section contains some documented common scenarios:\n\n Scenario | Description | Platform |\n| -------- | ----------- | ----- |\n[Animal Face Image Captioning and clustering](/walkthroughs/animal_faces/captioning_and_clustering/README.md) | End to end walkthrough of captioning against an animal face dataset | Synapse\n[Animal Face Feature Extraction and clustering](/walkthroughs/animal_faces/feature_extraction_and_clustering/README.md) | End to end walkthrough of clustering against an animal face dataset | Synapse\n| [Interactive Image Cluster dashboard](/walkthroughs/powerBI/README.md) | Setting up a dashboard from scratch in 2 minutes | PowerBI\n| [BBC Sports Similarity Matrix](walkthroughs/bbc/graph_similarity.ipynb) | A notebook that illustrates how to use locality-sensitive-hashing (LSH) to create a similarity matrix. Could be useful when dealing with large amounts of text documents | Synapse\n| [Image Situation dataset bias](/walkthroughs/imsitu) | A notebook that shows how to detect biases in a labeled image dataset | Synapse\n| [Applying simple heuristics for classification](/walkthroughs/heuristics/standalone_text_heuristics.ipynb) | A notebook that shows how run simple search techniques to quickly get a baseline | Synapse\n| [Classify BBC Sports documents with Azure OpenAI](/walkthroughs/OpenAi-classification/OpenAI-Classification.ipynb) | A notebook that shows how to use OpenAI one-shot classification to quickly get a baseline | Synapse\n| [EDA with Azure OpenAI](/walkthroughs/OpenAI-EDA/BBC_OpenAI_EDA.ipynb) | A notebook that shows how to use OpenAI for EDA including OpenAI Retrieval Augmented Generation Pattern evaluation using Azure Cognitive Search, Azure Cognitive Semantic Search, Azure Synapse/Trident and evaluation, clustering and automated classification  | Synapse\n\n## Azure Services used in this repository\n\n### Azure Synapse\n\n[Azure Synapse Analytics](https://azure.microsoft.com/en-gb/services/synapse-analytics/#overview) is a limitless analytics service that brings together data integration, enterprise data warehousing and big data analytics. It gives you the freedom to query data on your terms, using either serverless or dedicated options \u2013 at scale. Azure Synapse brings these worlds together with a unified experience to ingest, explore, prepare, transform, manage and serve data for immediate BI and machine learning needs.\n\n### PowerBI\n\n[PowerBI](https://powerbi.microsoft.com/en-us/). Connect to and visualize any data using the unified, scalable platform for self-service and enterprise business intelligence (BI) that\u2019s easy to use and helps you gain deeper data insight.\n\n### Azure Cognitive Search\n\n[Azure Cognitive Search](https://azure.microsoft.com/en-us/services/search/) is a fully managed search as a service to reduce complexity and scale easily including:\n\n* Auto-complete, geospatial search, filtering, and faceting capabilities for a rich user experience\n* Built-in AI capabilities including OCR, key phrase extraction, and named entity recognition to unlock insights\n* Flexible integration of custom models, classifiers, and rankers to fit your domain-specific needs\n\n## Other services used\n\n[Graphframes](http://graphframes.github.io/graphframes/docs/_site/index.html) is a package for Apache Spark which provides DataFrame-based Graphs. It provides high-level APIs in Scala, Java, and Python. It aims to provide both the functionality of GraphX and extended functionality taking advantage of Spark DataFrames. This extended functionality includes motif finding, DataFrame-based serialization, and highly expressive graph queries.\n\nThe notebooks contain a basic graph implementation that can be amended to run functions such as BFS, DFS, find communities and label propagation amongst others.\n\n## Datasets used in this repository \ud83d\udcbe\n\nDataset | Description | Labels |\n| -------- | ----------- | ----- |\n[BBC sports](http://mlg.ucd.ie/datasets/bbc.html) | Consists of 737 documents from the BBC Sport website corresponding to sports news articles in five topical areas from 2004-2005 | Class Labels: 5 (athletics, cricket, football, rugby, tennis)\n\n## Contributors \u2728\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/gxjorge\"><img src=\"https://avatars.githubusercontent.com/u/3693420?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jorge Garcia Ximenez</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/ayabel\"><img src=\"https://avatars.githubusercontent.com/u/95618207?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Aya Bellicha</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/lidaghr\"><img src=\"https://avatars.githubusercontent.com/u/101302183?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Lida Ghahremanlou</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/melmatlis\"><img src=\"https://avatars.githubusercontent.com/u/93650751?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Malvina Matlis</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/shanepeckham\"><img src=\"https://avatars1.githubusercontent.com/u/9840775?s=400&u=eedd334991b280967060ba797c9fb5bb6d7ffc0a&v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Shane Peckham</b></sub></a><br /></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/alyshaarshad\"><img src=\"https://avatars.githubusercontent.com/u/32840883?s=400&u=f50728f865e8fe8a3c151c3997b0f21c30b19ba6&v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Alysha Arshad</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/ivan-shaporov\"><img src=\"https://avatars.githubusercontent.com/u/4342797?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Ivan Shaporov</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/sujoysahacodes\"><img src=\"https://avatars.githubusercontent.com/u/88181686?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Sujoy Saha</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/yothikulk\"><img src=\"https://avatars.githubusercontent.com/u/90093485?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>\n    Jyothi Hanamant Kulkarni</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/ilyas-it83\"><img src=\"https://avatars.githubusercontent.com/u/10421745?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>\n    Ilyas F</b></sub></a><br /></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/alexhock\"><img src=\"https://avatars.githubusercontent.com/u/7644157?v=4\" width=\"100px;\" alt=\"\"/><br><sub><b>\n    Alex Hocking</b></sub></a><br /></td>\n    <td align=\"center\"><a href=\"https://github.com/sweanan\"><img src=\"https://avatars.githubusercontent.com/sweanan\" width=\"100px;\" alt=\"\"/><br><sub><b>\n    Swetha Anand</b></sub></a><br /></td>\n  </tr>\n</table>\n\n<!-- markdownlint-enable -->\n<!-- prettier-ignore-end -->\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services.Authorized use of Microsoft\ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Additional References\n\nPlease refer to the following references for additional relevant material:\n\n### Azure Cognitive Search additional links\n\n* [Azure Search Knowledge mining](https://github.com/Azure-Samples/azure-search-knowledge-mining)\n* [Azure Search Knowledge Mining Bootcamp](https://github.com/Azure/LearnAI-KnowledgeMiningBootcamp)\n* [Azure Search custom skills](https://github.com/microsoft/SkillsExtractorCognitiveSearch)\n", "repo_name": "Data-Discovery-Toolkit", "org_name": "microsoft", "org_repo": "microsoft/Data-Discovery-Toolkit", "platform_org_repo": "github+microsoft/Data-Discovery-Toolkit", "link_to_repo": "https://github.com/microsoft/Data-Discovery-Toolkit", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 22, "watchers_count": 22}, {"README_text": "\ufeff# azure-openai-in-a-day-workshop\r\n\r\n> *In this technical workshop, you will get a comprehensive introduction to Azure OpenAI Service and Azure OpenAI Studio. You will learn how to create and refine prompts for various scenarios using hands-on exercises. You will also discover how to leverage Azure OpenAI Service to access and analyze your company data. Moreover, you will explore existing solution accelerators and best practices for prototyping and deploying use cases end-to-end. The workshop will end with a Q&A session and a wrap-up.*\r\n\r\n## Workshop agenda\r\n\r\n### \ud83c\udf05 Morning (9:00 \u2013 12:00)\r\n\r\n> *Fokus: Introduction and first steps*\r\n\r\n* \ud83d\udce3 Intro (90min)\r\n  * Into Workshop (15mins)\r\n  * Intro Azure OpenAI Service (30mins)\r\n  * Azure Azure OpenAI Studio (45mins)\r\n* \ud83e\uddd1\ud83c\udffc\u200d\ud83d\udcbb Prompt Engineering Exercises using Studio (90mins)\r\n\r\n### \ud83c\udf06 Afternoon (1:00 \u2013 4:30)\r\n\r\n> *Focus: Solutions*\r\n\r\n* Recap (15min)\r\n* \ud83d\udce3 Using Azure OpenAI Service to access company data (60min)\r\n  * How to bring your own data\r\n  * Fine-tuning and embedding\r\n  * Solutions Accelerators\r\n* QnA session (30min)\r\n* \ud83d\udcbb Hands-on lab on two exemplay use-cases (90min)\r\n\r\n<sup>\r\n\ud83d\udce3 Presentation, \ud83e\uddd1\ud83c\udffc\u200d\ud83d\udcbb Hands-on lab\r\n</sup>\r\n\r\n-------------------\r\n\r\n## Preparation\r\n\r\n> *This is only required for the hands-on lab. If you are only attending the presentation, you can skip this section.*\r\n\r\n### Azure OpenAI Service subscription and deployments\r\n\r\nGrant the participant access to the Azure OpenAI Service subscription and create the required deployments.\r\n\r\nIdeally, grant the participants access to Azure OpenAI Service service be assigning the `Cognitive Service OpenAI user`. If the participant is a `Cognitive Service OpenAI contributor`, they can create the following deployments themselves.\r\n\r\nOtherwise, create 'text-davinci-003' and 'text-embedding-ada-002' deployments (and assign the participant to the deployments).\r\n\r\nThere are two ways to authenticate (see Jupyter notebooks):\r\n1. (Recommended) Use the Azure CLI to authenticate to Azure and Azure OpenAI Service\r\n2. Using a token (not needed if using the Azure CLI)\r\n\r\nGet the Azure OpenAI Service endpoint (and key) from the Azure portal.\r\n\r\n### Workspace environment\r\n\r\nChoose one of the following options to set up your environment: Codespaces, Devcontainer or bring your own environment (Anaconda). Building the environment can take a few minutes, so please start early.\r\n\r\n#### 1\ufe0f\u20e3 Codespaces\r\n\r\n> \ud83c\udf1f Highly recommended: *Best option if you already have a Github account. You can develop on local VSCode or in a browser window.*\r\n\r\n* Go to Github repository and click on `Code` button\r\n* Create and edit the `.env` file in the base folder including Azure OpenAI Service endpoint and key before starting any notebooks\r\n\r\n#### 2\ufe0f\u20e3 Devcontainer\r\n\r\n> *Usually a good option if VSCode and Docker Desktop are already installed.*\r\n\r\n* Install [Docker](https://www.docker.com/products/docker-desktop)\r\n* Install [Visual Studio Code](https://code.visualstudio.com/)\r\n* Install [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension\r\n* Clone this repository\r\n* Open the repository in Visual Studio Code\r\n* Click on the green button in the bottom left corner of the window\r\n* Select `Reopen in Container`\r\n* Wait for the container to be built and started\r\n* Create and edit `.env` file in the base folder including Azure OpenAI Service endpoint and key before starting any notebooks\r\n\r\n#### 3\ufe0f\u20e3 Bring your own environment\r\n\r\n> *If you already have a Python environment with Jupyter Notebook and the Azure CLI installed.*\r\n\r\nMake sure you have the requirements installed in your Python environment using `pip install -r requirements.txt`.\r\n\r\n-------------------\r\n\r\n## Content of the repository\r\n\r\n* :bulb: [Guideline for writing better prompts](lectures/prompt_writing_help.md)\r\n\r\n## Exercises\r\n\r\n* :muscle: [Simple prompt writing exercises](exercises/exercises.md)\r\n* :muscle: [Quickstart](exercises/quickstart.ipynb) - just to make sure everything works!\r\n* :muscle: [Preprocessing](exercises/preprocessing.ipynb) - principles of preprocessing and token handling!\r\n* :muscle: [Q&A with embeddings](exercises/qna_with_embeddings_exercise.ipynb)\r\n* :muscle: [Unsupervised movie classification and recommendations](exercises/movie_classification_unsupervised_incl_recommendations_exercise.ipynb)\r\n* :muscle: [Email Summarization and Answering App](exercises/email_app.md)\r\n\r\n## Solutions\r\n\r\nDo not cheat! :sweat_smile:\r\n\r\n* :bulb: Solution - [Q&A with embeddings](exercises/solutions/qna_with_embeddings_solution.ipynb)\r\n* :bulb: Solution - [Unsupervised movie classification and recommendations](exercises/solutions/movie_classification_unsupervised_incl_recommendations_solution.ipynb)\r\n* :bulb: Solution - [Email Summarization and Answering App](exercises/solutions/email_app.py)\r\n\r\n## Q&A Quick Start\r\n\r\nIf you want to quickly create a Q&A webapp using your own data, please follow the [quickstart guide notebook](qna-quickstart-template/qna-app-quickstart.ipynb).\r\n\r\nIf you want to use LangChain to build an interactive chat experience on your own data, follow the [quickstart chat on private data using LangChain](qna-chat-with-langchain/qna-chat-with-langchain.ipynb).\r\n\r\nIf you want to use LlamaIndex \ud83e\udd99 (GPT Index), follow the [quickstart guide notebook with llama-index](qna-quickstart-with-gpt-index/qna-quickstart-with-llama-index.ipynb).", "repo_name": "azure-openai-in-a-day-workshop", "org_name": "microsoft", "org_repo": "microsoft/azure-openai-in-a-day-workshop", "platform_org_repo": "github+microsoft/azure-openai-in-a-day-workshop", "link_to_repo": "https://github.com/microsoft/azure-openai-in-a-day-workshop", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 122, "watchers_count": 122}, {"README_text": "# Graph Notification Broker\n\n|  [Architecture](/docs/architecture.md#overall-architecture) |  [Deployment Guide](/docs/deployment-guide.md#Deployment-Guide) |\n| ---- | ---- |\n\n[MS Graph](https://learn.microsoft.com/en-us/graph/overview) provides an easy way to access the data in Microsoft 365, including information about users, documents, meetings, and more. When data changes, timely notifications are needed. For example, a chat application needs to know when someone is available or out of the office. This asset simplifies the development complexity through immediate notice of Graph changes.\n\nGraph Notification Broker (GNB) enables apps to receive change notifications from MS Graph in real time without the application having to poll repeatedly. GNB offers a pub/sub service of notifications which reduces the load on Graph API and provides for automatic renewal of the subscription to ensure delivery of notifications. GNB is architected to function even when the apps are behind a private network.\n\nFor collaborative applications, it is important to be able to be notified on changes in Microsoft 365 entities. Examples include:\n\n* The arrival of an email,\n* A reply on a chat,\n* A change in \"presence\" of a user\n* A change to a document.\n* The change of a calendar item.\n\nThe whole list of resources where a notification can be received from can be found [here](https://docs.microsoft.com/en-us/graph/api/resources/webhooks?view=graph-rest-1.0).\n\nNotification of these changes is essential for applications to react properly to these changes. Following are some important considerations in design.\n\n1. Change notifications sent by Graph via webhooks. Many large enterprises block the inbound traffic for security reasons, which limits the ability for the webhook to reach its endpoint.\n1. Change notifications have a limit lifetime and needs to be renewed before they expire. Many background business processes that need to subscribe to changes typically run longer than the lifetime of a subscription.\n1. The Graph API use throttling to protect itself. This is important for SaaS Applications that can execute large amount of graph calls. The number of calls to Graph API need to be reduced to prevent throttling and impact on the performance. So, using notifications instead of polling and sharing notifications between multiple clients can reduce the number of calls to Graph API.\n\n## Traverse Restricted inbound http traffic\n\nThe Microsoft Graph allows third-party applications to subscribe to Office 365 activity through change notifications. When first introduced, change notifications had to be implemented with a webhook accessible by the Microsoft Graph. The network ingress of webhooks made them a non-starter for many organizations. An approach is required where incoming webhooks can traverse inbound network policies that prevent inbound http traffic.\n\n## Subscription lifetime management\n\nA subscription has a limited lifetime and the client that creates that is responsible to renew it before it has expired. This is a complex task where limited solution patterns exist. A re-usable solution or pattern is required that manages and renews subscriptions on behalf of the client.\n\n## Reduce load on Graph and enable real-time notifications\n\nDue to the complexity in managing subscriptions, applications that use graph change notifications often use a polling strategy. Frequent polling can increase load on Graph API substantially. A pub-sub solution will reduce this load and allow multiple applications to use the same notification real-time.\n\nThis repo is a solution for managing [Microsoft Graph API Change Notifications](https://docs.microsoft.com/en-us/graph/api/resources/webhooks).\nThis solution takes out the complexities of managing the subscriptions with MS Graph\nand exposes the notifications via SignalR connections.\n\n## Client Applications\n\nPlease refer to the the [Graph Notification Samples](https://github.com/microsoft/GraphNotificationSamples)\nrepo for client application samples that consume the Graph Notification Broker.\n\nA sample Test Client application is included and deployed to the Azure Function and provided as an example.\n\nTo run the test client use dotnet serve to host static files. Navigate to\nthe Test client directory and run:\n`dotnet serve -h \"Cache-Control: no-cache, no-store, must-revalidate\" -p 8080`\n\nSee section Testing below for more guidance on testing the Graph Notification Broker.\n\n## Notes\n\n### High Throughput\n\nFor high throughput scenarios where the amount of requests per second (rps) to get/create/renew subscriptions begin to get throttled, you can implement an [Async Request Reply Pattern](https://learn.microsoft.com/en-us/azure/architecture/patterns/async-request-reply). This will add all requests to a queue and then will be processed off of the queue. The processor will send the result back to the client via a push notification with SignalR.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "GraphNotificationBroker", "org_name": "microsoft", "org_repo": "microsoft/GraphNotificationBroker", "platform_org_repo": "github+microsoft/GraphNotificationBroker", "link_to_repo": "https://github.com/microsoft/GraphNotificationBroker", "platform": "github", "language": "C#", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# vivainsights_ingressupload\n\nOpen the `C#_Solution` folder to access the DescriptiveDataUpload console application and related information.\n", "repo_name": "vivainsights_ingressupload", "org_name": "microsoft", "org_repo": "microsoft/vivainsights_ingressupload", "platform_org_repo": "github+microsoft/vivainsights_ingressupload", "link_to_repo": "https://github.com/microsoft/vivainsights_ingressupload", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Dev Containers for ML feasibility study with VS Code\r\n\r\nA machine learning and data science project template that makes it easy to work with multiple Docker based [VSCode Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers) in the same repository. The template also makes it easy to transition projects to the cloud and production by including automated code quality checks, pytest configuration, CI pipeline templates and a sample for running on Azure Machine Learning.\r\n\r\n## Contents\r\n\r\n- [Introduction and Overview](#introduction-and-overview)\r\n  - [Features](#features)\r\n- [Getting Started](#getting-started)\r\n- [How to create a new directory under src with a new environment?](#how-to-create-a-new-directory-under-src-with-a-new-environment)\r\n- [Directory Structure](#directory-structure)\r\n  - [notebooks directory vs src directory](#notebooks-directory-vs-src-directory)\r\n- [AML Example](#aml-example)\r\n- [CI pipeline](#ci-pipeline)\r\n  - [Running all unit tests with ci-tests.sh](#running-all-unit-tests-with-ci-testssh)\r\n  - [How to Configure Azure DevOps CI Pipeline](#how-to-configure-azure-devops-ci-pipeline)\r\n  - [How to Configure Github Actions CI Pipeline](#how-to-configure-github-actions-ci-pipeline)\r\n- [Using SSH Keys in Dev Containers](#using-ssh-keys-in-dev-containers)\r\n- [Future Roadmap and TODOs](#future-roadmap-and-todos)\r\n- [Contributing](#contributing)\r\n- [Trademarks](#trademarks)\r\n\r\n## Introduction and Overview\r\n\r\nThis repository provides a [VSCode Dev Container](https://code.visualstudio.com/docs/devcontainers/containers) based project template that can help accelerate your Machine Learning inner-loop development phase. The template covers the phases from early ML experimentation (local training/testing) until production oriented ML model training (cloud based training/testing with bigger CPUs and GPUs).\r\n\r\nDuring the early phase of Machine Learning project, you may face challenges such as each data scientist creating various different python environments that span across CPU and GPU that tend to have different setup procedures. With the power of Dev Containers, you can automate environment setup process across the team and every data scientist will get the exact same environment automatically. This template provides both CPU and GPU Dev Container setup as examples. To support multiple different ML approaches with different python environments to be experimented in one project, this solution allows multiple different Dev Containers to be used in one repository while having a \"common\" module that will be installed into all Dev Container to enable code reuse across different Dev Containers.\r\n\r\nAnother challenge you may face is each data scientist creating a low quality codebase. That is fine during the experimentation stage to keep the team agility high and maximize your team\u2019s experimentation throughput. But when you move to the model productionization stage, you experience the burden of bringing code quality up to production level. With the power of python tools and VSCode extensions configured for this template on top of Dev Containers, you can keep the code quality high automatically without losing your team\u2019s agility and experimentation throughput and ease the transition to the productionization phase.\r\n\r\n### Features\r\n\r\n- Multiple Dev Container samples (both CPU and GPU) with many common config steps already configured as following:\r\n  - Automated code quality checks (linter and auto formatter) with black, flake8, isort and bandit on VSCode on save\r\n  - Automated code quality checks (linter and auto formatter) with black, flake8, isort and bandit as precommit hook\r\n  - Zero effort transition from local env to Azure Machine Learning (cloud based env) by leveraging the same Dockerfile\r\n  - Pre-configured VSCode extensions installed such as python, jupyter, shellcheck, code-spell-checker, git tools etc\r\n- [Github Actions and Azure DevOps CI pipelines](#ci-pipeline) that run linters (flake8 and bandit) and pytest with test result reporting and coverage reporting\r\n- Pull Request templates that helps you to write a good PR description for both Github and Azure DevOps\r\n\r\nThis template automates all tedious setup process as much as possible and saves time and reduces setup errors for the entire data scientist team.\r\n\r\n## Getting Started\r\n\r\nThis section provides a comprehensive guide on how to set up a development environment using Dev Containers in Visual Studio Code with step-by-step instructions.\r\n\r\n### How to setup dev environment?\r\n\r\n1. Install [Visual Studio Code](https://code.visualstudio.com/)\r\n1. If your team has a commercial license for Docker Desktop, follow [VS Code Remote Containers | Docker Desktop](https://code.visualstudio.com/docs/remote/containers#_installation). Otherwise, go to [VS Code Remote Containers | Rancher Desktop Docs](https://docs.rancherdesktop.io/how-to-guides/vs-code-remote-containers/) and finish the first step (Install and launch Rancher Desktop. Select dockerd (moby) as the Container Runtime from the Kubernetes Settings menu.)\r\n1. Install [VSCode Remote - Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) on vscode\r\n1. **If you forget this step, you will get an error when you try to build the container so make sure you have `.env` at root of this directory before you move on to the next step.**. Copy `.env.example` and rename it to `.env`. This is where you store your credentials etc. `.env` is automatically loaded into dev container as environment variables. When you add new environment variables `.env`, update `.env.example` as well to share that with others but don't include any credentials there. `.env` is gitignored so your credentials in that file won't be accidentally committed.\r\n1. Run `Dev Containers: Open Folder in Container...` from the Command Palette (F1) and select the `notebooks` directory.\r\n1. VS Code will then build and start up a container, connect this window to service `notebooks`, and install VS Code extensions specified in `notebooks/.devcontainer/devcontainer.json`.\r\n1. Run `pre-commit install` in vscode terminal within dev container you just opened. This will setup your git precommit hook. This needs to run only once in the project and you don't need to rerun this every container rebuild.\r\n1. Now set up is done. If you want to develop in another directory for example under `src`, run `Dev Containers: Open Folder in Container...` and go to that directory that has `.devcontainer` and that will setup an dev environment for that directory.\r\n1. When you or others update either `requirements.txt` or `Dockerfile` in your working directory, make sure to rebuild your container to apply those changes to container. Run `Dev Containers: Rebuild and Reopen in Container...` for that.\r\n\r\n## How to create a new directory under src with a new environment\r\n\r\n1. Copy `src/sample_cpu_project/` under `src` and rename it. If you need gpu environment, base off of `src/sample_pytorch_gpu_project` instead\r\n1. Edit `.devcontainer/devcontainer.json` under the new directory and replace `sample_cpu_project` with the new directory name\r\n1. Edit `.devcontainer/docker-compose.yml` under the new directory and replace `sample_cpu_project` with the new directory name\r\n1. Update `COPY sample_cpu_project/.devcontainer/requirements.txt` in `Dockerfile` with a new path\r\n1. Update other parts of `Dockerfile` if you need\r\n1. Update `requirements.txt` if you need\r\n1. Run `Dev Containers: Open Folder in Container...` from the Command Palette (F1) and select the new directory and make sure you can successfully open the new directory on VS Code running in a container\r\n\r\n## Directory Structure\r\n\r\nThis section gives you overview of the directory structure of this template. Only essential files are covered in this structure graph for simplicity. The directory structure is as follows:\r\n\r\n```bash\r\n.\r\n\u251c\u2500\u2500 .azuredevops                   # CI pipelines for Azure DevOps. Details at section: How to Configure Azure DevOps CI Pipeline \r\n\u251c\u2500\u2500 .github                        # CI pipelines for Github Actions. Details at section: How to Configure Github Actions CI Pipeline \r\n\u251c\u2500\u2500 .pre-commit-config.yaml        # pre-commit config file with formatting and linting. Setup is covered in Section: Getting Started\r\n\u251c\u2500\u2500 .env.example                   # Example of .env file. Setup is covered in Section: Getting Started\r\n\u251c\u2500\u2500 bandit.yml                     # Setting file for bandit, security linter\r\n\u251c\u2500\u2500 ci-tests.sh                    # Details at Section: Running all unit tests with ci-tests.sh\r\n\u251c\u2500\u2500 data                           # Directory to keep your data for local training etc. This directory is gitignored \r\n\u251c\u2500\u2500 notebooks                      # Setup process is covered in Section: How to setup dev environment?\r\n\u2502   \u251c\u2500\u2500 .devcontainer              # dev container related configuration files goes to here following VSCode convention\r\n\u2502   \u2502   \u251c\u2500\u2500 devcontainer.json      # dev container configuration and VS Code settings, extensions etc.\r\n\u2502   \u2502   \u251c\u2500\u2500 docker-compose.yml     # referred in devcontainer.json\r\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile             # referred in docker-compose.yml\r\n\u2502   \u2502   \u2514\u2500\u2500 requirements.txt       # includes python package list for notebooks. used in Dockerfile\r\n\u2502   \u2514\u2500\u2500 sample_notebook.py         # example of interactive python script\r\n\u251c\u2500\u2500 pytest.ini                     # Setting file for pytest\r\n\u2514\u2500\u2500 src\r\n    \u251c\u2500\u2500 common                     # this module is accessible from all modules under src. put functions  you want to import across the projects here\r\n    \u2502   \u2514\u2500\u2500 requirements.txt       # python package list for common module. installed in all Dockerfile under src. python tools for src goes to here too\r\n    \u251c\u2500\u2500 sample_cpu_project         # cpu project example. Setup process is covered in Section: How to setup dev environment?\r\n    \u2502   \u251c\u2500\u2500 .devcontainer          # dev container related configuration files goes to here following VSCode convention\r\n    \u2502   \u2502   \u251c\u2500\u2500 devcontainer.json  # dev container configuration and VS Code settings, extensions etc.\r\n    \u2502   \u2502   \u251c\u2500\u2500 docker-compose.yml # referred in devcontainer.json\r\n    \u2502   \u2502   \u251c\u2500\u2500 Dockerfile         # referred in docker-compose.yml. Supports only CPU\r\n    \u2502   \u2502   \u2514\u2500\u2500 requirements.txt   # includes python package list for sample_cpu_project. used in Dockerfile\r\n    \u2502   \u251c\u2500\u2500 sample_main.py         \r\n    \u2502   \u2514\u2500\u2500 tests                  # pytest scripts for sample_cpu_project goes here\r\n    \u2502       \u2514\u2500\u2500 test_dummy.py      # pytest script example\r\n    \u2514\u2500\u2500 sample_pytorch_gpu_project # gpu project example with pytorch. Setup process is covered in Section: How to setup dev environment?\r\n        \u251c\u2500\u2500 README.md              # README for AML example contained in sample_pytorch_gpu_project\r\n        \u251c\u2500\u2500 .devcontainer          # dev container related configuration files goes to here following VSCode convention\r\n        \u2502   \u251c\u2500\u2500 devcontainer.json  # dev container configuration and VS Code settings, extensions etc.\r\n        \u2502   \u251c\u2500\u2500 docker-compose.yml # referred in devcontainer.json\r\n        \u2502   \u251c\u2500\u2500 Dockerfile         # referred in docker-compose.yml. Supports GPU\r\n        \u2502   \u2514\u2500\u2500 requirements.txt   # includes python package list for sample_pytorch_gpu_project. used in Dockerfile\r\n        \u251c\u2500\u2500 aml_example/           # Sample AML CLI v2 Components-based pipeline, including setup YAML. See sample_pytorch_gpu_project/README for full details of files in this directory.\r\n        \u251c\u2500\u2500 sample_main.py        \r\n        \u251c\u2500\u2500 inference.py           # Example pytorch inference/eval script that also works with aml_example\r\n        \u251c\u2500\u2500 train.py               # Example pytorch model training script that also works with aml_example\r\n        \u2514\u2500\u2500 tests                  # pytest scripts for sample_pytorch_gpu_project goes here\r\n            \u2514\u2500\u2500 test_dummy.py      # pytest script example\r\n```\r\n\r\n### `notebooks` directory vs `src` directory\r\n\r\nThere are two places to put python scripts/modules in this template. The `notebooks` directory is for experimental or throw-away python scripts and jupyter notebooks that you want to run cell by cell interactively. For example, EDA, one-off visualization codes, new model approaches you are not certain yet if you want to maintain over time typically go to this directory. The `src` directory is for python scripts and modules that you want to reuse and maintain over time. The `src` directory is also where you would put unit tests (typically under a `src/your_module/tests` directory).\r\n\r\nGiven the nature of each directory's responsibility, there is also a different quality governance required. One big difference is that pre-commit hooks and CI pipelines run flake8 over `src` but not over `notebooks` (black, isort and bandit still run for both). For scripts in `notebooks`, we recommend you use [interactive python scripts](https://code.visualstudio.com/docs/python/jupyter-support-py#_convert-jupyter-notebooks-to-python-code-file) where you can have jupyter-like code cells within `.py` files rather than jupyter notebooks `.ipynb`. Interactive python files gives you the following benefits:\r\n\r\n- Comes with full benefits of python extension in VSCode such as code completion, linting, auto formatting, debugging etc\r\n- pre-commit hooks and CI pipelines will work as they run over `.py` files (but not `.ipynb` files)\r\n- Python file format is easier to review during a pull request review\r\n\r\nInteractive python scripts and jupyter notebooks are interchangeable as described in [Convert Jupyter notebooks to Python code file](https://code.visualstudio.com/docs/python/jupyter-support-py#_convert-jupyter-notebooks-to-python-code-file) so you can switch between them easily too if you want to use both formats during the development.\r\n\r\n## AML Example\r\n\r\nAn Azure Machine Learning (AML) example is provided under `src/sample_pytorch_gpu_example`. The example is a AML Components-based ML pipeline, that runs a pytorch based training step followed by a inference/evaluation step. This example shows the seemless transition of moving from a local run (inside the Dev Container) of pytorch based training/inference and running in the cloud in the exact same Docker environment with flexible compute options. See the [AML Components-based Pipeline Example README](src/sample_pytorch_gpu_project/README.md) for a detailed explanation and instructions of the example code.\r\n\r\n## CI Pipeline\r\n\r\nThis repository contains templates for running a Continuous Integration (CI) pipeline on either Azure DevOps (under `.azuredevops` directory) or on Github Actions (under `.github` directory). Each of the CI pipeline configurations provided have the following features at a high level:\r\n\r\n- Run code quality checks including flake8 and bandit over the repository\r\n- Find all subdirectories under `src` and run all pytest tests inside the associated Docker containers\r\n- Publish test results and code coverage statistics\r\n\r\nWe recommend setting up pipeline triggers for PR creation, editing and merging. This will ensure the pipeline runs continuously and will help catch any issues earlier in your development process.\r\n\r\nSee the sections below for links on how to setup pipelines with [Azure DevOps](#how-to-configure-azure-devops-ci-pipeline) and [Github Actions](#how-to-configure-github-actions-ci-pipeline). Note that if you are only using one of these platforms to host a pipeline (or neither), you can safely delete either (or both) the `.azuredevops` directory or the `.github` directory.\r\n\r\n### Running all unit tests with `ci-tests.sh`\r\n\r\nAs multiple independent directories can be added under `src`, each with its own Dockerfile and requirements, running unit tests for each directory under `src` needs to be done within the Docker container of each `src` subdirectory. The `ci-tests.sh` script automates this task of running all unit tests for the repository with the following steps:\r\n\r\n1. Finds all subdirectories under `src` that have at least one `test_*.py` under a `tests` folder\r\n2. Builds each Docker image for each subdirectory with tests, using the Dockerfile in the associated `.devcontainer` directory\r\n3. Runs pytest for each subdirectory with tests, inside the matching Docker container built in step 2\r\n4. Combine all test results and coverage reports from step 3, with reports in a valid format for publishing in either Azure DevOps or Github Actions hosted pipeline\r\n\r\nNote that the `ci-test.sh` script can be run locally as well and it is assumed that all tests are written with pytest.\r\n\r\n### How to Configure Azure DevOps CI Pipeline\r\n\r\nSee [create your first pipeline](https://learn.microsoft.com/en-us/azure/devops/pipelines/create-first-pipeline?view=azure-devops) for how to setup a pipeline in Azure DevOps. Note that to use the provided template in this repository, you will need to specify the path to `.azuredevops/ado-ci-pipeline-ms-hosted.yml` during the pipeline setup process in Azure DevOps.\r\n\r\n#### Choosing between Azure DevOps Microsoft-hosted vs Self-hosted CI Pipeline\r\n\r\nThere are two templates for running a CI pipeline in Azure DevOps, a pipeline configuration that uses a Microsoft hosted agent to run the pipeline (`.azuredevops/ado-ci-pipeline-ms-hosted.yml`) and a pipeline configuration that uses a self-hosted agent to run the pipeline (`.azuredevops/ado-ci-pipeline-self-hosted.yml`).\r\n\r\nThe Microsoft hosted version is easiest to start with and recommended. Where you may consider switching to the self-hosted version, is when you have added several directories under `src` that have individual containers and the size of all the docker builds in the CI pipeline comes up against the 10GB disk storage limit for Microsoft hosted pipelines (see [resource limitations of Microsoft hosted agents](https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#capabilities-and-limitations)). In this case (or when other resource constraints are met) switching to a self-hosted agent pipeline may be an option and the template at `.azuredevops/ado-ci-pipeline-self-hosted.yml` includes additional steps to help manage space consumed by CI pipeline runs. The two versions are otherwise identitical in terms of building each docker container under `src`, running pytest within each of these containers and publishing test results and coverage information.\r\n\r\n### How to Configure Github Actions CI Pipeline\r\n\r\nGithub Actions CI pipeline is defined in `.github/workflows/ci.yaml`. As long as this repository is hosted in github, the pipeline will be automatically triggered when a PR is made or updated as well as when a PR is merged into your main branch with the setting below, so **no additional setting is required**.\r\n\r\n```yaml\r\non:\r\n  push:\r\n    branches:\r\n      - main\r\n  pull_request:\r\n    branches:\r\n      - main\r\n```\r\n\r\n## Using SSH Keys in Dev Containers\r\n\r\nIf you have connected to the origin repository using SSH authentication, you will need to do a bit of setup to reuse your local SSH key inside a Dev Container automatically, which will allow you to interact with the origin repository (git push, git pull etc.) inside the Dev Container.\r\n\r\n1. Try the recommendations in the official docs for [sharing git credentials](https://code.visualstudio.com/remote/advancedcontainers/sharing-git-credentials)\r\n1. If the previous step doesn't work, try the below method, that includes a bit of additional code to add keys to the SSH agent.\r\n\r\nAdd the following to your ~/.bash_profile, ~/.profile, ~/.zprofile or similar (by default most WSL users will have only a ~/.profile) so an ssh-agent will be started when needed and default keys will be added to the agent. The ssh-agent will then automatically forward keys to your Dev Container when its launched.\r\n\r\n```sh\r\n# this part taken from https://code.visualstudio.com/remote/advancedcontainers/sharing-git-credentials\r\n# check that link for the latest version or updates\r\nif [ -z \"$SSH_AUTH_SOCK\" ]; then\r\n   # Check for a currently running instance of the agent\r\n   RUNNING_AGENT=\"`ps -ax | grep 'ssh-agent -s' | grep -v grep | wc -l | tr -d '[:space:]'`\"\r\n   if [ \"$RUNNING_AGENT\" = \"0\" ]; then\r\n        # Launch a new instance of the agent\r\n        ssh-agent -s &> $HOME/.ssh/ssh-agent\r\n   fi\r\n   eval `cat $HOME/.ssh/ssh-agent`\r\nfi\r\n\r\n# ADD SSH Keys to the SSH agent\r\n# if using non-default SSH key, add it to ssh-add command like:\r\n# ssh-add /path/to/your/ssh-key\r\nssh-add\r\n```\r\n\r\n## Future Roadmap and TODOs\r\n\r\n- Add Docker build caching to Azure DevOps MS hosted CI pipeline\r\n- Add tensorflow GPU example\r\n- Investigate making `src/common` installed with `pip -e`\r\n- Use a common requirements.txt for code quality dependencies\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\n\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\n\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "dstoolkit-devcontainers", "org_name": "microsoft", "org_repo": "microsoft/dstoolkit-devcontainers", "platform_org_repo": "github+microsoft/dstoolkit-devcontainers", "link_to_repo": "https://github.com/microsoft/dstoolkit-devcontainers", "platform": "github", "language": "Python", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# GPT-Enabled-HSR-CoSpeechGestures\n![HSR](images/interesting.gif)\n\n\n## About this repository\nThis repository contains source code to generate HSR co-speech gestures using the official Python interface of the Toyota HSR robot. The chat system utilizes GPT as its backend.\n\n\n## Pipeline\n![Pipeline](images/pipeline.jpg)\nWhen a user inputs a message, GPT-3/ChatGPT generates the robot\u2019s textual response based on a prompt carefully crafted to create a chat-like experience. The pipeline then utilizes a gesture engine to analyze the text and select an appropriate gesture from a library associated with the conceptual meaning of the speech. A speech generator converts the text into speech, while a gesture generator executes co-speech gestures, providing audio-visual feedback expressed through an HSR robot.\nRelated materials:\n* [Our blog post](https://www.microsoft.com/en-us/research/?post_type=msr-blog-post&p=927303&preview=true)\n* [GPT-based chatbot (MSRAbot)](https://github.com/microsoft/LabanotationSuite/tree/master/MSRAbotChatSimulation)\n\n\n## Labanotation to HSR motion\nGestures are expressed based on Labanotation, which is a notation system for human dance ([example](https://github.com/microsoft/LabanotationSuite/blob/master/MSRAbotChatSimulation/data/LabanotationLibrary/away.json)). By mapping the human body onto the robot body, the robot's motion is generated. As HSR has only one arm, we mapped the left hand and head movements of Labanotation to HSR. In particular, we reflected the direction from the elbow to the wrist specified in Labanotation onto the direction of HSR's arm.\nFor reference, we proveide examples of Labanotation to HSR motion in the [gesture_videos](./gesture_videos) folder.\n\n\n![Labanotation](images/laban_mapping.jpg)\n\n## Prerequisites\nYou need to have a laptop setup for the use of the HSR robot. See [this website](http://hsr.io/) for instructions on how to set up a laptop for use with the HSR robot.\nInstall necessary packages:\n```bash\npip install -r requirements.txt\n```\nIn addition, download the [data](https://github.com/Naoki-Wake/LabanotationSuite/tree/master/MSRAbotChatSimulation/data) folder from [here](https://github.com/Naoki-Wake/LabanotationSuite/tree/master/MSRAbotChatSimulation/data) and put it in the root directory of this repository.\n\n\n## Azure subscription\nTo use this repository, you need an [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview)subscription for text generation and an [Azure speech service](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/overview) subscription for speech generation. We also use [Azure Language Understanding](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/conversational-language-understanding/overview) for concept estimation, but it is optional and you will need to train the model separately. We trained our Language Understanding models on [this database](data/sentence-concept_database.csv).\nTo insert your subscription keys, open the [secrets.json](./secrets.json) file.\n\n\n## Preparation (when using a simulator)\nLaunch an environment for the HSR robot:\n```bash\nroslaunch hsrb_gazebo_launch hsrb_empty_world.launch\n```\nClick the play button in the Gazebo window to start the simulation.\nimage:\n\n\n![Gazebo](./images/setup.jpg)\n\n\n## How to run\nOpen a terminal and run the following command:\n```bash\npython main.py\n```\n\n\n## Notes when using a simulator\nThe code sends motion commands to the simulator while the speech synthesized by the Azure Speech Service is played in the background. However, since the HSR motion is moving in simulation time, it is not guaranteed that the voice and motion will be played synchronously.\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "GPT-Enabled-HSR-CoSpeechGestures", "org_name": "microsoft", "org_repo": "microsoft/GPT-Enabled-HSR-CoSpeechGestures", "platform_org_repo": "github+microsoft/GPT-Enabled-HSR-CoSpeechGestures", "link_to_repo": "https://github.com/microsoft/GPT-Enabled-HSR-CoSpeechGestures", "platform": "github", "language": "Python", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Cohesion-based robot teaching for LfO\n## About this repository\nThis repository provides a user interface to teach robots how to perform household manipulative operations, using the framework of [Learning-from-Observation (LfO)](https://www.microsoft.com/en-us/research/project/interactive-learning-from-observation/). The LfO project aims to generate programs for robots to execute by teaching them through multimodal instruction using language and videos. Robots perform household tasks by combining basic actions called \"tasks,\" such as grasping, lifting, and carrying objects.\n\n## What is Cohesion?\nWhen instructing household tasks in language, users do not usually say \"Grasp the tissue paper, lift it, carry it, and release it.\" Instead, they would say \"Throw away the tissue paper.\" The concept of Cohesion serves as a bridge between the granularity of language instructions and task granularity. Cohesion is a data structure that packages the correspondence between everyday actions and task sequences ([see the cohesion file](./webapp/function_database/functions.json)). When the source code in this repository is executed, it reads the registered Cohesion from the language instructions. Detailed parameters for executing each task described in Cohesion (e.g., object name, distance moved) are obtained from the video from Azure Kinect. This teaching is achieved through interaction between the system via a browser and users. Furthermore, this repository provides functions for registering task cohesion through step-by-step instruction and collecting images associated with object names for registration in [Azure Custom Vision](https://azure.microsoft.com/en-us/products/cognitive-services/custom-vision-service/).\nFor more details, please refer to the following paper:\n[Interactive Learning-from-Observation through multimodal human demonstration](https://arxiv.org/abs/2212.10787)\n\nBibliography:\n```\n@article{wake2022interactive,\n  title={Interactive Learning-from-Observation through multimodal human demonstration},\n  author={Wake, Naoki and Kanehira, Atsushi and Sasabuchi, Kazuhiro and Takamatsu, Jun and Ikeuchi, Katsushi},\n  journal={arXiv preprint arXiv:2212.10787},\n  year={2022}\n}\n```\n\n\n### Overview of the pipeline:\n![LfO pipeline](./src/LfO_pipeline.png)\n\n### GUI while teaching:\n![LfO GUI](./src/LfO_GUI.png)\n\n## Installation\nTested with Python 3.9\n```bash\npip install -r requirements.txt\n```\n\n## Preparation\n* Download the latest browser drivers.\nhttps://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\nLocate the driver in the [speech_handler](./speech_handler) folder.\nDirectory structure should look like this:\n```bash\nLfO_interface\n\u251c\u2500\u2500\u2500speech_handler\n\u2502   \u2502\u2500\u2500\u2500recognizer.py\n\u2502   \u2502\u2500\u2500\u2500edgedriver_win64\n\u2502   \u2502   \u2502\u2500\u2500\u2500msedgedriver.exe\n```\nYou can use any browser, but you need to modify the [recognizer.py](./speech_handler/recognizer.py) to use the browser of your choice. You may need to enable long path if you are using Windows 10 or Windows 11.\n[Reference](https://www.thewindowsclub.com/how-to-enable-or-disable-win32-long-paths-in-windows-11-10)\n\n* Be sure to connect Azure Kinect DK to your computer.\n\n* You need to prepare analyzers to compile task models.\nSee this [repository](https://github.com/microsoft/analyzer-for-robot-teaching-demonstrations) for reference.\n\n* Fill in the [secrets.json](./secrets.json) with your credential information to utilize Azure.\n\n\n## How to run\nTo run the server, in the [webapp](./webapp) folder, run:\n```bash\npython -m uvicorn sample:app --reload  --reload-exclude '*__init__.py' --host localhost --port 9100\n```\n\nTo run the client, in the [speech_handler](./speech_handler) folder, run:\n```bash\npython .\\recognizer.py\n```\n\n## Other useful scripts\n* [recompile.py](./webapp/recompile.py)\nRecompile a task model from existing data.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cohesion-based-robot-teaching-interface", "org_name": "microsoft", "org_repo": "microsoft/cohesion-based-robot-teaching-interface", "platform_org_repo": "github+microsoft/cohesion-based-robot-teaching-interface", "link_to_repo": "https://github.com/microsoft/cohesion-based-robot-teaching-interface", "platform": "github", "language": "Python", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project\n\nWindows natively utilizes many HID controls, exposing hardware integration-points to enhance user-experience.  While HID is a well-understood industry standard, the great flexibility of HID Report descriptors introduces complexity for the consumer (e.g. Applications, Hosts) to parse and support, so only a subset of what HIDs can describe may be supported (with great variance amongst consumers).  The inevitability of errors/deviations in shipped device/consumer implementations from the HID-spec and the need to maintain backwards compatibility, further reduce what can be usefully described.\n\nThis project provides reference-implementations of HIDs that Windows natively consumes as hardware integration-points (e.g. Shell-interaction, Application-WindowMessages, UserMode APIs).  (*Note: 3rd-Party applications are naturally not limited to only what Windows natively consumes*).  References document (inline) any Windows expectations/limitations/work-arounds, and generally why things are the way they are.\n\n> References are intended to be used as a starting-point for 3rd-Party HID vendors, and to be consumed as-is by hobbyists.\n\n> This project is available as a library via the [Arduino Package Manager](https://docs.arduino.cc/software/ide-v2/tutorials/ide-v2-installing-a-library).  Search for `Microsoft_HidForWindows`.\n\nSee also the [Waratah project](https://github.com/microsoft/hidtools) for HID Descriptor composition.\n\n## Supported Devices\n\n> Additional devices will be continually added over the coming months.  To make suggestions for the next device-type, please file an issue.\n\n- Consumer Control  (see `Microsoft_HidConsumerControl.h`)\n- LampArray (see `Microsoft_HidLampArray.h`)\n  - New support in Windows as part of [Dynamic Lighting](https://blogs.windows.com/windows-insider/2023/06/07/announcing-windows-11-insider-preview-build-23475/)\n- Wireless Radio Control (see `Microsoft_HidWirelessRadioControl.h`)\n\n## Caveats\n- Untested on non-Windows systems (e.g. macOS, Linux, etc\u2026)\n- Comprehension/simplicity was always chosen over efficiency  (e.g. multiple Reports used, where one would be sufficient).  See [HID-spec](https://usb.org/document-library/device-class-definition-hid-111) and [Waratah](https://github.com/microsoft/hidtools) for rolling-your-own Report Descriptor.\n- A subset of HID Report Descriptor variations within the HID-spec are naturally supported, but likely not all.\n- Documented expectations/work-arounds may change at any time, and do not guarantee compatibility.\n  - Last Validated on Windows 11 (22621.1702) (`22621.1.amd64fre.ni_release.220506-1250`)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ArduinoHidForWindows", "org_name": "microsoft", "org_repo": "microsoft/ArduinoHidForWindows", "platform_org_repo": "github+microsoft/ArduinoHidForWindows", "link_to_repo": "https://github.com/microsoft/ArduinoHidForWindows", "platform": "github", "language": "C++", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# AGIEval\nThis repository contains information about AGIEval, data, code and output of baseline systems for the benchmark.\n\n# Introduction\nAGIEval is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving. \nThis benchmark is derived from 20 official, public, and high-standard admission and qualification exams intended for general human test-takers, such as general college admission tests (e.g., Chinese College Entrance Exam (Gaokao) and American SAT), law school admission tests, math competitions, lawyer qualification tests, and national civil service exams. \nFor a full description of the benchmark, please refer to our paper: [AGIEval: A Human-Centric Benchmark for\nEvaluating Foundation Models](https://arxiv.org/pdf/2304.06364.pdf).\n\n# Tasks and Data\n\nAGIEval v1.0 contains 20 tasks, including two cloze tasks (Gaokao-Math-Cloze and MATH) and 18 multi-choice question answering tasks (the rest). Among the multi-choice question answering tasks, Gaokao-physics and JEC-QA have one or more answers, and the other tasks only have one answer. You can find the full list of tasks in the table below.\n![The datasets used in AGIEVal](AGIEval_tasks.png)\n\nYou can download all post-processed data but JEC-QA in the [data/v1](data/v1) folder. For JEC-QA, please refer to the [JEC-QA website](https://jecqa.thunlp.org/) to request the data. We use the first 1,000 instances of JEC-QA training data as the test set. \n\nThe data format for all datasets is as follows:\n```\n{\n    \"passage\": null,\n    \"question\": \"\u8bbe\u96c6\u5408 $A=\\\\{x \\\\mid x \\\\geq 1\\\\}, B=\\\\{x \\\\mid-1<x<2\\\\}$, \u5219 $A \\\\cap B=$ ($\\\\quad$)\\\\\\\\\\n\",\n    \"options\": [\"(A)$\\\\{x \\\\mid x>-1\\\\}$\", \n        \"(B)$\\\\{x \\\\mid x \\\\geq 1\\\\}$\", \n        \"(C)$\\\\{x \\\\mid-1<x<1\\\\}$\", \n        \"(D)$\\\\{x \\\\mid 1 \\\\leq x<2\\\\}$\"\n        ],\n    \"label\": \"D\",\n    \"answer\": null\n}\n```\nThe `passage` field is available for gaokao-chinese, gaokao-english, both of logiqa, all of LSAT, and SAT. The answer for multi-choice tasks is saved in the `label` field. The answer for cloze tasks is saved in the `answer` field. \n\nWe provide the prompts for few-shot learning in the [data/v1/few_shot_prompts](data/few_shot_prompts.csv) file.\n# Baseline Systems\nWe evaluate the performance of the baseline systems on AGIEval v1.0. The baseline systems are based on the following models: text-davinci-003, ChatGPT (gpt-3.5-turbo), and GPT-4.\nYou can replicate the results by following the steps below:\n1. fill in your OpenAI API key in the [openai_api.py](openai_api.py) file.\n2. run the [run_prediction.py](run_prediction.py) file to get the results.\n\n# Model Outputs\nYou can download the zero-shot, zero-shot-Chain-of-Thought, few-shot and few-shot-Chain-of-Thought outputs of the baseline systems in the [Onedrive](https://1drv.ms/u/s!Amt8n9AJEyxcg8YQKFm1rSEyV9GU_A?e=VEfJVS) link. \nNote: we fixed typos in 52 instances of SAT-en and will release the updated outputs of the dataset soon.\n# Evaluation\nYou can run the [post_process_and_evaluation.py](post_process_and_evaluation.py) file to get the evaluation results.\n\n# Citation\nIf you use AGIEval dataset or the code in your research, please cite our paper:\n```\n@misc{zhong2023agieval,\n      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, \n      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},\n      year={2023},\n      eprint={2304.06364},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\nPlease make sure to cite all the individual datasets in your paper when you use them. We provide the relevant citation information below:\n```\n@inproceedings{ling-etal-2017-program,\n    title = \"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\",\n    author = \"Ling, Wang  and\n      Yogatama, Dani  and\n      Dyer, Chris  and\n      Blunsom, Phil\",\n    booktitle = \"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2017\",\n    address = \"Vancouver, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/P17-1015\",\n    doi = \"10.18653/v1/P17-1015\",\n    pages = \"158--167\",\n    abstract = \"Solving algebraic word problems requires executing a series of arithmetic operations{---}a program{---}to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.\",\n}\n\n@inproceedings{hendrycksmath2021,\n  title={Measuring Mathematical Problem Solving With the MATH Dataset},\n  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},\n  journal={NeurIPS},\n  year={2021}\n}\n\n@inproceedings{Liu2020LogiQAAC,\n  title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning},\n  author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},\n  booktitle={International Joint Conference on Artificial Intelligence},\n  year={2020}\n}\n\n@inproceedings{zhong2019jec,\n  title={JEC-QA: A Legal-Domain Question Answering Dataset},\n  author={Zhong, Haoxi and Xiao, Chaojun and Tu, Cunchao and Zhang, Tianyang and Liu, Zhiyuan and Sun, Maosong},\n  booktitle={Proceedings of AAAI},\n  year={2020},\n}\n\n@article{Wang2021FromLT,\n  title={From LSAT: The Progress and Challenges of Complex Reasoning},\n  author={Siyuan Wang and Zhongkun Liu and Wanjun Zhong and Ming Zhou and Zhongyu Wei and Zhumin Chen and Nan Duan},\n  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n  year={2021},\n  volume={30},\n  pages={2201-2216}\n}\n```\n\n\n\n# Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AGIEval", "org_name": "microsoft", "org_repo": "microsoft/AGIEval", "platform_org_repo": "github+microsoft/AGIEval", "link_to_repo": "https://github.com/microsoft/AGIEval", "platform": "github", "language": "Python", "stargazers_count": 291, "watchers_count": 291}, {"README_text": "# The Zero Trust Lab Guide\n\nThe Zero Trust Lab Guide aims to provide a level 200 learning path that will help you understand the Microsoft end-to-end Zero Trust story through logically ordered guidance across Microsoft 365 and Azure with the hope that it sovles some of the challenges that you can face when building a lab utulising Microsoft 365, such as:\n\n- Where do I start?\n- How complicated should my lab be?\n- Build for learning? (repeatability \u2013 easy to tear down and rebuild)\n- Build for demo\u2019s (build once, low maintenance, longevity is key)\n- How do I know the correct path and order to take?\n- How do I know when I\u2019m done with a part of the deployment and that it was done right?\n\n## Advantages of using the lab guide\n\n- The guide is modular and logically structured in scenario-based phases following best practices where possible.\n- Provides a choice of doing Hybrid (aka On-Premises AD) or Cloud Only deployment\n- Clearly articulated exit criteria for each phase and step\n- Complexity of the Hybrid Identity option goes no deeper than enabling Password Hash Sync to avoid ADFS\n- No AAD Connect \u2013 Cloud Sync Only\n- No public DNS or cert requirements \u2013 Everything works with *.onmicrosoft.com (even on-prem scenarios)\n\n## What does it cover?\n\n- AAD Premium features across P1 & P2\n- Intune\n- Purview\n- Sentinel\n- Defender for Identity/Endpoint/O365/Cloud Apps\n- Azure IaaS\n- Azure Bastion\n- Microsoft Security Benchmark\n- Azure Policies\n\n## Lab Guide Map\n\nA visual represenation of the lab and deployment order is located at https://aka.ms/ztlabguidemap\n\n## Lab Guide \n\nThe actual labs are located at https://ztlabguide.com \n", "repo_name": "ztlabguide", "org_name": "microsoft", "org_repo": "microsoft/ztlabguide", "platform_org_repo": "github+microsoft/ztlabguide", "link_to_repo": "https://github.com/microsoft/ztlabguide", "platform": "github", "language": "JavaScript", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Activate GitHub Copilot using Nodejs and .NET\n\nDemo project for running labs to evaluate Copilot viability\n\n- [Goal](#goal)\n- [Pre-requisites](#pre-requisites)\n- [Work with Github Codespaces](#work-with-github-codespaces)\n- [Work locally](#work-locally)\n- [Instructions](#instructions)\n\n## Goal\n\nThe goal of a GitHub Copilot Activate is to learn how to use it, using an exercise that consist of building a web server using Nodejs with different functionalities and a .NET Web API.\n\n Copilot is an AI-powered code assistant that helps developers write better code faster. It uses machine learning models trained on billions of lines of code to suggest whole lines or entire functions based on the context of what you\u2019re working on. By using Copilot, you can learn how to write better code and improve your productivity.\n\nRemember:\n\n- As you type GitHub Copilot will make suggestions, you can accept them by pressing Tab.\n- If nothing shows up after Copilot write some lines , press enter and wait a couple of seconds.\n- On Windows, MacOS or Linux, press Ctrl + Enter, then click Open GitHub Copilot.\n\n\n## Pre-requisites\n\n**CoPilot access**\n\nA 60 day trial can be requested here: https://github.com/github-copilot/signup\n\n## Work with Github Codespaces\n\nEnvironment is already configured to work with Github Codespaces, you can find the configuration files in the .devcontainer folder.\n\nTo start programming just start a new codespace and you are ready to go, don't need to install anything.\n\n### Work locally\n\n**VisualStudio Code**\n\nhttps://code.visualstudio.com/\n\n**Install Node and npm**\n https://docs.npmjs.com/downloading-and-installing-node-js-and-npm\n\n\n**Install mocha**\n\nrun:\n\n``` bash\n npm install --global mocha\n npm install axios\n```\n\n**Install Docker**\n\nhttps://docs.docker.com/engine/install/\n\n**Install .NET**\n\nhttps://dotnet.microsoft.com/download\n\n\n## Instructions\n\n- [Node Server](./exercisefiles/node/README.md)\n- [.NET Web API](./exercisefiles/dotnet/README.md)\n", "repo_name": "CopilotHackathon", "org_name": "microsoft", "org_repo": "microsoft/CopilotHackathon", "platform_org_repo": "github+microsoft/CopilotHackathon", "link_to_repo": "https://github.com/microsoft/CopilotHackathon", "platform": "github", "language": "JavaScript", "stargazers_count": 29, "watchers_count": 29}, {"README_text": "# Tokenizer\n\nThis repo contains C# and Typescript implementation of byte pair encoding(BPE) tokenizer for OpenAI LLMs, it's based on open sourced rust implementation in the [OpenAI tiktoken](https://github.com/openai/tiktoken). Both implementation are valuable to run prompt tokenization in .NET and Nodejs environment before feeding prompt into a LLM.\n\n# C# implementation\n\nThe TokenizerLib is built in .NET Standard 2.0, which can be consumed in projects on any version of .NET later than .NET Core 2.0 or .NET Framework 4.6.1.\n\nYou can download and install the nuget package of TokenizerLib [here](https://www.nuget.org/packages/Microsoft.DeepDev.TokenizerLib/).\n\nExample C# code to use TokenizerLib in your code. In production setting, you should pre-download the BPE rank file and call `TokenizerBuilder.CreateTokenizer` API to avoid downloading the BPE rank file on the fly.\n```csharp\nusing System.Collections.Generic;\nusing Microsoft.DeepDev;\n\nvar IM_START = \"<|im_start|>\";\nvar IM_END = \"<|im_end|>\";\n\nvar specialTokens = new Dictionary<string, int>{\n                                            { IM_START, 100264},\n                                            { IM_END, 100265},\n                                        };\ntokenizer = await TokenizerBuilder.CreateByModelNameAsync(\"gpt-4\", specialTokens);\n\nvar text = \"<|im_start|>Hello World<|im_end|>\";\nvar encoded = tokenizer.Encode(text, new HashSet<string>(specialTokens.Keys));\nConsole.WriteLine(encoded.Count);\n\nvar decoded = tokenizer.Decode(encoded.ToArray());\nConsole.WriteLine(decoded);\n```\n\n## C# performance benchmark\n\nPerfBenchmark result based on [PerfBenchmark.csproj](Tokenizer_C%23/PerfBenchmark/PerfBenchmark.csproj):\n``` ini\nBenchmarkDotNet=v0.13.3, OS=Windows 11 (10.0.22621.1702)\nIntel Core i7-1065G7 CPU 1.30GHz, 1 CPU, 8 logical and 4 physical cores\n.NET SDK=7.0.300-preview.23179.2\n  [Host]     : .NET 6.0.16 (6.0.1623.17311), X64 RyuJIT AVX2\n  DefaultJob : .NET 6.0.16 (6.0.1623.17311), X64 RyuJIT AVX2\n\n| Method |    Mean |    Error |   StdDev |\n|------- |--------:|---------:|---------:|\n| Encode | 2.414 s | 0.0303 s | 0.0253 s |\n```\n\n# Typescript implementation\n\nPlease follow [README](tokenizer_ts/README.md).\n\n# Contributing\n\nWe welcome contributions. Please follow [this guideline](CONTRIBUTING.md).\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Tokenizer", "org_name": "microsoft", "org_repo": "microsoft/Tokenizer", "platform_org_repo": "github+microsoft/Tokenizer", "link_to_repo": "https://github.com/microsoft/Tokenizer", "platform": "github", "language": "C#", "stargazers_count": 47, "watchers_count": 47}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fr-dai-hol-iac", "org_name": "microsoft", "org_repo": "microsoft/fr-dai-hol-iac", "platform_org_repo": "github+microsoft/fr-dai-hol-iac", "link_to_repo": "https://github.com/microsoft/fr-dai-hol-iac", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-multipart-dotnet", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-multipart-dotnet", "platform_org_repo": "github+microsoft/kiota-serialization-multipart-dotnet", "link_to_repo": "https://github.com/microsoft/kiota-serialization-multipart-dotnet", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "kiota-serialization-multipart-go", "org_name": "microsoft", "org_repo": "microsoft/kiota-serialization-multipart-go", "platform_org_repo": "github+microsoft/kiota-serialization-multipart-go", "link_to_repo": "https://github.com/microsoft/kiota-serialization-multipart-go", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Partner Center .NET SDK\n\nThe Partner Center SDK (Software Development Kit) is a set of tools and resources provided by Microsoft for developers to integrate their applications with the Partner Center API. The Partner Center is a platform that allows Microsoft partners to manage their accounts, customers, and orders for various Microsoft products and services. The SDK provides libraries, sample code, and documentation that developers can use to build applications that can access and manipulate data in the Partner Center. This enables partners to automate their workflows, enhance their reporting capabilities, and manage their operations more efficiently.\n\nThis repository has been archived and Microsoft no longer maintains this project and no future releases are planned.\n\n## Where is the latest version of the SDK?\n\nThe latest version of the .NET SDK for Partner Center can be found on the [here](https://www.nuget.org/packages/Microsoft.Store.PartnerCenter).\n\n## Can I still use this code?\n\nYou can still use the code in this repository, but we cannot guarantee that it will work with the latest version of the Partner Center API or that it is secure. \n\n## What is included in this project\n\nThe features included in this Partner Center .NET SDK archival project are describred in the [.NET SDK release notes](https://learn.microsoft.com/en-us/partner-center/developer/dotnet-release-notes)\n\n## Our recommendation?\n\nPartner Center .NET SDK and archive it on GitHub. No further updates to the SDK will be made. Partners are encouraged to integrate through the [Partner Center REST APIs](https://learn.microsoft.com/en-us/partner-center/developer/partner-center-rest-api-reference) instead. The final version of the Partner Center .NET SDK will be available on Microsoft Archive on GitHub for local download and maintenance, but contributions will not be accepted.\n\n\n# Setting up Partner Center SDK development environment\n\nThis guide will walk you through the steps to set up your development environment for using the Partner Center SDK\n\n## Prerequisites\n\nBefore getting started, you will need:\n\n - Visual Studio 2019 or later installed on your computer\n - .NET Framework 4.6.1 or later installed on your computer\n - A Partner Center account\n\n ## Getting Started\n\n1. Clone or download the Partner Center SDK repository from the GitHub repository.\n    ```git\n    git clone https://github.com/microsoft/partner-center-sdk-for-dotNET.git\n    ```\n2. Launch the Visual Studio IDE on your computer.\n3. In the start window, select \"***Open a project or solution***.\" You can also navigate to \"**File** > **Open** > **Project/Solution**\" from the menu bar. Select the \"***PartnerCenterSdk.sln***\" to open.\n3. Once you have opened the solution, navigate to \"**Build** > **Build Solution**\" from the menu bar. This will compile your project and ensure that it is ready to run.\n4. Next, set the startup project for your solution. You can do this by right-clicking on the \"__PartnerSdk.FeatureSamples__\" project in the Solution Explorer and selecting \"***Set as Startup Project***.\"\n    \n    ### Note\n    > The \"__PartnerSdk.FeatureSamples__\" feature sample project sample have snippets that demonstrate how to use the Microsoft Partner Center SDK with .NET-based applications. Feature sample provides examples of how to perform various tasks using the Partner Center API, such as creating and managing customers, retrieving customer subscriptions and usage data, and managing support tickets.Developers can use these code samples as a starting point for their own Partner Center API integration projects, or they can simply use them as a reference to learn more about how to work with the API. The \"**Feature Sample** project is a valuable resource for developers who need to integrate with the Microsoft Partner Center API in their .NET-based applications.\n---\n5. Locate \"__App.config__\" file under \"__PartnerSdk.FeatureSamples__\" project and double click on it.\n6. Add all the required values\n    ```xml\n    <?xml version=\"1.0\" encoding=\"utf-8\"?>\n    <configuration>\n    <startup>\n        <supportedRuntime version=\"v4.0\" sku=\".NETFramework,Version=v4.6.1\" />\n    </startup>\n    <appSettings>\n        <add key=\"applicationSignIn\" value=\"false\"/>\n        <add key=\"partnerServiceApiRoot\" value=\"https://api.partnercenter.microsoft.com/\"/>\n        <add key=\"customerWithServiceCosts\" value=\"\"/>\n        <add key=\"customerWithAgreements\" value=\"\"/>\n        <add key=\"partnerUserIdForAgreement\" value=\"\"/>\n        <add key=\"customerWithTrialOffer\" value=\"\"/>\n        <add key=\"trialOfferId\" value=\"\"/>\n        <add key=\"customerWithServiceRequests\" value=\"\"/>\n        <add key=\"customerWithDevices\" value=\"\"/>\n        <add key=\"customerWithProducts\" value=\"\"/>\n        <add key=\"directResellerTestAccountCustomerTenantId\" value=\"\"/>\n        <add key=\"directResellerTestAccountUserId\" value=\"\"/>\n        <add key=\"directResellerIntegrationTestAccountUser\" value=\"\"/>\n        <add key=\"offerWithAddonId\" value=\"\"/>\n        <add key=\"offerAddonOneId\" value=\"\"/>\n        <add key=\"offerAddonTwoId\" value=\"\"/>\n        <add key=\"deviceBatchId\" value=\"Prd1\"/>\n        <add key=\"defaultProductTargetView\" value=\"\"/> <!-- Example: azure -->\n        <add key=\"defaultProductPromotionsSegment\" value=\"\"/> <!-- Example: commercial -->\n        <add key=\"defaultProductCollection\" value=\"\"/> <!-- Example: azure -->\n        <add key=\"deviceId\" value=\"\"/>\n        <add key=\"configurationPolicyId\" value=\"\"/>\n        <add key=\"trackingIdForDeviceDeployment\" value=\"\"/>\n        <add key=\"customerForUsageDemo\" value=\"\"/>\n        <add key=\"subscriptionForUsageDemo\" value=\"\"/>\n        <add key=\"customerForRegistrationDemo\" value=\"\"/>\n        <add key=\"subscriptionForRegistrationDemo\" value=\"\"/>\n        <add key=\"customerServiceRequestId\" value=\"\"/> <!-- Example: 615121092223011 -->\n        <add key=\"defaultMpnId\" value=\"\"/> <!-- Example: 1234567 -->\n        <add key=\"defaultTenantId\" value=\"\"/>\n        <add key=\"selectedInvoiceKey\" value=\"\"/> <!-- Example: D02005YFHI -->\n        <add key=\"selectedReceiptKey\" value=\"\"/> <!-- Example: 8602768 -->\n        <add key=\"customerForActivationLinksDemo\" value=\"\"/>\n        <add key=\"subscriptionForActivationLinksDemo\" value=\"\"/>\n        <add key=\"modernOrderIdDemo\" value=\"\"/> <!-- Example: 3WVl63zjolJvaVoNmJvMSIcaexSp5WvL1 -->\n        <add key=\"orderIdForAttachments\" value=\"\"/> <!-- Example: 2297c718a6d7 -->\n        <add key=\"orderAttachmentId\" value=\"\"/> <!-- Example: 0bde1dac54290b -->\n\n        <!-- Modern Customers and Orders -->\n        <add key=\"defaultCustomerId\" value=\"\" />\n        <add key=\"customerIdForOrderSvc\" value=\"\" />\n        <add key=\"productUpgradeCustomerId\" value=\"\" />\n        <add key=\"productResourceName\" value=\"\" /> <!-- Example: Azure plan -->\n        <add key=\"defaultBillingCycleType\" value=\"\" /> <!-- Example: OneTime -->\n        <add key=\"azurePlanProductId\" value=\"DZH318Z0BPS6\" />\n        <add key=\"azurePlanSkuId\" value=\"0001\" />\n        <add key=\"defaultSubscriptionId\" value=\"\" />\n        <add key=\"azuresubscriptionId\" value=\"\" />\n        <add key=\"defaultProductFamily\" value=\"azure\" />\n        <add key=\"aad.authority\" value=\"https://login.windows.net\" />\n        <add key=\"aad.graphEndpoint\" value=\"https://graph.windows.net\" />\n        <add key=\"aad.organizationsDomain\" value=\"organizations\" />\n\n        <!-- DIRECT RESELLER TEST ACCOUNT FOR CUSTOMER USER LICENSE -->\n        <add key=\"directResellerTestAccount.clientId\" value=\"\" />\n        <add key=\"directResellerTestAccount.userName\" value=\"\" /> <!-- Example: testaccountusername@PRIMARYDOMAINNAME.onmicrosoft.com -->\n        <add key=\"directResellerTestAccount.password\" value=\"\" />\n\n        <!-- RI ACCOUNT -->\n        <add key=\"aad.clientId\" value=\"\" />\n        <add key=\"aad.userName\" value=\"\" /> <!-- Example: testaccountusername@PRIMARYDOMAINNAME.onmicrosoft.com -->\n        <add key=\"aad.password\" value=\"\" />\n        <add key=\"rIAccountPartnerTenantId\" value=\"\" />\n\n        <add key=\"aad.resourceUrl\" value=\"https://api.partnercenter.microsoft.com\" />\n        <add key=\"aad.redirectUri\" value=\"http://localhost\" />\n        <add key=\"aad.applicationId\" value=\"\" />\n        <add key=\"aad.applicationSecret\" value=\"\" />\n        <add key=\"aad.applicationDomain\" value=\"\" />\n        <add key=\"tipAccount.application.id\" value=\"\" />\n        <add key=\"tipAccount.application.secret\" value=\"\" />\n        <add key=\"tipAccount.application.domain\" value=\"\" />\n        <add key=\"tipAccount.aad.clientId\" value=\"\" />\n        <add key=\"tipAccount.aad.userName\" value=\"\" /> <!-- Example: testaccountusername@PRIMARYDOMAINNAME.onmicrosoft.com -->\n        <add key=\"tipAccount.aad.password\" value=\"\" />\n        <add key=\"TestMultiTierScenario\" value=\"false\" />\n        <add key=\"TestDevicesScenario\" value=\"false\" />\n        <add key=\"TestModernScenarios\" value=\"false\" />\n        <add key=\"TestModernOfficeScenarios\" value=\"true\" />\n        <add key=\"TestRIScenarios\" value=\"false\" />\n        <add key=\"TestModernAzureScenarios\" value=\"false\" />\n    </appSettings>\n    <runtime>\n        <assemblyBinding xmlns=\"urn:schemas-microsoft-com:asm.v1\">\n        <dependentAssembly>\n            <assemblyIdentity name=\"Microsoft.Identity.Client\" publicKeyToken=\"31bf3856ad364e35\" culture=\"neutral\" />\n            <bindingRedirect oldVersion=\"0.0.0.0-4.31.0.0\" newVersion=\"4.31.0.0\" />\n        </dependentAssembly>\n        </assemblyBinding>\n    </runtime>\n    </configuration>\n    ```\n7. Finally, you can run the project by selecting \"**Debug** > **Start Debugging**\" from the menu bar or by pressing the F5 key on your keyboard. This will launch the application and allow you to interact with it.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](./CODE_OF_CONDUCT.md). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq) or contact opencode@microsoft.com with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n\n## License\n\nThis code is licensed under the [MIT License](https://opensource.org/licenses/MIT). Feel free to use it as you wish.\n", "repo_name": "partner-center-sdk-for-dotNET", "org_name": "microsoft", "org_repo": "microsoft/partner-center-sdk-for-dotNET", "platform_org_repo": "github+microsoft/partner-center-sdk-for-dotNET", "link_to_repo": "https://github.com/microsoft/partner-center-sdk-for-dotNET", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# JARVIS\n\n**This project is under construction and we will have all the code ready soon.**\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)\n[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)\n\n\n## Updates\n+  [2023.04.16] Jarvis now supports the OpenAI service on the Azure platform and the GPT-4 model.\n+  [2023.04.06] We added the Gradio demo and built the web API for `/tasks` and `/results` in `server` mode.\n   +  The Gradio demo is now hosted on Hugging Face Space. (Build with `inference_mode=hybrid` and `local_deployment=standard`)\n   +  The Web API `/tasks` and `/results` access intermediate results for `Stage #1`: task planning and `Stage #1-3`: model selection with execution results. See <a href=\"#Server\">here</a>.\n+  [2023.04.03] We added the CLI mode and provided parameters for configuring the scale of local endpoints.\n   +  You can enjoy a lightweight experience with Jarvis without deploying the models locally. See <a href=\"#Configuration\">here</a>.\n   +  Just run `python awesome_chat.py --config configs/config.lite.yaml` to experience it.\n+  [2023.04.01] We updated a version of code for building.\n\n## Overview\n\nLanguage serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!\n\nSee our paper: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](http://arxiv.org/abs/2303.17580), Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang (the first two authors contribute equally)\n\n<p align=\"center\"><img src=\"./assets/overview.jpg\"></p>\n\nWe introduce a collaborative system that consists of **an LLM as the controller** and **numerous expert models as collaborative executors** (from HuggingFace Hub). The workflow of our system consists of four stages:\n+ **Task Planning**: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n+ **Model Selection**: To solve the planned tasks, ChatGPT selects expert models hosted on Hugging Face based on their descriptions.\n+ **Task Execution**: Invokes and executes each selected model, and return the results to ChatGPT.\n+ **Response Generation**: Finally, using ChatGPT to integrate the prediction of all models, and generate responses.\n\n## System Requirements\n\n### Default (Recommended)\n\nFor `configs/config.default.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ VRAM >= 24GB\n+ RAM > 12GB (minimal), 16GB (standard), 80GB (full)\n+ Disk > 284GB \n  + 42GB for `damo-vilab/text-to-video-ms-1.7b`\n  + 126GB for `ControlNet`\n  + 66GB for `stable-diffusion-v1-5`\n  + 50GB for others\n  \n### Minimum (Lite)\n\nFor `configs/config.lite.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ Nothing else\n\nThe configuration `configs/config.lite.yaml` does not require any expert models to be downloaded and deployed locally. However, it means that Jarvis is restricted to models running stably on HuggingFace Inference Endpoints.\n\n## Quick Start\n\nFirst replace `openai.key` and `huggingface.token` in `server/configs/config.default.yaml` with **your personal OpenAI Key** and **your Hugging Face Token**, or put them in the environment variables `OPENAI_API_KEY` and `HUGGINGFACE_ACCESS_TOKEN` respectfully. Then run the following commands:\n\n<span id=\"Server\"></span>\n\n### For Server:\n\n```bash\n# setup env\ncd server\nconda create -n jarvis python=3.8\nconda activate jarvis\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\npip install -r requirements.txt\n\n# download models. Make sure that `git-lfs` is installed.\ncd models\nbash download.sh # required when `inference_mode` is `local` or `hybrid`. \n\n# run server\ncd ..\npython models_server.py --config configs/config.default.yaml # required when `inference_mode` is `local` or `hybrid`\npython awesome_chat.py --config configs/config.default.yaml --mode server # for text-davinci-003\n```\n\nNow you can access Jarvis' services by the Web API. \n\n+ `/hugginggpt` --method `POST`, access the full service.\n+ `/tasks` --method `POST`, access intermediate results for Stage #1.\n+ `/results` --method `POST`, access intermediate results for Stage #1-3.\n\nFor example:\n\n```bash\n# request\ncurl --location 'http://localhost:8004/tasks' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image\"\n        }\n    ]\n}'\n\n# response\n[{\"args\":{\"image\":\"/examples/d.jpg\"},\"dep\":[-1],\"id\":0,\"task\":\"openpose-control\"},{\"args\":{\"image\":\"/examples/e.jpg\"},\"dep\":[-1],\"id\":1,\"task\":\"image-to-text\"},{\"args\":{\"image\":\"<GENERATED>-0\",\"text\":\"<GENERATED>-1\"},\"dep\":[1,0],\"id\":2,\"task\":\"openpose-text-to-image\"}]\n```\n\n\n### For Web:\n\nWe provide a user-friendly web page. After starting `awesome_chat.py` in a server mode, you can run the commands to communicate with Jarvis in your browser:\n \n- you need to install `nodejs` and `npm` first.\n- [ IMPORTANT ] if you are running the web client on another machine, you need set `http://{LAN_IP_of_the_server}:{port}/` to `HUGGINGGPT_BASE_URL` of `web/src/config/index.ts`.\n- if you want to use the video generation feature, you need to compile `ffmpeg` manually with H.264.\n- you can switch to ChatGPT by `double click` on the setting icon!\n\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\n```bash\n# Optional: Install ffmpeg\n# This command need be executed without errors.\nLD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4\n```\n\n<span id=\"Gradio\"></span>\n\n### For Gradio\n\nThe Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:\n\n```bash\npython models_server.py --config configs/config.gradio.yaml\npython run_gradio_demo.py --config configs/config.gradio.yaml\n\n# or run the HF Space as a Docker image (Build with `inference_mode=hibrid` and `local_deployment=standard`)\ndocker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py\n```\n\n### For CLI:\n\nYou can also run Jarvis more easily in CLI mode:\n\n```bash\ncd server\npython awesome_chat.py --config configs/config.default.yaml --mode cli\n```\n\nExamples of CLI mode:\n\n```\nWelcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).\n[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?\n[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"a cat sitting on a window sill looking out\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/a3dc.jpg', which contains two objects with labels 'potted plant' and 'cat'. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"two zebras standing in a field of dry grass\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/f5df.jpg', which contains three objects with labels 'zebra'. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text \"a zebra standing in a field of tall grass\". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box '/images/69c7.jpg', which contains one object with label 'zebra'. Finally, I used the canny-control model canny-control to generate the image '/images/7e18.png' based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?\n[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.\n[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?\n[ User ]: Please generate a canny image based on /examples/f.jpg\n[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is \"a herd of giraffes and zebras grazing in a field\". Then I used the canny-control model to generate the canny image of the original image, which is stored in the path \"/images/1ea2.png\". Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path \"/images/ba66.png\". Do you need anything else?\n```\n\n\n## Configuration\n\nThe server-side configuration file is `server/configs/config.default.yaml`, and some parameters are presented as follows:\n\n+ `model`: LLM, currently supports `text-davinci-003`. We are working on integrating more open-source LLMs.\n+ `inference_mode`: mode of inference endpoints\n  + `local`: only use the local inference endpoints\n  + `huggingface`: only use the Hugging Face Inference Endpoints **(free of local inference endpoints)**\n  + `hybrid`: both of `local` and `huggingface`\n+ `local_deployment`: scale of locally deployed models, works under `local` or `hybrid` inference mode:\n  +  `minimal` (RAM>12GB, ControlNet only)\n  +  `standard` (RAM>16GB, ControlNet + Standard Pipelines)\n  +  `full` (RAM>42GB, All registered models)\n\nOn a personal laptop, we recommend the configuration of `inference_mode: hybrid `and `local_deployment: minimal`. But the available models under this setting may be limited due to the instability of remote Hugging Face Inference Endpoints.\n\n## NVIDIA Jetson Embedded Device Support\nA [Dockerfile](./Dockerfile.jetson) is included that provides experimental support for [NVIDIA Jetson embedded devices](https://developer.nvidia.com/embedded-computing).  This image provides accelerated ffmpeg, pytorch, torchaudio, and torchvision dependencies.  To build the docker image, [ensure that the default docker runtime is set to 'nvidia'](https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime).  A pre-built image is provided at https://hub.docker.com/r/toolboc/nv-jarvis.\n\n```bash\n#Build the docker image\ndocker build --pull --rm -f \"Dockerfile.jetson\" -t toolboc/nv-jarvis:r35.2.1 \n```\n\nDue to to memory requirements, JARVIS is required to run on Jetson AGX Orin family devices (64G on-board RAM device preferred) with config options set to:\n* `inference_mode: local` \n* `local_deployment: standard`  \n\nModels and configs are recommended to be provided through a volume mount from the host to the container as shown in the `docker run` step below.  It is possible to uncomment the `# Download local models` section of the [Dockerfile](./Dockerfile.jetson) to build a container with models included.\n\n### Start the model server, awesomechat, and web app on Jetson Orin AGX\n\n```bash\n# run the container which will automatically start the model server\ndocker run --name jarvis --net=host --gpus all -v ~/jarvis/configs:/app/server/configs -v ~/src/JARVIS/server/models:/app/server/models toolboc/nv-jarvis:r35.2.1\n\n# (wait for model server to complete initialization)\n\n# start awesome_chat.py \ndocker exec jarvis python3 awesome_chat.py --config configs/config.default.yaml --mode server\n\n#start the web application (application will be acessible at http://localhost:9999)\ndocker exec jarvis npm run dev --prefix=/app/web\n```\n\n## Screenshots\n\n<p align=\"center\"><img src=\"./assets/screenshot_q.jpg\"><img src=\"./assets/screenshot_a.jpg\"></p>\n\n## Citation\nIf you find this work useful in your method, you can cite the paper as below:\n\n    @article{shen2023hugginggpt,\n        title   = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},\n        author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},\n        journal = {arXiv preprint arXiv:2303.17580},\n        year    = {2023}\n    }\n\n## Acknowledgement\n\n- [ChatGPT](https://platform.openai.com/)\n- [Hugging Face](https://huggingface.co/)\n- [ControlNet](https://github.com/lllyasviel/ControlNet)\n- [ChatGPT-vue](https://github.com/lianginx/chatgpt-vue)\n", "repo_name": "JARVIS", "org_name": "microsoft", "org_repo": "microsoft/JARVIS", "platform_org_repo": "github+microsoft/JARVIS", "link_to_repo": "https://github.com/microsoft/JARVIS", "platform": "github", "language": "Python", "stargazers_count": 20882, "watchers_count": 20882}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "devicescript-stm32", "org_name": "microsoft", "org_repo": "microsoft/devicescript-stm32", "platform_org_repo": "github+microsoft/devicescript-stm32", "link_to_repo": "https://github.com/microsoft/devicescript-stm32", "platform": "github", "language": "C", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Resource Leak Checker (RLC#) for C# code using CodeQL\n\nRLC# is a light-weight and modular resource leak checker for C# code. It is inspired by [Checker Framework's](https://checkerframework.org/) resource leak checker (RLC) for Java.\nRLC# is developed as a CodeQL query.\n\n## Prerequisites\n\nThis setup currently works only for Windows machine\n\n- Install [WSL](https://learn.microsoft.com/en-us/windows/wsl/install).\n- Install [CodeQL CLI](https://docs.github.com/en/code-security/codeql-cli/using-the-codeql-cli/getting-started-with-the-codeql-cli#checking-out-the-codeql-source-code-directly).\n\n## How to use?\n\nFirst download the CodeQL databases from LGTM and extract the database folders from the compressed files. You can also [create a CodeQL database](https://docs.github.com/en/code-security/codeql-cli/using-the-codeql-cli/creating-codeql-databases).\n\nUpdate the `HOME` variable in `run-all-services.sh` to provide path to the CodeQL repository that is cloned when you install CodeQL CLI.\n\nTo run RLC# on a list of CodeQL databases, run the following script with a list of paths to the CodeQL database folders.\n```bash\n./scripts/run-all-services.sh <list-of-codeql-databases>\n```\n\nFor each database, a sub-directory is created inside results/rlc-warnings that contains a file named `f-rlc-i-all.csv`. The resource leak warnings generated by RLC# are listed in this file.\nEach row in the file `f-rlc-i-all.csv` corresponds to a potential resource leak. The second column in a row gives the name of the source file in which the resource leak is detected and the third column gives the start line number. The first column provides meta information (type of resource, `L` stands for library type and `C` stands for custom type) for the resource leak warning.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "global-resource-leaks-codeql", "org_name": "microsoft", "org_repo": "microsoft/global-resource-leaks-codeql", "platform_org_repo": "github+microsoft/global-resource-leaks-codeql", "link_to_repo": "https://github.com/microsoft/global-resource-leaks-codeql", "platform": "github", "language": "CodeQL", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "tmdl-language-services", "org_name": "microsoft", "org_repo": "microsoft/tmdl-language-services", "platform_org_repo": "github+microsoft/tmdl-language-services", "link_to_repo": "https://github.com/microsoft/tmdl-language-services", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "tmdl-parser", "org_name": "microsoft", "org_repo": "microsoft/tmdl-parser", "platform_org_repo": "github+microsoft/tmdl-parser", "link_to_repo": "https://github.com/microsoft/tmdl-parser", "platform": "github", "language": null, "stargazers_count": 10, "watchers_count": 10}, {"README_text": "![image](https://github.com/microsoft/devhomegithubextension/blob/main/src/GithubPluginServer/Assets/StoreDisplay-150.png)\r\n\r\n# Welcome to the Dev Home GitHub Extension repo\r\n\r\n\r\nThis repository contains the source code for:\r\n\r\n* [Dev Home GitHub Extension](https://aka.ms/devhomegithubextension)\r\n* Dev Home GitHub widgets\r\n\r\nRelated repositories include:\r\n\r\n* [Dev Home](https://github.com/microsoft/devhome)\r\n\r\n## Installing and running Dev Home GitHub Extension\r\n\r\n> **Note**: The Dev Home GitHub Extension requires Dev Home. Dev Home requires Windows 11 21H2 (build 22000) or later.\r\n\r\n### Microsoft Store [Recommended]\r\n\r\nInstall [Dev Home from the Microsoft Store](https://aka.ms/devhome) and the Dev Home GitHub Extension will automatically be installed on first launch of Dev Home.\r\nThis allows you to always be on the latest version when we release new builds with automatic upgrades.\r\n\r\nThis is our preferred method.\r\n\r\nYou can also install the Dev Home GitHub Extension from its own [Microsoft Store listing](https://aka.ms/devhomegithubextension).\r\n\r\n### Other install methods\r\n\r\n#### Via GitHub\r\n\r\nFor users who are unable to install the Dev Home GitHub Extension from the Microsoft Store, released builds can be manually downloaded from this repository's [Releases page](https://github.com/microsoft/devhomegithubextension/releases).\r\n\r\n---\r\n\r\n## Dev Home GitHub Extension overview\r\n\r\nPlease take a few minutes to review the overview below before diving into the code:\r\n\r\n### Widgets\r\n\r\nThe Dev Home GitHub Extension provides widgets for Dev Home's dashboard, which is built as a Windows widget renderer. These widgets are built using the Windows widget platform, which relies on Adaptive Cards.\r\n\r\n### Machine configuration repository recommendations\r\n\r\nThe machine configuration tool utilizes the Dev Home GitHub Extension to recommend repositories to clone, but isn't required to clone and install apps. The app installation tool is powered by winget.\r\n\r\n---\r\n\r\n## Documentation\r\n\r\nDocumentation for the Dev Home GitHub Extension can be found at https://aka.ms/devhomedocs.\r\n\r\n---\r\n\r\n## Contributing\r\n\r\nWe are excited to work alongside you, our amazing community, to build and enhance the Dev Home GitHub Extension!\r\n\r\n***BEFORE you start work on a feature/fix***, please read & follow our [Contributor's Guide](https://github.com/microsoft/devhomegithubextension/blob/main/CONTRIBUTING.md) to help avoid any wasted or duplicate effort.\r\n\r\n## Communicating with the team\r\n\r\nThe easiest way to communicate with the team is via GitHub issues.\r\n\r\nPlease file new issues, feature requests and suggestions, but **DO search for similar open/closed preexisting issues before creating a new issue.**\r\n\r\nIf you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:\r\n\r\n* Kayla Cinnamon, Product Manager: [@cinnamon_msft](https://twitter.com/cinnamon_msft)\r\n* Clint Rutkas, Senior Product Manager: [@crutkas](https://twitter.com/crutkas)\r\n* Ujjwal Chadha, Developer: [@ujjwalscript](https://twitter.com/ujjwalscript)\r\n\r\n## Developer guidance\r\n\r\n* You must be running Windows 11 21H2 (build >= 10.0.22000.0) to run Dev Home\r\n* You must [enable Developer Mode in the Windows Settings app](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development)\r\n\r\n## Building the code\r\n\r\n* Clone the repository\r\n* Uninstall the Preview version of the Dev Home GitHub Extension (Dev Home has a hard time choosing which extension to use if two versions exist)\r\n* Open the GITServices.sln in Visual Studio 2022 or later, and build from the IDE, or run build.ps1 from a Visual Studio command prompt.\r\n\r\n### OAuth App\r\nSince secrets cannot be checked in to the repository, developers must create their own test OAuth app for local tests.\r\n\r\nFollow this link https://docs.github.com/en/developers/apps/building-oauth-apps/creating-an-oauth-app to create a Git OAuth app (with RedirectUri = \"devhome://oauth_redirect_uri/\").\r\n\r\nThe OAuth App ClientId and ClientSecret can be added as environment variables using the following instructions:\r\n\r\nHow to set the environment variables:\r\n\r\n    On an elevated cmd window:\r\n        setx GITHUB_CLIENT_ID \"Your OAuth App's ClientId\" /m\r\n        setx GITHUB_CLIENT_SECRET \"Your OAuth App's ClientSecret\" /m\r\n\r\n---\r\n\r\n## Code of conduct\r\n\r\nWe welcome contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "devhomegithubextension", "org_name": "microsoft", "org_repo": "microsoft/devhomegithubextension", "platform_org_repo": "github+microsoft/devhomegithubextension", "link_to_repo": "https://github.com/microsoft/devhomegithubextension", "platform": "github", "language": "C#", "stargazers_count": 127, "watchers_count": 127}, {"README_text": "# Chronos\n\n> Measure, experiment with & govern your threading layer with an iron-fist\n\nChronos (named after Greek god of time) is a thread orchestration, experimentation, monitoring and governance library for Android applications that work for large scale mobile apps. It enables a teams of developers on multiple features to utilise threading in a safe and controllable way.\n- Addresses the thread explosion problem for large teams with a centralised threadpool management system.\n- Improves the throughput of work by monitoring each threadpool via a central event stream firing multiple metrics to identify bottlenecks and high wait times.\n- Allows teams to experiment with different threadpool configurations with respect to thread priority, pool size, keep alive time etc. and monitor the impact of these changes on the performance of the feature.\n- Improves governance on large codebases by ensuring that all background work is done only on threadpools that are defined at initialisation via centralised configurations and linting.\n- Provides a Base threading layer configuration for your app inspired from GCD out of the box that can be used by apps to prioritise work across Executors based on importance to the user.\n\n\n## Usage\n**1.** Create a BaseExecutorConfig for threadpools that would contain all the threadpools. Centralising this will ensure governance of background work is possible. Each Executor is represented by an ExecutorSetting and the name should be passed to the get() APIs to ensure this is used.\nPlease check ExecutorSettings for possible values and their defaults.\n\nFor eg, you can follow a [GCD](https://developer.apple.com/documentation/dispatch/dispatchqos) based threadpool approach for creating your threadling layer to run work on based on it's priority to the user like in iOS.\n\n```kotlin\nobject BaseExecutorConfig : ExecutorConfig {\n    @StringDef(\n        ExecutorName.USER_INTERACTIVE,\n        ExecutorName.USER_INITIATED\n        // ...\n    )\n    annotation class ExecutorName {\n        companion object {\n            const val USER_INTERACTIVE = \"USER_INTERACTIVE\"\n            const val USER_INITIATED = \"USER_INITIATED\"\n            // ...\n        }\n    }\n    override val executors = setOf(\n        ExecutorSettings(\n            executorId = ExecutorName.USER_INTERACTIVE,\n            allowThreadTimeout = false,\n            corePoolSize = Integer.MAX_VALUE,\n            threadPriority = ExecutorPriority.PThreadPriority.MAX\n        ),\n        ExecutorSettings(\n            executorId = ExecutorName.USER_INITIATED,\n            allowThreadTimeout = false,\n            corePoolSize = Integer.MAX_VALUE,\n            threadPriority = ExecutorPriority.PThreadPriority.HIGH\n        ),\n        // ...\n    )\n}\n```\n\n**2.** Initialise an EventStream and register collectors to collect ExecutorEvent and ExecutionEvents from these threadpools. The EventStreamConfig will decide the monitoring related settings we will be using for these events. For eg, we can decide to collect events only when the app is in debug mode or with apply backpressure to the collectors. Transformers can be applied to add more metadata to certain events or filter some out. \n\nAll events collected pass through all the transformers to the collectors.\n\n```kotlin\n    private fun getEventStream(): EventStream {\n    return FlowEventStream(FlowEventStreamConfig.default).apply {\n        registerTransformer(object : EventTransformer<MeasureEvent> {\n            override fun transform(event: MeasureEvent): MeasureEvent? {\n                event.meta.isForeground = true\n            }\n        }, MeasureEvent::class)\n        registerCollector(\n            object : EventCollector<MeasureEvent> {\n                override fun onEvent(event: MeasureEvent) {\n                    telemetry.send(event)\n                }\n            }, MeasureEvent::class\n        )\n    }\n}\n```\n\n**3.** Initialise Executors class with EventStream and BaseExecutorConfig. Please check Experiments section to see how this BaseExecutorConfig can also be modified at runtime.\n\n```kotlin\n    private val executors = Executors(\n        getExecutorConfig(args),\n        getExecutorEventsConfig(args)\n    )\n```\n\n\n**4.** Use Executors class at all places to post work as coroutines in Kotlin\n```kotlin\n    CoroutineScope(executors.getExecutor(ExecutorName.USER_INITIATED).asCoroutineDispatcher()).launch {\n        someCriticalWork()\n    }\n```\n\nor runnables in Java\n\n```java\n    executors\n        .getExecutor(ExecutorName.USER_INITIATED)\n        .execute(\n            new Runnable() {\n                @Override\n                public void run() {\n                    someCriticalWork();\n                }\n        }\n    );\n```\n\n## Experimentation\nExperiments can be enabled on the Executors with different ExecutorSettings via CustomExperiment.\n\nInitialise ExecutorConfig by parsing your server side\n\n```kotlin\n    private fun getExecutorConfig(args: String?): ExecutorConfig {\n        return args?.let {\n            return CustomExperiment.apply(BaseExecutorConfig, it)\n        } ?: BaseExecutorConfig\n    }\n```\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "chronos", "org_name": "microsoft", "org_repo": "microsoft/chronos", "platform_org_repo": "github+microsoft/chronos", "link_to_repo": "https://github.com/microsoft/chronos", "platform": "github", "language": "Kotlin", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/reinmax) \n![GitHub](https://img.shields.io/github/license/microsoft/ReinMax) \n[![Maintenance](https://img.shields.io/badge/doc-yes-success.svg)](https://microsoft.github.io/ReinMax/) \n![PyPI](https://img.shields.io/pypi/v/reinmax) \n\n<h2 align=\"center\">ReinMax</h2>\n<h4 align=\"center\"> Beyond Straight-Through</h4>\n\n<p align=\"center\">\n  <a href=\"#st\">Straight-Through</a> \u2022\n  <a href=\"#reinmax\">ReinMax</a> \u2022\n  <a href=\"#how-to-use\">How To Use</a> \u2022\n  <a href=\"#examples\">Examples</a> \u2022\n  <a href=\"#citation\">Citation</a> \u2022\n  <a href=\"https://github.com/microsoft/reinmax/tree/main/LICENSE\">License</a>\n</p>\n\n[ReinMax](https://arxiv.org/pdf/2304.08612.pdf) achieves **second-order** accuracy and is **as fast as** the original Straight-Through, which has first-order accuracy.\n\n<h3 align=\"center\" id=\"st\"><i>What is Straight-Through</i></h4>\n<!-- ## Straight-Through and How It Works -->\n\nStraight-Through (as below) bridges discrete variables (`y_hard`) and back-propagation. \n```python\ny_soft = theta.softmax()\n\n# one_hot_multinomial is a non-differentiable function\ny_hard = one_hot_multinomial(y_soft) \n\n# with straight-through, the derivative of s_hard will\n# act as if you had `p_soft` in the forward\ny_hard = y_soft - y_soft.detach() + y_hard \n```\nIt is a long-standing mystery on how straight-through works, lefting doubts on many problems like whether we should use:\n- `y_soft - y_soft.detach()`,\n- ` (theta/tau).softmax() - (theta/tau).softmax().detach()`,\n- or what?\n\n\n\n<h3 align=\"center\" id=\"reinmax\"><i>Understand Straight-Through and Go Beyond</i></h3>\n<!-- ## Better Performance with Negligible Computation Overheads -->\n\n[We reveal](https://arxiv.org/pdf/2304.08612.pdf) that Straight-Through works as a special case of the forward Euler method, a numerical methods with first-order accuracy. \nInspired by Heun's Method, a numerical method achieving second-order accuracy without requiring Hession or other second-order derivatives, we propose ReinMax, which *approximates gradient with second-order accuracy with negligible computation overheads.*\n\n### How to use?\n\n`reinmax` can be installed via `pip`\n```\npip install reinmax\n```\n\nTo replace Straight-Through Gumbel-Softmax with ReinMax: \n\n```diff\nfrom reinmax import reinmax\n...\n- y_hard = torch.nn.functional.gumbel_softmax(logits, tau=tau, hard=True)\n+ y_hard, _ = reinmax(logits, tau) # note that reinmax prefers to set tau >= 1, while gumbel-softmax prefers to set tau < 1\n...\n```\n\nTo replace Straight-Through with ReinMax:\n```diff\nfrom reinmax import reinmax\n...\n- y_hard = one_hot_multinomial(logits.softmax()) \n- y_soft_tau = (logits/tau).softmax()\n- y_hard = y_soft_tau - y_soft_tau.detach() + y_hard \n+ y_hard, y_soft = reinmax(logits, tau) \n...\n```\n### Examples\n\n- [Polynomial Programming](https://github.com/LiyuanLucasLiu/reinmax/tree/main/poly)\n- [MNIST-VAE](https://github.com/LiyuanLucasLiu/reinmax/tree/main/mnist_vae)\n- [ListOps](https://github.com/LiyuanLucasLiu/reinmax/tree/main/listops)\n\n### Citation\nPlease cite the following papers if you found our model useful. Thanks!\n\n>Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, and Jianfeng Gao (2023). Bridging Discrete and Backpropagation: Straight-Through and Beyond. *ArXiv, abs/2304.08612*.\n```\n@inproceedings{liu2023bridging,\n  title={Bridging Discrete and Backpropagation: Straight-Through and Beyond},\n  author = {Liu, Liyuan and Dong, Chengyu and Liu, Xiaodong and Yu, Bin and Gao, Jianfeng},\n  booktitle = {arXiv:2304.08612 [cs]},\n  year={2023}\n}\n```\n", "repo_name": "ReinMax", "org_name": "microsoft", "org_repo": "microsoft/ReinMax", "platform_org_repo": "github+microsoft/ReinMax", "link_to_repo": "https://github.com/microsoft/ReinMax", "platform": "github", "language": "Python", "stargazers_count": 43, "watchers_count": 43}, {"README_text": "# ChatGPT-Robot-Manipulation-Prompts\n\nThis repository provides a set of prompts that can be used with OpenAI's ChatGPT to enable natural language communication between humans and robots for executing tasks. The prompts are designed to allow ChatGPT to convert natural language instructions into a sequence of executable robot actions, with a focus on robot manipulation tasks. The prompts are easy to customize and integrate with existing robot control and visual recognition systems.\nFor more information, please see our [blog post](https://www.microsoft.com/en-us/research/group/applied-robotics-research/articles/gpt-models-meet-robotic-applications-long-step-robot-control-in-various-environments/) and our paper, [ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application](https://www.microsoft.com/en-us/research/uploads/prod/2023/04/chatgpt_robot_manipulation_prompts_.pdf).\n\n\n![overview](./img/overview.jpg)\n## How to use\n1. We provide sample code for using ChatGPT through [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/overview). Fill in the [secrets.json](./secrets.json) with your credential information. Even if you do not have a subscription, you can try it out by copying and pasting the prompts into the [OpenAI's interface](https://chat.openai.com/).\n\n2. If you have a Azure subsctiption, install the required python packages by running the following command in a terminal session (note: we have confirmed that the sample codes work with python 3.9.16):\n```bash\n> pip install -r requirements.txt\n```\nThen, go to a subfolder in [example/](./examples) (for example, [example/task_decomposition](./examples/task_decomposition)), run the following command to run the sample code:\n```bash\npython aimodel.py --scenarios <scenario_name>\n```\nReplace `<scenario_name>` with the name of the scenario you want to run. Specific scenario names can be found in the aimodel.py.\n\n## Bibliography\n```\n@misc{wake2023chatgpt,\n      title={ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application}, \n      author={Naoki Wake and Atsushi Kanehira and Kazuhiro Sasabuchi and Jun Takamatsu and Katsushi Ikeuchi},\n      year={2023},\n      eprint={2304.03893},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "ChatGPT-Robot-Manipulation-Prompts", "org_name": "microsoft", "org_repo": "microsoft/ChatGPT-Robot-Manipulation-Prompts", "platform_org_repo": "github+microsoft/ChatGPT-Robot-Manipulation-Prompts", "link_to_repo": "https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts", "platform": "github", "language": null, "stargazers_count": 197, "watchers_count": 197}, {"README_text": "# Azure Health AI Services Samples\n\nThis repository contains different samples applications and code to help you get started with the different Microsoft Health-AI services.\nBased on these samples you will learn how to use our services, and accelerate your implementations.\n\nThis project hosts open-source samples for services developed by the Microsoft Health-AI team. \nTo learn more about the Health-AI Services, please refer to the managed service documentation: \n\n- Azure Health bot [Documentation](https://learn.microsoft.com/azure/health-bot/)\n- Azure Health Insights [Documentation](https://learn.microsoft.com/azure/azure-health-insights)\n- Text Analytics for Health [Documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/text-analytics-for-health/overview?tabs=ner)\n\n\n# Samples\n\nThis project provides samples outlining example implementations of various use cases across health. The \"samples\" folder contains all the sample apps. The samples are listed here:\n\n## 1. Intelligent Dashboard powered by Text Analytics for Health\n\nThe Intelligent Dashboard is designed to help healthcare professionals and organizations efficiently gather insights from unstructured healthcare information. By utilizing [Text Analytics for Health] (https://learn.microsoft.com/azure/azure-health-insights), the sample application is able to receive unstructured healthcare information and generate structured insights. The insights generated from Text Analytics for Health are then used to create population insights dashboards, which are accessible in a single Power App. With this innovative sample, healthcare professionals can easily access and analyze data in real-time. Say goodbye to the tedious process of manually analyzing healthcare data and hello to our Intelligent Dashboard  sample!\n\n![\"A screenshot of the Intelligent Dashboard PowerApp - PowerBI screen\"](/media/intelligent-dashboard-ta4h/dashboard.png)\n\n[I want to try this sample](/samples/intelligent-dashboard-ta4H/README.md)\n\n## 2. Day in the life of a Nurse. \n\nThis repository contains several open-source example [Power Apps](https://make.powerapps.com/) which were created based on a study called 'The Day in the Life of a Nurse'. One of the outcomes were several minimal viable products that could support nurses in their daily job. These starter Power Apps solutions are enhanced with [Nuance Speech to Text](https://www.nuancehealthcaredeveloper.com/?q=Dragon-Medical-SpeechKit-Home), and utilize [Text Analytics for Health](https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/text-analytics-for-health/overview ) for medical structuring. The data is being served from [FHIR API](https://docs.microsoft.com/en-us/azure/healthcare-apis/healthcare-apis-overview) and utilize the [FHIRBase](https://docs.microsoft.com/en-us/connectors/fhirbase/) and [FHIRClinical](https://docs.microsoft.com/en-us/connectors/fhirclinical/) Power Platform connectors. The application can also be linked to [Microsoft Shifts](https://support.microsoft.com/en-us/office/get-started-in-shifts-5f3e30d8-1821-4904-be26-c3cd25a497d6) where you can get real-time shift info from your colleagues.\n\n\n![\"A screenshot of the easy reporting power app for nurses\"](/media/day-in-the-life-of-a-nurse/easy-reporting.png)\n\n[I want to try this sample](https://github.com/microsoft/nurseempowerment)\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Disclaimers\n\nThe Health-AI Samples is an open-source project. It is not a managed service, and it is not part of the Health-AI Services. The sample apps and sample code provided in this repo are used as examples only. You bear sole responsibility for compliance with local law and for any data you use when using these samples. Please review the information and licensing terms on this GitHub website before using the Health-AI Services Samples repo. \n\nThe Azure Health-AI Samples Github repo is intended only for use in transferring and formatting data. It is not intended for use as a medical device or to perform any analysis or any medical function and the performance of the software for such purposes has not been established. You bear sole responsibility for any use of this software, including incorporation into any product intended for a medical purpose. \n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-health-AI-services-samples", "org_name": "microsoft", "org_repo": "microsoft/azure-health-AI-services-samples", "platform_org_repo": "github+microsoft/azure-health-AI-services-samples", "link_to_repo": "https://github.com/microsoft/azure-health-AI-services-samples", "platform": "github", "language": null, "stargazers_count": 25, "watchers_count": 25}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PowerApps-Sample-Apps-Japan", "org_name": "microsoft", "org_repo": "microsoft/PowerApps-Sample-Apps-Japan", "platform_org_repo": "github+microsoft/PowerApps-Sample-Apps-Japan", "link_to_repo": "https://github.com/microsoft/PowerApps-Sample-Apps-Japan", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Catalog in Power Platform\r\n\r\nThis is a discussion forum for reporting issues, requesting features and providing feedback for the Power Platform.\r\n\r\nDocumentation for Catalog can be found here: https://learn.microsoft.com/en-us/power-platform/developer/catalog\r\n\r\nPower Platform CLI documentation can be found here: https://learn.microsoft.com/en-us/power-platform/developer/cli/reference/catalog\r\n\r\nPower Platform actions have been published : https://github.com/microsoft/powerplatform-actions \r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n## Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \r\ntrademarks or logos is subject to and must follow \r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "PowerPlatform-Catalog", "org_name": "microsoft", "org_repo": "microsoft/PowerPlatform-Catalog", "platform_org_repo": "github+microsoft/PowerPlatform-Catalog", "link_to_repo": "https://github.com/microsoft/PowerPlatform-Catalog", "platform": "github", "language": null, "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# [Preview] Sample Chat App with AOAI\n\nThis repo contains sample code for a simple chat webapp that integrates with Azure OpenAI. Some portions of the app use preview APIs and features that should not be used for any production dependency.\n\n## Run the app\nUpdate the environment variables listed in `app.py`. At minimum, you need to specify `AZURE_OPENAI_RESOURCE`, `AZURE_OPENAI_MODEL`, and `AZURE_OPENAI_KEY`.\nStart the app with `start.cmd`.\nThis will build the frontend, install backend dependencies, and then start the app.\nYou can see the local running app at http://127.0.0.1:5000.\nNote: this app is under construction!\n\n## Deploy the app\n\n### One click Azure deployment\n[![Deploy to Azure](https://aka.ms/deploytoazurebutton)](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fsample-app-aoai-chatGPT%2Fmain%2Finfrastructure%2Fdeployment.json)\n\nClick on the Deploy to Azure button and configure your settings in the Azure Portal as described in the [Environment variables](#environment-variables) section.\n\nPlease be aware that you need:\n-   an existing Azure OpenAI resource with models deployment\n-   OPTIONALLY - an existing Azure Cognitive Search\n\n### Deploy from your local machine\n\nYou can use the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) to deploy the app from your local machine. Make sure you have version 2.48.1 or later.\n\nIf this is your first time deploying the app, you can use [az webapp up](https://learn.microsoft.com/en-us/cli/azure/webapp?view=azure-cli-latest#az-webapp-up). Run the following command from the root folder of the repo, updating the placeholder values to your desired app name, resource group, location, and subscription. You can also change the SKU if desired.\n\n`az webapp up --runtime PYTHON:3.10 --sku B1 --name <new-app-name> --resource-group <resource-group-name> --location <azure-region> --subscription <subscription-name>`\n\nIf you've deployed the app previously from the AOAI studio, first run this command to update the appsettings to allow local code deployment:\n\n`az webapp config appsettings set -g <resource-group-name> -n <existing-app-name> --settings WEBSITE_WEBDEPLOY_USE_SCM=false`\n\nThen, use the `az webapp up` command to deploy your local code to the existing app:\n\n`az webapp up --runtime PYTHON:3.10 --sku B1 --name <existing-app-name> --resource-group <resource-group-name>`\n\nMake sure that the app name and resource group match exactly for the app that was previously deployed.\n\nDeployment will take several minutes. When it completes, you should be able to navigate to your app at {app-name}.azurewebsites.net.\n\n## Best Practices\nFeel free to fork this repository and make your own modifications to the UX or backend logic. For example, you may want to expose some of the settings in `app.py` in the UI for users to try out different behaviors. We recommend keeping these best practices in mind:\n\n- Reset the chat session (clear chat) if the user changes any settings. Notify the user that their chat history will be lost.\n- Clearly communicate to the user what impact each setting will have on their experience.\n- When you rotate API keys for your AOAI or ACS resource, be sure to update the app settings for each of your deployed apps to use the new key.\n\n## Environment variables\n\n| App Setting | Value | Note |\n| --- | --- | ------------- |\n|AZURE_SEARCH_SERVICE|||\n|AZURE_SEARCH_INDEX|||\n|AZURE_SEARCH_KEYv\n|AZURE_SEARCH_USE_SEMANTIC_SEARCH|False||\n|AZURE_SEARCH_SEMANTIC_SEARCH_CONFIG|||\n|AZURE_SEARCH_INDEX_IS_PRECHUNKED|False||\n|AZURE_SEARCH_TOP_K|5||\n|AZURE_SEARCH_ENABLE_IN_DOMAIN|False||\n|AZURE_SEARCH_CONTENT_COLUMNS|||\n|AZURE_SEARCH_FILENAME_COLUMN|||\n|AZURE_SEARCH_TITLE_COLUMN|||\n|AZURE_SEARCH_URL_COLUMN|||\n|AZURE_OPENAI_RESOURCE|||\n|AZURE_OPENAI_MODEL||The name of your model deployment|\n|AZURE_OPENAI_MODEL_NAME|gpt-35-turbo|The name of the model|\n|AZURE_OPENAI_KEY|||\n|AZURE_OPENAI_TEMPERATURE|0||\n|AZURE_OPENAI_TOP_P|1.0||\n|AZURE_OPENAI_MAX_TOKENS|1000||\n|AZURE_OPENAI_STOP_SEQUENCE|||\n|AZURE_OPENAI_SYSTEM_MESSAGE|You are an AI assistant that helps people find information.||\n|AZURE_OPENAI_PREVIEW_API_VERSION|2023-06-01-preview||\n|AZURE_OPENAI_STREAM|True||\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "sample-app-aoai-chatGPT", "org_name": "microsoft", "org_repo": "microsoft/sample-app-aoai-chatGPT", "platform_org_repo": "github+microsoft/sample-app-aoai-chatGPT", "link_to_repo": "https://github.com/microsoft/sample-app-aoai-chatGPT", "platform": "github", "language": "Python", "stargazers_count": 54, "watchers_count": 54}, {"README_text": "# Jumpstart Agora Applications\n\nThis applications source code repository is part of [Jumpstart Agora](https://aka.ms/JumpstartAgora). This repo holds the codebase for industry-based applications designed to showcase various industry applications and increase developer velocity using Azure Arc Jumpstart and serves as a comprehensive resource for developers looking to explore and leverage industry-specific applications built on Azure.\n\n## Features\n\nJumpstart Agora offers the following key features:\n\n1. Industry-Based Applications: The repository contains a curated collection of industry applications, demonstrating how Azure Arc can be utilized for \"cloud and edge\" development across various domains such as retail, manufacturing, healthcare, finance, and more.\n\n2. Codebase: Each application in Jumpstart Agora has its own dedicated folder within the repository, containing all the necessary code, scripts, and configuration files. Developers can explore, study, and leverage this codebase to understand the implementation details and apply them to their own projects.\n\n3. Documentation: This repository leverage the detailed documentation provided in the [Jumpstart Agora](https://aka.ms/JumpstartAgora) page. Developers can refer to these guides to understand the application's architecture, deployment instructions, best practices, and any additional considerations specific to the industry.\n\n4. Community Engagement: Developers are encouraged to contribute to the repository by adding new industry applications, improving existing code, fixing bugs, or enhancing documentation. By fostering a collaborative environment, Jumpstart Agora aims to build a vibrant community of developers who can collectively drive innovation in the industry.\n\n## Start contributing\n\nTo start contributing to Jumpstart Agora apps repository follow these steps:\n\n1. This repo is part of [Jumpstart Agora](https://aka.ms/JumpstartAgora). Visit the page to learn more about it and its goals.\n\n2. Depending on your interest, you can contribute to either this applications codebase repository or the [Jumpstart Agora](https://aka.ms/JumpstartAgora) repository (or both). To contribute to this repository, follow the steps below:\n\n    1. Fork the repository to your GitHub account.\n\n    2. Clone the forked repository to your local machine.\n\n        ```shell\n        git clone https://github.com/microsoft/jumpstart-agora-apps.git\n        ```\n\n    3. Create a new branch for your changes.\n\n        ```shell\n        git checkout -b <branch-name>\n        ```\n\n    4. Make the necessary changes to the codebase.\n\n    5. Commit and push the changes to your forked repository.\n\n        ```shell\n        git commit -m \"Add my contribution\"\n        git push origin <branch-name>\n        ```\n\n    6. Create a pull request to the main repository.\n\n    7. Once the pull request is approved, the changes will be merged to the main repository.\n\n3. To contribute to the [Jumpstart Agora](https://aka.ms/JumpstartAgora) repository, follow it's contribution instructions and guidelines.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit Microsoft [CLA open source page](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\n\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "jumpstart-agora-apps", "org_name": "microsoft", "org_repo": "microsoft/jumpstart-agora-apps", "platform_org_repo": "github+microsoft/jumpstart-agora-apps", "link_to_repo": "https://github.com/microsoft/jumpstart-agora-apps", "platform": "github", "language": "CSS", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Marinara\n\nBuild distroless images for **CBL-Mariner**!\n\nMarinara is a distroless image builder for CBL-Mariner. It is intended to be modular to create several variations of a distroless image such as `standard` distroless image, distroless image with `debug` option, distroless image with `nonroot` user, or distroless image with both `debug` option and `nonroot` user.\n\nIt is designed to be easily used with Docker.\n\n## Getting Started\n-  How to use Marinara\n\n    1.  Clone the source repository.\n    2.  Change directory to `marinara`.\n    3.  Run the docker build command that is suitable for your use case. Follow the [Docker Commands](/instructions/dockercommands.md) (located at `marinara/instructions`) to learn how a specific docker build command can be used to produce or extend a distroless image for your specific use case.\n\n    For more details, please read the [Quick Guide](/instructions/quickstart.md) under `marinara/instructions`.\n\n-\tSoftware dependencies\n    - Linux System (For example, CBL-Mariner)\n    - Docker CLI with Docker Engine version 20.10\n\n## Features\n\n### Create a distroless image from scratch\nThere are a few distroless image variants that can be created with Marinara. For example, you can create a `standard` distroless image with Mariner packages, or you can create a `debug` version of the image. There are also options to create a distroless image with `nonroot` user in it, and the final variant is a distroless image with **both** the `debug` option and a `nonroot` user.\n\nUse a template dockerfile and a simple docker build command to produce a distroless image composed of Mariner packages from scratch. The template dockerfile `dockerfile-new-image` allows for passing build arguments to docker via the docker build command to create an image. Read more about these build arguments in the Build Arguments section.\n\n### Extend a distroless image\n\nMarinara can be used to extend a CBL-Mariner distroless image by adding packages to it. A CBL-Mariner distroless image has a couple of **container manifest files** (`container-manifest-1` and `container-manifest-2`) located at `/var/lib/rpmmanifest/` that are used to keep track of installed packages in the image. Extending a distroless image relies on these manifests and keeps these files up-to-date. The template dockerfile `dockerfile-extend-image` allows for passing build arguments to docker via the docker build command to extend an image. Read more about these build arguments in the Build Arguments section.\n\n## Tools\n\n### Marinara docker image\n\nMarinara docker image is developed by Mariner team with the goal of producing distroless images. It is based on CBL-Mariner base container image. It is available on MCR for CBL-Mariner version `2.0`. Although, the `1.0` version of the image is not available in MCR, you can create it locally by following instructions in [Marinara image instructions](/instructions/marinaraimage.md).\n\nAdditionally, it comes with essential utilities and packages preinstalled that can be used to create a distroless image from scratch or extend a distroless image by adding more packages to it:\n\n- **marinaracreate.py** - Composes an image, given an `image type`, `mariner version`, `packages to install`, and `user`. Some of the arguments can be optional depending on the image type.\n\n- **marinaraextend.py** - Extends a CBL-Mariner distroless image, given an `image to extend`, `mariner version`, and `packages to install`.\n\nMarinara uses `tdnf` package manager under the hood to install packages and their dependencies. The tooling included in the marinara docker image abstracts away the internals of tdnf installation process, along with the process of producing or keeping the container manifest files up-to-date. Although you have the ability to further modify, for a general use case, the starter dockerfiles do not require any modification to produce different flavors of a distroless image.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "marinara", "org_name": "microsoft", "org_repo": "microsoft/marinara", "platform_org_repo": "github+microsoft/marinara", "link_to_repo": "https://github.com/microsoft/marinara", "platform": "github", "language": "Python", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "<img src=\"docs/eBPF%20logo%20png%20800px.png\" width=75 height=75 align=left />\n\n# eBPF for Windows\n\n![CodeQL](https://github.com/microsoft/ebpf-for-windows/workflows/CI/CD/badge.svg?branch=main&event=schedule)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5742/badge)](https://bestpractices.coreinfrastructure.org/projects/5742)\n[![codecov](https://codecov.io/gh/microsoft/ebpf-for-windows/branch/main/graph/badge.svg?token=TXa0UAMvYf)](https://codecov.io/gh/microsoft/ebpf-for-windows)\n\neBPF is a well-known technology for providing programmability and agility, especially for extending an\nOS kernel, for use cases such as DoS protection and observability. This project is a work-in-progress that\nallows existing eBPF\ntoolchains and APIs familiar in the Linux ecosystem to be used on top of Windows.  That is, this project\ntakes existing eBPF projects as submodules and adds the layer in between to make them run on top of Windows.\n\n## New to eBPF?\n\nSee our [basic eBPF tutorial](docs/tutorial.md) and our\n[tutorial on debugging eBPF verification failures](docs/debugging.md).\n\n## Architectural Overview\n\nThe following diagram shows the basic architecture of this project and related components:\n\n![Architectural Overview](docs/ArchitectureDiagram.png)\n\nAs shown in the diagram, existing eBPF toolchains (clang, etc.) can be used to generate eBPF bytecode from\nsource code in various languages.  Bytecode can be consumed by any application, or via bpftool or the Netsh command line tool, which use a shared library\nthat exposes [Libbpf APIs](https://github.com/libbpf/libbpf),\nthough this is still in progress.\n\nThe eBPF bytecode is sent to a static verifier (the [PREVAIL verifier](https://github.com/vbpf/ebpf-verifier))\nthat is hosted in a secure user-mode environment such as a system service (which is the case at present),\nenclave, or trusted VM.\nIf the eBPF program passes all the verifier checks, it can be loaded into the kernel-mode execution context.\nTypically this is done by being JIT compiled (via the [uBPF](https://github.com/iovisor/ubpf) JIT compiler) into native code that is passed to the execution context.  In a debug build,\nthe byte code can instead be directly loaded into\nan interpreter (from [uBPF](https://github.com/iovisor/ubpf) in the kernel-mode execution context) though\nthe interpreter is not present in a release build as it is considered less secure.  See also the HVCI FAQ answer\nbelow.\n\neBPF programs installed into the kernel-mode execution context can attach to various\n[hooks](https://microsoft.github.io/ebpf-for-windows/ebpf__structs_8h.html#a0f8242763b15ec665eaa47c6add861a0)\nand call various helper APIs exposed by the eBPF shim,\nwhich internally wraps public Windows kernel APIs, allowing the use of eBPF on existing versions of Windows.\nMany [helpers](https://microsoft.github.io/ebpf-for-windows/bpf__helper__defs_8h.html)\nalready exist, and more hooks and helpers will be added over time.\n\n## Getting Started\n\nThis project supports eBPF on Windows 10 or later, and on Windows Server 2019 or later.\nTo try out this project, see our [Getting Started Guide](docs/GettingStarted.md).\n\nWant to help?  We welcome contributions!  See our [Contributing guidelines](CONTRIBUTING.md).\nFeel free to take a look at our [Good First Issues](https://github.com/microsoft/ebpf-for-windows/labels/good%20first%20issue)\nlist if you're looking for somewhere to start.\n\nWant to chat with us?  We have a:\n* [Slack channel](https://cilium.slack.com/messages/ebpf-for-windows) (If you are new, sign up at http://slack.cilium.io/)\n* Zoom meeting for github issue triage: see [meeting info](https://github.com/microsoft/ebpf-for-windows/discussions/427)\n\nFor tracking Q&A and general discussion, we use [Discussions](https://github.com/microsoft/ebpf-for-windows/discussions)\nin github.  This can also function similar to a mailing list if you subscribe to discussion notifications by\nclicking \"Watch\" (or \"Unwatch\") and selecting \"Custom\" -> \"Discussions\" (or by selecting \"All Activity\" if\nyou want to receive notifications about everything else too).\n\n## Frequently Asked Questions\n\n### 1. Is this a fork of eBPF?\n\nNo.\n\nThe eBPF for Windows project leverages existing projects, including\nthe [IOVisor uBPF project](https://github.com/iovisor/ubpf) and\nthe [PREVAIL verifier](https://github.com/vbpf/ebpf-verifier),\nrunning them on top of Windows by adding the Windows-specific hosting environment for that code.\n\n### 2. Does this provide app compatibility with eBPF programs written for Linux?\n\nThe intent is to provide source code compatibility for code that uses common\nhooks and helpers that apply across OS ecosystems.\n\nLinux provides many hooks and helpers, some of which are very Linux specific (e.g., using\nLinux internal data structs) that would not be applicable to other platforms.\nOther hooks and helpers are generically applicable and the intent is to support them for eBPF\nprograms.\n\nSimilarly, the eBPF for Windows project exposes [Libbpf APIs](https://github.com/libbpf/libbpf)\nto provide source code compatibility for applications that interact with eBPF programs.\n\n### 3. Will eBPF work with HyperVisor-enforced Code Integrity (HVCI)?\n\nYes. With HVCI enabled, eBPF programs cannot be JIT compiled, but can be run either natively or in interpreted mode\n(but the interpreter is disabled in release builds and is only supported in debug builds). To understand\nwhy JIT compiled mode does not work, we must first understand what HVCI does.\n\n[HyperVisor-enforced Code Integrity (HVCI)](https://techcommunity.microsoft.com/t5/windows-insider-program/virtualization-based-security-vbs-and-hypervisor-enforced-code/m-p/240571)\nis a mechanism\nwhereby a hypervisor, such as Hyper-V, uses hardware virtualization to protect kernel-mode processes against\nthe injection and execution of malicious or unverified code. Code integrity validation is performed in a secure\nenvironment that is resistant to attack from malicious software, and page permissions for kernel mode are set and\nmaintained by the hypervisor.\n\nSince a hypervisor doing such code integrity checks will refuse to accept code pages that aren't signed by\na key that the hypervisor trusts, this does impact eBPF programs running natively.  As such, when HVCI\nis enabled, eBPF programs work fine in interpreted mode, but not when using JIT compilation because the JIT\ncompiler does not have a key that the hypervisor trusts.  And since interpreted\nmode is absent in release builds, neither mode will work on an HVCI-enabled production system.\n\nInstead, a third mode is also supported by eBPF for Windows, in addition to JIT compiled and interpreted modes.\nThis third mode entails compiling eBPF programs into regular Windows drivers that can be accepted by HVCI.\nFor more discussion, see the [Native Code Generation documentation](docs/NativeCodeGeneration.md).\n", "repo_name": "ebpf-for-windows-release", "org_name": "microsoft", "org_repo": "microsoft/ebpf-for-windows-release", "platform_org_repo": "github+microsoft/ebpf-for-windows-release", "link_to_repo": "https://github.com/microsoft/ebpf-for-windows-release", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# AOAI Virtual Assistant Accelerator\n\nQuickly get started with virtual assistants using Azure OpenAI\n\n## Setup\n\n- Set up an Azure OpenAI resource. You may need to request access if your subscription is not enabled yet\n![Configure AOAI Service](./readme_images/configure-aoai.png)\n- Create a GPT 3.5 or GPT 4 deployment\n![Configure AOAI Deployment](./readme_images/configure-deployment.png)\n- Clone the repository:\n\n    `git clone https://github.com/microsoft/aoai-virtual-assistant.git`\n\n- Create the app settings file at `./GPTVirtualAssistant/settings/appsettings.json`. Use the example file provided as a template. You will need to fill in the information in the \"openai\" field.\n![Configure AOAI Credentials](./readme_images/configure-settings.png)\n- Open the project in Bot Framework Composer\n![Configure AOAI Credentials](./readme_images/open-botcomposer.png)\n- [Publish your bot](https://learn.microsoft.com/en-us/composer/how-to-publish-bot?tabs=v2x)\n![Publish bot](./readme_images/publish-bot.png)\n- [Configure end-user channels](https://learn.microsoft.com/en-us/azure/bot-service/bot-service-manage-channels?view=azure-bot-service-4.0)\n![Configure end-user channels](./readme_images/configure-channels.png)\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aoai-virtual-assistant", "org_name": "microsoft", "org_repo": "microsoft/aoai-virtual-assistant", "platform_org_repo": "github+microsoft/aoai-virtual-assistant", "link_to_repo": "https://github.com/microsoft/aoai-virtual-assistant", "platform": "github", "language": "JavaScript", "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# User-centric Web Attestations\n\n*This is a sample project, as such, it shouldn't be used as-is in a production environment.*\n\nThis project implements a proof-of-concept prototype for User-Centric Web Attestations (UWA). These attestations are statements (encoded as [U-Prove tokens](https://microsoft.com/uprove)) certified by an Issuer that can be attached to a web site by their User and verified by anyone. The UWA are only valid on the page (scope) to which they are attached. UWA protects the privacy of users; they contain no traceable information other than the application-specific data a user might want to disclose. Users are in control of which attestations to present where.\n\nDetails can be found in the [UWA specification](./doc/uwa-spec.md).\n\nThe repository contains two projects, which need to be setup separately:\n* an Express [sample server](./sample-issuer/README.md)\n* a Edge/Chrome [web browser extension](./browser-extension/README.md)\n\n## System overview\n\nThe system consists of three participants:\n* an Issuer that issues U-Prove tokens to Users\n* a User that obtains U-Prove tokens from Issuers, and creates web attestations\n* a Verifier that verifies web attestations\n\nThe [sample server](./sample-issuer/README.md) implements the Issuer role, while the [web browser extension](./browser-extension/README.md) implements both the User and Verifier roles.\n\nThe UWA lifecycle is as follows:\n1. The Issuer sets up its Issuer parameters and publishes them in a publicly accessible location. These specify the contents of the U-Prove tokens, which can contain an application-specific label. Users and Verifiers must obtain the Issuer parameters before creating or verifying web attestations.\n2. The User obtains U-Prove tokens from an Issuer. Authentication to the Issuer is application-specific. U-Prove tokens are stored in the web browser extension; new tokens will be automatically obtained if they expired or if they are running out.\n3. When visiting a web site, the User can create a web attestation from an issued token using the web browser extension (encoded either as a string or a QR code), and attach it to the site. The U-Prove token is then deleted from the browser extension to prevent linkability with newly created attestations.\n4. Other users visiting the same web site can verify attached web attestations from trusted Issuers using the web browser extension. Unknown Issuers can be added to the trusted list by the User. Invalid attestations (for example: forged, or copied from a different site) are marked as such; malformed ones are simply ignored.\n\n![UWA architecture](./doc/UWA_arch.svg)\n\n## Deployment example\n\nThis section describes an example of how a user could create a web attestation for their *soc.ial* profile attesting their membership in the amazing *commun.ity* group, and how another user can prove they are human using an attestation from the *human.iam* service (note that *soc.ial*, *commun.ity*, and *human.iam* are fictional web sites).\n\n### Issuer setup\n\nThe *commun.ity* admin first needs to create their U-Prove Issuer parameters and setup their issuance service; this can be achieved using the [sample server](./sample-issuer/README.md) project.\n\nThe admin first modifies the [settings.ts](./sample-issuer/src/settings.ts) file to correctly setup the Express server (to be deployed at `https://commun.ity`), which would then need to be modified and integrated into the *commun.ity* web environment to authenticate incoming users. In this example, the admin decides on a 7-day token validity, on an about page location (e.g., `https://commun.ity/uwa/about.html`), and decides to add a \"Membership level\" label to the tokens with three possible values: \"Gold\", \"Silver\", and \"Bronze\", representing the community's three membership types it offers.\n\nThe admin then creates the Issuer parameters by running `npm run setup-issuer` resulting in the creation of a JSON web key (JWK) set (publicly available at `https://commun.ity/.well-known/jwks.json`) and the corresponding private key (that remains, of course, secret).\n\nThe admin creates the about page explaining its UWA issuance criteria, and adds a `<meta name=\"uwa\" content=\"https://commun.ity/issue\">` tag in the page's HTML allowing a user's browser extension to discover the token issuance endpoint.\n\nThe token issuance server can be deployed by running `npm run deploy-issuer`.\n\n### Token issuance\n\nAlice, a member of *commun.ity*, visits the community web site, and sees the new feature to obtain tokens on its about page. She downloads and install the UWA browser extension for Edge, and in the popup Tokens tab, she clicks `Get tokens` (the browser extension enables this button after parsing the `<meta>` tag in the about page). This triggers the U-Prove issuance protocol after which she obtains some U-Prove tokens signed by *commun.ity* (5 tokens by default); the tokens and corresponding private keys are stored in the browser extension.\n\n### Attestation creation\n\nAlice has a pseudonymous account on *soc.ial*, with name tag *@pr1v4cy*; she wants to add a *commun.ity* membership attestation to her profile. She navigates to her profile page `https://soc.ial/@pr1v4cy`, and from the browser extension's Tokens tab, she selects the `https://commun.ity` issuer and clicks `Create`. The browser extension selects an unused token from the selected issuer, and creates a web attestation by signing the scope URL `https://soc.ial/@pr1v4cy` and current time using the token's private key before deleting it (expired and low-count tokens are automatically renewed by the browser extension). The resulting UWA string and equivalent QR code image are displayed in the popup. Alice copies the UWA string and edits her profile's bio adding the following text:\n\n```\nI'm a member of the amazing commun.ity: uwa://eyJhbGciOiJVUDI1NiJ9.eyJzY29wZSI6Imh0dHBzOi8vc29jLmlhbC9AcHIxdjRjeSIsInRpbWVzdGFtcCI6MTY4NDk1MDA0Mzg5MX0.eyJ1cHQiOnsiVUlEUCI6ImJvTGNUdTA1M2NNeGtOVEJtdEMyTmFkb2VIUnEyMzVtTmlQLVRNblM1Y2siLCJoIjoiQkVrZHJWU1padV9lUmNXc25YNld4M0Z3QmxwdWxnSkU2NUEyeWtyUmxkMjdUN3VxY3E4ZTZKOVl1NkNVaDJOQzNYZVo3QmlvTGQ4M2VMWlhGQURNeW00IiwiVEkiOiJleUpwYzNNaU9pSm9kSFJ3Y3pvdkwyTnZiVzExYmk1cGRIa2lMQ0psZUhBaU9qSXdNREF5TENKc1ltd2lPakY5IiwiUEkiOiIiLCJzWnAiOiJCR2RlT01rcTNnX1VEdHp2R1ZOTTYzckY2VGh6WDlPX0RoWFA4Q3I0Y3dlQW9QaHAxMEpKWnZBZ3NsN2tjLWJXTDEtX2V4eHpzdEhuS1JVT2ZpX0N2dE0iLCJzQ3AiOiJHaWJXdDYybGt3TG1Od3hYM3F5U2szblR5Y05HQy1pNmRiTkJhUUdBbzZjIiwic1JwIjoiWFNfY1dTOTdRNm5udTJpSlRjVXpKWHJHamdFdXBWLWZiOWZNRFlhRGpaMCJ9LCJwcCI6eyJhIjoiMFl3VUZWQjBKUmJIM2tRM1cyaElzb3VnLTJnQ21wbTVUSnBTLXJQYXZfdyIsInIiOlsiZGdqTU9mR2U1MWlpV0JHWTB0bVQ5VHJ1dVRRWjhXZHBpaFhZY2pBSUM5VSJdfX0\n```\n\nIn addition, she downloads the QR code image, and adds it in a pinned message on her profile page.\n\n![soc.ial QR code](./doc/soc.ial.qr.png)\n\n### Attestation verification\n\nBob, who also has the UWA browser extension installed, navigates to `https://soc.ial/@pr1v4cy` to learn more about this insightful user. The browser extension automatically parses the UWA string in the bio text, and since Bob is also a member of *commun.ity* (and therefore already trusts this Issuer), it renders it as a verified blue badge. Clicking on the badge reveals that *@pr1v4cy* is a Gold member of *commun.ity* (wow!) and this already increases Bob's confidence in this user's *soc.ial* posts. \n\n<img src=\"./doc/verified_soc.ial.png\" alt=\"Verified blue badge\" width=\"40%\"/>\n\nBob would like to know who is behind this *@pr1v4cy* tag name, unfortunately he can't even if he bribed the *commun.ity* admin with a million dollars (this could be any of the site's Gold members, nothing else to learn due to the U-Prove unlinkability property).\n\nBob later navigates to `https://soc.ial/@h4ck3r`, who also claims to be part of *commun.ity*. This malicious user is however not part of the reputable community, they simply copied the UWA string from *@pr1v4cy*'s page and added it to their own. Fortunately, the browser extension isn't fooled by this subterfuge, and renders the string as a red invalid badge (the U-Prove presentation proof is invalid, because the signed scope doesn't match the current page).\n\n<img src=\"./doc/invalid_soc.ial.png\" alt=\"Invalid red badge\" width=\"40%\"/>\n\nFinally, Bob visits the page of `https://soc.ial/2cool4u` and starts reading this user's numerous but interesting posts. Bob starts to wonder if this is a bot account or simply someone with a lot of free time! This user's profile pic is a UWA QR code, so Bob right click on it and selects \"Verify QR\". The QR-encoded UWA string has been issued by `https://human.iam`, an issuer unknown to Bob (and his browser extension), so a yellow unknown badge is displayed. \n\n<img src=\"./doc/untrusted_soc.ial.png\" alt=\"Untrusted yellow badge\" width=\"40%\"/>\n\nAfter navigating to their website and performing a quick web search, Bob learns that this is a web site that validates users' humanness after validating their real-life identity. Bob decides to click on the `Trust` button in the UWA badge, which prompts the browser extension to download and trust the Issuer parameters from `https://human.iam/.well-known/jwks.json` and validate the UWA. Bob now inspect the information of the verified blue badge, learning that *2cool4u* was verified using a government-issued ID, which convinces Bob that this is indeed a real person. From now on, UWA from `https://human.iam` will be automatically verified by his browser extension.\n\n## Frequently Asked Questions\n\n### How can I trust the Issuer of an attestation?\n\nJust like in identity federations (e.g., in OAuth/OpenID) or in PKI where verifiers must trust the keys of token/certificate issuers, U-Prove Verifiers must trust the parameters of an attestation's Issuers. Issuers are identified by a URL, from which the parameters can be retrieved. The validation therefore relies on trust of  ownership of the TLS endpoint. In an open environment, Users can make this decisions on an Issuer-by-Issuer basis, or delegate the trust to other authorities (e.g., an organization listing trusted issuers it oversees or audits).\n\n### Why U-Prove? Can't you do this with standard cryptography?\n\nWe could have designed the UWA framework encoding an Issuer signature and a User's proof-of-possession using conventionally signed JWS, which would be perfectly acceptable in many scenarios (e.g., for posting alumni or employment attestations in one's professional social profiles), but might be problematic in others (e.g., posting controversial or sensitive memberships on a pseudonymous forum). It would be trivial for an Issuer (e.g., an insider with access to the issuance logs) to recognize attestations it issued that are attached to web sites (therefore identifying the User behind them). Since U-Prove tokens are unlinkable, usage of the technology supports all use cases by protecting the full privacy spectrum: from anonymity, to pseudonymity, to full-identification.  \n\n### Why do label values need be pre-determined in the Issuer parameters?\n\nIt is sometimes useful to augment an Issuer attestation with some addition information. For example, a \"humanness validation\" attestation could encode a \"level of assurance\" value describing how the User's identity was confirmed (e.g., phone, email, in-person), an alumni attestation could encode a graduation year, an employment attestation could encode a company division where the User works. Since the UWA framework aims at protecting privacy to the upmost degree, free-form labels are not allowed; this is to prevent Issuers from encoding unique trackable values in a User's tokens. Issuers must therefore list the possible values in its parameters, and encode the corresponding value index in the issued tokens. Richer User attributes could be encoded in a token and disclosed in an attestation; see the [extensions](#extensions) section for details. \n\n### Can U-Prove tokens be revoked?\n\nAlthough U-Prove tokens can be revoked using various mechanisms, the UWA framework specifies the simplest validity method to deploy: short-lived tokens. Limiting the validity period of tokens foregoes the need for a token revocation scheme. Issuer keys can however be revoked by removing the corresponding parameters from the issuer JWK set as described in the [UWA specification](https://github.com/microsoft/web-attestation-sample/blob/main/doc/uwa-spec.md#issuer-setup). Doing so will invalidate all UWA generated using tokens issued using these parameters.\n\n### How long is a U-Prove token valid for?\n\nThe Issuer decides on the validity period of the U-Prove tokens they issue. The expiration date is described as the number of days since the Unix epoch; all tokens expire at midnight UTC on the specified day. These bucketized expiration values enhance privacy by reducing token trackability, when compared to using fine-grained expiration values. \n\n### Why are multiple tokens retrieved in batch for each issuance?\n\nThe issuance and attachment of a UWA is unlinkable (by virtue of the U-Prove unlinkability property); reusing a token to create multiple UWAs attached to different sites would however be linkable (they would have the same Issuer signature). Using a different token for each UWA protects against undesired User tracking. Obtaining tokens in advance in batch reduces the Issuer's visibility into their usage, thus better protecting privacy. If tokens were retrieved on-demand just before creating a UWA, an Issuer comparing issuance and UWA creation times could infer a lot of information on the Users behind them.  \n\n### Can someone steal my web attestation and attach it to another page?\n\nNo. The page URL (a.k.a. scope) where the UWA is attached has been signed by the User when creating it; moving the UWA to another page would result in an invalid signature that would be rejected by Verifiers; the corresponding badge would consequently be rendered as invalid. An attacking accessing issued tokens private keys could however create new attestations; it is therefore important to protect them accordingly (see the [extensions](#extensions) section for a discussion).\n\n## Extensions\n\nThis initial release focusses on the core concept of creating user-controlled and privacy-preserving web attestations. Many additional features and improvements are possible (by making use current or upcoming features of the [U-Prove JSON Framework (UPJF)](https://github.com/microsoft/uprove-node-reference/blob/main/doc/U-Prove_JSON_Framework.md)), and some of these are needed for a secure and robust real-life deployment. The following list describes some of them:\n\n* *Selectively-disclosable attributes*: a UWA can currently encode only a single Issuer-specific label value. To make badges more informative, the issued U-Prove tokens could be augmented with various attributes that could be selectively-disclosed on a page by page basis (e.g., a User could disclose its Issuer-verified real name on their professional social media page, while withholding it on their pseudonymous gaming profile).\n* *Optimized serialization*: the UWA URI is a direct encoding of the token presentation JWS as specified in the UPJF. Defining a more compact serialization in the framework would reduce the size of the UWA strings and their corresponding QR encoding.\n* *Key protection*: the User's token keys are stored as-is in the browser extension's local storage. Keys should be encrypted under a user-controlled key to help prevent theft. It is also desirable to prevent Users from extracting the private keys to prevent undesired token transfer (for example, to prevent Users from sharing or selling their attestations), which is difficult to do in a JavaScript environment. Stronger cryptographic techniques, such as the U-Prove device binding (see section 6 of the U-Prove [technical overview](https://github.com/microsoft/uprove-node-reference/blob/main/doc/U-Prove%20Technology%20Overview%20V1.1%20Revision%203.pdf)) could be use to tie a token to a specific 2nd-factor device (e.g., a TPM, a phone).\n* *UWA links*: a web attestation URI might be too big to fit into some web environments with a character limit, and its alternative QR code form might not suitable for text-only environment. Another possibility is to define a UWA link, pointing to an external location from which the UWA content could be retrieved from.\n* *Verifier UI protections*: malicious code could be used to trick a user into believing a fake UWA is valid by manipulating the page's UI (e.g., displaying verified badges or fake content popup on top of the ones created by the browser extensions). Isolating the extension's UI element in their own iframe could help mitigate this issue. Another useful feature would be for the browser extension to display the current's page validated UWAs in its popup. \n* *Site integration support*: the system currenty requires users to integrate the UWA string or QR code into a web site themselves. Participating sites could make the experience more user friendly by providing a mechanisms to accept attestations from the browser extension, and integrate them in the HTML they serve in a non-intrusive way, or by verifying the information once and directly displaying validated badges for all site visitors.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "web-attestation-sample", "org_name": "microsoft", "org_repo": "microsoft/web-attestation-sample", "platform_org_repo": "github+microsoft/web-attestation-sample", "link_to_repo": "https://github.com/microsoft/web-attestation-sample", "platform": "github", "language": "JavaScript", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# CLANDESTINO\n\n_Para espa\u00f1ol, [ve aqu\u00ed](./LEEME.md)_\n\nThis is the repository for CLANDESTINO, the Spanish toxic language dataset. Spanish is spoken by almost half a billion people in over 40 countries as a first language. This makes toxic language detection challenging: blanket detection could cause erasure (a term offensive in one locale might not be offensive in another), but underblocking can cause harm and perpetuate stereotypes.\n\nAdditionally, the phonetic nature of Spanish means that there are multiple ways to spell the same word. Statistical, text-only approaches fail to account for this. \nCultural nuances of the Hispanosphere also play an important role: the legacy of colonialism and imperialism means that offensiveness and toxicity are measured with a different bar than that of US-based speech; and it varies considerably over locales.\nHence, it would be inaccurate to consider all Spanish-speaking countries as a cultural monolith.\n\nCLANDESTINO is a corpus designed for toxic-language detection, bearing in mind the points above. It is meant to also act as a seed for further data synthesization, and has the following features:\n- Native speaker annotation, spanning seven countries. \n- Extensive (yet non-exhaustive, see [below](#responsible-ai-considerations)) coverage across multiple toxic categories (e.g., hate speech, microaggressions, positive stereotyping, self-harm, disinformation) and groups both traditionally considered vulnerable as well as those specific to the Hispanosphere.\n- Coverage of both informal speech (e.g., text-based spelling) and formal speech.\n- Inclusion of locale-specific language, along with locale tags.\n- A combination of AI and human-generated toxic content, and labelled as such.\n\n**WARNING: This repository contains and discusses content that is offensive or upsetting. All materials are intended to support research that improves toxicity detection methods. Included examples of toxicity do not represent how the authors or sponsors feel about any identity groups.**\n\n\n## Updates\n_Stay tuned for updates since this corpus is actively under development_\n\n- May 15th: CLANDESTINO's first version released! \n\n\n## Citation\n\nIf you use CLANDESTINO in your work, please consider citing our paper:\n\n\n```\nTBD\n```\n\n\n## Responsible AI Considerations\n\n>Note that there is still a lot that this dataset is not capturing about what constitutes problematic language in Spanish. \n>Our annotations might not capture the full complexity of these issues, given problematic language is context-and-culture-dependent, dynamic, and can manifest in different forms and different severities. Problematic language is also fundamentally a human-centric problem and should be studied in conjunction with human experience. There is need for multi-disciplinary work to better understand these aspects.\n\n>Also note that this dataset only captures toxicity in a non-exhaustive manner, and due to its large scale can naturally be noisy. Our goal in this project is to provide the community with means to improve toxicity detection in Spanish across multiple locales, and <ins>there exist limitations to this dataset and models trained on it</ins>. \n>Remember as well that all annotations introduce an intrinsic bias into the dataset. \n>All of these limitations can and should be the subject of future research.\n\n\n## Contributing & Trademarks\n\nSee [here](./CONTRIBUTING.md).\n", "repo_name": "Clandestino", "org_name": "microsoft", "org_repo": "microsoft/Clandestino", "platform_org_repo": "github+microsoft/Clandestino", "link_to_repo": "https://github.com/microsoft/Clandestino", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# BrevE and CLaro\n\nPara espa\u00f1ol, ve [aqu\u00ed](./LEEME.md).\n\nThis is the repository for our paper \"BrevE and CLaro: an evaluation of Spanish text simplification\". \n- BrevE is a corpus designed for Spanish complex sentence identification (CSI).\n- CLaro is a corpus for plain language identification (PLI).\n\nBoth corpora have been annotated by native Spanish speakers. This is important because:\n\n- Spanish is a language spoken by almost half a billion people!\n- Spanish morphology is quite different than English, and the sentence structure / word order allows for nuances that are not easily translated.\n\nText simplification (TS) is an important area of NLP, and is designed to make written material more accessible to a variety of users. \nHowever, it has been shown that identifying sentences that _need_ simplification on the first place considerably improves the performance of any TS system. \nOur work is meant to benchmark this.\n\nSee `documents` for documentation about this dataset, and our paper (link TBD) for our findings around CSI and PLI in Spanish. \n\nIf you use BrevE or CLaro in your work, please considering citing our paper:\n\n```\n@article{placeholderpaper\n   TBD\n}\n```\n\nAs well as the original works for the corpora we used to build BrevE and CLaro:\n\n```\n@inproceedings{oscar1,\n  author    = {Julien Abadji and Pedro Javier Ortiz Su\\'{a}rez and Laurent Romary and Beno\\^{i}t Sagot},\n  title     = {Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus},\n  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event)},\n  editor    = {Harald L{\\\"u}ngen and Marc Kupietz and Piotr Ba\u0144ski and Adrien Barbaresi and Simon Clematide and Ines Pisetta},\n  publisher = {Leibniz-Institut f{\\\"u}r Deutsche Sprache},\n  address   = {Mannheim},\n  doi       = {10.14618/ids-pub-10468},\n  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-104688},\n  pages     = {1 -- 9},\n  year      = {2021},\n}\n\n@inproceedings{oscar2,\n  author    = {Pedro Javier {Ortiz Su\\'{a}rez} and Beno\\^it Sagot and Laurent Romary},\n  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},\n  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},\n  editor    = {Piotr Ba\\'nski and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L\\\"{u}ngen and Caroline Iliadi},\n  publisher = {Leibniz-Institut f\\\"{u}r Deutsche Sprache},\n  address   = {Mannheim},\n  doi       = {10.14618/ids-pub-9021},\n  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},\n  pages     = {9 -- 16},\n  year      = {2019},\n}\n\n@inproceedings{yimam1,\n    title = \"Multilingual and Cross-Lingual Complex Word Identification\",\n    author = \"Yimam, Seid Muhie  and\n      {\\v{S}}tajner, Sanja  and\n      Riedl, Martin  and\n      Biemann, Chris\",\n    booktitle = \"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017\",\n    month = sep,\n    year = \"2017\",\n    address = \"Varna, Bulgaria\",\n    publisher = \"INCOMA Ltd.\",\n    url = \"https://doi.org/10.26615/978-954-452-049-6_104\",\n    doi = \"10.26615/978-954-452-049-6_104\",\n    pages = \"813--822\",\n}\n\n@inproceedings{yimam2,\n    title = \"{CWIG}3{G}2 - Complex Word Identification Task across Three Text Genres and Two User Groups\",\n    author = \"Yimam, Seid Muhie  and\n      {\\v{S}}tajner, Sanja  and\n      Riedl, Martin  and\n      Biemann, Chris\",\n    booktitle = \"Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)\",\n    month = nov,\n    year = \"2017\",\n    address = \"Taipei, Taiwan\",\n    publisher = \"Asian Federation of Natural Language Processing\",\n    url = \"https://aclanthology.org/I17-2068\",\n    pages = \"401--407\",\n}\n\n```\n\n# Updates\n\n- 5th June 2023: BrevE and CLaro are out!\n\n# Contributing\n\nSee [here](./CONTRIBUTING.md).\n\n# Legal Notices\n\nSee [here](./NOTICE.md)\n", "repo_name": "BrevE-CLaro", "org_name": "microsoft", "org_repo": "microsoft/BrevE-CLaro", "platform_org_repo": "github+microsoft/BrevE-CLaro", "link_to_repo": "https://github.com/microsoft/BrevE-CLaro", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# didx509go\n\n[![go.dev](https://pkg.go.dev/badge/github.com/microsoft/didx509go.svg)](https://pkg.go.dev/github.com/microsoft/didx509go)\n[![tests](https://github.com/microsoft/didx509go/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/didx509go/actions?query=workflow%3Aci)\n\nDID:x509 resolver for Go\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "didx509go", "org_name": "microsoft", "org_repo": "microsoft/didx509go", "platform_org_repo": "github+microsoft/didx509go", "link_to_repo": "https://github.com/microsoft/didx509go", "platform": "github", "language": "Go", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# cosesign1go\n\n[![go.dev](https://pkg.go.dev/badge/github.com/microsoft/cosesign1go.svg)](https://pkg.go.dev/github.com/microsoft/cosesign1go)\n[![tests](https://github.com/microsoft/cosesign1go/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/cosesign1go/actions?query=workflow%3Aci)\n\nA Go library to handle COSE Sign1 documents\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cosesign1go", "org_name": "microsoft", "org_repo": "microsoft/cosesign1go", "platform_org_repo": "github+microsoft/cosesign1go", "link_to_repo": "https://github.com/microsoft/cosesign1go", "platform": "github", "language": "Go", "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project Butterfly\n   \n## High level view of the Steps to follow\n\n<img src=\"./media/image7.png\" style=\"width:10.99365in;height:4.58333in\"\nalt=\"Project Butterfly High level view of the Steps to follow\" />\n\n#      \n## Detailed Stages to follow\n\n<img src=\"./media/image1.png\" style=\"width:10.99365in;height:5.70764in\"\nalt=\"Project Butterfly Deatailed Stages to follow\" />\n\n# \n\n<img src=\"./media/image2.png\" style=\"width:8.94653in;height:2.39861in\" />\n\n\n**Activities:**\n\n- Meet customer to listen and understand, leverage the \u201cWindows Server and SQL Server on Azure pitch deck \u201d as needed \u2013 SS, TS\n\n- Meet with Microsoft Account Team and Microsoft GPS Tech Team to Develop technical strategy according to customer necessities \u2013 SS, TS\n\n- Qualify opportunity for Windows server and SQL server migration to Azure \u2013 SS\n\n**Exit Criteria:**\n\n- High level definition of Migration Scope and Opportunity/Lead definition and registration on Partner Connect - SS;\n\n**Key Resources:**\n\n- FY23 Data & AI Partner Play book (\u201cMigrate and modernize your data estate\u201d section)- [Link](https://assetsprod.microsoft.com/en-us/fy23-data-and-ai-azure-playbook-1.pdf)\n\n- Windows Server and SQL Server on Azure pitch deck - [Link](https://aka.ms/wandsqlmigratepitch)\n\n- Migrate Windows and SQL Server in a Box \u2013 [Link](https://assetsprod.microsoft.com/mpn/en-us/migrate-windows-server-sql-server-pib.pptx)\n\n- Data Driven digital transformation \u2013 [Link](https://aka.ms/cdoppt)\n\n- Migrate & Modernize Data Estate Pitch Deck-\u00a0 [Link](https://aka.ms/datamod)\n\n# \n\n<img src=\"./media/image3.png\" style=\"width:8.98238in;height:2.16672in\"/>\n\n**Activities:**\n\n- Partner creates inbound deal via partner center with \\#DMWL \u2013 SS, TS\n\n- Partner review Customer Priorities & Success Criteria based in the discussions with the customer stakeholders. \u2013 SS, TS\n\n- Partner initiate a technical assessment or nominate the opportunity to the Solution Assessment Program \u2013 SS,TS\n\n- Create and deliver an Architecture Design Session - TS\n\n- Engage GPS Tech team to support in the Technical/Technology architecture, Understand Critical dependencies and\u00a0 inputs for deep architecture discussions.\n\n**Exit Criteria:**\n\n- Customer reviewed and endorsed the business value - SS\n\n- Technical/Business proof requirements identified -\u00a0 TS\n\n**Key Resources:**\n\n- Azure SQL Migration Architecture Center: [link](https://learn.microsoft.com/en-us/azure/architecture/guide/migration/migration-start-here)\u00a0\n\n- Azure TCO Calculator - [Link](https://azure.microsoft.com/en-us/pricing/tco/)\n\n- Application and Database Modernization Assessment - [Link](https://partner.microsoft.com/en-us/asset/collection/gdpr-solution-assessment)\n\n- Migrate & Modernize Demos - [Link](https://github.com/microsoft/CSAAzureSQLDemo/tree/main/azsql-migration-demos)\n\n# \n\n<img src=\"./media/image4.png\" style=\"width:8.98833in;height:2.17863in\"/>\n\n**Activities:**\n\n- Incorporate technical patterns/practices (ESLZ, CAF, WAF, WMF) to defined Solution - TS\n\n- Partner confirms solution to GPS Tech Team and nominates to AMMP and to SQL Migration Factory (SMF) \u2013 SS\n\n- Define, with key customer decision maker, and deploy a functional small\u2010scale proof of concepts and evaluate the feasibility of the solution and gain a deeper understanding of the solution needs at full scale using Key Resources described bellow and with GPS Tech team guidance and support\u2013 TS\n\n- Present results of PoC to customer key stakeholders \u2013 SS, TS.\n\n- Gain customer go ahead for proposal \u2013 SS\n\n- Create Proposal incorporating AMMP and SMF strategies as a \u201cOneTeam\u201c with GPS and STU Microsoft teams \u2013 SS, TS\n\n**Exit Criteria:**\n\n- High level definition of Migration Scope and Opportunity/Lead definition and registration on Partner Connect - SS;\n\n- Change opportunity status to \u201cCommitted\u201d \u2013 SS\n\n- Consumption plan with milestones to track progress during project execution \u2013 SS, TS\n\n**Key Resources:**\n\n- Migrate (PoC)- Migrate SQL Server to SQL Server on Azure Virtual Machine online using Azure Data Studio- [Link](https://learn.microsoft.com/azure/dms/tutorial-sql-server-to-virtual-machine-online-ads)\n\n- CAF Migration Process - [Link](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/migrate/)\n\n- Azure SQL Managed Instance Demo - [Link](https://github.com/microsoft/bobsql/tree/master/demos/sqlmidemo)\n\n- MCW-Migrating-SQL-databases \u2013 [Link](https://github.com/microsoft/MCW-Migrating-SQL-databases-to-Azure/blob/master/Hands-on%20lab/HOL%20step-by-step%20-%20Migrating%20SQL%20databases%20to%20Azure.md)\n\n- Azure SQL Workshop - [Link](https://github.com/microsoft/sqlworkshops-azuresqlworkshop)\n\n# \n\n<img src=\"./media/image5.png\" style=\"width:8.95857in;height:2.24411in\" />\n\n**Activities:**\n\n- Partner coordinate the Customer Success Plan execution, defining the kick-off date, consumption, user training, set up a rhythm of the business cadence with the customer for the implementation, and initiate support.\u00a0\u00a0SS, TS\n\n- Perform a technical validation: If needed engage GPS Tech Team and any other Microsoft team to validate the technical solution. - TS\n\n- Develop / Implement recommended practices for operations,\u00a0 governance, and adoption & organizational change/transformation. \\#AzureLandingZone - TS\n\n- Mitigate of\u00a0any technical blockers\u00a0that may affect\u00a0solution implementation - TS\n\n**Exit Criteria:**\n\n- Customer reviewed and endorsed the business value - SS\n\n**Key Resources:**\n\n- Azure SQL Architecture Review - [Link](https://learn.microsoft.com/en-us/azure/architecture/framework/services/data/azure-sql-database-well-architected-framework)  \n  Azure Well-Architected Framework Review \u2013 Assessment : [Link](https://learn.microsoft.com/en-us/assessments/azure-architecture-review/)\n\n- Azure Landing Zone User Guide - [Link](https://github.com/Azure/Enterprise-Scale/wiki/Deploying-Enterprise-Scale-Pre-requisites)\n\n# \n<img src=\"./media/image6.png\" style=\"width:8.95261in;height:2.32149in\" />\n\n**Activities:**\n\n- Drive customer business value achievement and Monitor usage trends and refresh Consumption Plans appropriately - SS\n\n- Analyze digital signals to maintain and optimize customer success (retention) and Proactive backlog review and action new success plans - TS\n\n**Exit Criteria:**\n\n- Final customer acceptance of project delivery\n\n**Key Resources**\n\n- Cloud Adoption Framework Manage \u2013 [Link](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/manage/)\n\n- Well Architected Deep-dive - [Link](https://learn.microsoft.com/en-us/training/paths/azure-well-architected-framework/)\n\n#\n## End of content\n# \n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n", "repo_name": "Project-Butterfly-LATAM", "org_name": "microsoft", "org_repo": "microsoft/Project-Butterfly-LATAM", "platform_org_repo": "github+microsoft/Project-Butterfly-LATAM", "link_to_repo": "https://github.com/microsoft/Project-Butterfly-LATAM", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Durable Task Client SDK for Python\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Build Validation](https://github.com/microsoft/durabletask-python/actions/workflows/pr-validation.yml/badge.svg)](https://github.com/microsoft/durabletask-python/actions/workflows/pr-validation.yml)\n\nThis repo contains a Python client SDK for use with the [Durable Task Framework for Go](https://github.com/microsoft/durabletask-go) and [Dapr Workflow](https://docs.dapr.io/developing-applications/building-blocks/workflow/workflow-overview/). With this SDK, you can define, schedule, and manage durable orchestrations using ordinary Python code.\n\n\u26a0\ufe0f **This SDK is currently under active development and is not yet ready for production use.** \u26a0\ufe0f\n\n> Note that this project is **not** currently affiliated with the [Durable Functions](https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview) project for Azure Functions. If you are looking for a Python SDK for Durable Functions, please see [this repo](https://github.com/Azure/azure-functions-durable-python).\n\n\n## Supported patterns\n\nThe following orchestration patterns are currently supported.\n\n### Function chaining\n\nAn orchestration can chain a sequence of function calls using the following syntax:\n\n```python\n# simple activity function that returns a greeting\ndef hello(ctx: task.ActivityContext, name: str) -> str:\n    return f'Hello {name}!'\n\n# orchestrator function that sequences the activity calls\ndef sequence(ctx: task.OrchestrationContext, _):\n    result1 = yield ctx.call_activity(hello, input='Tokyo')\n    result2 = yield ctx.call_activity(hello, input='Seattle')\n    result3 = yield ctx.call_activity(hello, input='London')\n\n    return [result1, result2, result3]\n```\n\nYou can find the full sample [here](./examples/activity_sequence.py).\n\n### Fan-out/fan-in\n\nAn orchestration can fan-out a dynamic number of function calls in parallel and then fan-in the results using the following syntax:\n\n```python\n# activity function for getting the list of work items\ndef get_work_items(ctx: task.ActivityContext, _) -> List[str]:\n    # ...\n\n# activity function for processing a single work item\ndef process_work_item(ctx: task.ActivityContext, item: str) -> int:\n    # ...\n\n# orchestrator function that fans-out the work items and then fans-in the results\ndef orchestrator(ctx: task.OrchestrationContext, _):\n    # the number of work-items is unknown in advance\n    work_items = yield ctx.call_activity(get_work_items)\n\n    # fan-out: schedule the work items in parallel and wait for all of them to complete\n    tasks = [ctx.call_activity(process_work_item, input=item) for item in work_items]\n    results = yield task.when_all(tasks)\n\n    # fan-in: summarize and return the results\n    return {'work_items': work_items, 'results': results, 'total': sum(results)}\n```\n\nYou can find the full sample [here](./examples/fanout_fanin.py).\n\n### Human interaction and durable timers\n\nAn orchestration can wait for a user-defined event, such as a human approval event, before proceding to the next step. In addition, the orchestration can create a timer with an arbitrary duration that triggers some alternate action if the external event hasn't been received:\n\n```python\ndef purchase_order_workflow(ctx: task.OrchestrationContext, order: Order):\n    \"\"\"Orchestrator function that represents a purchase order workflow\"\"\"\n    # Orders under $1000 are auto-approved\n    if order.Cost < 1000:\n        return \"Auto-approved\"\n\n    # Orders of $1000 or more require manager approval\n    yield ctx.call_activity(send_approval_request, input=order)\n\n    # Approvals must be received within 24 hours or they will be canceled.\n    approval_event = ctx.wait_for_external_event(\"approval_received\")\n    timeout_event = ctx.create_timer(timedelta(hours=24))\n    winner = yield task.when_any([approval_event, timeout_event])\n    if winner == timeout_event:\n        return \"Canceled\"\n\n    # The order was approved\n    ctx.call_activity(place_order, input=order)\n    approval_details = approval_event.get_result()\n    return f\"Approved by '{approval_details.approver}'\"\n```\n\nAs an aside, you'll also notice that the example orchestration above works with custom business objects. Support for custom business objects includes support for custom classes, custom data classes, and named tuples. Serialization and deserialization of these objects is handled automatically by the SDK.\n\nYou can find the full sample [here](./examples/human_interaction.py).\n\n## Feature overview\n\nThe following features are currently supported:\n\n### Orchestrations\n\nOrchestrations are implemented using ordinary Python functions that take an `OrchestrationContext` as their first parameter. The `OrchestrationContext` provides APIs for starting child orchestrations, scheduling activities, and waiting for external events, among other things. Orchestrations are fault-tolerant and durable, meaning that they can automatically recover from failures and rebuild their local execution state. Orchestrator functions must be deterministic, meaning that they must always produce the same output given the same input.\n\n### Activities\n\nActivities are implemented using ordinary Python functions that take an `ActivityContext` as their first parameter. Activity functions are scheduled by orchestrations and have at-least-once execution guarantees, meaning that they will be executed at least once but may be executed multiple times in the event of a transient failure. Activity functions are where the real \"work\" of any orchestration is done.\n\n### Durable timers\n\nOrchestrations can schedule durable timers using the `create_timer` API. These timers are durable, meaning that they will survive orchestrator restarts and will fire even if the orchestrator is not actively in memory. Durable timers can be of any duration, from milliseconds to months.\n\n### Sub-orchestrations\n\nOrchestrations can start child orchestrations using the `call_sub_orchestrator` API. Child orchestrations are useful for encapsulating complex logic and for breaking up large orchestrations into smaller, more manageable pieces.\n\n### External events\n\nOrchestrations can wait for external events using the `wait_for_external_event` API. External events are useful for implementing human interaction patterns, such as waiting for a user to approve an order before continuing.\n\n### Continue-as-new (TODO)\n\nOrchestrations can be continued as new using the `continue_as_new` API. This API allows an orchestration to restart itself from scratch, optionally with a new input.\n\n### Suspend, resume, and terminate\n\nOrchestrations can be suspended using the `suspend_orchestration` client API and will remain suspended until resumed using the `resume_orchestration` client API. A suspended orchestration will stop processing new events, but will continue to buffer any that happen to arrive until resumed, ensuring that no data is lost. An orchestration can also be terminated using the `terminate_orchestration` client API. Terminated orchestrations will stop processing new events and will discard any buffered events.\n\n### Retry policies (TODO)\n\nOrchestrations can specify retry policies for activities and sub-orchestrations. These policies control how many times and how frequently an activity or sub-orchestration will be retried in the event of a transient error.\n\n## Getting Started\n\n### Prerequisites\n\n- Python 3.7 or higher (Python 3.8 or higher is recommended when running tests or making contributions)\n- A Durable Task-compatible sidecar, like [Dapr Workflow](https://docs.dapr.io/developing-applications/building-blocks/workflow/workflow-overview/)\n\n### Installing the Durable Task Python client SDK\n\nInstallation is currently only supported from source. Ensure pip, setuptools, and wheel are up-to-date.\n\n```sh\npython3 -m pip install --upgrade pip setuptools wheel\n```\n\nTo install this package from source, clone this repository and run the following command from the project root:\n\n```sh\npython3 -m pip install .\n```\n\n### Run the samples\n\nSee the [examples](./examples) directory for a list of sample orchestrations and instructions on how to run them.\n\n## Development\n\nThe following is more information about how to develop this project. Note that development commands require that `make` is installed on your local machine. If you're using Windows, you can install `make` using [Chocolatey](https://chocolatey.org/) or use WSL.\n\n### Generating protobufs\n\nProtobuf definitions are stored in the [./submodules/durabletask-proto](./submodules/durabletask-proto) directory, which is a submodule. To update the submodule, run the following command from the project root:\n\n```sh\ngit submodule update --init\n```\n\nOnce the submodule is available, the corresponding source code can be regenerated using the following command from the project root:\n\n```sh\nmake proto-gen\n```\n\n### Running unit tests\n\nUnit tests can be run using the following command from the project root. Unit tests _don't_ require a sidecar process to be running.\n\n```sh\nmake test-unit\n```\n\n### Running E2E tests\n\nThe E2E (end-to-end) tests require a sidecar process to be running. You can use the Dapr sidecar for this or run a Durable Task test sidecar using the following `docker` command:\n\n```sh\ndocker run --name durabletask-sidecar -p 4001:4001 --env 'DURABLETASK_SIDECAR_LOGLEVEL=Debug' --rm cgillum/durabletask-sidecar:latest start --backend Emulator\n```\n\nTo run the E2E tests, run the following command from the project root:\n\n```sh\nmake test-e2e\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "durabletask-python", "org_name": "microsoft", "org_repo": "microsoft/durabletask-python", "platform_org_repo": "github+microsoft/durabletask-python", "link_to_repo": "https://github.com/microsoft/durabletask-python", "platform": "github", "language": "Python", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Power Platform Developer Hub\n\n> This repo contains sample files that are intended to be used as a reference and learning \n> resource for developers who are following tutorials in the [Developer Hub](https://developer.powerplatform.microsoft.com). The purpose is\n> to provide hands-on examples that demonstrate the concepts covered in the tutorials \n> and help developers build their skills and understanding.\n\n## Build custom controls\n\nRefer to the samples in folder **custom-controls/linear-input** to build a custom control for your app.\n\n## Setup CI/CD\n\nRefer to the samples in folder **.github/workflows** to setup CI/CD using GitHub Actions.\n\n\n", "repo_name": "powerplatform-developerhub", "org_name": "microsoft", "org_repo": "microsoft/powerplatform-developerhub", "platform_org_repo": "github+microsoft/powerplatform-developerhub", "link_to_repo": "https://github.com/microsoft/powerplatform-developerhub", "platform": "github", "language": "TypeScript", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# ATP Course Demos and Assignments Repository\n\nWelcome to the ATP course demos and assignments repository! This repository is designed to provide university students and other interested learners with demo projects, example code, and assignments to practice the knowledge and skills acquired during the ATP course.\n\nThe ATP course aims to teach advanced technology concepts, techniques, and best practices to help students build a solid foundation in modern software development and other technology-related fields.\n\nRepository Structure\nThis repository is organized as follows:\n\n{CourseId}/demos/: This directory contains demo projects and example code that will be presented during the course. Each demo is organized in a separate folder and includes a README file with instructions and explanations.\n\n{CourseId}/assignments/: This directory contains the assignments that students are required to complete after each class. Each assignment is organized in a separate folder and includes a README file with detailed instructions, requirements, evaluation criteria, and additional resources, such as reading materials, reference documentation, and useful links related to the course topics.\n\n# Getting Started\nTo get started, clone this repository to your local machine using the following command:\n\nbash\nCopy code\ngit clone https://github.com/microsoft/atp-edu.git\nThen, navigate to the demos/ and assignments/ directories to explore the available demo projects and assignments.\n\n## Contributing\n\nWe encourage students and other learners to contribute to this repository by submitting pull requests with improvements, bug fixes, or additional resources. Before submitting a pull request, please make sure to:\n\n- Fork the repository.\n- Create a new branch with a descriptive name.\n- Make your changes in the new branch.\n- Commit your changes and write a clear, concise commit message.\n- Push your changes to your fork.\n- Create a pull request and explain your changes.\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## License\nThis repository is licensed under the MIT License. Please read the LICENSE file for more information.\n\n## Contact\nIf you have any questions, suggestions, or issues, please feel free to open an issue on this repository or contact the course instructor via email at msatp@microsoft.com.\n\nHappy learning and coding!\n", "repo_name": "atp-edu", "org_name": "microsoft", "org_repo": "microsoft/atp-edu", "platform_org_repo": "github+microsoft/atp-edu", "link_to_repo": "https://github.com/microsoft/atp-edu", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Batch Inference Toolkit\n\nBatch Inference Toolkit(batch-inference) is a Python package that batches model input tensors coming from multiple requests dynamically, executes the model, un-batches output tensors and then returns them back to each request respectively. This will improve system throughput because of better compute parallelism and better cache locality. The entire process is transparent to developers. \n\n## When to use\n\nWhen you want to host Deep Learning model inference on Cloud servers, especially on GPU\n\n## Why to use\n\nIt can improve your server throughput up to multiple times\n\n## Advantage of batch-inference\n\n* Platform independent lightweight python library\n* Only few lines code change is needed to onboard using built-in [batching algorithms](https://microsoft.github.io/batch-inference/batcher/what_is_batcher.html)\n* Flexible APIs to support customized batching algorithms and input types\n* Support [multi-process remote mode](https://microsoft.github.io/batch-inference/remote_model_host.html) to avoid python GIL bottleneck\n* Tutorials and benchmarks on popular models:\n\n| Model | Throughput Comparing to Baseline | Links |\n| :-----| :---- | :---- |\n| Bert Embedding | 4.7x | [Tutorial](https://microsoft.github.io/batch-inference/examples/bert_embedding.html)  |\n| GPT Completion | 16x | [Tutorial](https://microsoft.github.io/batch-inference/examples/gpt_completion.html) |\n\n## Installation\n\nInstall from Pip\n\n```bash\npython -m pip install batch-inference --upgrade\n```\n\nBuild and Install from Source _(for developers)_\n\n```bash\ngit clone https://github.com/microsoft/batch-inference.git\npython -m pip install -e .[docs,testing]\n\n# if you want to format the code before commit\npip install pre-commit\npre-commit install\n\n# run unittests\npython -m unittest discover tests\n```\n\n## Example\n\nLet's start with a toy model to learn the APIs. Firstly, you need to define a **predict_batch** method in your model class, and then add the **batching** decorator to your model class.\n\nThe **batching** decorator adds host() method to create **ModelHost** object. The **predict** method of ModelHost takes a single query as input, and it will merge multiple queries into a batch before calling **predict_batch** method. The predict method also splits outputs from predict_batch method before it returns result.\n\n```python\nimport numpy as np\nfrom batch_inference import batching\nfrom batch_inference.batcher.concat_batcher import ConcatBatcher\n\n@batching(batcher=ConcatBatcher(), max_batch_size=32)\nclass MyModel:\n    def __init__(self, k, n):\n        self.weights = np.random.randn(k, n).astype(\"f\")\n\n    # shape of x: [batch_size, m, k]\n    def predict_batch(self, x):\n        y = np.matmul(x, self.weights)\n        return y\n\n# initialize MyModel with k=3 and n=3\nhost = MyModel.host(3, 3)\nhost.start()\n\n# shape of x: [1, 3, 3]\ndef process_request(x):\n    y = host.predict(x)\n    return y\n\nhost.stop()\n```\n\n**Batcher** is responsible to merge queries and split outputs. In this case ConcatBatcher will concat input tensors into a batched tensors at first dimension. We provide a set of built-in Batchers for common scenarios, and you can also implement your own Batcher. See [What is Batcher](https://microsoft.github.io/batch-inference/batcher/what_is_batcher.html) for more information.\n", "repo_name": "batch-inference", "org_name": "microsoft", "org_repo": "microsoft/batch-inference", "platform_org_repo": "github+microsoft/batch-inference", "link_to_repo": "https://github.com/microsoft/batch-inference", "platform": "github", "language": "Python", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# Polyglot Notebook - the new way to teach\n\nThis repo shows how you can teach several courses within the computer science space. It's also a 0-config environment where it's all setup for you and you can use a service like GitHub Codespaces to get started in seconds.\n\n## Anatomy of this repo\n\n- `.devcontainer/` a directory containing needed files for this to run on GitHub Codespaces\n    - `Dockerfile`, installs the needed runtime, in this case .NET 7\n    - `decontainer.json`, contains instructions on what extensions to install when you boot up your Codespaces environment.\n\n- `samples/` a samples directory showcasing different Notebooks that shows how you can teach various courses.\n    - `javascript.dib`, shows how you can teach the JavaScript programming language.\n    - `csharp.dib`, shows how you can teach the C# programming language.\n    - `frontend.dib`, shows how you can teach \"frontend\" meaning things such as HTML and CSS\n    - `api.dib`, shows how you can teach building an API. It has support for setting up backend routes and invoke the same.\n\n\n## Run repo\n\nTo run this repo, take the following steps:\n\n1. Fork this repo\n1. in your forked repo select \"Code\" and select \"Create codespace on main\"\n\n![Start Codespace](Codespaces-start.png)\n\nYou should now have an environment ready to use. See below video on what you're able to do:\n\n![TODO video embed](video.mp4)\n\n## References\n\n- Polyglot Notebooks\n- Mermaid\n- Codespaces", "repo_name": "polyglot-education", "org_name": "microsoft", "org_repo": "microsoft/polyglot-education", "platform_org_repo": "github+microsoft/polyglot-education", "link_to_repo": "https://github.com/microsoft/polyglot-education", "platform": "github", "language": "Dockerfile", "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Microsoft AI SDK for SAP\n\n## Links\n[Microsoft AI SDK for SAP Documentation](https://microsoft.github.io/aisdkforsapabap/)\n\n[Discussion Forum](https://github.com/microsoft/aisdkforsapabap/discussions)\n\n[Issues/Bug reporting](https://github.com/microsoft/aisdkforsapabap/issues)\n\n> ***Note**: Documentation website is still in progress. Please bookmark the above documentation link for future reference.* \n\n## What is Microsoft AI SDK for SAP?\nAI SDK for SAP is a software development kit (SDK) designed to provide SAP ABAP developers with the tools needed to create intelligent enterprise applications using artificial intelligence (AI) technologies.\n\nMicrosoft AI SDK for SAP ABAP is designed to be user-friendly, with an intuitive interface that allows developers to easily integrate AI capabilities into ABAP applications. Currently in early preview phase and Version 1.0, It provides ability to integrate and use Azure Open AI capabilities. Future versions will expand this capability to include other AI engines.\n\n*Azure Open AI is a comprehensive set of AI services and tools provided by Microsoft Azure. It includes powerful machine learning algorithms, natural language processing tools, and cognitive services that can be used to build intelligent applications that can recognize patterns, process natural language, and make predictions based on data. Azure Open AI features pre-built AI models and algorithms, as well as tools for custom model training and deployment \u2013 [all with strong security, compliance, and data privacy](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy?context=%2Fazure%2Fcognitive-services%2Fopenai%2Fcontext%2Fcontext).*\n\nUsing AI SDK for SAP ABAP v 1.0 and integrating Azure Open AI with SAP ABAP/SAP business processes, developers can create innovative applications that can automate manual tasks, make smarter business decisions, and provide more personalized customer experiences.\n\n\n## Contributing\n\nThis project is currently in active development. At this time, suggestions and feature requests are most welcome. Please use the discussion forum for exchanging ideas and brainstorming with the community.\n\nIn future, this project will be open to contributions.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aisdkforsapabap", "org_name": "microsoft", "org_repo": "microsoft/aisdkforsapabap", "platform_org_repo": "github+microsoft/aisdkforsapabap", "link_to_repo": "https://github.com/microsoft/aisdkforsapabap", "platform": "github", "language": "ABAP", "stargazers_count": 56, "watchers_count": 56}, {"README_text": "# Azure SDK for .NET\n\n[![Packages](https://img.shields.io/badge/packages-latest-blue.svg)](https://azure.github.io/azure-sdk/releases/latest/dotnet.html) [![Dependencies](https://img.shields.io/badge/dependency-report-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-net/dependencies/dependencies.html) [![Dependencies Graph](https://img.shields.io/badge/dependency-graph-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-net/dependencies/dependencyGraph/dagre.html)\n\nThis repository is for active development of the Azure SDK for .NET. For consumers of the SDK we recommend visiting our [public developer docs](https://docs.microsoft.com/dotnet/azure/) or our versioned [developer docs](https://azure.github.io/azure-sdk-for-net).\n\n## Getting started\n\nTo get started with a library, see the README.md file located in the library's project folder. You can find these library folders grouped by service in the /sdk directory.\n\nFor tutorials, samples, quick starts, and other documentation, go to [Azure for .NET Developers](https://docs.microsoft.com/dotnet/azure/).\n\n## Packages available\nEach service might have a number of libraries available from each of the following categories:\n* [Client - New Releases](#client-new-releases)\n* [Client - Previous Versions](#client-previous-versions)\n* [Management - New Releases](#management-new-releases)\n* [Management - Previous Versions](#management-previous-versions)\n\n### Client: New Releases\n\nNew wave of packages that we are announcing as **GA** and several that are currently releasing in **preview**. These libraries follow the [Azure SDK Design Guidelines for .NET](https://azure.github.io/azure-sdk/dotnet/guidelines/) and share a number of core features such as HTTP retries, logging, transport protocols, authentication protocols, etc., so that once you learn how to use these features in one client library, you will know how to use them in other client libraries. You can learn about these shared features at [Azure.Core](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/core/Azure.Core/README.md).\n\nThese new client libraries can be identified by the naming used for their folder, package, and namespace. Each will start with `Azure`, followed by the service category, and then the name of the service. For example `Azure.Storage.Blobs`. \n\nFor a complete list of available packages, please see the [latest available packages](https://azure.github.io/azure-sdk/releases/latest/dotnet.html) page.\n\n> NOTE: If you need to ensure your code is ready for production we strongly recommend using one of the stable, non-preview libraries.\n\n### Client: Previous Versions\n\nLast stable versions of packages that are production-ready. These libraries provide similar functionalities to the preview packages, as they allow you to use and consume existing resources and interact with them, for example: upload a storage blob. Stable library directories typically contain 'Microsoft.Azure' in their names, e.g. 'Microsoft.Azure.KeyVault'. They might not implement the [guidelines](https://azure.github.io/azure-sdk/dotnet_introduction.html) or have the same feature set as the November releases. They do however offer wider coverage of services.\n\n### Management: New Releases\n\nA new set of management libraries that follow the [Azure SDK Design Guidelines for .NET](https://azure.github.io/azure-sdk/dotnet_introduction.html) and based on [Azure.Core libraries](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/core/Azure.Core) are now in Public Preview. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more. You can find the list of new packages [on this page](https://azure.github.io/azure-sdk/releases/latest/dotnet.html). \n\nTo get started with these new libraries, please see the [quickstart guide here](https://github.com/Azure/azure-sdk-for-net/blob/main/doc/mgmt_preview_quickstart.md). These new libraries can be identified by namespaces that start with `Azure.ResourceManager`, e.g. `Azure.ResourceManager.Network` \n\n> NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.\n\n### Management: Previous Versions\n\nFor a complete list of management libraries which enable you to provision and manage Azure resources, please check [here](https://azure.github.io/azure-sdk/releases/latest/all/dotnet.html). They might not have the same feature set as the new releases but they do offer wider coverage of services. Previous versions of management libraries can be identified by namespaces that start with `Microsoft.Azure.Management`, e.g. `Microsoft.Azure.Management.Network`\n\nDocumentation and code samples for these libraries can be found [here](https://azure.github.io/azure-sdk-for-net).\n\n## Need help?\n\n* For reference documentation visit the [Azure SDK for .NET API Reference](https://aka.ms/net-docs).\n* For tutorials, samples, quick starts, and other documentation, go to [Azure for .NET Developers](https://docs.microsoft.com/dotnet/azure/).\n* File an issue via [Github Issues](https://github.com/Azure/azure-sdk-for-net/issues/new/choose).\n* Check [previous questions](https://stackoverflow.com/questions/tagged/azure+.net) or ask new ones on StackOverflow using `azure` and `.net` tags.\n\n### Community\n\n* Chat with other community members [![Join the chat at https://gitter.im/azure/azure-sdk-for-net](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/azure/azure-sdk-for-net?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n### Reporting security issues and security bugs\n\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) <secure@microsoft.com>. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the [Security TechCenter](https://www.microsoft.com/msrc/faqs-report-an-issue).\n\n## We want your thoughts!\n\n### Feature Requests\n\nWhat features are important to you?  You can let us know by looking at our open [feature requests](https://github.com/Azure/azure-sdk-for-net/issues?q=is%3Aopen+label%3Afeature-request+sort%3Areactions-%2B1-desc) and sharing your thoughts by giving the issue a thumbs up or thumbs down.  (Note the list is sorted by number of thumbs up in descending order.)\n\n\n### Design Discussions\n\nWe would love to incorporate the community's input into our library design process. Here's a list of [design discussions](https://github.com/Azure/azure-sdk-for-net/labels/design-discussion) that we're currently having. Participate in the discussions by leaving your comments in the issue!\n\n## Contributing\nFor details on contributing to this repository, see the [contributing guide](https://github.com/Azure/azure-sdk-for-net/blob/main/CONTRIBUTING.md).\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n![Impressions](https://azure-sdk-impressions.azurewebsites.net/api/impressions/azure-sdk-for-net%2FREADME.png)\n", "repo_name": "azure-sdk-for-net-ace", "org_name": "microsoft", "org_repo": "microsoft/azure-sdk-for-net-ace", "platform_org_repo": "github+microsoft/azure-sdk-for-net-ace", "link_to_repo": "https://github.com/microsoft/azure-sdk-for-net-ace", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# GitHub Copilot Chat in Visual Studio Code\n\nThis repository is for providing feedback on the Copilot Chat UX in VS Code. You can use the repository to report issues or submit feature requests on the chat user experience and interface.\n\nGitHub Copilot Chat is a companion extension to GitHub Copilot that houses experimental chat features. To learn more about GitHub Copilot Chat, check out our [blog post](https://code.visualstudio.com/blogs/2023/03/30/vscode-copilot).\n\n# Providing Feedback\n\nYou can use this repository to file issues on the UX and UI for Copilot Chat in VS Code:\n\n* Up-vote a feature or request a new one.\n* Search for existing issues already reported for potential workarounds.\n* Report a problem if you don't find what you are looking for.\n\nIf you'd like to report an issue for autocomplete-style suggestions, please use the [GitHub Copilot Discussions](https://github.com/orgs/community/discussions/categories/copilot). If you'd like to report an issue with the AI model, please instead use the [VS Code Copilot survey](https://aka.ms/vscode-copilot-survey).\n", "repo_name": "vscode-copilot-release", "org_name": "microsoft", "org_repo": "microsoft/vscode-copilot-release", "platform_org_repo": "github+microsoft/vscode-copilot-release", "link_to_repo": "https://github.com/microsoft/vscode-copilot-release", "platform": "github", "language": null, "stargazers_count": 87, "watchers_count": 87}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "artifacts-credprovider-conda", "org_name": "microsoft", "org_repo": "microsoft/artifacts-credprovider-conda", "platform_org_repo": "github+microsoft/artifacts-credprovider-conda", "link_to_repo": "https://github.com/microsoft/artifacts-credprovider-conda", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project Verona Artifacts\n\nThis repo contains the artifacts associated with the research papers from Project Verona. Here is a summary:\n\n* [Wait free weak reference counting](./WFWeakRC/README.md) - The Verona Runtime uses strong and weak reference counts to manage the cowns lifetimes.  This directory contains the ISMM'23 paper that explains the algorithm to make this wait-free.  The classic algorithms used elsewhere are not wait-free.", "repo_name": "verona-artifacts", "org_name": "microsoft", "org_repo": "microsoft/verona-artifacts", "platform_org_repo": "github+microsoft/verona-artifacts", "link_to_repo": "https://github.com/microsoft/verona-artifacts", "platform": "github", "language": "C++", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "**Deprecated**\n\nPlease use the official way of requiring Electron: https://electronjs.org/docs/tutorial/first-app\n\n--\n\n# @vscode/gulp-electron\n\n### Installation\n\n```bash\nnpm install --save-dev @vscode/gulp-electron\n```\n\n### Usage\n\nYou can use this module in two distinct ways: **to package your application** and/or\n**to download a version of Electron** to disk.\n\n#### How to Package Your Application\n\nYou should source your app's files using `gulp.src` and pipe them through\n`@vscode/gulp-electron`. The following task will create your application in\nthe `app` folder, ready for launch.\n\n```javascript\nvar gulp = require(\"gulp\");\nvar symdest = require(\"gulp-symdest\");\nvar electron = require(\"@vscode/gulp-electron\");\n\ngulp.task(\"default\", function () {\n  return gulp\n    .src(\"src/**\")\n    .pipe(electron({ version: \"0.34.1\", platform: \"darwin\" }))\n    .pipe(symdest(\"app\"));\n});\n```\n\n**Note:** It is important to use `gulp-symdest` only because of the OS X\nplatform. An application bundle has symlinks within and if you use `gulp.dest`\nto pipe the built app to disk, those will be missing. `symdest` will make\nsure symlinks are taken into account.\n\nFinally, you can always pipe it to a **zip archive** for easy distribution.\n[joaomoreno/gulp-vinyl-zip](https://github.com/joaomoreno/gulp-vinyl-zip) is recommended:\n\n```javascript\nvar gulp = require(\"gulp\");\nvar zip = require(\"gulp-vinyl-zip\");\nvar electron = require(\"@vscode/gulp-electron\");\n\ngulp.task(\"default\", function () {\n  return gulp\n    .src(\"src/**\")\n    .pipe(electron({ version: \"0.34.1\", platform: \"darwin\" }))\n    .pipe(zip.dest(\"app-darwin.zip\"));\n});\n```\n\n#### How to Download Electron\n\nThere's also a very handy export `electron.dest()` function that\nmakes sure you always have the exact version of Electron in a directory:\n\n```javascript\nvar gulp = require(\"gulp\");\nvar electron = require(\"@vscode/gulp-electron\");\n\ngulp.task(\"default\", function () {\n  return electron.dest(\"electron-build\", {\n    version: \"0.34.1\",\n    platform: \"darwin\",\n  });\n});\n```\n\nThis will place a vanilla Electron build into the `electron-build` directory.\nIf you run it consecutively and it detects that the version in the destination directory\nis the intended one, it will end up in a no-op. Else it will download the provided version\nand replace it.\n\n### Options\n\nYou **must** provide the following options:\n\n- `version` - the [Electron version](https://github.com/atom/electron/releases) to use\n- `platform` - kind of OS (`darwin`, `linux`, `win32`)\n\nThe following options are **optional**:\n\n- `quiet` - suppress a progress bar when downloading\n- `token` - GitHub access token(to avoid request limit. You can grab it [here](https://github.com/settings/tokens))\n\n- `arch` - the processor architecture (`ia32`, `x64`)\n\n- **Windows**\n\n  - `winIcon` - path to an `.ico` file\n  - `companyName` - company name\n  - `copyright` - copyright statement\n\n- **Darwin**\n\n  - `darwinIcon` - path to an `.icns` file\n  - `darwinHelpBookFolder` - the `CFBundleHelpBookFolder` value\n  - `darwinHelpBookName` - the `CFBundleHelpBookName` value\n  - `darwinBundleDocumentTypes` - ([reference](https://developer.apple.com/library/ios/documentation/filemanagement/conceptual/documentinteraction_topicsforios/Articles/RegisteringtheFileTypesYourAppSupports.html)) array of dictionaries, each containing the following structure:\n    - `name` - the `CFBundleTypeName` value\n    - `role` - the `CFBundleTypeRole` value\n    - `ostypes` - the `CFBundleTypeOSTypes` value, a `string` array\n    - `utis` - the `LSItemContentTypes` value, a `string` array\n    - `extensions` - the `CFBundleTypeExtensions` value, a `string` array of file extensions\n    - `iconFile` - the `CFBundleTypeIconFile` value\n  - `darwinForceDarkModeSupport` - Forces Mojave dark mode support to be enabled for older Electron versions\n\n- **Linux**\n  - `linuxExecutableName` - overwrite the name of the executable in Linux\n", "repo_name": "vscode-gulp-electron", "org_name": "microsoft", "org_repo": "microsoft/vscode-gulp-electron", "platform_org_repo": "github+microsoft/vscode-gulp-electron", "link_to_repo": "https://github.com/microsoft/vscode-gulp-electron", "platform": "github", "language": "JavaScript", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "<p align=\"center\" >\n<img src=\"build/public/images/fabric-notes-header.png\" />\n\n<span>A collection of simple drawings illustrating the main concepts of <a href=\"https://fabric.microsoft.com\">Microsoft Fabric</a> to empower anyone to build stuff on Fabric.</span>\n![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/fabricnotes?style=social)\n\n## **[Browse all the notes here](https://microsoft.github.io/fabricnotes)**\n\n</p>\n\n\n<br />\n<br />\n<br />\n<hr/>\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\n**Please open an issue before starting to work on a note**. We are creating a set of templates to \nhelp you create notes with the same \"look & feel\". \n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "fabricnotes", "org_name": "microsoft", "org_repo": "microsoft/fabricnotes", "platform_org_repo": "github+microsoft/fabricnotes", "link_to_repo": "https://github.com/microsoft/fabricnotes", "platform": "github", "language": "C#", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fr-dai-team-portal", "org_name": "microsoft", "org_repo": "microsoft/fr-dai-team-portal", "platform_org_repo": "github+microsoft/fr-dai-team-portal", "link_to_repo": "https://github.com/microsoft/fr-dai-team-portal", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "blockchain", "org_name": "microsoft", "org_repo": "microsoft/blockchain", "platform_org_repo": "github+microsoft/blockchain", "link_to_repo": "https://github.com/microsoft/blockchain", "platform": "github", "language": null, "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# PSRule samples\n\nThis repository contains community samples for PSRule.\nTo learn more about PSRule, visit <https://aka.ms/ps-rule>.\n\n[![Open in vscode.dev](https://img.shields.io/badge/Open%20in-vscode.dev-blue)][1]\n\n  [1]: https://vscode.dev/github/microsoft/PSRule-samples\n\n## Browsing samples\n\nYou can find samples in `samples` directory broken down by topic.\nFor example `samples/azure`.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.\nIf you would like to a contribute a sample to this repository please [log an issue][2] to discuss the sample.\n\n  [2]: https://github.com/microsoft/PSRule-samples/issues/new/choose\n\n## Support\n\nThis project uses GitHub Issues to track bugs and feature requests.\nPlease search the existing issues before filing new issues to avoid duplicates.\n\n- For new issues, file your bug or feature request as a new [issue][3].\n- For help, discussion, and support questions about using this project, join or start a [discussion][4].\n\nSupport for this project/ product is limited to the resources listed above.\n\n  [3]: https://github.com/microsoft/PSRule-samples/issues\n  [4]: https://github.com/microsoft/PSRule-samples/discussions\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Maintainers\n\n- [Bernie White](https://github.com/BernieWhite)\n- [Sam Bell](https://github.com/ms-sambell)\n\n## License\n\nThis project is [licensed under the MIT License](LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services.\nAuthorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PSRule-samples", "org_name": "microsoft", "org_repo": "microsoft/PSRule-samples", "platform_org_repo": "github+microsoft/PSRule-samples", "link_to_repo": "https://github.com/microsoft/PSRule-samples", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "<!--\n{% comment %}\nCopyright (c) Microsoft Corporation.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n{% endcomment %}\n-->\n\n# LST-Bench\n\n[![CI Status](https://github.com/microsoft/lst-bench/workflows/Java%20CI/badge.svg?branch=main)](https://github.com/microsoft/lst-bench/actions?query=branch%3Amain)\n\nLST-Bench is a framework that allows users to run benchmarks specifically designed for evaluating the performance, efficiency, and stability of Log-Structured Tables (LSTs), also commonly referred to as table formats, such as [Delta Lake](https://delta.io/), [Apache Hudi](http://hudi.apache.org), and [Apache Iceberg](http://iceberg.apache.org).\n\n## Usage Guide\n\n### How to Build\n\n#### Prerequisites\n\nInstall open-source Java Development Kit. As a recommendation, install OpenJDK distribution from [Adoptium]('https://adoptium.net/en-GB/').\n\n#### Build\n\nTo build LST-Bench in Linux/macOS, run the following command:\n\n```bash\n./mvnw package\n```\n\nOr use the following command for Windows:\n\n```bat\nmvnw.cmd package\n```\n\nTo build LST-Bench for a specific database, you can use the profile name (`-P`) option. \nThis will include the corresponding JDBC driver in the `./target` directory. \nCurrently, the following profiles are supported: `spark-oss`, `spark-databricks`, and `trino-oss`.\nFor example, to build LST-Bench for open-source Spark in Linux/macOS, you can run the following command:\n\n```bash\n./mvnw package -Pspark-oss\n```\n\nOr use the following command for Windows:\n\n```bat\nmvnw.cmd package -Pspark-oss\n```\n\n### How to Run\n\nAfter building LST-Bench, if you are on Linux/macOS run `launcher.sh` or open a Powershell `launcher.ps1` if you are on Windows to display the usage options.\n\n```bash\nusage: ./launcher.sh -c <arg> -e <arg> -l <arg> -t <arg> -w <arg>\n -c,--connections-config <arg>   [required] Path to input file containing\n                                 connections config details\n -e,--experiment-config <arg>    [required] Path to input file containing\n                                 the experiment config details\n -l,--task-library <arg>         [required] Path to input file containing\n                                 the library with task templates\n -t,--input-log-config <arg>     [required] Path to input file containing\n                                 the telemetry gathering config details\n -w,--workload <arg>             [required] Path to input file containing\n                                 the workload definition\n```\n\n## Configuration Files\nThe configuration files used in LST-Bench are YAML files. \n\nYou can find their schema, which describes the expected structure and properties, [here](src/main/resources/schemas).\n\nAdditionally, you can find sample configurations that can serve as guidelines for creating your configurations [here](src/main/resources/config).\n\n## Architecture\n\nThe LST-Bench code is organized into two modules:\n\n1. **Java Application.** This module is written entirely in Java and is responsible for executing SQL workloads against a system under test using JDBC.\n   It reads input configuration files to determine the tasks, sessions, and phases to be executed.\n   The Java application handles the execution of SQL statements and manages the interaction with the system under test.\n\n2. **Python Processing Module.** The processing module is written in Python and serves as the post-execution analysis component.\n   It consolidates experimental results obtained from the Java application and computes metrics to provide insights into LSTs and cloud data warehouses.\n   The Python module performs data processing, analysis, and visualization to facilitate a deeper understanding of the experimental results.\n\n### LST-Bench Concepts\nIn LST-Bench, the following concepts are used to define and organize SQL workloads:\n\n- **Task**: A task is a collection of SQL statements grouped together in a sequence of files. Each file represents a step or subtask within the overall task.\n\n- **Session**: A session refers to a sequence of tasks. It represents a logical unit of work or a user session.\n\n- **Phase**: A phase consists of multiple concurrent sessions that need to be completed before proceeding to the next phase. Phases help simulate concurrent workload scenarios.\n\n- **Workload**: A workload is a sequence of phases, defining the complete set of tasks, sessions, and phases to be executed during the evaluation.\n\nIn LST-Bench, tasks are generated using task templates predefined in the task library.\nLST-Bench includes a default task library that encompasses tasks derived from the TPC-DS benchmark, along with workload definitions representing the original TPC-DS and multiple workload patterns. These resources can be located [here](src/main/resources/config/tpcds).\n\nAlthough LST-Bench provides this set of tasks and workload patterns,\nusers have the flexibility to incorporate additional task templates or even create a completely new task library to model specific scenarios.\nThis flexible model allows for the easy creation of diverse SQL workloads for evaluation purposes without the need to modify the application itself.\n\n### Telemetry and Metrics Processor\nLST-Bench captures execution telemetry during workload execution at multiple levels, including per experiment, phase, session, task, file, and statement.\nEach telemetry event is recorded with an associated identifier, such as the statement's name or the phase IDs defined in the workload YAML.\nThe event includes information on whether it succeeded or not, along with any additional associated data.\nSpecifically, each event includes a _start time_, _end time_, _event ID_, _event type_, _status_, and optional _payload_.\n\nThe telemetry registry in LST-Bench is configurable, providing flexibility for different systems and use cases.\nBy default, LST-Bench includes an implementation for a JDBC-based registry and supports writing telemetry to DuckDB or Spark.\nLST-Bench writes these telemetry events into a table within the specified systems, enabling any application to consume and gain insights from the results.\n\nAlternatively, if the LST-Bench [Metrics Processor](metrics) is used, you can simply point it to the same database.\nThe processor will then analyze and visualize the results, providing a streamlined solution for result analysis and visualization.\n\n## Documentation\nFor more details about LST-Bench, please refer to the accompanying [technical report](https://arxiv.org/pdf/2305.01120):\n\n```bibtex\n@article{2023lstbench,\n    title={LST-Bench: Benchmarking Log-Structured Tables in the Cloud},\n    author={Jes\u00fas Camacho-Rodr\u00edguez and Ashvin Agrawal and Anja Gruenheid and\n            Ashit Gosalia and Cristian Petculescu and Josep Aguilar-Saborit and\n            Avrilia Floratou and Carlo Curino and Raghu Ramakrishnan},\n    year={2023},\n    journal={arXiv preprint arXiv:2305.01120},\n    url={https://arxiv.org/abs/2305.01120},\n}\n```\n\n## Contributing\n\nHere are some ways you can contribute to the LST-Bench project:\n\n* Submit PRs to fix bugs or add new features.\n* Review currently [open PRs](https://github.com/microsoft/lst-bench/pulls).\n* Provide feedback and report bugs related to the software or the documentation.\n* Enhance our design documents, examples, tutorials, and overall documentation.\n\nTo get started, please take a look at the [issues](https://github.com/microsoft/lst-bench/issues) and leave a comment if any of them interest you.\n\nIf you plan to make significant changes, we recommend [discussing](https://github.com/microsoft/lst-bench/discussions) them with the LST-Bench community first.\nThis helps ensure that your contributions align with the project's goals and avoids duplicating efforts.\n\n## Contributor License Agreement\n\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nSee the [LICENSE](LICENSE) file for more details.\n", "repo_name": "lst-bench", "org_name": "microsoft", "org_repo": "microsoft/lst-bench", "platform_org_repo": "github+microsoft/lst-bench", "link_to_repo": "https://github.com/microsoft/lst-bench", "platform": "github", "language": "Java", "stargazers_count": 14, "watchers_count": 14}, {"README_text": "# gpt-review\n\n<p align=\"center\">\n<a href=\"https://github.com/microsoft/gpt-review/actions\"><img alt=\"Actions Status\" src=\"https://github.com/microsoft/gpt-review/workflows/Python%20CI/badge.svg\"></a>\n<a href=\"https://codecov.io/gh/microsoft/gpt-review\"><img alt=\"Coverage Status\" src=\"https://codecov.io/gh/microsoft/gpt-review/branch/main/graph/badge.svg\"></a>\n<a href=\"https://github.com/microsoft/gpt-review/blob/main/LICENSE\"><img alt=\"License: MIT\" src=\"https://black.readthedocs.io/en/stable/_static/license.svg\"></a>\n<a href=\"https://pypi.org/project/gpt-review/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/gpt-review\"></a>\n<a href=\"https://pepy.tech/project/gpt-review\"><img alt=\"Downloads\" src=\"https://pepy.tech/badge/gpt-review\"></a>\n<a href=\"https://github.com/psf/black\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a>\n</p>\n\nA Python based CLI and GitHub Action to use Open AI or Azure Open AI models to review contents of pull requests.\n\n## How to install CLI\n\nFirst, install the package via `pip`:\n\n```bash\npip install gpt-review\n```\n\n### GPT API credentials\n\nYou will need to provide an OpenAI API key to use this CLI tool. In order of precedence, it will check the following methods:\n\n1. Presence of a context file at `azure.yaml` or wherever `CONTEXT_FILE` points to. See `azure.yaml.template` for an example.\n\n2. `AZURE_OPENAI_API_URL` and `AZURE_OPENAI_API_KEY` to connect to an Azure OpenAI API:\n\n    ```bash\n    export AZURE_OPENAI_API=<your azure api url>\n    export AZURE_OPENAI_API_KEY=<your azure key>\n    ```\n\n3. `OPENAI_API_KEY` for direct use of the OpenAI API\n\n    ```bash\n    export OPENAI_API_KEY=<your openai key>\n    ```\n\n4. `AZURE_KEY_VAULT_URL` to use Azure Key Vault. Put secrets for the url at `azure-open-ai` and the API Key `azure-openai-key`, then run:\n\n    ```bash\n    export AZURE_KEY_VAULT_URL=https://<keyvault_name>.vault.azure.net/\n    az login\n    ```\n\n## Main Commands\n\nTo show help information about available commands and their usage, run:\n\n```bash\ngpt --help\n```\n\nTo display the current version of this CLI tool, run:\n\n```bash\ngpt --version\n```\n\nHere are the main commands for using this CLI tool:\n\n### 1. Ask a Question\n\nTo submit a question to GPT and receive an answer, use the following format:\n\n```bash\ngpt ask \"What is the capital of France?\"\n```\n\nYou can customize your request using various options like maximum tokens (`--max-tokens`), temperature (`--temperature`), top-p value (`--top-p`), frequency penalty (`--frequency-penalty`), presence penalty (`--presence-penalty`), etc.\n\n#### Ask a Question about a File\n\nTo submit a question to GPT with a file and receive an answer, use the following format:\n\n```bash\ngpt ask --files WordDocument.docx \"Summarize the contents of this document.\"\n```\n\n### 2. Review a PR\n\nTo review a PR, use the following format:\n\n```bash\ngpt github review \\\n    --access-token $GITHUB_ACCESS_TOKEN \\\n    --pull-request $PULL_REQUEST_NUMBER \\\n    --repository $REPOSITORY_NAME\n```\n\n### 3. Generate a git commit message with GPT\n\nTo generate a git commit message with GPT after having added the files, use the following format:\n\n```bash\ngit add .\n\ngpt git commit\n```\n\nFor more detailed information on each command and its options, run:\n\n```bash\ngpt COMMAND --help\n```\n\nReplace COMMAND with one of the main commands listed above (e.g., 'ask').\n\n## Developer Setup\n\nTo install the package in development mode, with additional packages for testing, run the following command:\n\n```bash\npip install -e .[test]\n```\n", "repo_name": "gpt-review", "org_name": "microsoft", "org_repo": "microsoft/gpt-review", "platform_org_repo": "github+microsoft/gpt-review", "link_to_repo": "https://github.com/microsoft/gpt-review", "platform": "github", "language": "Python", "stargazers_count": 68, "watchers_count": 68}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fabric-samples", "org_name": "microsoft", "org_repo": "microsoft/fabric-samples", "platform_org_repo": "github+microsoft/fabric-samples", "link_to_repo": "https://github.com/microsoft/fabric-samples", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 18, "watchers_count": 18}, {"README_text": "# Nimble: Rollback Protection for Confidential Cloud Services \n\nNimble is a service that helps applications running in trusted execution environments (TEEs) detect \nrollback attacks (i.e., detect whether a data item retrieved from persistent storage is the latest version).\n\nNimble can also be used as a generic tamper-proof fault-tolerant append-only ledger.\n\nNimble will appear at [OSDI 2023](https://www.usenix.org/conference/osdi23).\n\n\nTo reproduce the results in our paper, please follow the instructions below\nto build Nimble and then see [experiments/](experiments/).\n\n## Dependencies\n\nInstall make, gcc, protobuf-copiler, perl, libssl-dev, pkg-config. In Ubuntu, you can type:\n\n```text\nsudo apt install make gcc libssl-dev pkg-config perl protobuf-compiler\n```\n\n## Building and running tests\n\nInstall [`rustup`](https://rustup.rs/)\n\nClone the repository:\n\n```text\ngit clone https://github.com/Microsoft/Nimble\n```\n\nTo run tests:\n\n```text\ncargo test\n```\n\nTo build:\n\n```text\ncargo build --release\n```\n\nOptional: to build the Nimble endorser that runs in Intel SGX with open enclave, please folow the instructions [here](endorser-openenclave/).\n\n\nRunning a toy local setup with 2 endorsers, coordinator, REST endpoint, and sample REST client.\nRun each on a different terminal (or in the background, or with detached screen).\n\n\n  ```bash\n    ./target/release/endorser -p 9090\n    ./target/release/endorser -p 9091 \n    ./target/release/coordinator -e \"http://localhost:9090,http://localhost:9091\" \n    ./target/release/endpoint_rest\n    ./target/release/light_client_rest\n  ```\n\n\n## Details of Nimble's Rust binaries\n\nBelow are the different Nimble binaries, and some of the basic\noptions. Each binary has many other options. You can see them by\nrunning the binary and with the `--help` flag.\n\n\n### Endorser\n\n```\n  ./target/release/endorser\n    -t HOSTNAME\n    -p PORT \n```\n\n### Coordinator\n\n```\n  ./target/release/coordinator\n    -h HOSTNAME\n    -p PORT\n    -e \"http://HOST_ENDORSER_1:PORT,http://HOST_ENDORSER_2:PORT,http://HOST_ENDORSER_3:PORT\" \n    -s \"memory\" # use \"table\" to use Azure table instead and provide the following\n    -a AZURE_STORAGE_ACCOUNT_NAME\n    -k AZURE_STORAGE_MASTER_KEY\n```\n\nBelow is a helper tool to interact with the coordinator. After you\nkill some endorsers, you can add new ones (reconfiguration) by running.\n\n```\n  ./target/release/coordinator_ctrl \n    -c \"http://HOST_COORDINATOR:PORT\" \n    -a \"http://HOST_NEW_ENDORSER_1:PORT;http://HOST_NEW_ENDORSER_2:PORT\"\n```\n\n### REST Endpoint\n\n```\n  ./target/release/endpoint_rest\n    -t HOST\n    -p PORT\n    -c \"http://HOST_COORDINATOR:PORT\"\n```\n\n\n### REST Client \n\n```\n  ./target/release/endpoint_rest\n    -e \"http://HOST_ENDPOINT:PORT\"\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Nimble", "org_name": "microsoft", "org_repo": "microsoft/Nimble", "platform_org_repo": "github+microsoft/Nimble", "link_to_repo": "https://github.com/microsoft/Nimble", "platform": "github", "language": "Rust", "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# @vscode/gulp-vinyl-zip\n\n[![Build Status](https://dev.azure.com/monacotools/Monaco/_apis/build/status%2Fnpm%2Fvscode%2Fgulp-vinyl-zip?branchName=main)](https://dev.azure.com/monacotools/Monaco/_build/latest?definitionId=488&branchName=main)\n\nA library for creating and extracting ZIP archives from/to streams.\n\nUses [yazl](https://github.com/thejoshwolfe/yazl)\nand [yauzl](https://github.com/thejoshwolfe/yauzl).\n\n## Usage\n\n**Archive \u2192 Archive**\n\n```javascript\nvar gulp = require('gulp');\nvar zip = require('@vscode/gulp-vinyl-zip');\n\ngulp.task('default', function () {\n\treturn zip.src('src.zip')\n\t\t.pipe(/* knock yourself out */)\n\t\t.pipe(zip.dest('out.zip'));\n});\n```\n\nor\n\n```javascript\nvar gulp = require('gulp');\nvar zip = require('@vscode/gulp-vinyl-zip');\n\ngulp.task('default', function () {\n\treturn gulp.src('src.zip')\n\t\t.pipe(zip.src())\n\t\t.pipe(/* knock yourself out */)\n\t\t.pipe(zip.dest('out.zip'));\n});\n```\n\n**Archive \u2192 File System**\n\n```javascript\nvar gulp = require('gulp');\nvar zip = require('@vscode/gulp-vinyl-zip');\n\ngulp.task('default', function () {\n\treturn zip.src('src.zip')\n\t\t.pipe(/* knock yourself out */)\n\t\t.pipe(gulp.dest('out'));\n});\n```\n\n**File System \u2192 Archive**\n\n```javascript\nvar gulp = require('gulp');\nvar zip = require('@vscode/gulp-vinyl-zip');\n\ngulp.task('default', function () {\n\treturn gulp.src('src/**/*')\n\t\t.pipe(/* knock yourself out */)\n\t\t.pipe(zip.dest('out.zip'));\n});\n```\n\n**File System \u2192 Archive Stream \u2192 Disk**\n\n```javascript\nvar gulp = require('gulp');\nvar zip = require('@vscode/gulp-vinyl-zip').zip; // zip transform only\n\ngulp.task('default', function () {\n\treturn gulp.src('src/**/*')\n\t\t.pipe(/* knock yourself out */)\n\t\t.pipe(zip('out.zip'))\n\t\t.pipe(/* knock your zip out */)\n\t\t.pipe(gulp.dest('./'));\n});\n```\n", "repo_name": "gulp-vinyl-zip", "org_name": "microsoft", "org_repo": "microsoft/gulp-vinyl-zip", "platform_org_repo": "github+microsoft/gulp-vinyl-zip", "link_to_repo": "https://github.com/microsoft/gulp-vinyl-zip", "platform": "github", "language": "JavaScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "---\npage_type: sample\nlanguages:\n- C#\nproducts:\n- azure-iot-hub\n- azure-iot-hub-device-provisioning-service\n- azure-service-bus \n- azure-data-lake-storage\n- azure-logic-apps\n- azure-functions-app\n---\n\n![EAE Banner](./Anomaly_Detection/Docs/Media/SA-EAE-Banner.png)\n\n# Intelligent Field Services (IFS) Solution Accelerator \n\n## Project Description\n\nField services involve tasks that are performed at the customer's location, such as installations, repairs, and maintenance. The industry has been rapidly evolving to increase efficiency and customer satisfactions by task automation. Now field service providers can bring productivity and customer experience to a new level by providing more advanced services with digitalization and cloud services such as Azure IoT Hub, Azure Data and AI services, and Azure Digital Twins. \n\nThere are published open source projects that provide ready to deploy solutions. For example, the [Dynamics 365 Connected Field Service - Azure IoT Deployment Template](https://github.com/microsoft/Dynamics-365-Connected-Field-Service-Deployment) is a solution that uses IoT sensors and data analysis to monitor and service customers' equipment, allowing companies to detect issues remotely and dispatch technicians efficiently. It integrates with Dynamics 365 to provide a comprehensive view of customer data, service history, and technician availability. For more details, please refer to the [CFS for IoT Hub architecture and workflow description](./Anomaly_Detection/Docs/Connected_Field_Service/README.md).\n\nThe Intelligent Field Service (IFS) solution accelerator builds additional AI capabilities on top of the Dynamics 365 Connected Field Service for IoT Hub. We provide multivariate anomaly detection solution that leverages [Azure Cognitive Services](https://learn.microsoft.com/en-us/azure/cognitive-services/what-are-cognitive-services) and more AI driven capabilities are planned to be released in the coming months.\n\n## IFS Anomaly Detection Solution Overview \n\nIFS Anomaly Detection solution detects unusual patterns or anomalies in IoT telemetry data in real time. It collects IoT telemetry data and invokes API Services provided by the [Anomaly Detector](https://learn.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/overview). The Anomaly Detector uses machine learning algorithms to analyze time-series data, such as IoT telemetry, logs, and business metrics, and identify anomalies based on historical patterns and trends. Anomaly Detector also provides a confidence score to help assess the severity of the anomaly, enabling field service providers to take appropriate actions. \n\nTo test the anomaly detection solution, we created a machine learning model using [sample IoT sensor data](./Anomaly_Detection/Deployment/Data/sensordata.csv). The machine learning model ID is plugged into an Azure Functions App called `Invoker` that interacts with the Anomaly Detector. The model can be updated with newer data when necessary. When an anomaly is detected, the `Invoker` sends message to Dynamics 365 Field service with confidence score, severity, along with actual IoT telemetry data so that appropriate action can be taken based on this message. How the action is taken can be programmed in Dynamics 365. \n\nBelow architecture diagram illustrates the main components and information flow of this solution accelerator. The components for Anomaly Detection solution is noted in the diagram. For a description of the workflow, please refer to [anomaly detection solution architecture and workflow description](./Anomaly_Detection/Docs/README.md). \n\n![SA-Architecture-Anomaly-Detection](./Anomaly_Detection/Docs/Media/SA-Architecture-Anomaly-Detection-Stand-Out.png)\n\n## Deploy and Set up the Solution Accelerator\n\nPlease refer to the [Deployment Guide](./Anomaly_Detection/Deployment/README.md) for detailed instructions on how to deploy and set up the solution. \n\n# Planned Future Releases\n\nAdditional IFS solution accelerators are under construction and planned to be published in the coming months:\n\n1. Predictive Maintenance Powered by AI\n2. Digital Twins Enablement\n3. Insights and Analytics\n\n## License\nMIT License\n\nCopyright (c) Microsoft Corporation.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE\n\n## Contributing\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Data Collection\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n", "repo_name": "Intelligent-Field-Service-Solution-Accelerator", "org_name": "microsoft", "org_repo": "microsoft/Intelligent-Field-Service-Solution-Accelerator", "platform_org_repo": "github+microsoft/Intelligent-Field-Service-Solution-Accelerator", "link_to_repo": "https://github.com/microsoft/Intelligent-Field-Service-Solution-Accelerator", "platform": "github", "language": "C#", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Benchmarking Spatial Relationships in Text-to-Image Generation\n*Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang*\n\n<!-- ![](assets/motivating_example_4.png \"\") -->\n<p align=center>\n  <img src=\"assets/visor_example_detailed_new.png\" height=400px/>\n</p>\n\n- We introduce a large-scale challenge dataset SR<sub>2D</sub> that contains sentences describing two objects and the spatial relationship between them.\n- We introduce a metric called VISOR (short for **V**erify**I**ng **S**patial **O**bject **R**elationships) to quantify spatial reasoning performance.  \n- VISOR and SR<sub>2D</sub> can be used off-the-shelf with any text-to-image model.\n\n## SR<sub>2D</sub> Dataset\nOur dataset is hosted as a Huggingface Dataset [here](https://huggingface.co/datasets/tgokhale/sr2d_visor).  This contains \n1. The text prompt dataset in `.json` format (`text_spatial_rel_phrases.json`)\n2. Images generated using 7 models (GLIDE, CogView2, DALLE-mini, Stable Diffusion, GLIDE + Stable Diffusion + CDM, and Stable Diffusion v2.1) \n\nAlternatively, the text prompt dataset can also accessed from [`text_spatial_rel_phrases.json`](https://github.com/microsoft/VISOR/blob/main/text_spatial_rel_phrases.json). It contains all examples from the current version of the dataset (31680 text prompts) accompanied by the corresponding metadata.\nThis dataset can also be generated by running the script `python create_spatial_phrases.py`\n\n\n## Generating Images\nIn our experiments, we generated images for each text prompt in SR<sub>2d</sub> for 5 models: *[GLIDE](https://arxiv.org/abs/2112.10741), [DALLE-mini](https://github.com/borisdayma/dalle-mini), [CogView2](https://arxiv.org/abs/2204.14217), [DALLE-v2](https://cdn.openai.com/papers/dall-e-2.pdf), [Stable Diffusion](https://arxiv.org/abs/2112.10752) (SD)* as well as [Composable Diffusion Models](https://arxiv.org/abs/2206.01714) (CDM) with GLIDE (GLIDE + CDM) and SD (SD + CDM). See acknowledgments below for the corresponding codebases / APIs.\n\nIn this repository, we provide a few output images in the `images` directory, as well as a [webpage](https://visort2i.github.io/) or `viz.html` for visualizing them.\n\n## Scripts for Evaluation\n### Formatting for Object Detection results\n\nIn `objdet_results` we provide `.json` files that contain object detection results using OWL-ViT for images generated by eight state-of-the-art diffusion models (GLIDE, GLIDE+CDM, DALLE-mini, CogView2, DALLE-v2, Stable Diffusion, SD+CDM, and SD-2.1).\nThe format of these results file is simple -- each file contains a dictionary with keys being the filenames of images and values being the object detection results for that image.  These object detection results are themselves dictionaries, which contain a list of `classes`, corresponding `centroids` of bounding boxes, `recall` (number of detected objects), `sra` (`recall` divided by total number of expected objects) and the `text` prompt.\n\nAn example is shown below:\n```\n{\n\"1103_0\": {\n    \"classes\": [\n      \"kite\"\n    ],\n    \"centroid\": [\n      [\n        170.64540100097656,\n        135.6916046142578\n      ]\n    ],\n    \"recall\": 1,\n    \"sra\": 0,\n    \"text\": \"a kite to the right of a bicycle\"\n  },\n  \"11036_1\": {\n    \"classes\": [\n      \"cat\"\n    ],\n    \"centroid\": [\n      [\n        170.22688674926758,\n        108.26640701293945\n      ]\n    ],\n    \"recall\": 1,\n    \"sra\": 0,\n    \"text\": \"a cat below a bottle\"\n  }\n}\n```\nIf you want to use your own results, please convert them to the above format and name the file `results_<model_name>_<dataset_file_name>_<object_detector_name>_<threshold>.json`.\nFor example: `results_dallev2_text_spatial_rel_phrases_owlvit_0.1.json`\n\n### Running VISOR\nOnce the object detection results are setup as described above, run the visor evaluation code:\n```\npython visor.py --text_json <path_to_SR2D_data> \n```\nWe also provide the functionality of running the evaluation for specific models as comma-separated arguments:\n```\npython visor.py --text_json <path_to_SR2D_data> --mo glide,dallev2,stable-diffusion\n```\n\n## Notebook for Evaluation\nThe notebook `text2image_realtime_evaluate.ipynb` offers a walk-through for using VISOR to evaluate the generated images and obtaining the VISOR scores for each model.\nThis notebook also includes the pipeline for using OWL-ViT object detection.\n\n## Results from the Paper\nBenchmarking results are shown below:\n\n| Model  | OA | VISOR_uncond | VISOR_cond | VISOR_1 | VISOR_2 | VISOR_3 | VISOR_4 |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| GLIDE       |  3.36 |  1.98 | 59.06 |  6.72 |  1.02 |  0.17 | 0.03 |\n| GLIDE + CDM | 10.17 |  6.43 | 63.21 | 20.07 |  4.69 |  0.83 | 0.11 |\n| DALLE-mini  | 27.10 | 16.17 | 59.67 | 38.31 | 17.50 |  6.89 | 1.96 |\n| CogView2    | 18.47 | 12.17 | 65.89 | 33.47 | 11.43 |  3.22 | 0.57 |\n| DALLE-v2    | 63.93 | 37.89 | 59.27 | 73.59 | 47.23 | 23.26 | 7.49 |\n| SD          | 29.86 | 18.81 | 62.98 | 46.60 | 20.11 |  6.89 | 1.63 |\n| SD + CDM    | 23.27 | 14.99 | 64.41 | 39.44 | 14.56 |  4.84 | 1.12 |\n| SD 2.1      | 47.83 | 30.25 | 63.24 | 64.42 | 35.74 | 16.13 | 4.70 |\n\n## References\nCode for text-to-image generation:\n1. GLIDE: https://github.com/openai/glide-text2im\n2. DALLE-mini: https://github.com/borisdayma/dalle-mini\n3. CogView2: https://github.com/THUDM/CogView2\n4. Stable Diffusion: https://github.com/CompVis/stable-diffusion\n5. Composable Diffusion Models: https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\n6. OpenAI API for DALLE-2: https://openai.com/api/\n\n## Citation\nIf you find SR<sub>2D</sub> or VISOR useful in your research, please use the following citation:\n```\n@article{gokhale2022benchmarking,\n  title={Benchmarking Spatial Relationships in Text-to-Image Generation},\n  author={Gokhale, Tejas and Palangi, Hamid and Nushi, Besmira and Vineet, Vibhav and Horvitz, Eric and Kamar, Ece and Baral, Chitta and Yang, Yezhou},\n  journal={arXiv preprint arXiv:2212.10015},\n  year={2022}\n}\n```\n", "repo_name": "VISOR", "org_name": "microsoft", "org_repo": "microsoft/VISOR", "platform_org_repo": "github+microsoft/VISOR", "link_to_repo": "https://github.com/microsoft/VISOR", "platform": "github", "language": "HTML", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "scopestudio-vscode", "org_name": "microsoft", "org_repo": "microsoft/scopestudio-vscode", "platform_org_repo": "github+microsoft/scopestudio-vscode", "link_to_repo": "https://github.com/microsoft/scopestudio-vscode", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "\n# Project\nThis is project is primarliy to track customer issues with any of the following ADS (Azure Data Studio) extensions\n\n1. Oracle extension \n2. Database Migration Assessment for Oracle (DMAO) \n3. Database Schema Conversion Toolkit (DSCT) \n4. Oracle Migrations\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "OracleToAzure", "org_name": "microsoft", "org_repo": "microsoft/OracleToAzure", "platform_org_repo": "github+microsoft/OracleToAzure", "link_to_repo": "https://github.com/microsoft/OracleToAzure", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Semantic Kernel Starters\n\nThis repository contains starter projects for the [Semantic Kernel](https://github.com/microsoft/semantic-kernel). Each starter is a self-contained application using a different programming language and application runtime.\n\n## Usage\n\n- `git clone https://github.com/semantic-kernel/semantic-kernel-starters`\n- `code <any-sample-folder>`\n- Follow the instructions in each sample's README for setting up and running the sample\nAlternatively, you can use the [Semantic Kernel Tools](https://marketplace.visualstudio.com/items?itemName=ms-semantic-kernel.semantic-kernel) to \"Create a New App\" using the starters\n\n## Getting Started\n\n- [C# Hello World](sk-csharp-hello-world): The Hello World C# console application starter for the Semantic Kernel.\n- [C# Azure Functions](sk-csharp-azure-functions): The Hello World C# Azure Functions starter for the Semantic Kernel.\n- [Python Hello World](sk-python-hello-world): The Hello World Python console application starter for the Semantic Kernel.\n- [Python Azure Functions](sk-python-azure-functions): The Hello World Python Azure Functions starter for the Semantic Kernel.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "semantic-kernel-starters", "org_name": "microsoft", "org_repo": "microsoft/semantic-kernel-starters", "platform_org_repo": "github+microsoft/semantic-kernel-starters", "link_to_repo": "https://github.com/microsoft/semantic-kernel-starters", "platform": "github", "language": "C#", "stargazers_count": 27, "watchers_count": 27}, {"README_text": "# HackTogether: The Microsoft Teams Global Hack \n\n![TeamsGlobalHack Banner Dark](https://github.com/microsoft/hack-together-teams/assets/45178151/1a8b3b43-f674-4b35-a737-5a7cd1cf2e0c)\n\n<p align=\"center\">\n  <a href=\"https://aka.ms/hack-together-teams/register\"><img src=\"https://img.shields.io/badge/HackTogether-Register-6264A7?style=for-the-badge&logoColor=white&logo=MicrosoftTeams\" alt=\"Hackathon survey\" border=\"0\" /></a>\n</p>\n\n## Introduction\n\n\ud83d\udee0\ufe0fBuild, innovate, and **#HackTogether**!\ud83d\udee0\ufe0f It's time to get started building apps for Microsoft Teams. \ud83d\udd25\n\nHackTogether is your playground for coding and experimenting with Microsoft Teams. With mentorship from Microsoft experts and access to the latest tech, you will learn how to build Teams apps based on the top Microsoft Teams app scenarios. The possibilities are endless for what you can create... plus you can submit your hack for a chance to win exciting prizes! \ud83e\udd73\n\nThe hackathon starts on **June 1st** and ends on **June 15th**. It is recommended for participants to follow the HackTogether Roadmap for a successful hackathon.\n\n### Tips & Tricks\n\n* Whenever you struggle during the hackathon, ask your questions on **[\u2753GitHub Discussions](https://github.com/microsoft/hack-together-teams/discussions/new?category=q-a)**. Microsoft experts will be there to help you.\n* Are your looking for a project idea? Check out **[\ud83d\udcc3 Top Teams App Scenarios](https://github.com/microsoft/hack-together-teams/blob/main/TOP-SCENARIOS.md)** to pick a project idea to get started, or get insipired for your own project idea! Also, there are many samples available in the **[\u2728 Project samples to get inspired](https://github.com/microsoft/hack-together-teams/blob/main/README.md#samples-to-get-inspired-)** section if you are looking for more inspiration while working on your hack to win a digital badge!\n* Looking for documentation and guidance? Check out **[\ud83d\udcda Recommended Learning Materials](https://github.com/microsoft/hack-together-teams/blob/main/README.md#recommended-learning-materials-)** below.\n* Looking for code templates to start with? Check out available **[\ud83e\udea1 Templates](https://github.com/microsoft/hack-together-teams/blob/main/README.md#templates-to-get-started-)** in the repository.\n* Submit your project until June 15 to win **[\ud83c\udfc6 Judging and Prizes](https://github.com/microsoft/hack-together-teams/blob/main/README.md#judging-and-prizes-)**.\n\n## HackTogether Roadmap \ud83d\uddfa\ufe0f\n\n![TeamsGlobalHack Roadmap](https://github.com/microsoft/hack-together-teams/assets/45178151/8a97e909-9783-45e6-8484-e786250a5c7a)\n\nFollow the steps below to successfully complete the hackathon.\n\n### #00 - Pre-requisites\n\nRegister to Hack Together:\n\n[![Register](https://img.shields.io/badge/HackTogether-Register-6264A7?style=for-the-badge&logoColor=white&logo=MicrosoftTeams )](https://aka.ms/hack-together-teams/register)\n\nBefore you start hacking, you will need an environment setup. Review **[Environment Setup](https://github.com/microsoft/hack-together-teams/blob/main/SETUP.md)** guidelines or the following prerequisites or click the related link of each prerequisite to be directed to the official documentation:\n\n* [Prepare your Office 365 tenant](/SETUP.md#1---prepare-your-microsoft-365-tenant)\n* [Install Visual Studio or Visual Studio Code (Recommended)](/SETUP.md#2---install-visual-studio-or-visual-studio-code-recommended)\n* [Install Teams Toolkit for Visual Studio or Visual Studio Code (Recommended)](/SETUP.md#3---install-teams-toolkit-for-visual-studio-or-visual-studio-code-recommended)\n* [Install the browser of your choice](/SETUP.md#4---install-the-browser-of-your-choice)\n* [Setup your GitHub account](/SETUP.md#5---setup-your-github-account)\n* For Teams AI Library - [Get started with Teams AI Library](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams%20conversational%20ai/how-conversation-ai-get-started)\n* For SharePoint - [SharePoint Framework Environment Setup](/SETUP.md#sharepoint-framework-environment-setup)\n\nIntroduce yourself, we'd like to get to know you! \ud83e\udd73 [GitHub Discussions | Let's get to know each other \ud83c\udf89](https://github.com/microsoft/hack-together-teams/discussions/1)\n\n### #01 - Start hacking on June 1st \n\nEither team up (max 4 team members) or fly solo to join the hacking! Make sure to register for HackTogether and complete the pre-requisites before starting to build your project.\n\n### #02 - Join live sessions for learning and inspiration\n\n[![Calendar Invite](https://img.shields.io/badge/ADD%20TO%20CALENDAR-4285F4?style=for-the-badge&label=&labelColor=555555&logoColor=white&logo=googlecalendar)](https://aka.ms/hack-together-teams/sessions)\n\n* **[June 1st - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together-teams/session1):** Ready, Set, Hack: Empower the developer in you with Microsoft Teams app development!\n* **[June 5th - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together-teams/session2):** Intro to Teams tabs and building them with SharePoint Framework: Maximize Microsoft 365 investments\n* **[June 6th - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together-teams/session3):** Intro to Teams bots: Integrating AI into your bot logic\n* **[June 7th - Watch On Demand \ud83c\udfa5](https://aka.ms/hack-together-teams/session4):** Boost user engagement beyond Teams with Message Extensions and make your app intelligent using Microsoft Graph.\n* **[June 12th - Join Live \ud83d\udcfa](https://aka.ms/hack-together-teams/session5):** Ask the experts: Meet our Engineering team and ask your pressing questions!\n* **[June 15th - Join Live \ud83d\udcfa](https://aka.ms/hack-together-teams/session6):** Wrap up and take action: Join our community for the next Big Thing!\n\n### #03 - Submit your project until June 15th 23:59 PM PST\n\nYou may submit your project here when it's ready: **[\ud83d\ude80 Project Submission](https://github.com/microsoft/hack-together-teams/issues/new?assignees=&labels=&template=project.yml&title=Project%3A+%3Cshort+description%3E)**\n\nThere will be a grand prize winner out of all projects as well as a 'Best AI-powered solution' and 'Best productivity-focused solution,' so it is worthwhile to align your hack to one of these categories.\n\nCheck out this video for step by step project submission guidance:\n[Project Submission Video](https://github.com/microsoft/hack-together-teams/assets/3199282/572ea387-61ec-4b77-9885-23b5b2bd39bd)\n\nWe'd love to hear about your project, tell us what you are working on! [GitHub Discussions | Tell us more about your project \ud83d\uddfa\ufe0f](https://github.com/microsoft/hack-together-teams/discussions/2)\n\n## Recommended Learning Materials \ud83d\udcda\n\n* [Learn Path - Build and deploy apps for Microsoft Teams using Teams Toolkit for Visual Studio Code](https://learn.microsoft.com/en-us/training/paths/m365-teams-toolkit-vsc/)\n* [Learn Module - Introduction to building apps for Microsoft Teams](https://learn.microsoft.com/en-us/training/modules/intro-microsoft-teams-apps/)\n* [Hands-on labs - New \"App Camp\" labs - Extend a web service to be a Teams message extension](https://aka.ms/app-camp-new)\n* [Hands-on labs and videos - \"App Camp\" labs - Extend a web site to be a Teams app](https://aka.ms/app-camp)\n* [Documentation - Teams app that fits](https://learn.microsoft.com/en-us/microsoftteams/platform/overview)\n* [Documentation - From ideas to Teams app](https://learn.microsoft.com/en-us/microsoftteams/platform/overview-story)\n* [Documentation - Teams AI library](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams%20conversational%20ai/teams-conversation-ai-overview)\n* [Documentation - Teams AI library Quick start guide](https://learn.microsoft.com/en-us/microsoftteams/platform/bots/how-to/teams%20conversational%20ai/conversation-ai-quick-start)\n* [Tutorial - Build your first app using Teams AI library](https://learn.microsoft.com/en-us/microsoftteams/platform/sbs-botbuilder-conversation-ai?tabs=vscode%2Cviscode)\n* [Documentation - Overview of the SharePoint Framework](https://learn.microsoft.com/en-us/sharepoint/dev/spfx/sharepoint-framework-overview)\n\n## Templates to get started \ud83e\udea1\nIf you are looking for a code template to start your project, we have the following templates available for you in this repository created using Teams Toolkit for Visual Studio(.NET) and Visual Studio Code(JavaScript/TypeScript).\n\n* [Visual Studio with .NET](https://github.com/microsoft/hack-together-teams/tree/main/templates/vs-dotnet)\n* [Visual Studio Code with JavaScript/TypeScript](https://github.com/microsoft/hack-together-teams/tree/main/templates/vscode-js-ts)\n\n## Samples to get inspired \u2728\nIf you are looking for scenarios or project ideas to get inspired, we have many samples available for you to explore! Check out below repositories to discover what you can build and get some inspiration for your hack!\n\n* [Microsoft - Teams Framework (TeamsFx) Sample Gallery](https://github.com/officedev/teamsfx-samples)\n* [Microsoft 365 & Power Platform Community - Teams Development Samples](https://pnp.github.io/teams-dev-samples/)\n* [Teams AI Library Samples](https://github.com/microsoft/teams-ai)\n\n## Judging and Prizes \ud83c\udfc6\n\nProjects will be evaluated by a panel of judges, including Microsoft executives, product managers, and developer advocates. Judging criteria will include innovation, impact, technical usability, and alignment with corresponding hackathon category.\n\nThe winners will receive the following exciting prizes (up to 4 individuals if submitting as a team, prizes for each person on the team). Winning solutions and/or product feedback may also be considered for integration into the Microsoft Teams product roadmap. Winners will also have the opportunity to showcase their projects to experts and leaders on Microsoft 365 Community Calls and on our YouTube channels.\n\n**\ud83c\udfc6 Grand prize winner:** _The best of the best! This category rewards the solution that meets all judging criteria, wows judges, and has potential real-world value for the 300M Teams users._\n* Xbox \ud83c\udfae\n* $300 digital gift card \ud83d\udcb8\n\n**\ud83e\udd47 Best AI-powered solution:** _This category rewards the solution that integrates AI or machine learning in an innovative way. For example, a chatbot that automates common tasks in Teams or an app that uses AI to suggest relevant files for users._ \n* $300 digital gift card \ud83d\udcb8\n\n**\ud83e\udd47 Best productivity-focused solution:** _This category rewards the solution that facilitates productivity and collaboration within Teams. For example, extending an existing app to Teams or creating a custom solution that helps team members stay organized during meetings._\n* $300 digital gift card \ud83d\udcb8\n\n**\ud83e\udd47 Community hack winner:** _This category rewards the solution chosen by the European Collaboration Summit (ECS) panel of judges._\n* C64 retro computer \ud83d\udda5\ufe0f\n\n**\ud83c\udfc5 All hackathon participants who submit an app will receive a digital badge.**\n\n>**\u2139\ufe0f Hack Together winners will be selected as per the following judging criteria:**\n>\n>1. Innovation\n>2. Impact\n>3. Technical Usability/User Experience\n>4. Alignment with hackathon category (either AI-powered or productivity-focused)\n\n## Join the community \ud83d\udc9c\n**Do you have an existing Microsoft Teams app and want help from Microsoft experts?** [Fill out this form for 1:1 assistance.](https://aka.ms/TeamsApp/Support)\n\n[Join the Microsoft 365 and Power Platform Community](https://pnp.github.io/) to find like-minded people, attend [community calls](https://pnp.github.io/#community), and explore [resources](https://pnp.github.io/#samples) to see what other apps you can build for Microsoft Teams!\n\n<img width=\"1307\" alt=\"banner-m365-community\" src=\"https://github.com/microsoft/hack-together-teams/assets/36196437/5e38dfeb-8a00-4f59-819e-d94b74b3069e\">\n\n*Special thanks to our partner **[European Collaboration Summit](https://collabsummit.eu)** for supporting HackTogether: The Microsoft Teams Global Hack.*\n\n![summit](https://github.com/microsoft/hack-together-teams/assets/36196437/b4b996b2-a093-4c44-8890-b8067107131d)\n\n## Feedback \u267b\ufe0f\nWe are excited to learn from you! Please share your experience and feedback with us: [Feedback Survey](https://aka.ms/hack-together-teams/survey).\n\n<p align=\"center\">\n  <a href=\"https://aka.ms/hack-together-teams/register\"><img src=\"https://img.shields.io/badge/HackTogether-Register-6264A7?style=for-the-badge&logoColor=white&logo=MicrosoftTeams\" alt=\"Hackathon survey\" border=\"0\" /></a>\n</p>\n", "repo_name": "hack-together-teams", "org_name": "microsoft", "org_repo": "microsoft/hack-together-teams", "platform_org_repo": "github+microsoft/hack-together-teams", "link_to_repo": "https://github.com/microsoft/hack-together-teams", "platform": "github", "language": "C#", "stargazers_count": 400, "watchers_count": 400}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "D365DataExportServicesReplacement", "org_name": "microsoft", "org_repo": "microsoft/D365DataExportServicesReplacement", "platform_org_repo": "github+microsoft/D365DataExportServicesReplacement", "link_to_repo": "https://github.com/microsoft/D365DataExportServicesReplacement", "platform": "github", "language": null, "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Data Factory Tutorials\n\nWelcome to our Data Factory repo where you will find some great resources on how to get started with using Data Factory in Microsoft Fabric!\n\nHere are some useful links as you get started:\n\n- Start by taking a look at the [Data Integration in an Hour tutorial](https://github.com/microsoft/DataFactory/tree/main/DI%20in%20an%20Hour) with the overview and prerequisites [here](https://github.com/microsoft/DataFactory/blob/main/DI%20in%20an%20Hour/0%20-%20Overview.md). In this tutorial you will learn how to get started with pipelines and dataflows in Data Factory.\n- Then take a look at the [Data Integration and Data Engineering Lab](https://github.com/microsoft/DataFactory/tree/main/DI%20and%20DE%20Lab), where you can learn how to build an end-to-end lakehouse architecture all the way from ingestion to reporting. You will learn how to ingest data into the lakehouse, use notebooks and Spark to transform your data, and then consumer the lakehouse data using SQL and Power BI.\n\n\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "DataFactory", "org_name": "microsoft", "org_repo": "microsoft/DataFactory", "platform_org_repo": "github+microsoft/DataFactory", "link_to_repo": "https://github.com/microsoft/DataFactory", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# `rego-cpp`\n\nThis project is an effort to create a C++ interpreter for the OPA policy language,\n[Rego](https://www.openpolicyagent.org/docs/latest/policy-language/). Our goal is\nto build both a standalone executable and a library such that programmers who are working\nin C++ can interpret Rego programs natively. To achieve this we are building our\ninterpereter on top of the experimental term rewriter\n[Trieste](https://github.com/microsoft/trieste).\n\n> **Warning**\n> This project is still in the early stages, and we do not currently support many\n> language features.\n\n## Getting Started\n\nFirst you will need to install the build dependencies in the manner appropriate to\nyour system:\n\n1. `cmake`\n2. `ninja` (optional)\n3. `clang++` (optional)\n4. `clang-format` (optional)\n\n> **Note**\n> 2-4 are entirely optional (i.e. the build does not require them) but will produce\n> the most reliable results. Our CI currently tests the following configurations:\n> 1. Windows latest with MSBuild\n> 2. Ubuntu latest with Ninja/Clang\n\nCreate a build directory and initialize the cmake project:\n\n    mkdir build\n    cd build\n    cmake .. -DCMAKE_INSTALL_PREFIX=dist\n\nYou can then build and run the tests using:\n\n    cmake --build . --config Debug --target INSTALL\n    ctest -C Debug\n\nIf you wish to use Ninja, then be sure to pass `-G Ninja` as well when configuring the project.\nThen:\n\n    ninja install\n    ctest -C Debug\n\nSimilarly, if you want to use clang you can indicate this by passing `-DCMAKE_CXX_COMPILER=clang++`\nduring configuration.\n\n\nThe interpreter will be located at `build/dist/bin/rego_interpreter`. Here are\nsome example commands using the provided example files:\n\n    ./bin/rego_interpreter -d examples/scalars.rego -q data.scalars.greeting\n    \"Hello\"\n\n    ./bin/rego_interpreter -d examples/objects.rego -q data.objects.sites[1].name\n    \"smoke1\"\n\n    ./bin/rego_interpreter -d examples/data0.json -d examples/data1.json -d examples/objects.rego -d examples/input.json  -q \"[data.one, input.b, data.objects.sites[data.objects.index]]\"\n    [{\"bar\": \"Foo\", \"baz\": 5, \"be\": true, \"bop\": 23.4}, undefined, {\"name\": \"smoke1\"}]\n\n    ./bin/rego_interpreter -q \"5 + (2 - 4 * 0.25) * -3 + 7.4\"\n    9.4\n\n    ./bin/rego_interpreter -d examples/bodies.rego -q data.bodies.e\n    {\"one\": 15, \"two\": 15}\n    \n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "rego-cpp", "org_name": "microsoft", "org_repo": "microsoft/rego-cpp", "platform_org_repo": "github+microsoft/rego-cpp", "link_to_repo": "https://github.com/microsoft/rego-cpp", "platform": "github", "language": "C++", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Experiments with self-verification using LLMS for clinical tasks.\n\nCode for paper \"Self-Verification Improves Few-Shot Clinical Information Extraction\" ([Gero et al. 2023](arxiv.org/abs/2306.00024)).\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/license-mit-blue.svg\">\n  <img src=\"https://img.shields.io/badge/python-3.6+-blue\">\n  <img src=\"https://img.shields.io/badge/pytorch-1.0+-blue\">\n  <img src=\"https://img.shields.io/pypi/v/imodelsx?color=green\">  \n</p>  \n\n\n<p align=\"center\">\n  <img src=\"https://microsoft.github.io/clinical-self-verification/figs/logo.png\" width=\"90%\">\n</p>\n\n> Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning, which requires costly human annotations. However, despite drastic advances, modern LLMs such as GPT-4 still struggle with issues regarding accuracy and interpretability, especially in safety-critical domains such as health. We explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This framework is made possible by the asymmetry between verification and generation, where the former is often much easier than the latter. Experimental results show that our method consistently improves accuracy for various LLMs across standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios.\n\nThe self-verification pipeline here extracts clinical information along with evidence for each output:\n\n<p align=\"center\">\n  <img src=\"https://microsoft.github.io/clinical-self-verification/figs/med_status_ex.png\" width=\"60%\">\n</p>\n\n\n```\n@misc{gero2023selfverification,\n      title={Self-Verification Improves Few-Shot Clinical Information Extraction}, \n      author={Zelalem Gero and Chandan Singh and Hao Cheng and Tristan Naumann and Michel Galley and Jianfeng Gao and Hoifung Poon},\n      year={2023},\n      journal={arXiv preprint arXiv:2306.00024},\n}\n```\n", "repo_name": "clinical-self-verification", "org_name": "microsoft", "org_repo": "microsoft/clinical-self-verification", "platform_org_repo": "github+microsoft/clinical-self-verification", "link_to_repo": "https://github.com/microsoft/clinical-self-verification", "platform": "github", "language": "Python", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "\ud83d\udc49 **[For English](./README.en.md)** \ud83d\udc48\n\n# \ud574\ucee4\ub7f0 &ndash; \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc\n\n<div style=\"text-align: center;\">\n  <img src=\"assets/banner-1920x480.png\" alt=\"\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ubc30\ub108\" width=\"100%\"/>\n</div>\n\n**\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc** \uc774\ubca4\ud2b8\ub294 \ub300\uad6c/\uacbd\ubd81 \uc9c0\uc5ed\uc758 \uccad\ub144 \uac1c\ubc1c\uc790\ub4e4\uacfc \ud568\uaed8 [Microsoft \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc](https://azure.microsoft.com/ko-kr?WT.mc_id=dotnet-91712-juyoo)\ub97c \ud65c\uc6a9\ud574 \ub2f9\uba74\ud55c \uc9c0\uc5ed\uc758 \ud604\uc548 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ud574\ucee4\ud1a4\uc785\ub2c8\ub2e4.\n\n**\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0(\uc628\ub77c\uc778 \uc0ac\uc804\uad50\uc721)** \uc774\ubca4\ud2b8\ub294 \uc8fc\uc5b4\uc9c4 \uae30\uac04 \uc548\uc5d0 \uad00\ub828 \uc9c0\uc2dd\uc744 \uc9c1\uc811 \ubc30\uc6cc\uc11c \ud574\ucee4\ud1a4\uc5d0 \uc801\uc6a9\ud558\ub294 \uc774\ubca4\ud2b8\uc785\ub2c8\ub2e4. \uc774\ubc88 \ucc4c\ub9b0\uc9c0\uc758 \uc8fc\uc81c\ub294 \ubc14\ub85c 6\ub300 Microsoft \uae30\ucd08 \uacfc\uc815\uc785\ub2c8\ub2e4. 3\uc8fc\uac04 \uac01\uc790 \uacf5\ubd80\ud558\uace0 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4\uc5d0 \ucc38\uc5ec\ud574\uc11c \uc9c1\uc811 \uc11c\ube44\uc2a4\ub97c \ub9cc\ub4e4\uc5b4 \ubcf4\ub294 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc774\ubca4\ud2b8! \ub3c4\uc804\ud574 \ubcf4\uc138\uc694. \ud478\uc9d0\ud55c \uc0c1\ud488\uc774 \uae30\ub2e4\ub9bd\ub2c8\ub2e4!\n\n## \u2709\ufe0f \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4/\ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc548\ub0b4 \uba54\uc77c \uc2e0\uccad\n\n\uc544\ub798 \ud3fc\uc73c\ub85c \uc2e0\uccad\uc2dc \uad00\ub828 \uc548\ub0b4 \uba54\uc77c\uc744 \ubcf4\ub0b4\ub4dc\ub9b4 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n- [\ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0(\uc628\ub77c\uc778 \uc0ac\uc804\uad50\uc721) \uc9c0\uae08 \uc2e0\uccad\ud558\uae30](https://aka.ms/hackers-ground/register/csc) \ud83d\udc48 \ub4f1\ub85d\uacfc \ub3d9\uc2dc\uc5d0 \uc218\uac15 \uac00\ub2a5\ud569\ub2c8\ub2e4.\n- [\ud574\ucee4\ud1a4 \uc9c0\uae08 \uc2e0\uccad\ud558\uae30](https://aka.ms/hackers-ground/register/hackathon)\n\n## \ud83c\udfc5 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc218\uc0c1\uc790\n\n\ubaa8\ub4e0 \ucc4c\ub9b0\uc9c0 \uc644\ub8cc\uc790 \uc911 \ucd94\ucca8\uc744 \ud1b5\ud574 \uc0c1\ud488\n\n\n## \u23f0 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc77c\uc815\n\n- \ucc38\uac00 \uc2e0\uccad\uae30\uac04: 2023\ub144 5\uc6d4 29\uc77c 0\uc2dc - 2023\ub144 6\uc6d4 19\uc77c 0\uc2dc \ud83d\udc48 \ucc38\uac00 \uc2e0\uccad\uacfc \ub3d9\uc2dc\uc5d0 \uc218\uac15 \uac00\ub2a5\ud569\ub2c8\ub2e4.\n- \uacb0\uacfc\uc81c\ucd9c \ub9c8\uac10\uae30\ud55c: 2023\ub144 6\uc6d4 19\uc77c 0\uc2dc\n- \uc218\uc0c1\uc790 \ubc1c\ud45c: 2023\ub144 6\uc6d4 20\uc77c 18\uc2dc\n\n\n## \ud83d\ude46\ud83c\udffb\u200d\u2640\ufe0f \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \ucc38\uc5ec \uc790\uaca9\n\n- \ucc38\uc5ec\ud558\uace0 \uc2f6\uc740 \ub204\uad6c\ub098 \uac00\ub2a5\ud569\ub2c8\ub2e4! \ub2e8...\n  - \ub300\uad6c/\uacbd\ubd81 \ud65c\ub3d9 17\uc138 ~ 39\uc138 \uccad\ub144\uc774\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n  - \ud589\uc0ac \uae30\uac04 \uc911 \uc694\uccad\uc2dc \uc790\uc2e0\uc758 \uac70\uc8fc\uc9c0\ub97c \uc99d\ube59\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\n\n## \ud83c\udfc3\ud83c\udffb\u200d\u2642\ufe0f \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \ucc38\uc5ec \ubc29\ubc95\n\n1. \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc774\ubca4\ud2b8\uc5d0 \ucc38\uac00 \uc2e0\uccad\ud569\ub2c8\ub2e4.\n1. \ucc38\uac00\uc2e0\uccad \ud6c4 \uace7\ubc14\ub85c \uc218\uac15\uc744 \uc2dc\uc791\ud569\ub2c8\ub2e4.\n1. \ucd1d 6\uac1c\uc758 \ucc4c\ub9b0\uc9c0 \ubbf8\uc158\uc744 \ubaa8\ub450 \uc644\ub8cc\ud569\ub2c8\ub2e4.\n1. \uac01 \ucc4c\ub9b0\uc9c0 \ubbf8\uc158\uc744 \uc644\ub8cc\ud560 \ub54c \ub9c8\ub2e4 \uc774\uc288\ub97c \uc0dd\uc131\ud558\uc5ec \uc6b4\uc601\uc9c4\uc5d0\uac8c [\uc81c\ucd9c][submit]\ud569\ub2c8\ub2e4.\n1. \ucc4c\ub9b0\uc9c0 \ub3c4\uc911 \uad81\uae08\ud55c \uc0ac\ud56d\uc774 \uc788\uc744 \ub550 \uc5b8\uc81c\ub4e0 [\uc9c8\ubb38/\ub2f5\ubcc0][qna] \ubc29\uc744 \uc774\uc6a9\ud574 \uc8fc\uc138\uc694.\n\n\n## \ud83c\udfc6 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc0c1\ud488\n\n- \ucc4c\ub9b0\uc9c0 \ubbf8\uc158 \ub450 \uac1c \uc644\ub8cc\ud560 \ub54c \ub9c8\ub2e4 Microsoft \uc790\uaca9\uc99d \uc2dc\ud5d8 \ubb34\ub8cc \ubc14\uc6b0\ucc98 \uc81c\uacf5 \ud83d\udc49 \ucd5c\ub300 3\uac1c \ubc14\uc6b0\ucc98 \uc218\ub839 \uac00\ub2a5\n- \ubaa8\ub4e0 \ucc4c\ub9b0\uc9c0 \uc644\ub8cc\uc790 \uc911 \ucd94\ucca8\ud574\uc11c \uc544\ub798 \uc0c1\ud488 \uc81c\uacf5\n  - \uc0c1\ud488: Microsoft Surface Pro\n\n    ![Microsoft Surface Pro 9](./assets/microsoft-surface-pro-9.jpg)\n\n- \uc790\uaca9\uc99d \uc2dc\ud5d8 \ud569\uaca9\uc2dc \ucd94\uac00 \uc0c1\ud488 \uc81c\uacf5 (\ucd94\ud6c4 \uacf5\uc9c0)\n\n> \uc0c1\ud488 \uad00\ub828 \uc138\ubd80 \uc0ac\ud56d\uc740 \uc608\uace0 \uc5c6\uc774 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n## \u2705 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \ubbf8\uc158\n\n\ucd1d **\uc5ec\uc12f \uac00\uc9c0 \ucc4c\ub9b0\uc9c0 \ubbf8\uc158**\uc744 \uc644\ub8cc\ud574\uc57c \ud569\ub2c8\ub2e4. \uc804\uccb4 \ucc4c\ub9b0\uc9c0 \ubbf8\uc158\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n> \uac01 \ucc4c\ub9b0\uc9c0 \ubcc4\ub85c \uac01\uac01 \ucc38\uac00 \uc2e0\uccad\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n1. **Hackers Ground: \uc560\uc800 \uae30\ucd08 (AZ-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/az-900)\n1. **Hackers Ground: \uc560\uc800 AI \uae30\ucd08 (AI-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/ai-900)\n1. **Hackers Ground: \uc560\uc800 \ub370\uc774\ud130 \uae30\ucd08 (DP-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/dp-900)\n1. **Hackers Ground: \ud30c\uc6cc \ud50c\ub7ab\ud3fc \uae30\ucd08 (PL-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/pl-900)\n1. **Hackers Ground: \ubcf4\uc548 \uae30\ucd08 (SC-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/sc-900)\n1. **Hackers Ground: Microsoft 365 \uae30\ucd08 (MS-900) \ucc4c\ub9b0\uc9c0** \ud83d\udc49 [\ucc4c\ub9b0\uc9c0 \ubc14\ub85c\uac00\uae30](https://aka.ms/hg/csc/ms-900)\n\n\n## \ud83d\udccb \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud074\ub77c\uc6b0\ub4dc \uc2a4\ud0ac \ucc4c\ub9b0\uc9c0 \uc790\uaca9\uc99d \uc2dc\ud5d8\n\n- \uc628\ub77c\uc778\uc73c\ub85c \uc9d1\uc5d0\uc11c \ud3b8\ud788 \uc790\uaca9\uc99d \uc2dc\ud5d8\uc5d0 \uc751\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- \uc9d1\uc5d0\uc11c \uc790\uaca9\uc99d \uc2dc\ud5d8\uc744 \uce58\ub974\uae30 \uace4\ub780\ud55c \uacbd\uc6b0 \uc544\ub798 \ub9c1\ud06c\ub97c \ud1b5\ud574 \ubcc4\ub3c4\uc758 \uc2dc\ud5d8 \uc7a5\uc18c\ub97c \uc608\uc57d\ud55c \ud6c4 \uc751\uc2dc\ud558\uc138\uc694.\n  - \uc790\uaca9\uc99d \uc2dc\ud5d8 \uc2e0\uccad \ub9c1\ud06c \ud83d\udc49 [\uc900\ube44\uc911]\n\n---\n\n## \u23f0 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uc77c\uc815 \ubc0f \uc7a5\uc18c\n\n- \ucc38\uac00 \uc2e0\uccad\uae30\uac04: 2023\ub144 5\uc6d4 29\uc77c - 2023\ub144 6\uc6d4 19\uc77c\n- \uc0ac\uc804 \ud578\uc988\uc628\ub7a9 \uae30\uac04: 2023\ub144 6\uc6d4 20\uc77c\n- \ud574\ucee4\ud1a4 \uae30\uac04: 2023\ub144 6\uc6d4 21\uc77c - 23\uc77c\n- \uc7a5\uc18c: \ub300\uad6c \uc5d1\uc2a4\ucf54 \uadf8\ub79c\ub4dc\ubcfc\ub8f8 B (\ub300\uad6c\uad11\uc5ed\uc2dc \ubd81\uad6c \uc5d1\uc2a4\ucf54\ub85c 10)\n\n  ![\ub300\uad6c \uc5d1\uc2a4\ucf54 \uc9c0\ub3c4](./assets/map-exco.png)\n\n\n## \ud83d\ude46\ud83c\udffb\u200d\u2640\ufe0f \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \ucc38\uc5ec \uc790\uaca9 \ubc0f \uc870\uac74\n\n- \uc790\uc2e0\uc774 \uc9c1\uc811 \uc9c0\uc5ed\uc0ac\ud68c \ubb38\uc81c \ubc0f \ud604\uc548\uc744 \ud574\uacb0\ud558\uace0 \uc2f6\uc740 \ub300\uad6c/\uacbd\ubd81 \uc18c\uc7ac 17\uc138 ~ 39\uc138 \uccad\ub144\n  - \ud589\uc0ac \uae30\uac04 \uc911 \uc694\uccad\uc2dc \uc790\uc2e0\uc758 \uac70\uc8fc\uc9c0\ub97c \uc99d\uba85\ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n- [Microsoft \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc](https://azure.microsoft.com/ko-kr?WT.mc_id=dotnet-91712-juyoo)\ub97c \ud65c\uc6a9\ud558\uc5ec \ubb38\uc81c \ud574\uacb0\n\n\n## \ud83d\udcbb \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uc0ac\uc804 \ud578\uc988\uc628\ub7a9 \uc138\uc158\n\n\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uae30\uac04 \uc911 0\uc77c\ucc28\uc5d0 \uc624\ud504\ub77c\uc778\uc5d0\uc11c \ub2e4\uc591\ud55c \ud578\uc988\uc628 \ub7a9 \uc138\uc158\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \ud578\uc988\uc628\ub7a9 \uc138\uc158\uc744 \ud1b5\ud574 \uae30\ubcf8\uc801\uc778 \uc560\uc800 \uc11c\ube44\uc2a4\uc758 \ud65c\uc6a9 \ubc29\ubc95\uc744 \uc775\ud600\ubcf4\uace0 \uc5ec\ub7ec\ubd84\uc758 \uc11c\ube44\uc2a4 \uac1c\ubc1c\uc5d0 \uc801\uc6a9\uc2dc\ucf1c \ubcf4\uc138\uc694!\n\n- \uc77c\uc2dc: 2023\ub144 6\uc6d4 20\uc77c (\uc2dc\uac04 \ucd94\ud6c4 \uacf5\uc9c0)\n- \uc7a5\uc18c: \uacbd\ubd81\ub300\ud559\uad50 \uac15\uc758\uc2e4 (\uc7a5\uc18c \ucd94\ud6c4 \uacf5\uc9c0)\n- \ub0b4\uc6a9:\n\n  | &nbsp; | \uac15\uc758\uc2e4 1 | \uac15\uc758\uc2e4 2 | \uac15\uc758\uc2e4 3 |\n  |----------|----------|----------|----------|\n  | 1\uad50\uc2dc | \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc \uae30\ucd08 | \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc \uae30\ucd08 | \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc \uae30\ucd08 |\n  | 2\uad50\uc2dc | GitHub \ucf54\ub4dc\uc2a4\ud398\uc774\uc2a4 \ubc0f \ucf54\ud30c\uc77c\ub7ff \uae30\ucd08 | GitHub \ucf54\ub4dc\uc2a4\ud398\uc774\uc2a4 \ubc0f \ucf54\ud30c\uc77c\ub7ff \uae30\ucd08 | GitHub \ucf54\ub4dc\uc2a4\ud398\uc774\uc2a4 \ubc0f \ucf54\ud30c\uc77c\ub7ff \uae30\ucd08 |\n\n\n## \ud83d\udcb0 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \ucc38\uac00\uc790 \ubca0\ub124\ud54f\n\n- \ubaa8\ub4e0 \ucc38\uac00\uc790\ub294 [Microsoft \uc560\uc800 \ud074\ub77c\uc6b0\ub4dc](https://azure.microsoft.com/ko-kr?WT.mc_id=dotnet-91712-juyoo)\ub97c \ud589\uc0ac \uae30\uac04 \uc911 \ubb34\ub8cc\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4\n- \ubaa8\ub4e0 \ucc38\uac00\uc790\ub294 [GitHub \ucf54\ud30c\uc77c\ub7ff](https://github.com/features/copilot)\uc744 \ud589\uc0ac\uae30\uac04 \uc911 \ubb34\ub8cc\ub85c \uc0ac\uc6a9 \uac00\ub2a5\ud569\ub2c8\ub2e4.\n\n\n## \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udfeb \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uae30\uc220 \uba58\ud1a0\n\n\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \uae30\uac04\uc911 \ub2e4\uc591\ud55c \uae30\uc220\uc801 \ubb38\uc81c\ub4e4\uc5d0 \ub300\ud574 \uad81\uae08\ud55c \uc810\uc774 \uc788\uc73c\uc2e0\uac00\uc694? \uc5ec\uae30 Microsoft\uc758 \uc804\ubb38\uac00\uc640 Microsoft MVP\ub4e4\uc774 \ud574\ucee4\ud1a4 \uae30\uac04\uc911 \uacaa\uc744 \uc218 \uc788\ub294 \uc5ec\ub7ec\uac00\uc9c0 \uae30\uc220\uc801\uc778 \ubb38\uc81c \ud574\uacb0\uc5d0 \uba58\ud1a0\ub9c1\uc744 \ud574 \uc8fc\uc2e4 \uc608\uc815\uc785\ub2c8\ub2e4.\n\n- \ub0b4\uc6a9:\n  - \ud589\uc0ac\uc7a5 \ub0b4 Microsoft \uc5d4\uc9c0\ub2c8\uc5b4 \ubc0f Microsoft MVP \uc0c1\uc8fc \uba58\ud1a0\ub9c1\n  - \ubcc4\ub3c4\ub85c \ud300\ubcc4 \uba58\ud1a0\uac00 \ubc30\uc815\ub418\uc9c0 \uc54a\uc73c\uba70 \uae30\uc220\uc801\uc778 \ubb38\uc81c \ubc1c\uc0dd \uc2dc \uc624\ud53c\uc2a4 \uc544\uc6cc\ub97c \uc774\uc6a9\ud574 \uc2e4\uc2dc\uac04 \uc624\ud504\ub77c\uc778 \uba58\ud1a0\ub9c1 \uc9c4\ud589\n\n\n## \ud83c\udfc5 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uc2dc\uc0c1 \ubd80\ubb38\n\n- \ub300\uc0c1 (\ucd1d 1\ud300): Microsoft \uc0ac\uc7a5\uc0c1\n- \ucd5c\uc6b0\uc218\uc0c1 (\ucd1d 2\ud300): \ub300\uad6c\uacbd\ubd81\uc9c0\uc5ed\ud601\uc2e0\ud50c\ub7ab\ud3fc \uc804\uc790\uc815\ubcf4\uae30\uae30\uc0ac\uc5c5\ub2e8\uc7a5\uc0c1\n- \uc6b0\uc218\uc0c1 (\ucd1d 3\ud300): Microsoft \uc0ac\uc7a5\uc0c1\n\n\n## \ud83c\udfc6 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud574\ucee4\ud1a4 \uc2dc\uc0c1\ud488\n\n- \ub300\uc0c1 (\ucd1d 1\ud300): \uc0c1\uc7a5 + 150\ub9cc\uc6d0 \uc0c1\ub2f9 \uc0c1\ud488\n- \ucd5c\uc6b0\uc218\uc0c1 (\ucd1d 2\ud300): \uc0c1\uc7a5 + 70\ub9cc\uc6d0 \uc0c1\ub2f9 \uc0c1\ud488\n- \uc6b0\uc218\uc0c1 (\ucd1d 3\ud300): \uc0c1\uc7a5 + 40\ub9cc\uc6d0 \uc0c1\ub2f9 \uc0c1\ud488\n\n> \uc0c1\ud488 \uad00\ub828 \uc138\ubd80 \uc0ac\ud56d\uc740 \uc608\uace0 \uc5c6\uc774 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n## \ud83c\udfad \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \uac1c\uc778\uc815\ubcf4 \ubcf4\ud638\uc815\ucc45\n\n\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc\ub294 \uc5ec\ub7ec\ubd84\uc758 \uac1c\uc778 \uc815\ubcf4\ub97c \uc18c\uc911\ud788 \uc5ec\uae41\ub2c8\ub2e4. \uac1c\uc778 \uc815\ubcf4 \ubcf4\ud638\uc640 \uad00\ub828\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [\uac1c\uc778\uc815\ubcf4 \ubcf4\ud638\uc815\ucc45][privacy] \ud398\uc774\uc9c0\ub97c \ucc38\uace0\ud574 \uc8fc\uc138\uc694.\n\n\n## \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 \ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc \ud589\ub3d9 \uac15\ub839 Code of Conduct\n\n\ud574\ucee4\uadf8\ub77c\uc6b4\ub4dc\ub294 \ucc38\uac00\uc790 \uc5ec\ub7ec\ubd84\ub4e4 \ubaa8\ub450\uac00 \ud589\uc0ac \uae30\uac04 \uc911 \ub9c8\uc74c \ud3b8\uc548\ud558\uac8c \ucc38\uc5ec\ud558\uc2e4 \uc218 \uc788\uac8c\ub054 \ubaa8\ub450\uc5d0\uac8c \uacf5\uc815\ud558\uace0 \uc0c1\ud638 \uc874\uc911\ud558\ub3c4\ub85d \ud589\ub3d9 \uac15\ub839\uc744 \uc815\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc880 \ub354 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [\ud589\ub3d9 \uac15\ub839][coc] \ud398\uc774\uc9c0\ub97c \ucc38\uace0\ud574 \uc8fc\uc138\uc694.\n\n\n## \ud83e\udd51 \uc8fc\ucd5c\n\n\uc774 \ud589\uc0ac\ub294 Microsoft \ubc0f \uacbd\ubd81\ub300\ud559\uad50\uc5d0\uc11c \uc8fc\ucd5c\ud569\ub2c8\ub2e4.\n&nbsp;\n\n<div style=\"display: flex; justify-content: center; align-items: center;\">\n  <br/>\n\n  <a href=\"https://www.microsoft.com/ko-kr?WT.mc_id=dotnet-91712-juyoo\" target=\"_blank\" title=\"Microsoft \ud648\ud398\uc774\uc9c0\"><img src=\"assets/logo-microsoft-cropped.png\" alt=\"Microsoft \ub85c\uace0\" width=\"251\"/></a>&nbsp;\n  <a href=\"https://www.knu.ac.kr\" target=\"_blank\" title=\"Microsoft \ud648\ud398\uc774\uc9c0\"><img src=\"assets/logo-knu.png\" alt=\"\uacbd\ubd81\ub300\ud559\uad50 \ub85c\uace0\" width=\"210\"/></a>\n</div>\n\n## \ud83e\udd51 \uc8fc\uad00\n\n\uc774 \ud589\uc0ac\ub294 \uad50\uc721\ubd80, \ub300\uad6c\uad11\uc5ed\uc2dc, \ud55c\uad6d\uc5f0\uad6c\uc7ac\ub2e8, \ub300\uad6c\uacbd\ubd81\uc9c0\uc5ed\ud601\uc2e0\ud50c\ub7ab\ud3fc \uc804\uc790\uc815\ubcf4\uae30\uae30\uc0ac\uc5c5\ub2e8, (\uc8fc)\ud574\ub2ec\ud504\ub85c\uadf8\ub798\ubc0d, \ub300\uad6c\uacbd\ubd81\ub300\ud559\ud559\uc0dd\ud68c\uc5f0\ud569\uc5d0\uc11c \uc8fc\uad00\ud569\ub2c8\ub2e4.\n&nbsp;\n\n<div style=\"display: flex; justify-content: center; align-items: center;\">\n  <br/>\n\n  <a href=\"https://www.moe.go.kr/\" target=\"_blank\"><img src=\"assets/logo-moe.png\" alt=\"\uad50\uc721\ubd80 \ub85c\uace0\" width=\"15%\"/></a>&nbsp;\n  <a href=\"https://www.daegu.go.kr/\" target=\"_blank\"><img src=\"assets/logo-daegu.png\" alt=\"\ub300\uad6c\uad11\uc5ed\uc2dc \ub85c\uace0\" width=\"15%\"/></a>&nbsp;\n  <a href=\"https://www.nrf.re.kr/\" target=\"_blank\"><img src=\"assets/logo-nrf.jpg\" alt=\"\ud55c\uad6d\uc5f0\uad6c\uc7ac\ub2e8 \ub85c\uace0\" width=\"15%\"/></a>&nbsp;\n  <a href=\"https://www.dgplatform.or.kr/\" target=\"_blank\"><img src=\"assets/logo-dgplatform.jpg\" alt=\"\ub300\uad6c\uacbd\ubd81\uc9c0\uc5ed\ud601\uc2e0\ud50c\ub7ab\ud3fc \ub85c\uace0\" width=\"10%\"/></a>&nbsp;\n  <a href=\"https://dgei.or.kr/\" target=\"_blank\"><img src=\"assets/logo-dgei.jpg\" alt=\"\ub300\uad6c\uacbd\ubd81\uc9c0\uc5ed\ud601\uc2e0\ud50c\ub7ab\ud3fc \uc804\uc790\uae30\uae30\uc0ac\uc5c5\ub2e8 \ub85c\uace0\" width=\"10%\"/></a>\n</div>\n\n\n[submit]: https://github.com/microsoft/hackers-ground/issues\n[qna]: https://github.com/microsoft/hackers-ground/issues\n[privacy]: https://github.com/microsoft/hackers-ground/blob/main/PRIVACY_POLICY.md\n[coc]: https://github.com/microsoft/hackers-ground/blob/main/CODE_OF_CONDUCT.md\n\n", "repo_name": "hackers-ground", "org_name": "microsoft", "org_repo": "microsoft/hackers-ground", "platform_org_repo": "github+microsoft/hackers-ground", "link_to_repo": "https://github.com/microsoft/hackers-ground", "platform": "github", "language": null, "stargazers_count": 16, "watchers_count": 16}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "vscode-stac", "org_name": "microsoft", "org_repo": "microsoft/vscode-stac", "platform_org_repo": "github+microsoft/vscode-stac", "link_to_repo": "https://github.com/microsoft/vscode-stac", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Libraries for Linux Tracepoints and user_events\n\nThis repository contains libraries for using the\n[user_events](https://docs.kernel.org/trace/user_events.html) facility to\ngenerate Linux Tracepoints from user mode.\n\n## Overview\n\n- [libtracepoint](libtracepoint) -\n  low-level C/C++ tracing interface. Designed to support replacement at\n  link-time if a different implementation is needed.\n  - [Default implementation](libtracepoint/src/tracepoint.c)\n    writes directly to the Linux `user_events` facility.\n  - [tracepoint-provider.h](libtracepoint/include/tracepoint/tracepoint-provider.h) -\n    a developer-friendly C/C++ API for writing tracepoint events to any\n    implementation of the tracepoint interface.\n- [libeventheader-tracepoint](libeventheader-tracepoint) -\n  `eventheader` envelope that supports extended attributes including severity\n  level and optional field type information.\n  - [TraceLoggingProvider.h](libeventheader-tracepoint/include/eventheader/TraceLoggingProvider.h) -\n    a developer-friendly C/C++ API for writing `eventheader`-encapsulated\n    events to any implementation of the tracepoint interface.\n  - [EventHeaderDynamic.h](libeventheader-tracepoint/include/eventheader/EventHeaderDynamic.h) -\n    C++ API for writing runtime-defined `eventheader`-encapsulated events,\n    intended for use as an implementation layer for a higher-level API like\n    OpenTelemetry.\n- [libtracepoint-decode-cpp](libtracepoint-decode-cpp) -\n  C++ library for decoding tracepoints. Works on both Linux and Windows.\n  - `PerfEventInfo.h` defines the `PerfSampleEventInfo` and\n    `PerfNonSampleEventInfo` structures for raw event information.\n  - `PerfDataFile.h` defines the `PerfDataFile` class that decodes\n    `perf.data` files.\n  - `PerfEventMetadata.h` defines classes for parsing ftrace event metadata\n    information.\n- [libeventheader-decode-cpp](libeventheader-decode-cpp) -\n  C++ library for decoding events that use the `eventheader` envelope.\n  - `EventEnumerator` class parses an event into fields.\n  - `EventFormatter` class converts event data into a string.\n  - `decode-perf` tool that decodes `perf.data` files to JSON.\n- [libeventheader-decode-dotnet](libeventheader-decode-dotnet) -\n  .NET library for decoding events that use the `eventheader` envelope.\n- [Rust](rust) - support for generating `eventheader`-encapsulated events from\n  Rust.\n\n## General Usage\n\n- Configure a Linux system with the `user_events` feature enabled.\n  - Supported on Linux kernel 6.4 and later.\n  - Kernel must be built with `user_events` support (`CONFIG_USER_EVENTS=y`).\n  - Must have either `tracefs` or `debugfs` mounted. For example, you might add\n    the following line to your `/etc/fstab` file:\n    `tracefs /sys/kernel/tracing tracefs defaults 0 0`\n  - The user that will generate events must have `x` access to the `tracing`\n    directory and `rw` access to the `tracing/user_events_data` file. One\n    possible implementation is to create a `tracers` group, then:\n    - `chgrp tracers /sys/kernel/tracing`\n    - `chgrp tracers /sys/kernel/tracing/user_events_data`\n    - `chmod g+x /sys/kernel/tracing`\n    - `chmod g+rw /sys/kernel/tracing/user_events_data`\n- Use one of the event generation APIs to write a program that generates events.\n  - C/C++ programs can use\n    [tracepoint-provider.h](libtracepoint/include/tracepoint/tracepoint-provider.h)\n    to generate regular Linux Tracepoint events that are defined at compile-time.\n    (Link with `libtracepoint`.)\n  - C/C++ programs can use\n    [TraceLoggingProvider.h](libeventheader-tracepoint/include/eventheader/TraceLoggingProvider.h)\n    to generate eventheader-enabled Tracepoint events that are defined at\n    compile-time. (Link with `libtracepoint` and `libeventheader-tracepoint`.)\n  - C++ middle-layer APIs (e.g. an OpenTelemetry exporter) can use\n    [EventHeaderDynamic.h](libeventheader-tracepoint/include/eventheader/EventHeaderDynamic.h)\n    to generate eventheader-enabled Tracepoint events that are runtime-dynamic.\n    (Link with `libtracepoint` and `libeventheader-tracepoint`.)\n  - Rust middle-layer APIs (e.g. an OpenTelemetry exporter) can use the\n    [eventheader_dynamic](rust/eventheader_dynamic/README.md) crate\n    to generate eventheader-enabled Tracepoint events that are defined at\n    compile-time.\n- Running as a privileged user, use the Linux\n  [`perf`](https://www.man7.org/linux/man-pages/man1/perf.1.html) tool\n  to collect events to a `perf.data` file, e.g.\n  `perf record -e user_events:MyEvent1,user_events:MyEvent2`.\n  - The `perf` tool binary is typically available as part of the `linux-perf`\n    package (e.g. can be installed by `apt install linux-perf`). However, this\n    package installs a `perf_VERSION` binary rather than a `perf` binary, so\n    you will need to add an appropriate VERSION suffix to your `perf` commands\n    or use a wrapper script.\n  - To capture tracepoints using `perf`, you'll also need to install\n    `libtraceevent`, e.g. `apt install libtraceevent1`.\n  - The `linux-base` package installs a `perf` wrapper script that redirects to\n    the version of `perf` that matches your current kernel (if present) so that\n    you can run the appropriate version of `perf` without the VERSION suffix.\n    This frequently doesn't work because the latest `perf` binary from `apt`\n    doesn't always match the running kernel, so you may want to make your own\n    wrapper script instead.\n  - Note that for purposes of collecting events, it is usually not important\n    for the version of the `perf` tool to match the kernel version, so it's\n    ok to use e.g. `perf_5.10` even if you are running a newer kernel.\n- Note that tracepoints must be registered before you can start collecting\n  them. The `perf` command will report an error if the tracepoint is not yet\n  registered.\n  - You can usually register tracepoints by running the program that generates\n    them. Most programs will register all of their tracepoints when they start\n    running.\n  - For eventheader-enabled Tracepoint events, you can also use the\n    [`eventheader-register`](libeventheader-tracepoint/tools/eventheader-register.cpp)\n    tool to pre-register an event based on its tracepoint name so you can start\n    collecting it before starting the program that generates it.\n- Use the [`decode-perf`](libeventheader-decode-cpp/tools/decode-perf.cpp)\n  tool to decode the `perf.data` file to JSON text.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit [https://cla.opensource.microsoft.com](https://cla.opensource.microsoft.com).\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "LinuxTracepoints", "org_name": "microsoft", "org_repo": "microsoft/LinuxTracepoints", "platform_org_repo": "github+microsoft/LinuxTracepoints", "link_to_repo": "https://github.com/microsoft/LinuxTracepoints", "platform": "github", "language": "C", "stargazers_count": 23, "watchers_count": 23}, {"README_text": "# Visual Studio Desired State Configuration (DSC) Resource Provider\n\nDSC resources to simplify state management of installed Visual Studio instances.\n\n## VSComponents Resource\n\nThe `VSComponents` resource is used to modify a pre-existing Visual Studio instance in order to add additional [components](https://learn.microsoft.com/visualstudio/install/workload-and-component-ids). It is meant to accompany the [`winget configure`](https://learn.microsoft.com/windows/package-manager/winget/configure) command. \n\nYou currently need administrator permissions to use this resource to install or modify Visual Studio. Furthermore, Visual Studio must be closed in order to update or add components to it. \n\nRefer to [Use winget to install or modify Visual Studio](https://learn.microsoft.com/en-us/visualstudio/install/use-command-line-parameters-to-install-or-modify-visual-studio?#use-winget-to-install-visual-studio) for additional information. \n\n### Parameters\n\nAt least `VSConfigFile` or `Components` must be specified. You can also specify both simultaneously.\n\n**Parameter**|**Attribute**|**DataType**|**Description**|**Allowed Values**\n:-----|:-----|:-----|:-----|:-----\n`ProductId`|Key|String|The product identifier of the instance you are working with. EG: `Microsoft.VisualStudio.Product.Community`|See [workload and component ids](https://learn.microsoft.com/en-us/visualstudio/install/workload-and-component-ids)\n`ChannelId`|Key|String|The channel identifier of the instance you are working with. EG: `VisualStudio.17.Release`|See [channel identifiers](https://learn.microsoft.com/en-us/visualstudio/install/command-line-parameter-examples#using---channelId)\n`Components`|Optional|StringArray[]|Collection of component identifiers you wish to update the provided instance with.|See [workload and component ids](https://learn.microsoft.com/en-us/visualstudio/install/workload-and-component-ids)\n`VSConfigFile`|Optional|String|Path to the [Installation Configuration (VSConfig) file](https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/) you wish to update the provided instance with.|Valid file path to a .vsconfig file\n`IncludeRecommended`|Optional|Boolean|For the provided required components, also add recommended components into the specified instance|True/False\n`IncludeOptional`|Optional|Boolean|For the provided required components, also add optional components into the specified instance|True/False\n`InstalledComponents`|NotConfigurable|StringArray[]|A collection of components installed in the Visual Studio instance identified by the provided Product ID and Channel ID.|N/A\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "VisualStudioDSC", "org_name": "microsoft", "org_repo": "microsoft/VisualStudioDSC", "platform_org_repo": "github+microsoft/VisualStudioDSC", "link_to_repo": "https://github.com/microsoft/VisualStudioDSC", "platform": "github", "language": "PowerShell", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Azure Resource Manager QuickStart Templates\n\nThis repo contains all currently available Azure Resource Manager templates contributed by the community. A searchable template index is maintained at [azure.com](https://azure.microsoft.com/documentation/templates).\n\nSee the [**Contribution guide**](/1-CONTRIBUTION-GUIDE/README.md#contribution-guide) for how to use or contribute to this repo.\n\n## NOTE\n\nWe have finished migration of the samples to subfolders, see [azure.com](https://azure.microsoft.com/documentation/templates) if you need help finding a sample.  A few obsolete samples were removed in the migration.\n\n### Final Note\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n", "repo_name": "azure-quickstart-templates", "org_name": "microsoft", "org_repo": "microsoft/azure-quickstart-templates", "platform_org_repo": "github+microsoft/azure-quickstart-templates", "link_to_repo": "https://github.com/microsoft/azure-quickstart-templates", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Microsoft Supply Chain Center and Dynamics 365 Intelligent Order Management Providers\n\n## Overview\n\nThis project is a collabaration with the community on providers for Microsoft Supply Chain Center and Dynamics 365 Intelligent Order Management.\n\nProviders are a collection of Provider Schema, Connection References, Power Automates, Parameters and Power Query Transformations. These providers allow you to communicate with external systems to retrieve related data and transform it into a format that is understood by Dataverse.\n\n## Sample Code\n\nAll contributions to this project, including those from the community and Microsoft are considered sample code. Sample code can vary in levels of production readiness, and it is up to the person or entity who uses these providers to deploy these to a test environment, modify them, and ensure their level of readiness before deploying these into a production system.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "mscc-providers", "org_name": "microsoft", "org_repo": "microsoft/mscc-providers", "platform_org_repo": "github+microsoft/mscc-providers", "link_to_repo": "https://github.com/microsoft/mscc-providers", "platform": "github", "language": null, "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# An Active Learning Pipeline for Identifying Whales in High-resolution Satellite Imagery\n\n**Jump to: [Setup](#setup) | [Interesting point detector](#interesting-point-detector) | [Labeling tool](#labeling-tool) | [Results](#results)**\n\n\nThis repository contains code for an [_active learning_](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) pipeline for detecting whales in high-resolution satellite imagery. This consists of two parts:\n- A script for detecting __interesting points__ in imagery\n- A labeling tool that runs an active learning pipeline to classify each __interesting point__ as a whale or not\n\n\n<p align=\"center\">\n    <img src=\"images/screenshot.png\" width=\"800\" alt=\"Screenshot of labeling tool\"/>\n    <b>Figure 1: </b>Screenshot of the labeling tool interface.\n</p>\n\n## Setup\n\nFirst, run the following commands to create a conda environment, `whales`, with the necessary dependencies for running the scripts in this repository.\n```\nconda env create -f environment.yml\nconda activate whales\n```\n\nDownload and preprocess a coastline boundary for the United States from (1 : 500,000) Census cartographic boundary data:\n```\nwget https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_state_500k.zip\nunzip cb_2021_us_state_500k.zip\nogr2ogr -clipdst -125.156 24.207 -66.797 49.210 -nln out -f GeoJSON tmp.geojson cb_2021_us_state_500k.shp\nogr2ogr -f GPKG cb_2021_us_state_500k.gpkg tmp.geojson -nln out -dialect sqlite -sql \"SELECT ST_Union(geometry) as geometry FROM out\"\nmv cb_2021_us_state_500k.gpkg data/\nrm tmp.geojson\nrm cb_2021_us_state_500k.*\n```\n\n\n## Interesting point detector\n\nThe first part in our pipeline is an _interesting point detector_. There is a huge amount of variance in satellite imagery, even for images taken over the empty ocean, and there are few dataset of labeled whales in satellite imagery.\nAs such, we take an unsupervised modeling approach to identify anomalous points that are offshore in order to feed a human-in-the-loop labeling pipeline instead of attempting to directly train a model to segment/classify whales.\n\n**Usage example**: The following command will load Maxar satellite imagery off the coast of Turkey (released by Maxar as part of their Open Data Program), then look for groups of anomalous pixels, and save the centroid locations of such groups to `results/` in GeoJSON format.\n```bash\npython generate_interesting_points.py --input_url \"https://maxar-opendata.s3.amazonaws.com/events/Kahramanmaras-turkey-earthquake-23/ard/37/031133021120/2023-02-12/10300100E1B9D900-visual.tif\" --output_fn results/interesting_points.geojson --method big_window --difference_threshold 20\n```\n\n\n## Labeling tool\n\nThe second part of our pipeline is a web based active learning interface. This interface allows a user to label a set of interesting points as either a whale or not a whale. As a user labels points, an active learning algorithm selects the next set of points to label based on a lightweight model.\n\nRunning this labeling tool requires a preprocessing step that creates image chips around each interesting point:\n```bash\npython prepare_data_for_labeling_tool.py --run_name demo_run --image_url \"https://maxar-opendata.s3.amazonaws.com/events/Kahramanmaras-turkey-earthquake-23/ard/37/031133021120/2023-02-12/10300100E1B9D900-visual.tif\" --output_dir labeling-tool/inputs/demo_run/ --input_fn results/interesting_points.geojson --dar \"Demo Imagery\" --catid \"10300100E1B9D900\" --date \"2023-02-12\"\n```\n\nThe following command runs the labeling tool and allows us to access it at `http://localhost:12351`\n```bash\ncd labeling-tool\npython server.py --remote_host localhost --input_dir inputs/demo_run/ --port 12351\n```\n\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n\nThe datasets are licensed under the [Open Use of Data Agreement v1.0](https://cdla.dev/open-use-of-data-agreement-v1-0/).\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "whales", "org_name": "microsoft", "org_repo": "microsoft/whales", "platform_org_repo": "github+microsoft/whales", "link_to_repo": "https://github.com/microsoft/whales", "platform": "github", "language": "Python", "stargazers_count": 20, "watchers_count": 20}, {"README_text": "# Templates for Power Platform \n\nSolutions designed to expedite the implementation of the Power Platform within large-scale enterprises.\n\n## Currently Offered Solutions\n\n### SAP Procurement\nThis solution allows you to extend your core procure-to-pay business processes enabled within SAP. The accelerator contains the building blocks necessary to streamline all the SAP screens and attributes related to a core procure-to-pay process into one simple screen in Power Apps. The Power Apps are supported by further automation of processes behind the scenes using Power Automate flows, SAP ERP Connector and Microsoft Dataverse. These components are fully extensible to rapidly deploy and meet your business requirements including connecting into additional line of business systems and workflows.\n\n### Employee Kudos\n\nEnables individual teammates to celebrate co-workers for their contributions. There are many more exciting solutions on the roadmap such as Awards & Recognitions which enables employees to nominate individuals & teams for awards and Employee Onboarding which partners new employees with current employees and mentors to facilitate new employee onboarding, especially in a hybrid work environment. All with easy to use interface with data stored in Microsoft Dataverse. Available for both web and mobile devices.\n\n## Access on AppSource\n\n#### [Employee Kudos Template on AppSource](https://aka.ms/AccessEmployeeKudosTemplate)\n\n#### [SAP Procurement Template on AppSource](https://aka.ms/AccessSAPProcurementTemplate)\n\n## Documentation\n\n#### [Employee Kudos Template](https://aka.ms/LearnEmployeeKudosTemplate)\n\n#### [SAP Procurement Template](https://aka.ms/LearnSAPProcurementTemplate)\n\n#### [General Documenatation](https://aka.ms/LearnTemplatesForPowerPlatform)\n\n# Advanced:\n\n## Folder Structure\n\n#### [AppSourcePackages](./AppSourcePackages/README.md)\n    AppSource packages that are used to publish solutions for easy install.\n\n#### [DeploymentPackages](./DeploymentPackages/README.md)\n    Dynamics 356 packages that are used to install Solutions into a Microsoft Dataverse instance.\n\n#### [SampleData](./SampleData/README.md)\n    This folder contains sample data that can be imported into specific Power Apps after they have been installed into a Power Platform Environment.\n\n#### [Scripts](./Scripts/README.md)\n    Helpful scripts usually written in PowerShell.\n\n#### [Solutions](./Solutions/README.md)\n    Unpackaged Power Platform Solutions.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Templates-for-Power-Platform", "org_name": "microsoft", "org_repo": "microsoft/Templates-for-Power-Platform", "platform_org_repo": "github+microsoft/Templates-for-Power-Platform", "link_to_repo": "https://github.com/microsoft/Templates-for-Power-Platform", "platform": "github", "language": "C#", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Model Performance Toolkit\n\nModel Performance Toolkit (model-perf) is a Python package backed by test applications for different platforms (Windows/MacOS, Android/iOS, Web) to benchmark machine learning models on different target platforms and devices (e.g., Google Pixel for Android, iPhone for iOS).\n\n## Installation\n\nInstall from Pip\n\n```bash\npython -m pip install model-perf --upgrade\n```\n\nBuild and Install from Source _(for developers)_\n\n```bash\ngit clone https://github.com/microsoft/model-perf.git\n\n# We suggest don't use conda, please use native python.\npython -m pip install --upgrade aiohttp[speedups] build mypy pip setuptools twine virtualenv wheel\n\n# Pay attention to the auto-detected Python intepretor path in log. If it is wrong, specify the Python version to help detect the right one.\ncmake -S . -B build -A x64\ncmake --build build --config RelWithDebInfo\n```\n\n## Example\n### Run Android Benchmark\n```python\nfrom model_perf.metrics import accuracy_score\nfrom model_perf.mobile import AppCenterModelRunner\nfrom model_perf.model import ModelAssets\n\n# model runner to run model on android devices\nmodel_runner = AppCenterModelRunner(model=ModelAssets(path='./add.onnx'),\n                                    test_app='./test_apps/android/test_app/app-arm64-v8a-debug.apk',\n                                    test_driver_app='./test_apps/android/test_driver_app/target/upload',\n                                    output_dir='output_test_android',\n                                    appcenter_owner='test-owner',\n                                    appcenter_app='test-app',\n                                    appcenter_deviceset='pixel-4a')\n\n# inputs of model\ninputs = [[numpy.array([[1.1, 1.1]], dtype=numpy.float32), numpy.array([[2.2, 2.2]], dtype=numpy.float32)],\n          [numpy.array([[3.3, 3.3]], dtype=numpy.float32), numpy.array([[4.4, 4.4]], dtype=numpy.float32)],\n          [numpy.array([[5.5, 5.5]], dtype=numpy.float32), numpy.array([[6.6, 6.6]], dtype=numpy.float32)]]\n\n# predict and benchmark\npredict_outputs, benchmark_outputs = model_runner.predict_and_benchmark(inputs=inputs,\n                                                                        config={'model': {'input_names': ['x', 'y'], 'output_names': ['sum']}})\n\n# expected outputs of model\nexpected_outputs = [[numpy.array([[3.3, 3.3]], dtype=numpy.float32)],\n                    [numpy.array([[7.7, 7.7]], dtype=numpy.float32)],\n                    [numpy.array([[12.1, 12.1]], dtype=numpy.float32)]]\n\n# calculate accuracy\naccuracy = accuracy_score(expected_outputs, predict_outputs[0])\nprint(f\"accuracy = {accuracy}\")\n\n# print benchmark outputs\nprint(benchmark_outputs[0])\n```\n\n### Run Server Benchmark\n```python\nfrom model_perf.server import ServerModelRunner\n\n# the system under test\nclass SystemUnderTest:\n    def __init__(self) -> None:\n        pass\n\n    def run(self):\n        sum = 0\n        for i in range(1, 10000):\n            sum += i\n\n# model runner to run model on server\nmodel_runner = ServerModelRunner(SystemUnderTest, num_workers=8, num_threads=1, tensorboard=True)\n\n# start server model runner\nmodel_runner.start()\nreport = model_runner.benchmark(queries=[(), (), ()], target_qps=10000, min_duration_ms=120000)\nmodel_runner.stop()\n\n# print benchmark report\nprint(report)\n```\n\n\n## Build the Docs\n\nRun the following commands and open ``docs/_build/html/index.html`` in browser.\n\n```bash\npython -m pip install sphinx myst-parser sphinx-rtd-theme sphinxemoji\ncd docs/\n\nmake html         # for linux\n.\\make.bat html   # for windows\n```\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "model-perf", "org_name": "microsoft", "org_repo": "microsoft/model-perf", "platform_org_repo": "github+microsoft/model-perf", "link_to_repo": "https://github.com/microsoft/model-perf", "platform": "github", "language": "Python", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# API Manifest\n\nThis project is a parser and DOM(Document object model) for the API Manifest media type.  An early draft of the specification is [available](https://darrelmiller.github.io/api-manifest/draft-miller-api-manifest.html).\n\nAn \"api manifest\" is a way to store the dependencies that an application has on HTTP APIs. It contains characteristics of those dependencies including links to API descriptions, specifics of the types of HTTP API requests made by the application and related authorization information.\n\nYou can create an API manifest in code:\n\n```csharp\n var apiManifest = new ApiManifestDocument() {\n            Publisher = new() {\n                Name = \"Microsoft\",\n                ContactEmail = \"example@example.org\"\n            },\n            ApiDependencies = new() {\n                { \"example\", new()\n                    {\n                        ApiDescripionUrl = \"https://example.org\",\n                        Auth = new()\n                        {\n                            ClientIdentifier = \"1234\",\n                            Access = new() {\n                                new () { Type= \"application\", Content = new JsonObject() {\n                                        { \"scopes\", new JsonArray() {\"User.Read.All\"} }}\n                                     } ,\n                                new () { Type= \"delegated\", Content = new JsonObject() {\n                                        { \"scopes\", new JsonArray() {\"User.Read\", \"Mail.Read\"} }}\n                                     }\n                            }\n                        },\n                        Requests = new() {\n                            new() { Method = \"GET\", UriTemplate = \"/api/v1/endpoint\" },\n                            new () { Method = \"POST\", UriTemplate = \"/api/v1/endpoint\"}\n                        }\n                    }\n                }\n            }\n        };\n```\n\nor you can read it from a stream\n\n```csharp\nvar doc = JsonDocument.Parse(stream);\nvar apiManifest = ApiManifestDocument.Load(doc.RootElement);\n```\n\nAn ApiManifest object can be serialized to a JSON stream as follows:\n\n```csharp\nvar stream = new MemoryStream();\nvar writer = new Utf8JsonWriter(stream);\nexampleApiManifest.Write(writer);\nwriter.Flush();\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "OpenApi.ApiManifest", "org_name": "microsoft", "org_repo": "microsoft/OpenApi.ApiManifest", "platform_org_repo": "github+microsoft/OpenApi.ApiManifest", "link_to_repo": "https://github.com/microsoft/OpenApi.ApiManifest", "platform": "github", "language": "C#", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Project\n\nThis is a repository to aggregate content to be used if you are running a hackathon event focused on AI solutions.\n\nIf you encounter a word or phrase you don't understand, check out our [Vocabulary](Vocabulary.md) page.\n\nIf you aren't finding what you are looking for in this repository, it might be because we didn't want to reproduce data points in our [Interesting Links](InterestingLinks.md).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "globalopenaihack", "org_name": "microsoft", "org_repo": "microsoft/globalopenaihack", "platform_org_repo": "github+microsoft/globalopenaihack", "link_to_repo": "https://github.com/microsoft/globalopenaihack", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "SynapseVSCode", "org_name": "microsoft", "org_repo": "microsoft/SynapseVSCode", "platform_org_repo": "github+microsoft/SynapseVSCode", "link_to_repo": "https://github.com/microsoft/SynapseVSCode", "platform": "github", "language": null, "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Teams for Justice\n\nBeginning with Australian courts and tribunals, Teams for Justice addresses the need to facilitate hearings online with\ntooling that reflects the principles of case procedure. The COVID-19 pandemic has been a catalyst for this need.\nBy providing a Microsoft Teams experience that accommodates the general needs of courts and tribunals in an online setting,\nwe will materially improve what is currently considered a sub-optimal experience of managing and moderating online hearings.\n\nOur goal is enabling the moderator to \ufffdbring back order to the courts.\ufffd\n\n## Table of Contents <!-- omit in toc -->\n\n- [Teams for Justice](#teams-for-justice)\n  - [Soluton Architecture](#soluton-architecture)\n  - [Development Overview](#development-overview)\n  - [Azure Functions](#azure-functions)\n    - [Extension Management](#extension-management)\n  - [Development Container](#development-container)\n  - [Building](#building)\n  - [Deploying to Azure](#deploying-to-azure)\n  - [MEGA Linting](#mega-linting)\n  - [Azure Storage Emulation](#azure-storage-emulation)\n  - [Contributing](#contributing)\n  - [Trademarks](#trademarks)\n\n## Soluton Architecture\n\nAn overview of the architecture for the solution and its component parts cane be found [here](docs/wiki/architecture/architecture.md)\n\n## Development Overview\n\nThis project has been configured to work best with [VS Code](https://code.visualstudio.com/). There are two ways to open\nthis project:\n\n1. As a folder - simply open VS code from the root of the repository.\n2. As a workspace - After opening as a folder, click on the dialog in the bottom right to open the workspace, or go to\n   `File -> Open Workspace` and select the `workspace.code-workspace` file.\n\n## Azure Functions\n\nRunning and debugging Azure Functions locally works best when running as a workspace; the workspace gives access to most\nof the same code files, organized as projects. If a file is not present in the workspace, feel free to either it to the\nworkspace, or open as a folder for full access.\n\n### Extension Management\n\nAzure Functions that wish to interop with different Azure Services require bindings for those services. We are using an\nexplicit approach to manage these extensions to minimize the build times as well as to ensure we stay locked into the\nversions of these bindings we know work well through testing.\n\nIf you are just working with existing code, there is nothing to do as we have a build step that will synchronize your\nlocal project with the already configured extensions (i.e. runs `func extensions sync`) whenever you call `yarn build`.\nIf a new binding is necessary, the developer should either add this binding explicitly by referencing it in the\n`function.json` file and calling `func extensions install` which will crawl the `function.json` files in the project,\ndiscover all bindings used and update the `extensions.csproj` or manually install the extension using\n`func extensions install -p <extension-package-name> [-v <version>]`.\n\nFor more on registering extensions, [please refer to this Azure Functions CLI documentation](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Ccsharp%2Cbash#register-extensions).\n\n## Development Container\n\n**Prerequisites:**\n\n- [Docker Desktop](https://www.docker.com/products/docker-desktop)\n- [VS Code](https://code.visualstudio.com/)\n- [Remote Development extension pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n\nThis project supports the use of [development containers](https://code.visualstudio.com/docs/remote/containers). Dev\ncontainers are a great way to package all development requirements into a docker image and running VS code from the\ncontext of that docker image. Dev containers can also be used with [Github\nCodespaces](https://code.visualstudio.com/docs/remote/codespaces) for a hosted development environment.\n\nTo use the dev container, follow the [Quick Start\ninstructions](https://code.visualstudio.com/docs/remote/containers#_quick-start-open-an-existing-folder-in-a-container)\nin the VS code documentation. VS Code will restart itself and open in the context of the docker container with your\nlocal repo mounted directly into the container so you will not lose any work if the container stops.\n\n## Building\n\n**Pre-requisites**:\n\n- [Node 14](https://nodejs.org/en/download/)\n- Yarn (run `npm install -g yarn`)\n- [Azure Functions Core Tools v3](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=linux%2Ccsharp%2Cbash)\n  - Can be installed with `npm i -g azure-functions-core-tools@3 --unsafe-perm true`\n\nAll project source code is located in the `src` folder. This project utilizes [yarn\nworkspaces](https://yarnpkg.com/features/workspaces) to support all the projects as separate packages in a monorepo. To\ninstall dependencies and build and test all the projects, run the following from the `src` folder:\n\n```shell\nyarn install        # installs all dependencies\nyarn build          # builds all projects\nyarn test           # run all unit tests\n```\n\nUsing yarn workspaces means all dependencies are co-located in a single node_modules folder in the `src` folder. To run\nany script present in a package.json for one of the projects within the workspace, you can use the following command:\n\n```shell\nyarn <project name> <command>\n```\n\nOr to run against all projects:\n\n```shell\nyarn workspaces run <command>\n```\n\nSee [`src/package.json`](src/package.json) for examples of this usage.\n\n## Deploying to Azure\n\nTo deploy Teams for Justice (T4J) solution to Azure, you'd need to follow the\nsteps and provided instructions in the various documents spread across this Wiki\nfolder. To help you with navigating throught a myriad of documents for how to\ndeploy Teams for Justice into a new Azure and Teams environment, this section\nprovides you with an list of documents and helpful guidelines that you can\nfollow along in the process of deploying all Teams for Justice Azure and Teams\ncomponents.\n\nHere is the list of operations we suggest you to perform in the order they are outlined:\n\n1. Create a new or sign in as a global administrator into a targeted Microsoft Azure subscription\n2. Create a new or sign in as a Teams administrator into a targeted Microsoft Teams tenant\n3. Obtain M365 licenses (E3 or E5) as you'd need them later when provisioning Teams for Justice application\n4. Fork the `teams4justice` solution in GitHub to your own GitHub account\n5. Create Azure AD App Registrations for all T4J microservices (see more [Application Security](docs/wiki/application-security.md))\n6. Configure Microsoft Teams tenant with the PowerShell scripts (see more [Setting Up Teams Tenant](docs/wiki/setting-up-teams-tenant.md))\n7. Deploy baseline Azure infrastructure using Terraform scripts (see more [Set up Terraform Infrastructure](./terraform/backend/README.md))\n8. Follow the remaining steps in [Setting up a new environment](docs/wiki/setting-up-a-new-environment.md))\n\n## MEGA Linting\n\n**Prerequisites:**\n\n- [Docker Desktop](https://www.docker.com/products/docker-desktop)\n- [Node 14](https://nodejs.org/en/download/)\n\nThis project uses a variety of linters to ensure code quality and style. Most of the linters are integrated into the VS\nCode dev container or are recommended extensions. This project also uses the\n[mega-linter](https://github.com/nvuillam/mega-linter/) to run linting for all PRs automatically via GitHub Actions.\n\nThis tool is automatically installed in the dev container; to install it on your local machine, run\n`npm install -g mega-linter-runner`. To run the mega-linter locally, run `mega-linter-runner` in the root of the\nrepository. Configuration for the mega-linter can be found in `.mega-linter.yml`.\n\n## Azure Storage Emulation\n\nSeveral of the projects in this repo, such as the Azure Functions, are reliant on Azure Storage features to run.\n[Azurite] solves this problem well for local development. Azurite can either be installed as a global tool using\n[its `npm` package](https://www.npmjs.com/package/azurite) or as\n[a container](https://hub.docker.com/_/microsoft-azure-storage-azurite) which you can spin up using Docker Desktop.\nDocumentation for how to work with each specific approach is provided at each of the respective links.\n\nThen, in the project(s) that depends on Azure Storage, just make sure you set any Azure Storage connection strings\nsetting (e.g. `AzureWebJobsStorage`) in your configuration files (e.g. environment vars or appsettings) to the value:\n`UseDevelopmentStorage=true`. This tells the SDK to specifically look for the emulator running on a specific set of\ndefault ports.\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "teams4justice", "org_name": "microsoft", "org_repo": "microsoft/teams4justice", "platform_org_repo": "github+microsoft/teams4justice", "link_to_repo": "https://github.com/microsoft/teams4justice", "platform": "github", "language": "Bicep", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Microsoft.Advisor.Engagements", "repo_name": "Microsoft.Advisor.Engagements", "org_name": "microsoft", "org_repo": "microsoft/Microsoft.Advisor.Engagements", "platform_org_repo": "github+microsoft/Microsoft.Advisor.Engagements", "link_to_repo": "https://github.com/microsoft/Microsoft.Advisor.Engagements", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Streamlit ChatGPT Harness\n\nA fully python based Streamlit development harness for ChatGPT hosted in Azure OpenAI Service.\n\n## Getting Started\n\n### Running the App\n\nTo run the app, simply run the `streamlit run app.py`.  This will start the app on port 8501.  You can then access the app at `http://localhost:8501`.\n\n### Devcontainer\n\nThis project is designed to be used with VSCode and the Remote Containers extension.  Once you have the extension installed, open the project in VSCode and you will be prompted to open the project in a container.  This will build the container and install all the dependencies.\n\n### Python Dependencies\n\nThe `requirements.txt` file contains all the python dependencies for this project.  The `devcontainer.json` file will automatically install these dependencies when the container is built.\n\n## Credits\n\nThis project was forked from the [streamlit-chatgpt-ui](https://github.com/marshmellow77/streamlit-chatgpt-ui) under the MIT license and adapted to the Azure OpenAI Service.\n\n## Contributing\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.", "repo_name": "az-oai-chatgpt-streamlit-harness", "org_name": "microsoft", "org_repo": "microsoft/az-oai-chatgpt-streamlit-harness", "platform_org_repo": "github+microsoft/az-oai-chatgpt-streamlit-harness", "link_to_repo": "https://github.com/microsoft/az-oai-chatgpt-streamlit-harness", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Smart Word Suggestions for Writing Assistance\n\nThis repository contains the code and pre-trained models for our paper [Smart Word Suggestions for Writing Assistance](https://arxiv.org/pdf/2305.09975.pdf).\n\n## Overview\n\nThis paper introduces \"Smart Word Suggestions\" (SWS) task and benchmark. \nUnlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario. \nThis task involves identifying words or phrases that require improvement and providing substitution suggestions. \nThe benchmark includes human-labeled data for testing, a large distantly supervised dataset for training, and the framework for evaluation. \nThe test data includes 1,000 sentences written by English learners, accompanied by over 16,000 substitution suggestions annotated by 10 native speakers. \nThe training dataset comprises over 3.7 million sentences and 12.7 million suggestions generated through rules. \n\n![](https://tva1.sinaimg.cn/large/008vOhrAly1hdvfxw0o8fj30u00y2tgd.jpg)\n\nFor example, as illustrated in the figure above, the word \u201cinti-mate\u201d in the first sentence should be replaced with\n\u201cclose\u201d, as \u201cintimate\u201d is not suitable for describing relationships between colleagues.\n\n## Getting Started\n\nThe data folder contains all the data, the code folder contains all the baselines. There are corresponding READMEs in the folder. \n\n## Bugs or questions?\n\nIf you have any questions related to the code or the paper, feel free to email Chenshuo (iven@ivenwang.com) and Shaoguang (shaoguang.mao@microsoft.com). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!\n\n## Citation\n\nPlease cite our paper if you use SWS in your work:\n\n```\n\n```\n", "repo_name": "SmartWordSuggestions", "org_name": "microsoft", "org_repo": "microsoft/SmartWordSuggestions", "platform_org_repo": "github+microsoft/SmartWordSuggestions", "link_to_repo": "https://github.com/microsoft/SmartWordSuggestions", "platform": "github", "language": "Python", "stargazers_count": 19, "watchers_count": 19}, {"README_text": "# BingMaps.PTE\n\nWelcome to this Open Source Per Tenant Extension, which is an integration with Bing Maps inside Business Central.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bcsamples-bingmaps.pte", "org_name": "microsoft", "org_repo": "microsoft/bcsamples-bingmaps.pte", "platform_org_repo": "github+microsoft/bcsamples-bingmaps.pte", "link_to_repo": "https://github.com/microsoft/bcsamples-bingmaps.pte", "platform": "github", "language": "AL", "stargazers_count": 4, "watchers_count": 4}, {"README_text": "# Fundraising Proposal Assistant\n\nFundraising Proposal Assistant is a customized experience for the Microsoft Cloud for Nonprofit to allow fundraisers to generate a draft fundraising proposal email that can be edited and sent to prospective donors. The draft proposal is generated by Open AI based on the context of an opportunity, a donor\u2019s profile, interests, and funding priorities in the Microsoft Cloud for Nonprofit.\n\n\n\n## Prerequisites\n\n- Dynamics 365 Sales Enterprise.\n- Microsoft Cloud for Nonprofit \u2013 Fundraising and Engagement https://learn.microsoft.com/en-us/dynamics365/industry/nonprofit/fundraising-engagement-deploy-installer.\n- Admin access to Microsoft Power Platform, Microsoft Dynamics 365 or tenant admin.\n\n\n## API Key\n\n- Log in to your OpenAI account\n- Click on your name in the upper right corner and select View API keys from the drop-down menu.\n- To get a new secret key, click the Create new secret key button near the center of the page.\n- On the create new secret key dialog, enter a name for your API key and click Create secret key.\n\n\n## Manual Solution Installation\n\n1. Before installing the solution, make sure you have the solution file in your machine. (Select one of the files available on the \"Releases\" option on the right panel).\n2. Go to the maker portal https://make.powerapps.com/.\n3. On the top right, select the environment where you want to install the solution.\n4. On the left navigation pane, click Solutions and Import solution.\n5. Click Import solution.\n6. Browse to the location of the solution file downloaded at step 1 and click Next.\n7. On the confirmation window click Next.\n8. The import process will ask you to reestablish a connection. A connection is needed for the Power Automate Flow that is part of the solution. This connection provides access to the org-based database on Microsoft Dataverse in current environment. If a connection already exists, you can select it from the dropdown and you can jump to step 12. In case there are no connections, Click the dropdown next to the connection reference and click New Connection.\n9. A new window will open to create the connection. On the dialog, click Create.\n10. Pick an account or log in with an account that has access to the current Dataverse Environment.\n11. After creating the connection, go back to the previous window and click Refresh on the dialog shown. That will refresh the connections dropdown.\n12. Select the newly created connection from the dropdown next to the connection reference and click Next.\n13. Paste your API key from OpenAI on the Environment Variable OpenAI API Key and click Import. (to get an OpenAI API key see section Set up an OpenAI API Key).\n14. Wait until the import process completes.\n\n## Utilizing the AI Solution\nFundraisers can reach more prospective donors by quickly drafting personalized fundraising proposals with OpenAI in the Microsoft Cloud for Nonprofit.\nWith the click of a button, fundraisers can generate a draft fundraising proposal email that can be edited and sent to prospective donors. The draft proposal is generated by Open AI based on the context of an opportunity, a donor\u2019s profile, interests, and funding priorities in the Microsoft Cloud for Nonprofit.\n\nThe following guidelines will show how to use the predeveloped solution:\n1. Go to your organization\u2019s Dynamics 365 CRM portal (e.g. https://organizationid.crm.dynamics.com).\n2. From the list of apps, select the Fundraising and Engagement model-driven app.\n3. From the navigation pane, select Opportunities, and open an existing opportunity record.\n4. To draft a Fundraising Proposal Letter, click on the **Write Proposal** button inside the opportunity form.\n5. A side panel dialog shows, which allows to add more guidance or context before sending the data to OpenAI. Some examples of Additional Guidance for OpenAI include:\n    - \u201cThe letter should be written as if It was written by John Smith (Fundraising Director of ABC Organization)\u201d.\n    -  \u201cThe proposal must be written in Spanish\u201d.\n6. Once the additional context is added (is optional), click Generate Proposal. After the generation of the proposal completes, the dialog will show a confirmation.\n7. The Open Email button allows opening the newly created.\n\n## Developer Notes\n\n- Find instructions on how to deploy Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal).\n\n\n## FAQ\n### Should the assistant use GPT-3.5 or GPT-4?\nThe app was tested with GPT-3.5 \n\n### Can the assistant use Azure Open AI?\nThe app functions with both OpenAI or Azure Open AI. The repo currently uses Azure OpenAI. \n[Request access to Azure OpenAI.](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu)\n\n\n", "repo_name": "FundraisingProposalAssistant", "org_name": "microsoft", "org_repo": "microsoft/FundraisingProposalAssistant", "platform_org_repo": "github+microsoft/FundraisingProposalAssistant", "link_to_repo": "https://github.com/microsoft/FundraisingProposalAssistant", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "aicreator", "org_name": "microsoft", "org_repo": "microsoft/aicreator", "platform_org_repo": "github+microsoft/aicreator", "link_to_repo": "https://github.com/microsoft/aicreator", "platform": "github", "language": "Python", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Donor Engagement Assistant\n\nDonor Engagement Assistant is a customized experience for the Microsoft Dynamics 365 Fundraising and Engagement platform to allow users of the system to create targeted email communications to donors. Based on their needs, users will interact with the Microsoft Azure OpenAI Service to create a personal letter and send it to a donor.\n\n\n\n## Prerequisites\n\n- Dynamics 365 Sales Enterprise\n- Microsoft Cloud for Nonprofit \u2013 Fundraising and Engagement https://learn.microsoft.com/en-us/dynamics365/industry/nonprofit/fundraising-engagement-deploy-installer\n- Admin access to Microsoft Power Platform, Microsoft Dynamics 365 or tenant admin\n\n\n\n## Environment Variables\n\n- AI Letter Generator URL: the URL of the Canvas App in the deployed environment (You can leave this blank. It will be filled in later)\n- Azure OpenAI Base URL: the partial URL of a deployed model, e.g. \u201copenai/deployments/**gpt-turbo**\u201d (**remove all trailing slashes**), where \"gpt-turbo\" is the name of your Azure OpenAI deployment. \n- Azure OpenAI Host URL: the endpoint of your Azure OpenAI instance, e.g. \"**myazureopenai**.openai.azure.com\" (**remove all trailing slashes**), where \"myazureopenai\" is the name of your Azure OpenAI resource.\n\n## Manual Solution Installation\n\n1. Navigate to the Power Apps Portal\n2. Click on Solutions in the left-hand navigation\n3. Click on Import Solution in the top navigation bar\n4. Click on browse and locate the unmanaged solution zip file on your computer\n5. Follow the remaining prompts to import the solution\n\n## Setting AI Letter Generator URL\n\n1. Go to Power Apps Make Portal\n2. Click on \"Solutions\" on the left panel.\n3. Click on \"Donor Engagement Assistant\" from the list.\n4. Click on the \"OpenAI Letter Generator\" elipesis menu and click on \"Details\". Copy the \"Web link\" address.\n5. Click on the \"AI Letter Generator URL\" and paste the copied URL into the \"Current Value\" field.\n6. Click on the \"OpenAI Letter Generator\". The editor will open and prompt you for the OpenAI key.\n7. After setting the key, go back and save the changes.\n\n## Usage\n\n1. Import or create potential donors as Contacts\n    -Can also create/import organizations as Accounts. The Canvas App will appeal to the Primary Contact of the Account, using their biography and email address\n2. Create a Campaign and a Designation. Enter descriptions for each\n3. Create an Opportunity. The following fields will be pulled into the Canvas App:\n    - Potential Donor (or Primary Contact of Potential Donor)\n    - Expected Amount\n    - Source Campaign\n    - Default Designation of the Opportunity\n    - Opportunity Manager \u2013 this determines the generated email\u2019s \u201cFrom\u201d email address\n4. Open the Opportunity. A button labeled OpenAI Letter on the command bar/ribbon will open a side pane containing the Canvas App\n5. Upon first execution, you will be prompted to enter credentials for the custom Azure OpenAI connector. You may also be prompted to enter credentials for the Outlook connector\n\n## Developer Notes\n\n- Find instructions on how to deploy Azure OpenAI [here](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal).\n\n\n## FAQ\n### Should the assistant use GPT -3.5 or GPT-4?\nThe app was tested with GPT-3.5 \n\n### Can the assistant use Azure Open AI?\nThe app functions with both OpenAI or Azure Open AI. The repo currently uses Azure OpenAI. \n[Request access to Azure OpenAI.](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu)\n\n", "repo_name": "DonorEngagementAssistant", "org_name": "microsoft", "org_repo": "microsoft/DonorEngagementAssistant", "platform_org_repo": "github+microsoft/DonorEngagementAssistant", "link_to_repo": "https://github.com/microsoft/DonorEngagementAssistant", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Nonprofit Virtual Assistant (for Microsoft Teams)\n\nNonprofit Virtual Assistant is a Teams-based Virtual Assistant for nonprofit organizations, powered by OpenAI services. With the Nonprofit Virtual Assistant, nonprofit organizations can take advantage of the advanced language capabilities of OpenAI to simplify internal workflows and deepen their connection with their communities, patrons, donors, and stakeholders more effectively than ever before.\nThe assistant will focus on generating four essential letters: Funding Request Letter, In-Kind Donation Request Letter, Urgent Appeal Letter, and Volunteer Recruitment Letter.\n\nhttps://github.com/microsoft/NonprofitVirtualAssistant/assets/92055663/ef59725d-85c5-459b-9ee1-88375ecfc60d\n\n## Prerequisites\n\n- [OpenAI API account](https://platform.openai.com/signup)\n- Azure resource group with rights to resource creation\n- [Install Visual Studio 2022](https://visualstudio.microsoft.com/downloads/) (version 17.3+)\n- [Install Teams Toolkit for Visual Studio](https://learn.microsoft.com/en-us/microsoftteams/platform/toolkit/install-teams-toolkit?tabs=vscode&pivots=visual-studio)\n- [ngrok](https://ngrok.com/download) (needed for local debugging only)\n\n## Setup\n\n- Clone the repo. \n\n```\ngit clone https://github.com/microsoft/NonprofitVirtualAssistant.git\n```\n\n- Add an environment variable named `OPENAI_KEY` and set its value as the api key you get from [OpenAI API portal](https://platform.openai.com/account/api-keys). \n\n- Launch Visual Studio\n\n    - From a _Developer Command Prompt for Visual Studio_ you can do the following to launch Visual Studio with the environment variable set:\n    \n    ```\n    set OPENAI_KEY=<YOUR API KEY>\n    devenv\n    ```\n\n- Open the solution csharp/NonprofitVirtualAssistant.sln. Right click on the project and make sure you have the Teams Toolkit menu option:\n\n![Teams Toolkit Menu](NonprofitVirtualAssistant/images/TeamsToolkitMenu.png)\n\n- Then follow the steps below for local or cloud deployment. If desired, you can skip the steps for local and provision to the cloud directly.\n\n## Local deployment\n\n1. Start ngrok in a terminal: `ngrok http 5130`.\n2. Open solution in Visual Studio.\n3. Right click project > _Teams Toolkit_ > _Prepare Teams App Depndencies_.\n4. If prompted, sign in with a Microsoft 365 account for the Teams organization you want \nto install the app to.\n5. Run/Debug (F5) to launch the project in Visual Studio.\n6. In the launched browser, select the Add button to load the app in Teams.\n\n## Cloud deployment\n\n1. Open solution in Visual Studio.\n2. Right click project > _Teams Toolkit_ > _Provision in the Cloud_.\n3. If prompted, sign in with a Microsoft 365 account for the Teams organization you want \nto install the app to.\n4. Choose a subscription and resource group to provision the resources, wait till provisioning is complete.\n5. Right click project > _Teams Toolkit_ > _Deploy to the Cloud_.\n6. Right click project > _Teams Toolkit_ > _Zip App Package_ > _For Azure_.\n7. Find the generated zip in the _build_ folder alongside project and upload it to Teams as a custom app.\n\nYou can share/commit the generated `azure.parameters.env.json` file if you want to share the cloud provisioning details.\n\n## Developer Notes\n\n- You can use an existing Azure bot for deployment. Find instructions on how to do it [here](https://learn.microsoft.com/en-us/microsoftteams/platform/toolkit/provision?pivots=visual-studio#use-an-existing-azure-ad-app-for-your-teams-app-1).\n- The bot was developed on Windows using Visual Studio. However, there aren\u2019t any Visual Studio-specific dependencies, so you can use the Teams toolkit on VS Code as well if you so wish.\n\n## FAQ\n### Should the assistant use GPT -3.5 or GPT-4?\nAlthough the app was tested with both GPT-3.5 and GPT-4, GPT-4 is recommended for optimal performance.\n\n### Can the assistant use Azure Open AI?\nThe app functions with both OpenAI or Azure Open AI. The repo currently uses OpenAI. If you have access to Azure OpenAI, it is recommended to make the switch in the API call and also skip the call to OpenAI Moderation API.\n[Request access to Azure OpenAI.](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu)\n\n## List of third party javascript libraries/versions\n\n- [Azure OpenAI](https://www.nuget.org/packages/Azure.AI.OpenAI)\n- [TeamsFx](https://www.nuget.org/packages/Microsoft.TeamsFx)\n", "repo_name": "NonprofitVirtualAssistant", "org_name": "microsoft", "org_repo": "microsoft/NonprofitVirtualAssistant", "platform_org_repo": "github+microsoft/NonprofitVirtualAssistant", "link_to_repo": "https://github.com/microsoft/NonprofitVirtualAssistant", "platform": "github", "language": "C#", "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "powerplatformconnect", "org_name": "microsoft", "org_repo": "microsoft/powerplatformconnect", "platform_org_repo": "github+microsoft/powerplatformconnect", "link_to_repo": "https://github.com/microsoft/powerplatformconnect", "platform": "github", "language": null, "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Contextual Optimisation via Bayesian Experimental Design.\n\nAccepted at ICML 2023.\n\nPaper: https://arxiv.org/abs/2302.14015\n\n## Installation\nInstall the basic requirements with conda/mamba, e.g. by running\n\n    conda env create -f env.yml\n\nMake sure you are installing the correct torch version (cpu vs gpu).\n\n\n## Experiments\n\nTo reproduce the experiments in the paper follow the instructions below.\n\n-------------\n### **Experiment 1: Discrete treatments**\n-------------\nThis is the example with 4 treatments.\n**MODEL** Each of the four treatments $a = 1, 2, 3, 4$ is a random function with two parameters $\\psi_k = (\\psi_{k,1}, \\psi_{k,2})$ with the following Gaussian priors (parameterized by mean and covariance matrix):\n\n$$\n    \\begin{align}\n    \\psi_1 ~ &\\sim \\mathcal{N} \\left(\\begin{pmatrix} 5.00 \\\\ 15.0  \\end{pmatrix},\n    \\begin{pmatrix}\n     9.00 & 0 \\\\\n     0 & 9.00\n    \\end{pmatrix} \\right) \\\\\n    \\psi_2 ~ &\\sim \\mathcal{N} \\left(\\begin{pmatrix} 5.00 \\\\ 15.0  \\end{pmatrix},\n    \\begin{pmatrix}\n     2.25 & 0 \\\\\n     0 & 2.25\n    \\end{pmatrix} \\right) \\\\\n    \\psi_3 ~ &\\sim \\mathcal{N} \\left(\\begin{pmatrix} -2.0 \\\\ -1.0  \\end{pmatrix},\n    \\begin{pmatrix}\n     1.21 & 0 \\\\\n     0 & 1.21\n    \\end{pmatrix} \\right) \\\\\n    \\psi_4 ~ &\\sim \\mathcal{N} \\left(\\begin{pmatrix} -7.0 \\\\ 3.0  \\end{pmatrix},\n    \\begin{pmatrix}\n     1.21 & 0 \\\\\n     0 & 1.21\n    \\end{pmatrix} \\right)\n    \\end{align}\n$$\n\nand reward (outcome) likelihoods:\n\n$$\n    \\begin{align}\n    y | c, a, \\psi &\\sim  \\mathcal{N} \\left( f(c, a, \\psi), 0.1 \\right) \\\\\n    f(c, a, \\psi) &= -c^2 + \\beta(a, \\psi)c + \\gamma(a, \\psi) \\\\\n    \\gamma &= (\\psi_{a, 1} + \\psi_{a, 2} + 18) / 2 \\\\\n    \\beta &= (\\psi_{a, 2} - \\gamma + 9) / 3\n    \\end{align}\n$$\n\n\n**Run an experiment**\n\nThe model is implemented as `ContinuousContextDiscreteTreatment` class, defined in `bayesian_simulators/continuous_scalar_context_discrete_treatment.py`.\n\n\nThe training loop is implemented in\n**`research_experiments/discrete_design_main.py`**. In addition to training loop (`train_net`), that script also:\n- defines a function to compute UCB designs: `compute_ucb_design`;\n- defines a function to calculate regret: `calculate_regret`. The \"real process\" is `ContinuousContextAndTreatment` with a fixed realisation of the parameters $\\psi$, drawn from the prior. It differentiates the REAL WORLD SIMULATOR from the model emulator by denoting the former in ALL CAPS;\n- defines `main` which defines a prior, sets up logging, runs all the above (train loop, regret evaluation), stores the outputs and creates some plots.\n\nTo run CO-BED:\n\n```\npython discrete_design_main.py \\\n    --seed-reps 3 \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --tau 5 \\\n    --optimise-design \\\n    --device <cpu/cuda>\n```\n\nTo run random baseline, simply remove the `optimise-design` flag:\n\n```\npython discrete_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --tau 5 \\\n    --device <cpu/cuda>\n```\n\nTo run UCB baseline, use `ucb-baseline` flag, setting to the appropriate level of $\\alpha$, e.g. 0.0 (or $1.0$ or $2.0$):\n\n```\npython discrete_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --tau 5 \\\n    --ucb-baseline 0.0 \\\n    --device <cpu/cuda>\n```\n\nTo run Thompson sampling baseling, use `thompson-sampling-baseline` flag:\n```\npython discrete_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --tau 5 \\\n    --thompson-sampling-baseline \\\n    --device <cpu/cuda>\n```\n\n----------------\n### **Experiment 2: Continuous treatments**\n--------------\n**MODEL** For the continuous treatment example we use the following model:\n\n$$\n \\text{Prior: }  \\quad  \\psi = (\\psi_1, \\psi_2, \\psi_3, \\psi_4 ), \\quad  \\psi_i \\sim \\text{Uniform}[0.1, 1.1] \\; \\text{iid}\n$$\n$$\n \\text{Likelihood: } \\quad y  | c, a, \\psi \\sim \\mathcal{N} (f(\\psi, a, c), \\sigma^2),\n$$\nwhere\n$$\n    f(\\psi, a, c) = \\exp\\left( -\\frac{\\big(a - g(\\psi, c)\\big)^2}{h(\\psi, c)} \\right) \\quad\n    g(\\psi, c) = \\psi_0 + \\psi_1 c + \\psi_2 c^2 \\quad\n    h(\\psi, c) = \\psi_3\n$$\n\n\n\n**Run an experiment**\n\nThe model is implemented as `ContinuousContextAndTreatment` class, defined in `bayesian_simulators/continuous_scalar_context_scalar_treatment.py`.\n\nThe training loop is implemented in\n**`research_experiments/continuous_design_main.py`**. In addition to training loop (`train_net`), that script also:\n- defines a function to compute UCB designs: `compute_ucb_design`;\n- defines a function to calculate regret: `calculate_regret`. The \"real process\" is `ContinuousContextAndTreatment` with a fixed realisation of the parameters $\\psi$, drawn from the prior.It differentiates the REAL WORLD SIMULATOR from the model emulator by denoting the former in ALL CAPS;\n- defines `main` which defines a prior, sets up logging, runs all the above (train loop, regret evaluation), stores the outputs and creates some plots.\n\nTo run CO-BED:\n\n```\npython continuous_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --optimise-design \\\n    --device <cpu/cuda>\n```\n\nTo run random baseline, simply remove the `optimise-design` flag:\n\n```\npython continuous_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --device <cpu/cuda>\n```\n\nTo run UCB baseline, use `ucb-baseline` flag, setting to the appropriate level of $\\alpha$, e.g. 0.0 (or $1.0$ or $2.0$):\n\n```\npython continuous_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --ucb-baseline 0.0 \\\n    --device <cpu/cuda>\n```\n\nTo run Thompson sampling baseling, use `thompson-sampling-baseline` flag:\n```\npython continuous_design_main.py \\\n    --batch-size 512 \\\n    --hidden-dim 512 \\\n    --encoding-dim 16 \\\n    --lr 0.001 \\\n    --gamma 0.9 \\\n    --num-jobs -1 \\\n    --num-steps 500 \\\n    --design-dim 10 \\\n    --seed-reps 5 \\\n    --num-true-models-to-sample 3 \\\n    --thompson-sampling-baseline \\\n    --device <cpu/cuda>\n```\n\nIf you want to learn a batch of $D$, e.g. `D=60` experiments modify `design_dim` accordingly. e.g:\n```\npython\ndesign_dim: 60\n```\n\n\n# Logging\n\nLogging is done with MlFlow, to start a local server, run\n\n    mlflow ui\n\nand navigate to the appropriate experiment to view all logged metrics.\n\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "co-bed", "org_name": "microsoft", "org_repo": "microsoft/co-bed", "platform_org_repo": "github+microsoft/co-bed", "link_to_repo": "https://github.com/microsoft/co-bed", "platform": "github", "language": "Python", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Apps Command Center / Enterprise Apps Accelerator\n\nDeveloped by the Microsoft GPS LATAM Team, the ACC Solution Accelerator provides a set of templates to help you accelerate your deployment process. The ACC Solution Accelerator is designed to help you build solutions that are secure, scalable, and reliable based on Microsoft WAF Principals.\n\n### Make your choice\n\n```mermaid\nflowchart TD\n    A[Start] --> B{Windows OS};\n    B -- Yes --> C{Does Windows App need console access?};\n    B -- No --> D{Does Linux App need console access?};\n    C -- Yes --> E[Enjoy Azure VM for Windows];\n    C -- No --> F{Code in Repo?};\n    D -- Yes --> G[Enjoy Azure VM for Linux];\n    D -- No --> H{Code in Repo?};\n    F -- Yes --> I[Enjoy App Service for Windows]\n    F -- No --> J[Enjoy Azure VM for Windows];\n    H -- No --> K[Enjoy Azure VM for Linux];\n    H -- Yes --> L{Do you need to support containers?};\n    L -- Yes --> M[Enjoy Azure Container Apps];\n    L -- No --> N[Enjoy App Service for Linux];\n```\n### This solution is open source. You can adapt the templates to create an architecture that meets your needs.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "acc", "org_name": "microsoft", "org_repo": "microsoft/acc", "platform_org_repo": "github+microsoft/acc", "link_to_repo": "https://github.com/microsoft/acc", "platform": "github", "language": "Bicep", "stargazers_count": 2, "watchers_count": 2}, {"README_text": "# Business Central Samples - Warehouse Helper\n\nThe Warehouse Helper sample consists of an AL extension, a Power App, and a Power Automate flow. The app can scan a barcode and attempt to map the value to a Business Central item's GTIN property.\n\nThe AL Extension adds relevant APIs to Business Central and includes a sample data page called **Warehouse Helper sample data**. The sample page has an action that adds GTIN values to a set of items that match the [Sample barcodes](https://github.com/microsoft/bcsamples-warehousehelper/blob/main/SampleBarCodes/Sample%20Barcodes.pdf).\n\n## Try the sample yourself\n\nFollow the steps in the [Try our sample apps](https://github.com/microsoft/AL-Go/blob/PPPreview/Scenarios/TryPowerPlatformSamples.md) guide to try the sample with your own tenant.\n\n## Per Tenant Extension Project\n\nThis repository is based on the AL-Go for GitHub PTE template, which is available [here](https://github.com/microsoft/AL-Go-PTE).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow.\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bcsamples-warehousehelper", "org_name": "microsoft", "org_repo": "microsoft/bcsamples-warehousehelper", "platform_org_repo": "github+microsoft/bcsamples-warehousehelper", "link_to_repo": "https://github.com/microsoft/bcsamples-warehousehelper", "platform": "github", "language": "AL", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Business Central Samples - Take Order\n\nThe **Take Order** sample demonstrates how you can build a customized user experience for servers at restaurants, using Power Apps powered by Business Central.\n\nThe app allows servers to seat guests at a table and provides an intuitive UI for easily taking their orders. The sample includes a Business Central extension, a Power App, and a Power Automate flow. The extension adds the appropriate APIs and extends the items and customer tables with the following fields:\n\n- **Items**\n  - *SoldInRestaurant*: Indicates if the item should appear in the app\n  - *LongItemDescription* and *AllergenInformation*: Optional fields that the app can utilize\n\n- **Customers**\n  - *IsTable*: Indicates that the customer should appear as a table within the app\n\nThe extension also includes a sample data page called **Take Order sample data**, which contains an action to generate sample items and tables for the app to use. \n\n*Note: The sample data generator has only been tested with US versions of Business Central. Adjustments may be necessary for other locales.*\n\n## Try the sample yourself\n\nFollow the steps in the [Try our sample apps](https://github.com/microsoft/AL-Go/blob/PPPreview/Scenarios/TryPowerPlatformSamples.md) guide to try the sample with your own tenant.\n\n## Per Tenant Extension Project\n\nThis repository is based on the AL-Go for GitHub PTE template, which is available [here](https://github.com/microsoft/AL-Go-PTE).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow.\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "bcsamples-takeorder", "org_name": "microsoft", "org_repo": "microsoft/bcsamples-takeorder", "platform_org_repo": "github+microsoft/bcsamples-takeorder", "link_to_repo": "https://github.com/microsoft/bcsamples-takeorder", "platform": "github", "language": "AL", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# [PREVIEW] AL-Go Template\n\n## Per Tenant Extension Project with Power Platform support\nThis template repository can be used for managing Per Tenant Extensions for Business Central.\n\nIt is the PPPreview version of https://github.com/microsoft/AL-Go-PTE.\n\nPlease consult https://github.com/microsoft/AL-Go/#readme for scenarios on usage\n", "repo_name": "AL-Go-PTE-PPPreview", "org_name": "microsoft", "org_repo": "microsoft/AL-Go-PTE-PPPreview", "platform_org_repo": "github+microsoft/AL-Go-PTE-PPPreview", "link_to_repo": "https://github.com/microsoft/AL-Go-PTE-PPPreview", "platform": "github", "language": "PowerShell", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# [PREVIEW] AL-Go Template\n\n## Per Tenant Extension Project\nThis template repository can be used for managing Per Tenant Extensions for Business Central.\n\nIt is the preview version of https://github.com/microsoft/AL-Go-PTE.\n\nPlease consult https://github.com/microsoft/AL-Go/#readme for scenarios on usage\n", "repo_name": "AL-Go-PTE-Preview", "org_name": "microsoft", "org_repo": "microsoft/AL-Go-PTE-Preview", "platform_org_repo": "github+microsoft/AL-Go-PTE-Preview", "link_to_repo": "https://github.com/microsoft/AL-Go-PTE-Preview", "platform": "github", "language": "PowerShell", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# [PREVIEW] AL-Go Template\n\n## AppSource App Project\nThis template repository can be used for managing AppSource Apps for Business Central.\n\nIt is the preview version of https://github.com/microsoft/AL-Go-AppSource.\n\nPlease consult https://github.com/microsoft/AL-Go/#readme for scenarios on usage\n", "repo_name": "AL-Go-AppSource-Preview", "org_name": "microsoft", "org_repo": "microsoft/AL-Go-AppSource-Preview", "platform_org_repo": "github+microsoft/AL-Go-AppSource-Preview", "link_to_repo": "https://github.com/microsoft/AL-Go-AppSource-Preview", "platform": "github", "language": "PowerShell", "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Our Pride is open source\n\nLast year, we open-sourced our [Pride flag design](https://github.com/microsoft/Pride-flag) representing 40 LGBTQIA+ communities. Our latest update reflects almost 50 identities and comes with new layouts. Plus, we are open-sourcing the entire Pride design and invite you to remix, share and make more Pride.\n\nLearn more about [Pride at Microsoft](https://unlocked.microsoft.com/pride/)\n\n![FY23Pride-Hero_MakePride_OpenSource](https://github.com/microsoft/Pride/assets/113071293/9c5595fc-4e8b-4317-989a-bac74067ac17)\n\n# An emblem of unity\n\nCreated by the LGBTQIA+ people at Microsoft together with our creative partners, this Pride expression celebrates the nuance, intersectionality, and strength of LGBTQIA+ communities by bringing together almost 50 community flags in immersive and evocative designs. The hand-drawn brush strokes and sketch textures that make up LGBTQIA+ flags are woven together in a variety of compositions that are designed to ingite imagination and inspire action towards more LGBTQIA+ equity and equality.\n\nThe following flags are represented in the graphics: A-spec, Abrosexual, Aceflux, Achillean, Agender, Ally, Ambiamorous, Androgynous, Aroace, Aroflux, Abrosexual, Aceflux, Achillean, Agender, Ally, Ambiamorous, Androgynous, Aroace, Aroflux, Aromantic, Asexual, A-spec, Bigender, Bisexual, Demiboy, Demifluid, Demigender, Demigirl, Demiromantic, Demisexual, Diamoric, Gay / MLM, Gender questioning, Genderfluid, Genderflux, Genderqueer, Graysexual, Intersex, Lesbian, Maverique, Multigender, Multisexual, Neutrois, Nonbinary, Omnisexual, Pangender, Pansexual, Polyamorous, Polysexual, Pride, Queer, Sapphic, Transfeminine, Transgender, Transmasculine, Transneutral, Trigender, Two Spirits and Unlabeled.\n\n![overlap2 - no white](https://github.com/microsoft/Pride/assets/113071293/288b6b9a-1144-46a1-99c7-1f1a52b3461f)\n\n## Expansive and flexible\nOur Pride design system is built for you to create almost infinite compositions and layouts. With a custom Microsoft Pride font, almost 50 individual flag graphics, ready-made hero assets, and guides on crafting your own images - your imagination is your only limit. Make your own work with open source files and ready-made assets. \n\nExplore assets on [Figma](https://www.figma.com/community/file/1158808367098375909), plus you can create clips and videos with these desings availaible on [Clipchamp](https://app.clipchamp.com/login).\n\n![guidance - no white](https://github.com/microsoft/Pride/assets/113071293/c5ed1430-43e5-4d11-bf08-393e01d78aeb)\n\n## Ever growing and evolving\n\nJune 1st marks our first drop, and we\u2019ll be adding more assets and guidance over the coming weeks. Specifically, ready-made localized assets in over 20 languages, plus motion files. Follow for updates.\n\n![Picture10](https://github.com/microsoft/Pride/assets/113071293/c598f777-5883-4493-8beb-e397e93d977d)\n\n\n## Make it yours\n\nWe welcome your contributions to this project! Please note that most contributions require you to agree to a Contribution License Agreement (CLA.) Visit https://cla.opensource.microsoft.com.\n\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\n\n## License\n\nMicrosoft and any contributors grant you a license to content in this repository under the Creative Commons BY-NC-SA 4.0, see the LICENSE file. Privacy information can be found at https://privacy.microsoft.com/en-us/. Progress Pride Flag \u00a9quasar.digital LLC 2022. Polyamory Pride Flag by Molly Colleen Bennett Wilvich.\n\n![Picture8](https://github.com/microsoft/Pride/assets/113071293/17eebc44-ef33-463a-a428-054fd8ceea0f)\n", "repo_name": "Pride", "org_name": "microsoft", "org_repo": "microsoft/Pride", "platform_org_repo": "github+microsoft/Pride", "link_to_repo": "https://github.com/microsoft/Pride", "platform": "github", "language": null, "stargazers_count": 22, "watchers_count": 22}, {"README_text": "# cpp-async\n\nC++ 20 contains the core language support needed to make coroutines (async/await) possible, but it does not provide the\nrelated types needed to write an actual coroutine or functions to consume coroutines in common ways. This repository\nprovides support types and functions to fill that gap, until such capabilities are made available as part of the C++\nStandard Library.\n\n# awaitable_get()\n\nThis function allows blocking the calling thread until an awaitable completes and returns the awaitable's value (or\nvoid, as appropriate). It works with any awaitable type.\n\nExample usage:\n```c++\ninline /*<some_awaitable_type>*/ run_async() { co_return; }\n\nint main()\n{\n    async::awaitable_get(run_async());\n}\n```\n\n```c++\ninline /*<some_awaitable_type>*/ read_file_async()\n{\n    co_await /* some awaitable object */;\n    co_return std::string{ \"file contents\" };\n}\n\nint main()\n{\n    std::string text{ async::awaitable_get(read_file_async()) };\n    printf(\"%s\\n\", text.c_str());\n}\n```\n\n# awaitable_then()\n\nThis function allows scheduling a single continuation to run when an awaitable completes. It provides a single argument\nto the continuation, which, when invoked, returns the awaitable's (or void, as appropriate) if the awaitable succeeded\nor throws if the awaitable failed. It works with any awaitable type.\n\nIf the provided continuation throws, await_then terminates the process. (A caller can avoid this behavior by surrounding\nthe continuation in a try/catch block.)\n\nExample usage:\n```c++\ninline /*<some_awaitable_type>*/ read_file_async()\n{\n    co_await /* some awaitable object */;\n    co_return std::string{ \"file contents\" };\n}\n\nint main()\n{\n    async::event_signal done{};\n    async::awaitable_then(read_file_async(), [&done](awaitable_result<std::string> result)\n        {\n            printf(\"%s\\n\", result().c_str());\n            done.set();\n        });\n    done.wait();\n}\n```\n\n```c++\ninline /*<some_awaitable_type>*/ read_file_async()\n{\n    co_await /* some awaitable object */;\n    co_return std::string{ \"file contents\" };\n}\n\nstd::future<std::string> read_file_future()\n{\n    std::shared_ptr<std::promise<std::string>> promise{ std::make_shared<std::promise<std::string>>() };\n    async::awaitable_then(read_file_async(), [promise](awaitable_result<std::string> result)\n        {\n            try\n            {\n                promise->set_value(result());\n            }\n            catch (...)\n            {\n                promise->set_exception(std::current_exception());\n            }\n        });\n    return promise->get_future();\n}\n\nint main()\n{\n    printf(\"%s\\n\", read_file_future().get().c_str());\n}\n```\n\n# task<T>\n\nThis type is a coroutine return type; it allows writing a function as a coroutine (calling co_await/co_return).\n\ntask<T> supports the following return types:\n* T is void\n* T is a move-only type\n* T is a type that does not have a default constructor\n* T is a reference type\n\nA caller may only co_await a task<T> once, and the result of the task is moved out when returning from co_await.\n\nWhen working with the task<T> directly (rather than only passing it directly to co_await), a caller may cancel execution\nof any code that would resume after the task completes by destructing the task. (Destructing the task does not stop the\ntask's coroutine from running, just any continuation that would run after the task's coroutine completes.) As a result,\nan exception thrown from a task will be ignored if no caller consumers the task's result. (A caller can avoid this\nbehavior by consuming the task's result and handling the exception differently; for example by co_awaiting the task from\nanother coroutine and calling std::terminate when it throws.)\n\nInternally, task<T> destroys the coroutine frame as soon as the coroutine completes; it does not wait until the task has\ndestructed (which would only be after executing any code that will run next). Both options here involve tradeoffs; the\nalternative would avoid the need for an internal heap allocation when creating the task at the expense of keeping\n(potentially large) coroutine frame memory in use longer.\n\nExample usage:\n```c++\ninline async::task<void> do_async()\n{\n    /* do some work */\n    co_await /* some awaitable object */;\n    /* do some more work */\n    co_return;\n}\n```\n\n```c++\ninline async::task<std::string> read_async()\n{\n    co_await /* some awaitable object */;\n    co_return std::string{ \"contents\" };\n}\n```\n\n```c++\ninline async::task<std::unique_ptr<std::string>> return_move_only_async()\n{\n    co_return std::make_unique<std::string>(\"contents\");\n}\n\ninline async::task<void> use_move_only_async()\n{\n    std::unique_ptr<std::string> text{ co_await return_move_only_async() };\n    printf(\"%s\\n\", text->c_str());\n}\n```\n\n```c++\ninline async::task<int&> return_reference_type_async(int& a, int& b)\n{\n    int& larger{ a > b ? a : b };\n    co_return larger;\n}\n\ninline async::task<void> use_reference_type_async()\n{\n    int first{ 3 };\n    int second{ 6 };\n    const int& larger{ co_await return_reference_type_async(first, second) };\n    ++second;\n    printf(\"%i\\n\", larger); // 7\n}\n```\n\n```c++\ninline async::task<void> fire_and_forget()\n{\n    /* do some work */\n\n    co_await /* some awaitable object */;\n\n    if (/* something bad happens*/)\n    {\n        throw std::runtime_error{ \"fire_and_forget failed\" };\n    }\n\n    co_return;\n}\n\ninline async::task<void> fire_and_forget_except_crash_on_failure()\n{\n    try\n    {\n        co_await fire_and_forget();\n    }\n    catch (...)\n    {\n        std::terminate();\n    }\n}\n\nint main()\n{\n    {\n        // uncomment one of the next lines (to ignore or crash on exception):\n        //fire_and_forget();\n        fire_and_forget_except_crash_on_failure();\n    }\n\n    std::this_thread::sleep_for(std::chrono::seconds{ 1 });\n}\n```\n\n# task_completion_source<T>\n\nThis type controls a task and allows it to be returned and completed separately. Is is designed for producing an\nawaitable type from a function that is not itself a coroutine.\n\nThe design of this type is intentionally similar to TaskCompletionSource<TResult> in .NET.\n\nExample usage:\n```c++\nasync::task<int> compute_async(std::thread& callbackThread)\n{\n    std::shared_ptr<async::task_completion_source<int>> promise{\n        std::make_shared<async::task_completion_source<int>>() };\n    // Warning: without the sleep below, the may run and be destroyed before it is assigned to callbackThread.\n    // Capture the task_completion_source shared_ptr by value, or it will get destructed before the callbackThread runs.\n    callbackThread = std::thread{ [promise]()\n        {\n            std::this_thread::sleep_for(std::chrono::seconds{ 1 });\n            promise->set_value(123);\n        }\n    };\n    printf(\"Returning the task now; will complete it later... \");\n    return promise->task();\n}\n\nint main()\n{\n    std::thread callbackThread{};\n    printf(\"[completed]\\nThe task returned %i\\n\", async::awaitable_get(compute_async(callbackThread)));\n    callbackThread.join();\n}\n```\n\n# to_future()\n\nThis function produces a std::future<T> for an awaitable; it downgrades a C++ 20 awaitable/coroutine to a C++ 11\nstd::future<T> (for use with legacy code).\n\nThis function works with any awaitable type whose return value type can be used with std::promise<T>. Note that types\nwithout default constructors are not supported for std::promise<T>\n\nExample usage:\n```c++\ninline /*<some_awaitable_type>*/ read_file_async()\n{\n    co_await /* some awaitable object */;\n    co_return std::string{ \"file contents\" };\n}\n\n// Convert to legacy C++ 11 async; for example, if needed to defer refactoring the caller.\ninline std::future<std::string> read_file_future()\n{\n    return async::to_future(read_file_async());\n}\n\nint main()\n{\n    std::string text{ read_file_future().get() };\n    printf(\"%s\\n\", text.c_str());\n}\n```\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "cpp-async", "org_name": "microsoft", "org_repo": "microsoft/cpp-async", "platform_org_repo": "github+microsoft/cpp-async", "link_to_repo": "https://github.com/microsoft/cpp-async", "platform": "github", "language": "C++", "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Fluent UI contribution packages \ud83d\udcaa\n\nThis repository hosts packages that are published under the `@fluentui-contrib/` scope. Contributor packages\nare extensions of [Fluent UI](https://github.com/microsoft/fluentui).\n\n| Controls | README                                                                                             | Docs                                                                 | NPM                                                                                                                                                       |\n|----------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Chat     | [README.md](https://github.com/microsoft/fluentui-contrib/blob/main/packages/react-chat/README.md) | [Storybook](https://microsoft.github.io/fluentui-contrib/react-chat) | [![npm version](https://img.shields.io/npm/v/@fluentui-contrib/react-chat?style=flat-square)](https://www.npmjs.com/package/@fluentui-contrib/react-chat) |\n\n## [Contributing](./Contributing.md)\n\nPlease read the [CONTRIBUTING.md file](./CONTRIBUTING.md) for instructions on how to onboard to the repository and\nrun basic tasks.\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "fluentui-contrib", "org_name": "microsoft", "org_repo": "microsoft/fluentui-contrib", "platform_org_repo": "github+microsoft/fluentui-contrib", "link_to_repo": "https://github.com/microsoft/fluentui-contrib", "platform": "github", "language": "TypeScript", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "dataActivator", "org_name": "microsoft", "org_repo": "microsoft/dataActivator", "platform_org_repo": "github+microsoft/dataActivator", "link_to_repo": "https://github.com/microsoft/dataActivator", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Differentially Private Synthetic Data via Foundation Model APIs\n\nThis repo is a Python library to **generate differentially private (DP) synthetic data without the need of any ML model training**. It is based on the following papers that proposed a new DP synthetic data framework that only utilizes the blackbox inference APIs of foundation models (e.g., Stable Diffusion).\n\n* Differentially Private Synthetic Data via Foundation Model APIs 1: Images  \n\t[[paper (arxiv)](https://arxiv.org/abs/2305.15560)]  \n    **Authors:** [[Zinan Lin](https://zinanlin.me/)], [[Sivakanth Gopi](https://www.microsoft.com/en-us/research/people/sigopi/)], [[Janardhan Kulkarni](https://www.microsoft.com/en-us/research/people/jakul/)], [[Harsha Nori](https://www.microsoft.com/en-us/research/people/hanori/)], [[Sergey Yekhanin](http://www.yekhanin.org/)]\n\n\n#### Potential Use Cases\nGiven a private dataset, this tool can generate a new DP synthetic dataset that is statistically similar to the private dataset, while ensuring a rigorous privacy guarantee called Differential Privacy. The DP synthetic dataset can replace real data in various use cases where privacy is a concern, for example:\n* Sharing them with other parties for collaboration and research.\n* Using them in downstream algorithms (e.g., training ML models) in the normal non-private pipeline.\n* Inspecting the data directly for easier product debugging or development.\n\n\n#### Supported Data Types\nThis repo currently supports the following data types and foundation models.\n\n| Foundation Model APIs | Data Type | Size of Generated Images (`--image_size`) |\n|--------|--------|--------|\n|    [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview) |   Images  | Preferably 512x512 |\n|    [improved diffusion](https://github.com/openai/improved-diffusion)    |   Images | 64x64 |\n|    [DALLE2](https://platform.openai.com/docs/api-reference/images)    |    Images     | 256x256, 512x512, or 1024x1024 |\n\n\n\n## Quick Examples\n\nSee the [docker file](docker/Dockerfile) for the environment.\n\n#### CIFAR10 Images\n```sh\npushd data; python get_cifar10.py; popd  # Download CIFAR10 dataset\npushd models; ./get_models.sh; popd  # Download the pre-trained improved diffusion model\n./scripts/main_improved_diffusion_cifar10_conditional.sh  # Run DP generation\n```\n\n#### Camelyon17 Images\n```sh\npushd data; python get_camelyon17.py; popd  # Download Camelyon17 dataset\npushd models; ./get_models.sh; popd  # Download the pre-trained improved diffusion model\n./scripts/main_improved_diffusion_camelyon17_conditional.sh  # Run DP generation\n```\n\n\n#### Cat Images\n\n* Download the dataset from [https://www.kaggle.com/datasets/fjxmlzn/cat-cookie-doudou](https://www.kaggle.com/datasets/fjxmlzn/cat-cookie-doudou), and put them under `data/cookie` and `data/doudou`.\n* For Cat Cookie:\n```\n./scripts/main_stable_diffusion_cookie.sh  # Run DP generation\n```\n* For Cat Doudou:\n```sh\n./scripts/main_stable_diffusion_doudou.sh  # Run DP generation\n``` \n\nSee [scripts folder](scripts) for more examples.\n\n\n## Detailed Usage\n\n`main.py` is the main script for generation. Please refer to `python main.py --help` for detailed descriptions of the arguments. \n\nFor each foundation model API (e.g., Stable Diffusion, improved diffusion), there could be more arguments. Please use `--api_help` argument, e.g., `python main.py --api stable_diffusion --data_folder data --api_help`, to see detailed descrptions of the API-specific arguments.\n\nSee Appendices H, I, J of the [paper](https://arxiv.org/abs/2305.15560) for examples/guidelines of parameter selection.\n\n## Generate DP Synthetic Data for Your Own Dataset\nPlease put all images in a folder (which can contain any nested folder structure), and the naming of the image files should be `<class label without '_' character>_<the remaining part of the filename>.<jpg/jpeg/png/gif>`. Pass the path of this folder to `--data_folder` argument.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Responsible Uses\n\nThis project uses foundation model APIs to create [synthetic image data](https://en.wikipedia.org/wiki/Synthetic_data) with [differential privacy](https://en.wikipedia.org/wiki/Differential_privacy) guarantees. Differential privacy (DP) is a formal framework that ensures the output of an algorithm does not reveal too much information about its inputs. Without a formal privacy guarantee, a synthetic data generation algorithm may inadvertently reveal sensitive information about its input datapoints.\n\nUsing synthetic data in downstream applications can carry risk. Synthetic data may not always reflect the true data distribution, and can cause harms in downstream applications. Both the dataset and algorithms behind the foundation model APIs may contain various types of bias, leading to potential allocation, representation, and quality-of-service harms. Additionally, privacy violations can still occur if the \u03b5 and \u03b4 privacy parameters are set inappropriately, or if multiple copies of a sample exist in the seed dataset. It is important to consider these factors carefully before any potential deployments.  \n", "repo_name": "DPSDA", "org_name": "microsoft", "org_repo": "microsoft/DPSDA", "platform_org_repo": "github+microsoft/DPSDA", "link_to_repo": "https://github.com/microsoft/DPSDA", "platform": "github", "language": "Python", "stargazers_count": 11, "watchers_count": 11}, {"README_text": "\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.\n\nPrivacy information can be found at https://privacy.microsoft.com/en-us/\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel or otherwise.\n", "repo_name": "taser", "org_name": "microsoft", "org_repo": "microsoft/taser", "platform_org_repo": "github+microsoft/taser", "link_to_repo": "https://github.com/microsoft/taser", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Podcast Copilot\n\nThis code was demonstrated at the Build 2023 keynote by Microsoft CTO Kevin Scott, illustrating the architecture of a Copilot.  \n\nKevin Scott hosts a podcast, [Behind the Tech](https://www.microsoft.com/behind-the-tech).  This Podcast Copilot makes it easier to generate a social media post promoting a new episode of the podcast, when given the audio file for the podcast.  The Podcast Copilot uses a series of machine learning models orchestrated by LangChain to do this:\n+ Given the podcast audio file, the Whisper model performs speech-to-text to generate a transcript of the podcast.  \n+ Given this transcript, the Dolly 2 model extracts the name of the guest on the podcast.  \n+ Given the guest name, the Bing Search Grounding API retrieves a bio for the guest from the internet.  \n+ Given the transcript and guest's bio, the GPT-4 model generates a social media post promoting the podcast episode.  \n+ Given the social media post, we use GPT-4 to create a relevant DALL-E prompt. \n+ Given that DALL-E prompt, the DALL-E model generates a corresponding image for the post.  \n+ Finally, the user has an opportunity to review the content before posting, and if approved, a LinkedIn plugin will post the social media copy and image to LinkedIn.  \n\n![Diagram of the data flow and chain of machine learning models described above](./images/PodcastCopilotDataFlow.png)\n\nFor the demo, we ran Whisper and Dolly 2 locally.  The Bing Search Grounding API is available on Azure.  We used model deployments of GPT-4, DALL-E 2, and a plugins-capable model on the Azure OpenAI service.  \n\nPlease note that as of Build (May 2023):\n+ The DALL-E models are still in private preview. For the DALL-E model, you must request access using the form at https://aka.ms/oai/access and in question #22, request access to the DALL-E models for image generation.\n+ The plugins-capable models are not publicly released yet.\n\n## Setup\n\nThis project requires creating an Azure OpenAI resource to run several cloud-based models.  \n+ You can request access to Azure OpenAI at https://aka.ms/oai/access.  \n+ After approval, create an Azure OpenAI resource at https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI following the instructions at https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource.  \n+ You will need to create model deployments of the following models: gpt-4, dalle, and a plugins-capable model.  Follow the instructions [here](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource#deploy-a-model).  \n\nYou will also need to create a Bing search resource at https://portal.azure.com/#create/Microsoft.BingSearch.  \n\nNext, update the PodcastSocialMediaCopilot.py file with your settings.  \n+ Update **bing_subscription_key** with the API key of your Bing resource on Azure.  \n+ Update **openai_api_base** with the name of your Azure OpenAI resource; this value should look like this: \"https://YOUR_AOAI_RESOURCE_NAME.openai.azure.com/\"\n+ Update **openai_api_key** with the corresponding API key for your Azure OpenAI resource.  \n+ Update **gpt4_deployment_name** with the name of your model deployment for GPT-4 in your Azure OpenAI resource.  \n+ If your model deployments for gpt-4, dalle, and the plugins-capable model are all on the same Azure OpenAI resource, you're all set!  If not, you can override the individual endpoints and keys for the resources for the various model deployments using the variables **gpt4_endpoint**, **gpt4_api_key**, **dalle_endpoint**, **dalle_api_key**, **plugin_model_url**, and **plugin_model_api_key**.  \n+ Optionally, you can also update the **podcast_url** and **podcast_audio_file** to reflect your own podcast.  \n\nFinally, set up your environment and run the code using the following commands:\n```\npip install -r requirements.txt\npython PodcastSocialMediaCopilot.py\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PodcastCopilot", "org_name": "microsoft", "org_repo": "microsoft/PodcastCopilot", "platform_org_repo": "github+microsoft/PodcastCopilot", "link_to_repo": "https://github.com/microsoft/PodcastCopilot", "platform": "github", "language": "Python", "stargazers_count": 502, "watchers_count": 502}, {"README_text": "# \ud83d\udc27Pengi: An Audio Language Model for Audio Tasks\n[[`Paper`](https://arxiv.org/abs/2305.11834)] [`Checkpoints`]\n\nPengi is an Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions.\n![image](https://github.com/microsoft/Pengi/assets/28994673/abc714fb-cee3-4253-a753-0db4bd122144)\n\n## Coming soon\nCode to be updated after review.\n\n## Citation\n```BibTeX\n@inproceedings{Pengi,\n  title={Pengi: An Audio Language Model for Audio Tasks},\n  author={Soham Deshmukh and Benjamin Elizalde and Rita Singh and Huaming Wang},\n  journal={arXiv preprint arXiv:2305.11834},\n  year={2023}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Pengi", "org_name": "microsoft", "org_repo": "microsoft/Pengi", "platform_org_repo": "github+microsoft/Pengi", "link_to_repo": "https://github.com/microsoft/Pengi", "platform": "github", "language": null, "stargazers_count": 114, "watchers_count": 114}, {"README_text": "# LLaVA-Med: Large Language and Vision Assistant for BioMedicine\n\n*Visual instruction tuning towards buiding large language and vision models with GPT-4 level capabilities in the biomedicine space.*\n\n[[Paper](https://arxiv.org/abs/2306.00890)] \n\n<!-- [[Data](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)] [[Model](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0)] -->\n\n**LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day** <br>\n\n[Chunyuan Li*](https://chunyuan.li/), [Cliff Wong*](https://scholar.google.com/citations?user=Sl05ifcAAAAJ&hl=en), [Sheng Zhang*](https://scholar.google.com/citations?user=-LVEXQ8AAAAJ&hl=en), [Naoto Usuyama](https://www.microsoft.com/en-us/research/people/naotous/), [Haotian Liu](https://hliu.cc), [Jianwei Yang](https://jwyang.github.io/), [Tristan Naumann](https://scholar.google.com/citations?user=cjlSeqwAAAAJ&hl=en), [Hoifung Poon](https://scholar.google.com/citations?user=yqqmVbkAAAAJ&hl=en), [Jianfeng Gao](https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en) (*Equal Contribution)\n\n<p align=\"center\">\n    <img src=\"images/llava_med_logo.png\" width=\"50%\"> <br>\n \n  *Generated by  <a href=\"https://gligen.github.io/\">GLIGEN</a>  using the grounded inpainting mode, with three boxes: ``white doctor coat``, ``stethoscope``, ``white doctor hat with a red cross sign``.*\n \n</p>\n\n## Release\n\n- [June 1] \ud83d\udd25 We released **LLaVA-Med: Large Language and Vision Assistant for Biomedicine**, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities.  Checkout the [paper](https://arxiv.org/abs/2306.00890)\n\n<p align=\"center\">\n    <img src=\"images/llava_med_pipeline.png\" width=\"90%\"> <br>\n \n  *LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained in a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-tuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks.*\n</p>\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)\n**Usage and License Notices**: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\n\n\n## LLaVA-Med Dataset\n\n<p align=\"center\">\n    <img src=\"images/llava_med_dataset.png\" width=\"90%\"> <br>\n \n  *The data statistics of biomedical multimodal instruction-following data: (a,b) The root verb-noun pairs of instruction and responses, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) The distribution of images and QA pairs on the five domains, one image is shown per domain.*\n</p>\n\n## LLaVA-Med Performance\n\n<p align=\"center\">\n    <img src=\"images/llava_med_chat.png\" width=\"90%\"> <br>\n \n  *Performance comparison of mulitmodal chat instruction-following abilities, measured by the relative score via language GPT-4 evaluation.*\n</p>\n\n\n<p align=\"center\">\n    <img src=\"images/llava_med_chat_example1.png\" width=\"90%\"> <br>\n \n  *Example 1: comparison of medical visual chat. The language-only GPT-4 is considered as the performance upper bound, as the golden captions and inline mentions are fed into GPT-4 as the context, without requiring the model to understand the raw image.*\n</p>\n\n<p align=\"center\">\n    <img src=\"images/llava_med_chat_example2.png\" width=\"90%\"> <br>\n \n  *Example 2: comparison of medical visual chat. LLaVA tends to halluciate or refuse to provide domain-specific knowledgable response.*\n</p>\n\n\n<p align=\"center\">\n    <img src=\"images/llava_med_vqa.png\" width=\"90%\"> <br>\n \n  *Performance comparison of fine-tuned LLaVA-Med on established Medical QVA datasets.*\n</p>\n\n## Related Projects\n\n- [LLaVA](https://llava-vl.github.io/)\n- [BioMed CLIP](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224)\n- [Instruction Tuning with GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\n\n\n\n", "repo_name": "LLaVA-Med", "org_name": "microsoft", "org_repo": "microsoft/LLaVA-Med", "platform_org_repo": "github+microsoft/LLaVA-Med", "link_to_repo": "https://github.com/microsoft/LLaVA-Med", "platform": "github", "language": null, "stargazers_count": 122, "watchers_count": 122}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "lowcodeplugins", "org_name": "microsoft", "org_repo": "microsoft/lowcodeplugins", "platform_org_repo": "github+microsoft/lowcodeplugins", "link_to_repo": "https://github.com/microsoft/lowcodeplugins", "platform": "github", "language": null, "stargazers_count": 11, "watchers_count": 11}, {"README_text": "# Woodgrove groceries demo of the claims augmentation REST API\n\nThis dotnet C# Web API demonstrates how to use Microsoft Entra External ID's custom authentication extension for various events. \n\n## Endpoints\n\nThe sample code provides an implementation of the following endpoints:\n\n### Token issuance start\n\nThe *TokenIssuanceStart* event is triggered when a token is about to be issued by Microsoft Entra External ID to your application. When the event is triggered your custom extension REST API is called to fetch attributes from external systems. In this demo, the [TokenIssuanceStartController](./Controllers/TokenIssuanceStartController.cs) returns the following claims:\n\n- **CorrelationId** the correlation ID that was sent by the issuer to your REST API.\n- **ApiVersion** a fixed value with your REST API version. This attribute can help you debug your REST API and check if your latest version is in used.\n- **LoyaltyNumber** a random numeric value that represents an imaginary loyally number.\n- **LoyaltySince** a random date that the that represents an imaginary time the user joined the loyalty program.\n- **LoyaltyTier** a random string that the that represents an imaginary loyalty program tier.\n\nThe REST API endpoint URL:\n\n```http\nPOST https://api.wggdemo.net/TokenIssuanceStart\n```\n\n### On attribute collection start\n\nThe *OnAttributeCollectionStart* is fired at the beginning of the attribute collection process and can be used to prevent the user from signing up (such as based on the domain they are authenticating from) or modify the initial attributes to be collected (such as including additional attributes to collect based on the user\u2019s identity provider).\n\n> [!IMPORTANT]\n> The OnAttributeCollectionStart event type is not available yet.\n\n### On attribute collection submit\n\nOnAttributeCollectionSubmit event is fired after the user provides attribute information during signing up and can be used to validate the information provided by the user (such as an invitation code or partner number), modify the collected attributes (such as address validation), and either allow the user to continue in the journey or show a validation or block page.\n\n> [!IMPORTANT]\n> The OnAttributeCollectionSubmit event type is subject be to changed. Don't use it in your Microsoft Entra External ID tenant. \n\nThis demo validates the city name, against a list of cities and countries we compiled. You can find the list of countries and cities in the [OnAttributeCollectionSubmitController](./Controllers/OnAttributeCollectionSubmitController.cs). \n\nThe REST API endpoint URL:\n\n```http\nPOST https://api.wggdemo.net/OnAttributeCollectionSubmit\n```\n\n## Protect access to your REST API\n\nTo ensure the communications between Microsoft Entra custom extension and your REST API are secured appropriately, Microsoft Entra External ID uses OAuth 2.0 client credentials grant flow to issue an access token for the resource application registered with your custom authentication extension. \n\nWhen the custom extension calls your REST API, it sends an HTTP Authorization header with a bearer token issued by Azure AD. You REST API validate the access token and its claims values. \n\n### [Option 1] Validate the access token in your code\n\nThis example uses the [Microsoft.Identity.Web](https://www.nuget.org/packages/Microsoft.Identity.Web) library to validate the access token.\n\nIn the [appsettings.json](./appsettings.json) file, update the following keys under the `AzureAd` element:\n\n- **ClientId** the application ID that is associated with your custom extension. You can find this application under the API authentication in your custom extension.\n- **Audience** same as above\n- **TenantId** your tenant ID\n\nThis demo REST API can be used without authentication (see option 2 below). If you run your own REST API, uncomment the `[Authorize]` attribute in the controllers. The following example shows how a controller should look like:\n\n```csharp\n[Authorize]\n[ApiController]\n[Route(\"[controller]\")]\npublic class TokenIssuanceStartController : ControllerBase\n{\n    // Rest of your code\n}\n```\n\n### [Option 2] Validate the access token via Azure Service App\n\n[Azure App Service](https://learn.microsoft.com/azure/app-service/) enables you to build and host web apps and and RESTful APIs in the programming language of your choice without managing the infrastructure.\n\nAzure App Service provides built-in [authentication and authorization capabilities](https://learn.microsoft.com/azure/app-service/overview-authentication-authorization) (sometimes referred to as \"Easy Auth\"), so you can validate the access token sends by Microsoft Entra External ID by writing minimal code in RESTful API.\n\nTo enable authentication into your App Service app, follow these steps:\n\n1. Sign in to the [Azure portal](https://portal.azure.com/) and navigate to the app service hosting your web API.\n1. From the left navigation, select **Authentication** > **Add identity provider** > **Microsoft**.\n1. For **App registration type**, choose **Provide the details of an existing app registration** \n1. Fill in the following configuration details:\n\n    |Field|Description|\n    |-|-|\n    |Application (client) ID| The application ID that is associated with your custom extension. You can find this application under the API authentication in your custom extension. |\n    |Client Secret| Enter any value, such as 12345. |\n    |Issuer Url| Use `https://login.microsoftonline.com/<tenant-id>/v2.0`, and replace the *\\<tenant-id>* with the **Directory (tenant) ID** in which the app registration was created. |\n    |Allowed Token Audiences| Use the same value as the *Application (client) ID*. |\n    \n1. For the **Restrict access**, select **Require authentication**.\n1. For the **Unauthenticated requests**, select **HTTP 401 Unauthorized: recommended for APIs**.\n1. Unselect the **Token store** option.\n1. Select **Add**.\n\nYou're now ready to use the Microsoft identity platform for authentication in your app. The App Service makes the claims in the incoming token available to your code by injecting them into the `X-MS-CLIENT-PRINCIPAL` request header (Base64 encoded JSON representation of available claims). \n\nTo ensure the communications between the custom extension and your REST API are [secured appropriately](https://learn.microsoft.com/azure/active-directory/develop/custom-extension-overview#protect-your-rest-api), validate that the respective `azp` claim equals the `99045fe1-7639-4a75-9d4a-577b6ca3810f` value.\n\nIn your REST API use the code in the [AzureAppServiceClaims class](./Models/AzureAppServiceClaims.cs). Then, in the controller call the `Authorize` function that checks the `azp` claim value.\n\n```csharp\nif (!AzureAppServiceClaimsHeader.Authorize(this.Request))\n{\n    Response.StatusCode = (int)HttpStatusCode.Unauthorized;\n    return null;\n}\n```\n\n### [Option 3] Validate the access token via Azure APIM\n\n[Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts) offers a scalable, multi-cloud API management platform for securing, publishing, and analyzing APIs. The [validate-azure-ad-token](https://learn.microsoft.com/azure/api-management/validate-azure-ad-token-policy) policy enforces the existence and validity of a JSON web token (JWT) that was provided by the Microsoft Entra External ID.\n\nThe following example policy, when added to the `<inbound>` policy section, checks the value of the `audience` and the `azp` claims in an access token obtained from Microsoft Entra External ID that is presented in the `Authorization` header. It returns an error message if the token is not valid. Configure this policy at a policy scope that it protects all custom authentication extensions REST API endpoints.\n\n```xml\n<validate-azure-ad-token tenant-id=\"your-tenant-ID\" header-name=\"Authorization\" failed-validation-httpcode=\"401\" failed-validation-error-message=\"Unauthorized. Access token is missing or invalid.\">\n  <client-application-ids>\n     <application-id>99045fe1-7639-4a75-9d4a-577b6ca3810f</application-id>\n  </client-application-ids>\n  <audiences>\n     <audience>Your application ID</audience>\n  </audiences>\n</validate-azure-ad-token>\n``` \n\nUse the following values:\n\n- **tenant-id** your Microsoft Entra External ID tenant ID.\n- **Audience** the application ID that is associated with your custom extension. You can find this application under the API authentication in your custom extension.\n\n\n## Data models\n\nThe code sample has the following data models:\n\n- TokenIssuanceStart event [request](./Models/TokenIssuanceStartRequest.cs) and [response](./Models/TokenIssuanceStartResponse.cs)\n- OnAttributeCollectionSubmit event [request](./Models/OnAttributeCollectionSubmitRequest.cs) and [response](./Models/OnAttributeCollectionSubmitResponse.cs)\n", "repo_name": "woodgrove-api", "org_name": "microsoft", "org_repo": "microsoft/woodgrove-api", "platform_org_repo": "github+microsoft/woodgrove-api", "link_to_repo": "https://github.com/microsoft/woodgrove-api", "platform": "github", "language": "C#", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Woodgrove demo application \n\nComing soon...\n", "repo_name": "woodgrove", "org_name": "microsoft", "org_repo": "microsoft/woodgrove", "platform_org_repo": "github+microsoft/woodgrove", "link_to_repo": "https://github.com/microsoft/woodgrove", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# OpenAI in MS Teams Group Chat\n\n## Description\n\nInvite OpenAI to your teams calls to assist w/ QnA right in chat.\n\n### Details \ud83d\udd0d\n\nThis effectively gives attendees of your meeting access to ask ChatGPT Turbo 3.5, plus other LLMs, questions right in the meeting's chat using any of these keywords (genie, copilot, openai, chatgpt). Have you ever felt like you're the only SME on a call, and could use a little bit of help during a workshop? Well, this should help, plus drive more engagement.\n\nIt effectively integrates Azure OpenAI **/chat/completions** REST-API into Teams for a group chat using power automate connectors. Note, this integration will not chain the conversation due to volume & context, so it interacts 1:1.\n\n### Instructions \ud83e\udded\n\nImport openai*.zip package to Power Automate and configure the flow. Select your next meeting you want to invite OpenAI. \n\nIf you wish to create a dashboard of the **QnA-only** to measure latency, tunning, etc. Then create a free ADX cluster at https://aka.ms/adx.free, import the dashboard-openai.json and edit datasource to your free cluster URI. Otherwise, edit the flow to not execute the ADX queries.\n\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "openai-in-teams-chat", "org_name": "microsoft", "org_repo": "microsoft/openai-in-teams-chat", "platform_org_repo": "github+microsoft/openai-in-teams-chat", "link_to_repo": "https://github.com/microsoft/openai-in-teams-chat", "platform": "github", "language": null, "stargazers_count": 13, "watchers_count": 13}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Text-Prediction-Biases", "org_name": "microsoft", "org_repo": "microsoft/Text-Prediction-Biases", "platform_org_repo": "github+microsoft/Text-Prediction-Biases", "link_to_repo": "https://github.com/microsoft/Text-Prediction-Biases", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MSBuildSecurityLabs", "org_name": "microsoft", "org_repo": "microsoft/MSBuildSecurityLabs", "platform_org_repo": "github+microsoft/MSBuildSecurityLabs", "link_to_repo": "https://github.com/microsoft/MSBuildSecurityLabs", "platform": "github", "language": null, "stargazers_count": 3, "watchers_count": 3}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Education-Accelerator-for-Japan", "org_name": "microsoft", "org_repo": "microsoft/Education-Accelerator-for-Japan", "platform_org_repo": "github+microsoft/Education-Accelerator-for-Japan", "link_to_repo": "https://github.com/microsoft/Education-Accelerator-for-Japan", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "PostgresToAzure", "org_name": "microsoft", "org_repo": "microsoft/PostgresToAzure", "platform_org_repo": "github+microsoft/PostgresToAzure", "link_to_repo": "https://github.com/microsoft/PostgresToAzure", "platform": "github", "language": null, "stargazers_count": 2, "watchers_count": 2}, {"README_text": "<p align=\"center\"><img width=\"50%\" src=\"docs/images/ONNX_Runtime_logo_dark.png\" /></p>\n\n**ONNX Runtime is a cross-platform inference and training machine-learning accelerator**.\n\n**ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing)\n\n**ONNX Runtime training** can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-training)\n\n\n## Get Started & Resources\n\n* **General Information**: [onnxruntime.ai](https://onnxruntime.ai)\n\n* **Usage documention and tutorials**: [onnxruntime.ai/docs](https://onnxruntime.ai/docs)\n\n* **YouTube video tutorials**: [youtube.com/@ONNXRuntime](https://www.youtube.com/@ONNXRuntime)\n\n* [**Upcoming Release Roadmap**](https://github.com/microsoft/onnxruntime/wiki/Upcoming-Release-Roadmap)\n\n* **Companion sample repositories**: \n  - ONNX Runtime Inferencing: [microsoft/onnxruntime-inference-examples](https://github.com/microsoft/onnxruntime-inference-examples)\n  - ONNX Runtime Training: [microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)\n\n\n## Build Pipeline Status\n|System|Inference|Training|\n|---|---|---|\n|Windows|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47)||\n|Linux|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline?label=Linux+OpenVINO)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=55)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining/orttraining-ortmodule-distributed?label=Training+Distributed)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=148)|\n|Mac|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13)||\n|Android|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53)||\n|iOS|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134)||\n|Web|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/ONNX%20Runtime%20Web%20CI%20Pipeline?label=Web)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161)||\n|Other|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/onnxruntime-binary-size-checks-ci-pipeline?repoName=microsoft%2Fonnxruntime&label=Binary+Size+Check)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=187&repoName=microsoft%2Fonnxruntime)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/onnxruntime-python-checks-ci-pipeline?label=Python+Checks)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=164)||\n\n\n## Data/Telemetry\n\nWindows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the [privacy statement](docs/Privacy.md) for more details.\n\n## Contributions and Feedback\n\nWe welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).\n\nFor feature requests or bug reports, please file a [GitHub Issue](https://github.com/Microsoft/onnxruntime/issues).\n\nFor general discussion or questions, please use [GitHub Discussions](https://github.com/microsoft/onnxruntime/discussions).\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n", "repo_name": "onnxruntime-gh-pages", "org_name": "microsoft", "org_repo": "microsoft/onnxruntime-gh-pages", "platform_org_repo": "github+microsoft/onnxruntime-gh-pages", "link_to_repo": "https://github.com/microsoft/onnxruntime-gh-pages", "platform": "github", "language": "C++", "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# Azure blob Checksum Verification\n\nThis repo provides a simple mechanism to validate local file integrity against the checksums generated and stored by Azure blob storage.\n\n1.\tClone the code from Github and enter the repo:\n\n```\ngit clone https://github.com/microsoft/azure-blob-checksum-verification\ncd azure-blob-checksum-verification\n```\n\n2.\tInstall [azcopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10#download-azcopyi)\n\n3.\tLogin to azcopy, example below. \n\n```\nazcopy login\n```\n\nSet the right permissions\n\n \nImportant, to interact with the Azure Storage Account, you will need to set the right permissions for the account, even if you are the storage account owner.\n\nIf you want to upload files, you will need to assign Storage Blob Data Contributor or Storage Blob Data Owner.\n\n\n4.\tRun the following command to check file integrity\n\n```\n./file-verification.sh -a account -c container-path -f files -o outfile \n```\n\n5.\tYou should see a confirmation content matches\n\n```\nThe checksums match.\n```\n\nAlso a list of all checks and results are stored in an output file.\n\n```\nFile Name       Local MD5       On Azure MD5    Status\n\nCODE_OF_CONDUCT.md      c06b12caf3c901eb3156e3dd5b0aea56        c06b12caf3c901eb3156e3dd5b0aea56        PASS\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "azure-blob-checksum-verification", "org_name": "microsoft", "org_repo": "microsoft/azure-blob-checksum-verification", "platform_org_repo": "github+microsoft/azure-blob-checksum-verification", "link_to_repo": "https://github.com/microsoft/azure-blob-checksum-verification", "platform": "github", "language": "Shell", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Welcome to the (Name) Data User Group!\nWe are a community of data enthusiasts who meet regularly to share our knowledge, skills and insights on various topics related to data science, analytics, visualization and more. Whether you are a beginner or an expert, a student or a professional, you are welcome to join us and learn from each other.\n\n## About Us\n\nThe Data User Group was founded in 2022 by a group of friends who wanted to create a space for data lovers to network, collaborate and have fun. Since then, we have grown to over 100 members from different backgrounds and industries. Our mission is to promote data literacy, foster innovation and support each other in our data journeys.\n\n[You can find a list of our resources here.](https://github.com/microsoft/AzureDataCommunity/tree/main/Resources)\n\n## Location\n\nWe meet every second Wednesday of the month at 6:00 pm at the Data Hub, a co-working space located at 123 Main Street, Downtown. The Data Hub provides us with a comfortable and well-equipped venue for our meetings, as well as access to their data resources and tools.\n\n## Meetings\n\nOur meetings usually last for two hours and consist of two parts: a presentation and a discussion. The presentation is given by one of our members or a guest speaker on a topic of their choice related to data. The discussion is an opportunity for everyone to ask questions, share opinions and exchange ideas on the topic. We also have some time for networking and socializing before and after the meeting.\n\n## Past Topics\n\nSome of the topics we have covered in our past meetings are:\n\n- Introduction to Python for Data Analysis\n- How to Create Interactive Dashboards with Power BI\n- Data Storytelling: Tips and Tricks\n- Machine Learning Basics: Classification and Regression\n- Web Scraping with BeautifulSoup and Selenium\n- Natural Language Processing with NLTK and SpaCy\n- Data Ethics: Challenges and Best Practices\n\n## Upcoming Topics\n\nSome of the topics we are planning to cover in our upcoming meetings are:\n\n- Data Visualization with D3.js\n- Sentiment Analysis with TextBlob and Vader\n- Time Series Analysis with Pandas and Statsmodels\n- Deep Learning with TensorFlow and Keras\n- Data Engineering with Apache Spark and Hadoop\n- Recommender Systems with Surprise and LightFM\n- Data Governance: Frameworks and Standards\n\n## Links\n\nHere are some useful links for data enthusiasts:\n\n- [Azure Data Community](https://aka.ms/datacommunity/): The Microsoft Azure Data Network.\n- [Upcoming Data Conferences and Events](https://eventlist.azurewebsites.net/index.php): Find conferences, meetups, and sessions from around the world.\n- [DataCamp](https://www.datacamp.com/): Learn data skills online with interactive courses and projects.\n- [Kaggle](https://www.kaggle.com/): Join the world's largest data science community and compete in challenges.\n- [Towards Data Science](https://towardsdatascience.com/): Read and write articles on data science, machine learning and more.\n- [Data Science Central](https://www.datasciencecentral.com/): A hub for news, resources and events related to data science.\n- [Data Elixir](https://dataelixir.com/): A weekly newsletter with curated content on data science and analytics.\n\nWe hope you find what you need here and join us soon. If you are interested in joining our group, please fill out this [form](https://forms.gle/xyz) and we will get back to you soon. We look forward to meeting you at our next meeting! \ud83d\ude0a\n\n## Contributing\n\nOur group welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nIf you have a comment or topic you would like to present, use the **Issues** link above to submit. Be as complete in your description as possible.  \n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "AzureDataCommunity", "org_name": "microsoft", "org_repo": "microsoft/AzureDataCommunity", "platform_org_repo": "github+microsoft/AzureDataCommunity", "link_to_repo": "https://github.com/microsoft/AzureDataCommunity", "platform": "github", "language": null, "stargazers_count": 8, "watchers_count": 8}, {"README_text": "# Microsoft Fabric Real-Time Analytics \n\n## Flight Stream Demo \u2708\ufe0f\n\n### Description\nEncompasses a use-case using airline flight data for the following Microsoft Fabric features: \n1. Eventstream \n2. Notebook\n3. KQL Database\n4. Power BI Report\n\n### Scenario \n1. An aircraft is continuously sending data points in a real-time stream using Microsoft Fabric Eventstream. In Power BI, we see data points indicating a flight has departed Newark international.\n\nJames works for the port authority of a large city. His organization manages one of the local international airports. Part of his job is to mitigate noise pollution cause by civil aviation traffic in the area surrounding the airport.\n\nJames has access to a real-time feed of aircraft departing from the airport in the last hour stored in Microsoft Fabric KQL Database. Some residents have complained about louder than usual noises in their neighborhood. Part of the port authority\u2019s legal duties is to report any violation of operation procedures to the federal aviation administrations.\n\n2. In a Power BI Report, James zooms in on a few datapoints where the plane has executed a right turn north.\n\nToday James is looking if Flight number 753 has climbed to or above the required 2,500 feet altitude over residential area which is a recommended minimum flight altitude for commercial jet airliners.\n\n3. James hovers over a few points in sequence to show the latitude, longitude and altitude of the aircraft. In Power BI, we see the number is compliant with the regulations.\n\nUpon closer inspection, the plane reached 2,500 feet above an industrial sector and therefore is compliant with the federal recommendations.\n\nThis is but a small sample of the air traffic in the area. What if we could automate this? With Fabric real-time analytics, the port authority has defined restricted geographical zones where planes must be compliant with minimal altitude requirements.\n\nWith the power of the KQL language and its native geospatial functions, we can now detect in real time the aircrafts that are non-compliant.\n\n4. Additionally, we can display geojson shapes around areas; for example, Newark international. Then we can switch to a Microsoft Fabric KQL Queryset that detects violations to raise alerts using Microsoft Fabric Data Activator Reflex.\n\nNow that we have an easy way to detect these situations, James can focus his effort on proactive noise mitigations by working with air traffic control to avoid these situations in the future.\n\n5. Lastly, we have a KQL Queryset that forecasts the plane\u2019s altitude and uses that to predict if the next data point(s) will be in violation of the noise regulations.\n\nWelcome to the power of Real-time Analytics with Microsoft Fabric! \ud83d\udc9a\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Fabric-RTA-FlightStream", "org_name": "microsoft", "org_repo": "microsoft/Fabric-RTA-FlightStream", "platform_org_repo": "github+microsoft/Fabric-RTA-FlightStream", "link_to_repo": "https://github.com/microsoft/Fabric-RTA-FlightStream", "platform": "github", "language": "Jupyter Notebook", "stargazers_count": 7, "watchers_count": 7}, {"README_text": "This repo includes source used to build the Surface Duo kernel. \n\nFull details are published at https://aka.ms/SurfaceDuoOSS\n\n", "repo_name": "surface-duo-oss-sm8350.12.camera-devicetree", "org_name": "microsoft", "org_repo": "microsoft/surface-duo-oss-sm8350.12.camera-devicetree", "platform_org_repo": "github+microsoft/surface-duo-oss-sm8350.12.camera-devicetree", "link_to_repo": "https://github.com/microsoft/surface-duo-oss-sm8350.12.camera-devicetree", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "This repo includes source used to build the Surface Duo kernel. \n\nFull details are published at https://aka.ms/SurfaceDuoOSS\n\n", "repo_name": "surface-duo-oss-sm8350.12.devicetree", "org_name": "microsoft", "org_repo": "microsoft/surface-duo-oss-sm8350.12.devicetree", "platform_org_repo": "github+microsoft/surface-duo-oss-sm8350.12.devicetree", "link_to_repo": "https://github.com/microsoft/surface-duo-oss-sm8350.12.devicetree", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "#  Python + Power Apps Workshop\n\nWelcome to the Python and Power Apps workshop. In this workshop, you will learn how to integrate an OpenAPI backend built with Python, and a Power Apps frontend. \n\n![The complete ToDo Power App](/Lab0/assets/complete-power-app-with-more-items.png)\n\n## Agenda\n\n### Lab 0 - Setup your environment\nThis lab is all about setting up your environment. This is what you need to get started. Make sure to do this before starting the workshop.\n\n:link: [Lab 0 - Setup your environment](#)\n\n### Lab 1 - Getting the Python API up and running in Azure\nThis lab will help you deploy the Python API to Azure using the Azure Developer CLI.\n\n:link: [Lab 1 - Getting the Python API up and running in Azure](#)\n\n### Lab 2 - Adding data with the ToDo API\nNow that you've got our API up and running in Azure, let's add some data so it's there, ready and waiting, for us to play with in Power Apps.\n\n:link: [Lab 2 - Adding data with the ToDo API](#)\n\n### Lab 3 - Creating a Custom Connector\nThis lab will help you enable your Power App to interact with the API in the previous lab, by creating a Custom Connector.\n\n:link: [Lab 3 - Creating a Custom Connector](#)\n\n### Lab 4 - Testing a Custom Connector\nWith the Custom Connector created, this lab will take you through the necessary steps to test it and to make sure that it's working appropriately.\n\n:link: [Lab 4 - Testing a Custom Connector](#)\n\n### Lab 5 - Creating a Power Apps Canvas App\nIn this lab, you'll be creating the UI for a ToDo List Canvas App which will then be backed by the Custom Connector you created in Lab 3.\n\n:link: [Lab 5 - Creating a Power Apps Canvas App](#)\n\n### Lab 6 - Adding data to a Power Apps Canvas App\nIn this lab, you'll be finally connecting the Custom Connector you created in the previous labs to the Canvas App you've just built.\n\n:link: [Lab 6 - Adding data to a Power Apps Canvas App](#)\n", "repo_name": "PowerPlatformPythonAndNet", "org_name": "microsoft", "org_repo": "microsoft/PowerPlatformPythonAndNet", "platform_org_repo": "github+microsoft/PowerPlatformPythonAndNet", "link_to_repo": "https://github.com/microsoft/PowerPlatformPythonAndNet", "platform": "github", "language": null, "stargazers_count": 7, "watchers_count": 7}, {"README_text": "# Interactive Summarization\n\nThis repo releases the human evaluation data of paper \"Interactive Editing for Text Summarization\". In the data folder, you can find the human evaluations for the draft summaries (`summary 1`), human annotated summaries with and without interaction (`summary 2` and `summary 3`).\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Interactive-Summarization", "org_name": "microsoft", "org_repo": "microsoft/Interactive-Summarization", "platform_org_repo": "github+microsoft/Interactive-Summarization", "link_to_repo": "https://github.com/microsoft/Interactive-Summarization", "platform": "github", "language": null, "stargazers_count": 15, "watchers_count": 15}, {"README_text": "# Microsoft Let's Get Technical Series \n![Microsoft Lets Get Technical](./assets/Lets-Get-Technical-Header.png)\n\n\nHere's how you can partner Microsoft Reactor and get started to landing a Let's Get Technical series in your market. \n<br/>\n## Background\nThe Let\u2019s Get Technical Series is a series of three short virtual events piloting first at the regional markets in Asia. It first inaugurated in Indonesia, then the Philippines, recently in Thailand. \n<br/>\n## Objectives \nThe 60 minute-event series is aimed to amplify the voices of our Community Leaders and Subject Matter Experts, and to engage developers from the local markets, help them to achieve success, and at the same time, grow awareness of our Reactor programs. \n<br/>\n\n\ud83d\udc49 We are inviting you to partner with us to bring this series to your market, so that your voices of your local communities can be heard and the latest technologies can be brought to your market. \n<br/>\n## Processes \n| Tasks                                                 | Provision by The Reactor      | Provision by Subsidiary/Community Leader     |\n| -------------------------------------------------     | ------------------------------| -------------------------------------------- |\n| Event Structure/Consultation                          | x                             |                                              |\n| Event Brief                                           |                               | x                                            |\n| Creation of Registration Page                         | x                             |                                              |\n| Provision of Microsoft Reactor Branding               | x                             |                                              |\n| Provision of Speakers                                 | x (Cloud Advocates)           | x (Local Subject Matter Experts)             | \n| Creation of Content (Deck, promotional materials)     |                               | x                                            |\n| Localisation of Code of Conduct                       |                               | x                                            |\n| Resources                                             |                               | x                                            |\n| Demand Generation                                     | x                             | x                                            |\n| Weekly/Daily attendance updates                       | x                             |                                              |\n| Runsheet and Pre-event guidance                       | x                             |                                              |\n| Attendee Survey                                       | x                             |                                              |\n| Onsite Production                                     | x                             |                                              |\n| Post Event Report                                     | x                             |                                              |\n\n\n## Success Stories \nHere are the some examples of the Let's Get Technical series in the two markets. \n* [Let's Get Technical - Philippines](https://www.youtube.com/watch?v=fMBfQL7Tvkg)\n* [Let's Get Technical - Thailand](https://www.youtube.com/watch?v=OdSF9ZcoYT0)  \n\nThrough these great partnership with the various markets, it also allowed further collaborations including the Thailand Developer Week where we partnered to deliver a week-long hybrid event to the local developers. \n\n## Quick start\n\n1. Complete [brief](https://github.com/microsoft/Lets-Get-Technical/tree/main/assets/Lets-Get-Technical-Brief.docx) \n2. Refer to Calendar section below for a visibility of when it may be most ideal for your market\n3. Send brief over to suzanne.chen@microsoft.com \n4. We'll be in touch in the next three working days.  \n\n## Resources \nThese are some resources you may find useful for localisation. \n\n* [Speakers invitation](https://github.com/microsoft/Lets-Get-Technical/tree/main/assets/Speakers-Invitation.docx)\n* [Brand/Powerpoint Templates](https://github.com/microsoft/Lets-Get-Technical/tree/main/assets/Lets-Get-Technical-Cover-Slides.pptx) for your presentation  \n* Customisable [Promotional materials](https://github.com/microsoft/Lets-Get-Technical/tree/main/assets/Lets-Get-Technical-Promo-Slides.pptx)\n* Code of Conduct (in English) \n\n## Calendar\n| Event                      | February 2023        | March 2023           | April 2023           | May 2023             | June 2023            | July 2023            | August 2023          | September 2023       | \n| -------------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- | -------------------- |\n| Let's Get Technical        | Indonesia            | Philippines          |                      | Thailand             | Singapore            |  Vietnam             |                      |                      |\n\n<br/>\n", "repo_name": "Lets-Get-Technical", "org_name": "microsoft", "org_repo": "microsoft/Lets-Get-Technical", "platform_org_repo": "github+microsoft/Lets-Get-Technical", "link_to_repo": "https://github.com/microsoft/Lets-Get-Technical", "platform": "github", "language": null, "stargazers_count": 1, "watchers_count": 1}, {"README_text": "# wcnscripts\n\n> A collection of Windows container networking troubleshooting scripts and manifests, primarily targeting networking issues in the Azure Kubernetes Service.\n\n## Scripts directory\n[scripts](./scripts) contains network troubleshooting PowerShell scripts.\n\n## Manifests directory\n[manifests](./manifests) contains manifests (.yaml) to help monitor/remedy K8s Windows networking issues. \n\n### Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "wcnscripts", "org_name": "microsoft", "org_repo": "microsoft/wcnscripts", "platform_org_repo": "github+microsoft/wcnscripts", "link_to_repo": "https://github.com/microsoft/wcnscripts", "platform": "github", "language": "PowerShell", "stargazers_count": 5, "watchers_count": 5}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "Microsoft-Research-Video-Description-Corpus", "org_name": "microsoft", "org_repo": "microsoft/Microsoft-Research-Video-Description-Corpus", "platform_org_repo": "github+microsoft/Microsoft-Research-Video-Description-Corpus", "link_to_repo": "https://github.com/microsoft/Microsoft-Research-Video-Description-Corpus", "platform": "github", "language": null, "stargazers_count": 6, "watchers_count": 6}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "zerotrustassessment", "org_name": "microsoft", "org_repo": "microsoft/zerotrustassessment", "platform_org_repo": "github+microsoft/zerotrustassessment", "link_to_repo": "https://github.com/microsoft/zerotrustassessment", "platform": "github", "language": "C#", "stargazers_count": 12, "watchers_count": 12}, {"README_text": "# Fine-tuning language models with Advantage-Induced Policy Alignment\r\nThis repo contains the official implementation of paper \"Fine-tuning language models with Advantage-Induced Policy Alignment\", by Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao.\r\n\r\n\r\n### Abstract\r\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator.\r\nIn addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output.\r\nIn addition to empirical results, we also provide a theoretical justification supporting the design of our loss function.\r\n\r\n\r\n### Getting Started\r\nPython 3 is required for the current codebase. It's recommended to use Python 3.9 for installing the dependencies. Due to the current [Ray support issue](https://github.com/ray-project/ray/issues/33232), Python 3.11 may give error during executation.\r\n\r\nInstall the dependencies as follows.\r\n\r\n```shell \r\npip install -r requirements.txt \r\npip install -e . \r\n```\r\n\r\nTo reproduce the experiments in the paper, execute the following set of code:\r\n```shell \r\n## For running APA or AWR on HH dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/sppo_hh.py \r\n\r\n## For running PPO on HH dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/ppo_hh.py \r\n\r\n## For running APA or AWR on TLDR dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/sppo_tldr.py \r\n\r\n## For running PPO on TLDR dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/ppo_tldr.py \r\n\r\n## For running offline ILQL on HH dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/ilql_hh.py \r\n\r\n## For running offline APA or AWR on HH dataset\r\naccelerate launch  --config_file configs/accelerate/zero2-bf16.yaml examples/hh/sppo_off_hh.py  \r\n```\r\n\r\nInside each of the code file, one may adjust the random seed, model size and algorithm. Note that this code is not optimized with memory usage, only for a preliminary illustration of the differences between the existing policy iteration algorithms for RLHF. The code is tested on 4 V100 and 8 V100 for 125M and 1B models, and 4 MI200 for 6B models. We put reference model, reward model and value model in three difference GPUs. For smaller number of GPUs, you may need to change the device number in accelerate_sppo_trainer.py (and other corresponding accelerator files). \r\n\r\n\r\n### Acknowledgement\r\nOur codebase is built based on a stable version of [CarperAI/trlX](https://github.com/CarperAI/trlx). We thank the authors for the nicely organized code!\r\n\r\n\r\n### Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n\r\n### Trademarks\r\n\r\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \r\ntrademarks or logos is subject to and must follow \r\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\r\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\r\nAny use of third-party trademarks or logos are subject to those third-party's policies.\r\n", "repo_name": "RLHF-APA", "org_name": "microsoft", "org_repo": "microsoft/RLHF-APA", "platform_org_repo": "github+microsoft/RLHF-APA", "link_to_repo": "https://github.com/microsoft/RLHF-APA", "platform": "github", "language": "Python", "stargazers_count": 10, "watchers_count": 10}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "NTA", "org_name": "microsoft", "org_repo": "microsoft/NTA", "platform_org_repo": "github+microsoft/NTA", "link_to_repo": "https://github.com/microsoft/NTA", "platform": "github", "language": null, "stargazers_count": 9, "watchers_count": 9}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "MC4MAppSourceGallery", "org_name": "microsoft", "org_repo": "microsoft/MC4MAppSourceGallery", "platform_org_repo": "github+microsoft/MC4MAppSourceGallery", "link_to_repo": "https://github.com/microsoft/MC4MAppSourceGallery", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}, {"README_text": "# Project\n\n> This repo has been populated by an initial template to help get you started. Please\n> make sure to update the content to build a great experience for community-building.\n\nAs the maintainer of this project, please make a few updates:\n\n- Improving this README.MD file to provide a great experience\n- Updating SUPPORT.MD with content about this project's support experience\n- Understanding the security reporting process in SECURITY.MD\n- Remove this section from the README\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n", "repo_name": "nsaas", "org_name": "microsoft", "org_repo": "microsoft/nsaas", "platform_org_repo": "github+microsoft/nsaas", "link_to_repo": "https://github.com/microsoft/nsaas", "platform": "github", "language": null, "stargazers_count": 0, "watchers_count": 0}]